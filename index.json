{
  "README.html": {
    "href": "README.html",
    "title": "DocFX Documentation Pipeline | XMPro",
    "summary": "DocFX Documentation Pipeline This repository contains the source files for the documentation site and an Azure DevOps CI/CD pipeline to automatically build and deploy the documentation. How It Works The Azure DevOps pipeline follows this workflow: Triggers when changes are pushed to the main branch or when pull requests targeting the main branch are created Builds the DocFX documentation site If the build is successful and the changes are on the main branch, commits the generated _site folder to the public-docs-site repository Pipeline Configuration azure-pipelines.yml - Azure DevOps pipeline definition Setup Requirements To use this pipeline, you need to set up the following: Create a GitHub Personal Access Token (PAT) with the following permissions: If the target repository is private: repo scope (Full control of private repositories) If the target repository is public: public_repo scope (Access to public repositories) These permissions are required to push to the target repository (https://github.com/XMPro/public-docs-site.git) Important: Make sure the GitHub account associated with the PAT has write access to the target repository. If you're getting a 403 error (Permission denied), it means the account doesn't have the necessary permissions to push to the repository. The pipeline now uses a classic PAT token stored in the DOCS_DEPLOY_PAT_CLASSIC variable, which should have the necessary permissions to push directly to the target repository. Add the classic PAT as a pipeline variable or secret: Go to your Azure DevOps project Navigate to Pipelines > Library Create a new Variable Group or edit an existing one Add a new variable named DOCS_DEPLOY_PAT_CLASSIC Set the value to your GitHub classic PAT Check the \"Keep this value secret\" option Save the variable group Alternatively, you can add it directly to your pipeline: Go to your pipeline Click \"Edit\" Click the \"Variables\" button Add a new variable named DOCS_DEPLOY_PAT_CLASSIC Set the value to your GitHub classic PAT Check the \"Keep this value secret\" option Save the pipeline Local Development To build the documentation locally: Install .NET 8.0 SDK (required for the latest version of DocFX) Install DocFX: dotnet tool install -g docfx Navigate to the public-documentation directory: cd public-documentation Run docfx docfx.json to build the site The generated site will be in the public-documentation/_site folder File Structure public-documentation/docfx.json - Configuration file for DocFX public-documentation/index.md - Main landing page public-documentation/toc.yml - Table of contents public-documentation/docs/ - Documentation content azure-pipelines.yml - Azure DevOps pipeline definition .gitignore - Configured to ignore the _site folder in this repository Notes The _site folder is ignored in this repository (via .gitignore) because it contains the build output that is automatically deployed to the target repository. Only changes to the main branch will trigger a deployment to the target repository."
  },
  "docs/administration/administrative-accounts.html": {
    "href": "docs/administration/administrative-accounts.html",
    "title": "Administrative Accounts | XMPro",
    "summary": "Administrative Accounts Global Administrator Global Administrator is an account that is created when installing XMPro. The username for this account is _admin@xmpro.onxmpro.com_. As a Global Administrator, you will have full access to managing users, companies, subscriptions, and products. Company Administrator A Company Administrator is an account that has been given the Administration Role on the XMPro/Subscription Manager product for their Company. As a Company Administrator, you will have access to managing Users from your Company."
  },
  "docs/administration/companies/index.html": {
    "href": "docs/administration/companies/index.html",
    "title": "Companies | XMPro",
    "summary": "Companies Companies are the top-level organizational units in XMPro. Each company represents a separate organization that uses the XMPro platform. As an administrator, you can: Register a new company Manage existing companies Manage company subscriptions Manage licenses Companies have their own users, subscriptions, and settings. Users from one company cannot access resources from another company unless explicitly granted access."
  },
  "docs/administration/companies/manage-a-company.html": {
    "href": "docs/administration/companies/manage-a-company.html",
    "title": "Manage a Company | XMPro",
    "summary": "Manage a Company This page explains how to manage an existing company in the XMPro platform. Prerequisites You must have Global Administrator privileges to manage companies. The company must already be registered in the system. Accessing Company Management Log in to the XMPro platform using your Global Administrator account. Navigate to the Administration section. Select \"Companies\" from the menu. Find the company you want to manage in the list of companies. Click on the company name or the \"Manage\" button to access the company management page. Company Management Options Edit Company Details On the company management page, click the \"Edit\" button. Update the company details as needed: Company Name Company Code Address Contact Information Click \"Save\" to apply the changes. Configure Company Settings Navigate to the \"Settings\" tab on the company management page. Configure the company settings: Time Zone: Update the time zone for the company. Language: Change the default language for the company. Other settings as required. Click \"Save\" to apply the settings. Manage Company Users Navigate to the \"Users\" tab on the company management page. Here you can: View all users associated with the company Add new users to the company Remove users from the company Edit user details and permissions Manage Company Subscriptions For detailed information on managing company subscriptions, see Manage Company Subscriptions. Manage Company License For detailed information on managing company licenses, see Manage License. Deactivating a Company On the company management page, click the \"Deactivate\" button. Confirm the deactivation when prompted. The company will be deactivated, and users will no longer be able to access the platform. Reactivating a Company Navigate to the list of companies. Find the deactivated company in the list. Click the \"Reactivate\" button. The company will be reactivated, and users will be able to access the platform again. Notes Changes to company settings may affect all users within that company. Deactivating a company will prevent all users from that company from accessing the platform. Company management is typically performed by Global Administrators during the setup and maintenance of the XMPro platform."
  },
  "docs/administration/companies/manage-company-subscriptions.html": {
    "href": "docs/administration/companies/manage-company-subscriptions.html",
    "title": "Manage Company Subscriptions | XMPro",
    "summary": "Manage Company Subscriptions This page explains how to manage subscriptions for a company in the XMPro platform. Prerequisites You must have Global Administrator privileges to manage company subscriptions. The company must already be registered in the system. Accessing Company Subscription Management Log in to the XMPro platform using your Global Administrator account. Navigate to the Administration section. Select \"Companies\" from the menu. Find the company you want to manage in the list of companies. Click on the company name or the \"Manage\" button to access the company management page. Navigate to the \"Subscriptions\" tab. Managing Company Subscriptions View Current Subscriptions The Subscriptions tab displays all current subscriptions for the company, including: Product name Subscription type Start date End date Status (Active, Expired, Pending) Number of licenses Add a New Subscription On the Subscriptions tab, click the \"Add Subscription\" button. Select the product you want to add a subscription for. Choose the subscription type: Trial: Limited-time access for evaluation purposes. Standard: Regular subscription with standard features. Enterprise: Subscription with advanced features and capabilities. Specify the number of licenses required. Set the start and end dates for the subscription. Click \"Add\" to create the subscription. Edit an Existing Subscription Find the subscription you want to edit in the list. Click the \"Edit\" button for that subscription. Update the subscription details as needed: Subscription type Number of licenses End date Click \"Save\" to apply the changes. Renew a Subscription Find the subscription you want to renew in the list. Click the \"Renew\" button for that subscription. Specify the new end date for the subscription. Click \"Renew\" to extend the subscription. Cancel a Subscription Find the subscription you want to cancel in the list. Click the \"Cancel\" button for that subscription. Confirm the cancellation when prompted. The subscription will be marked as canceled, and users will no longer have access to the product when the subscription period ends. Managing User Access to Subscriptions For detailed information on managing user access to subscriptions, see Manage User Access. Setting Up Auto-Approval and Default Subscriptions For detailed information on setting up auto-approval and default subscriptions, see Setup Auto Approval/Default Subscriptions. Notes Changes to company subscriptions may affect all users within that company. Canceling a subscription will prevent users from accessing the associated product when the subscription period ends. Subscription management is typically performed by Global Administrators during the setup and maintenance of the XMPro platform."
  },
  "docs/administration/companies/manage-license.html": {
    "href": "docs/administration/companies/manage-license.html",
    "title": "Manage License | XMPro",
    "summary": "Manage License This page explains how to manage licenses for a company in the XMPro platform. Prerequisites You must have Global Administrator privileges to manage company licenses. The company must already be registered in the system. Accessing License Management Log in to the XMPro platform using your Global Administrator account. Navigate to the Administration section. Select \"Companies\" from the menu. Find the company you want to manage in the list of companies. Click on the company name or the \"Manage\" button to access the company management page. Navigate to the \"License\" tab. Managing Company Licenses View Current License Information The License tab displays the current license information for the company, including: License key License type Issue date Expiration date Licensed products Number of users allowed Current usage Apply a New License On the License tab, click the \"Apply New License\" button. Enter the new license key in the provided field. Click \"Apply\" to activate the new license. The system will validate the license key and update the license information if the key is valid. Update an Existing License On the License tab, click the \"Update License\" button. Enter the updated license key in the provided field. Click \"Update\" to apply the changes. The system will validate the license key and update the license information if the key is valid. Export License Information On the License tab, click the \"Export\" button. Choose the export format (PDF, CSV, etc.). The license information will be exported in the selected format. License Types XMPro offers different types of licenses: Trial License: Limited-time access for evaluation purposes. Standard License: Regular license with standard features. Enterprise License: License with advanced features and capabilities. License Allocation Licenses are allocated to users within the company. The number of users that can access the platform is determined by the license. View License Allocation On the License tab, navigate to the \"Allocation\" section. This section displays: Total number of licenses available Number of licenses currently in use Users who have been assigned licenses Manage License Allocation For detailed information on managing user access to subscriptions and licenses, see Manage User Access. Requesting a License For detailed information on requesting and applying a license, see Request and Apply a License. Notes Changes to company licenses may affect all users within that company. If a license expires, users may lose access to certain features or the entire platform. License management is typically performed by Global Administrators during the setup and maintenance of the XMPro platform."
  },
  "docs/administration/companies/register-a-company.html": {
    "href": "docs/administration/companies/register-a-company.html",
    "title": "Register a Company | XMPro",
    "summary": "Register a Company Register a Company Click on the Sign-Up button on the login page. Follow the steps below: Fill in your first name, last name, and email address. If this is the first time you or someone in your company is signing up to use Subscription Manager, fill in the company name and check the “Create a new company?” checkbox. By checking this box, your company will be registered in the system when your account is created. This check box will automatically become visible after you have filled in your company name and click anywhere else on the form. Choose a unique username, for example, “keith.miller“. Do not include your company name in your username. Choose a password and confirm your password. Click “Agree“. Wait for an email, confirming that your company has been registered and that you have been granted access to Subscription Manager. You will only be allowed to use the system after being granted access."
  },
  "docs/administration/index.html": {
    "href": "docs/administration/index.html",
    "title": "Administration | XMPro",
    "summary": "Administration The Administration section covers the management aspects of the XMPro platform, including user accounts, companies, subscriptions, and language settings. Administrative Accounts XMPro has different types of administrative accounts with varying levels of access and permissions: Global Administrator: Has full access to manage users, companies, subscriptions, and products. Company Administrator: Has access to manage users within their company. Language Settings XMPro follows industry standards for language and translation of technical terms and user interface elements. Companies Companies are the top-level organizational units in XMPro: Companies Overview Register a Company Manage a Company Manage Company Subscriptions Manage License Subscriptions Subscriptions control access to features and services: Subscriptions Overview Manage User Access Setup Auto Approval/Default Subscriptions Request and Apply a License Users User management is an important aspect of maintaining security: Users Overview Invite a User Register an Account Profile Change Password Reset Password Delete a User Change Business Role"
  },
  "docs/administration/language.html": {
    "href": "docs/administration/language.html",
    "title": "Language | XMPro",
    "summary": "Language Overview XMPro follows the common industry approach for how major no-code/low-code platforms handle translations for technical terms, i.e.: Keep in English: SQL-related terms Programming concepts Industry-standard acronyms Technical protocols Translate: UI navigation elements Error messages Help documentation Descriptive text Certain things are in the primary company language: product roles & email templates. User Preferred Language ... Company Preferred Language ... Exclusions ..."
  },
  "docs/administration/subscriptions-admin/index.html": {
    "href": "docs/administration/subscriptions-admin/index.html",
    "title": "Subscriptions | XMPro",
    "summary": "Subscriptions Subscriptions in XMPro control access to features and services. They determine what products and features users can access within the platform. As an administrator, you can: Manage user access to subscriptions Set up auto-approval and default subscriptions Request and apply licenses Subscriptions are assigned to users within a company. Different users may have different subscription levels based on their roles and responsibilities."
  },
  "docs/administration/subscriptions-admin/manage-user-access.html": {
    "href": "docs/administration/subscriptions-admin/manage-user-access.html",
    "title": "Manage User Access | XMPro",
    "summary": "Manage User Access This page explains how to manage user access to subscriptions in the XMPro platform. Prerequisites You must have Global Administrator or Company Administrator privileges to manage user access to subscriptions. The company and subscriptions must already be set up in the system. Accessing User Access Management Log in to the XMPro platform using your Administrator account. Navigate to the Administration section. Select \"Subscriptions\" from the menu. Navigate to the \"User Access\" tab. Managing User Access to Subscriptions View Current User Access The User Access tab displays all users and their current subscription access, including: User name Email address Assigned subscriptions Access status (Active, Pending, Denied) Role within each subscription Grant Access to a Subscription On the User Access tab, click the \"Grant Access\" button. Select the user you want to grant access to. Choose the subscription from the available subscriptions. Assign a role for the user within the subscription: Viewer: Can view content but cannot make changes. Contributor: Can view and contribute content but cannot manage settings. Administrator: Has full access to manage the subscription. Click \"Grant\" to provide access. Edit User Access Find the user whose access you want to edit in the list. Click the \"Edit\" button for that user. Update the user's access details as needed: Change the assigned role Modify access permissions Click \"Save\" to apply the changes. Revoke User Access Find the user whose access you want to revoke in the list. Click the \"Revoke\" button for that user. Confirm the revocation when prompted. The user will no longer have access to the subscription. Managing Access Requests View Access Requests Navigate to the \"Access Requests\" tab. This tab displays all pending requests for subscription access, including: User name Email address Requested subscription Request date Approve an Access Request Find the access request you want to approve in the list. Click the \"Approve\" button for that request. Assign a role for the user within the subscription. Click \"Confirm\" to approve the request. Deny an Access Request Find the access request you want to deny in the list. Click the \"Deny\" button for that request. Optionally, provide a reason for the denial. Click \"Confirm\" to deny the request. Setting Up Auto-Approval for Subscriptions For detailed information on setting up auto-approval for subscriptions, see Setup Auto Approval/Default Subscriptions. Notes Changes to user access may affect the user's ability to access certain features or products. Access management is typically performed by Administrators during the setup and maintenance of the XMPro platform. Users can request access to subscriptions through the platform, which will appear in the Access Requests tab for administrator approval."
  },
  "docs/administration/subscriptions-admin/request-and-apply-a-license.html": {
    "href": "docs/administration/subscriptions-admin/request-and-apply-a-license.html",
    "title": "Request and Apply a License | XMPro",
    "summary": "Request and Apply a License This page explains how to request and apply a license in the XMPro platform. Prerequisites You must have Global Administrator or Company Administrator privileges to request and apply a license. The company must already be registered in the system. Requesting a License Understanding License Types XMPro offers different types of licenses: Trial License: Limited-time access for evaluation purposes. Standard License: Regular license with standard features. Enterprise License: License with advanced features and capabilities. Steps to Request a License Log in to the XMPro platform using your Administrator account. Navigate to the Administration section. Select \"Subscriptions\" from the menu. Navigate to the \"Licenses\" tab. Click the \"Request License\" button. Fill in the license request form: Company Name: The name of the company for which the license is being requested. Contact Information: Your contact details. License Type: Select the type of license you need. Number of Users: Specify how many users will need access. Products: Select the XMPro products you need. Additional Requirements: Specify any additional requirements or notes. Click \"Submit Request\" to send the license request. Tracking License Requests Navigate to the \"License Requests\" tab. This tab displays all pending and completed license requests, including: Request ID Request date Status (Pending, Approved, Denied) License details (if approved) Applying a License Once you have received a license key, you can apply it to your XMPro instance. Steps to Apply a License Log in to the XMPro platform using your Administrator account. Navigate to the Administration section. Select \"Subscriptions\" from the menu. Navigate to the \"Licenses\" tab. Click the \"Apply License\" button. Enter the license key in the provided field. Click \"Apply\" to activate the license. The system will validate the license key and update the license information if the key is valid. Viewing Current License Information Navigate to the \"Licenses\" tab. This tab displays the current license information, including: License key License type Issue date Expiration date Licensed products Number of users allowed Current usage Updating an Existing License Navigate to the \"Licenses\" tab. Click the \"Update License\" button. Enter the updated license key in the provided field. Click \"Update\" to apply the changes. The system will validate the license key and update the license information if the key is valid. Managing Company Licenses For detailed information on managing company licenses, see Manage License. Notes License requests are typically processed within 1-2 business days. Once a license is applied, it may take a few minutes for all features to become available. If you encounter any issues with your license, contact XMPro support for assistance."
  },
  "docs/administration/subscriptions-admin/setup-auto-approvals-default-subscriptions.html": {
    "href": "docs/administration/subscriptions-admin/setup-auto-approvals-default-subscriptions.html",
    "title": "Setup Auto Approval/Default Subscriptions | XMPro",
    "summary": "Setup Auto Approval/Default Subscriptions This page explains how to set up auto-approval and default subscriptions in the XMPro platform. Prerequisites You must have Global Administrator or Company Administrator privileges to set up auto-approval and default subscriptions. The company and subscriptions must already be set up in the system. Accessing Auto-Approval and Default Subscription Settings Log in to the XMPro platform using your Administrator account. Navigate to the Administration section. Select \"Subscriptions\" from the menu. Navigate to the \"Auto-Approval & Defaults\" tab. Setting Up Auto-Approval for Subscriptions Auto-approval allows users to gain access to certain subscriptions automatically without requiring administrator approval. Configure Auto-Approval On the Auto-Approval & Defaults tab, locate the \"Auto-Approval Settings\" section. Click the \"Configure\" button. Select the subscriptions that should be auto-approved: Check the box next to each subscription that should be auto-approved. Uncheck the box for subscriptions that require manual approval. Configure auto-approval conditions: Domain-based: Auto-approve users from specific email domains. Role-based: Auto-approve users with specific roles. Department-based: Auto-approve users from specific departments. Click \"Save\" to apply the auto-approval settings. Edit Auto-Approval Settings On the Auto-Approval & Defaults tab, locate the \"Auto-Approval Settings\" section. Click the \"Edit\" button. Update the auto-approval settings as needed. Click \"Save\" to apply the changes. Disable Auto-Approval On the Auto-Approval & Defaults tab, locate the \"Auto-Approval Settings\" section. Click the \"Disable\" button. Confirm the action when prompted. Auto-approval will be disabled, and all subscription requests will require manual approval. Setting Up Default Subscriptions Default subscriptions are automatically assigned to new users when they are added to the system. Configure Default Subscriptions On the Auto-Approval & Defaults tab, locate the \"Default Subscriptions\" section. Click the \"Configure\" button. Select the subscriptions that should be assigned by default: Check the box next to each subscription that should be assigned by default. Uncheck the box for subscriptions that should not be assigned by default. Configure default subscription conditions: Domain-based: Assign default subscriptions to users from specific email domains. Role-based: Assign default subscriptions to users with specific roles. Department-based: Assign default subscriptions to users from specific departments. Click \"Save\" to apply the default subscription settings. Edit Default Subscription Settings On the Auto-Approval & Defaults tab, locate the \"Default Subscriptions\" section. Click the \"Edit\" button. Update the default subscription settings as needed. Click \"Save\" to apply the changes. Disable Default Subscriptions On the Auto-Approval & Defaults tab, locate the \"Default Subscriptions\" section. Click the \"Disable\" button. Confirm the action when prompted. Default subscriptions will be disabled, and new users will not be automatically assigned any subscriptions. Managing User Access to Subscriptions For detailed information on managing user access to subscriptions, see Manage User Access. Notes Auto-approval and default subscription settings apply to all new users who meet the specified conditions. These settings can help streamline the onboarding process for new users. Auto-approval and default subscription settings can be updated at any time to reflect changes in the organization's needs."
  },
  "docs/administration/users/business-role-for-a-user.html": {
    "href": "docs/administration/users/business-role-for-a-user.html",
    "title": "Change Business Role | XMPro",
    "summary": "Change Business Role This page explains how to change the business role for a user in the XMPro platform. Prerequisites You must have Global Administrator or Company Administrator privileges to change user business roles. The user must be a member of your company. Understanding Business Roles Business roles in XMPro define a user's position within the organization and can affect their access to certain features and functionality. Business roles are different from subscription roles, which determine a user's permissions within specific subscriptions. Common business roles include: Executive Manager Supervisor Operator Technician Engineer Analyst Developer Administrator Custom roles defined by your organization Accessing User Management Log in to the XMPro platform using your Administrator account. Navigate to the Administration section. Select \"Users\" from the menu. Changing a User's Business Role Steps to Change a Business Role On the Users page, find the user whose business role you want to change in the list. Click the \"Actions\" button for that user. Select \"Edit\" from the dropdown menu. In the Edit User dialog, locate the \"Business Role\" field. Select the new business role from the dropdown list. Click \"Save\" to apply the changes. Alternative Method: From User Details On the Users page, click on the user's name to open their details. Navigate to the \"Profile\" tab. Click the \"Edit\" button. Locate the \"Business Role\" field. Select the new business role from the dropdown list. Click \"Save\" to apply the changes. Creating Custom Business Roles If the default business roles do not meet your organization's needs, you can create custom business roles: Navigate to the Administration section. Select \"Settings\" from the menu. Navigate to the \"Business Roles\" tab. Click the \"Add Role\" button. Enter a name for the new business role. Optionally, provide a description for the role. Configure any role-specific settings. Click \"Save\" to create the new business role. Impact of Changing Business Roles Changing a user's business role may affect: Their access to certain features or functionality. The default subscriptions they are assigned when auto-approval is enabled. The way they are grouped or filtered in reports and user lists. Their visibility to certain content that is targeted to specific business roles. Notes Business role changes take effect immediately. Users will receive a notification when their business role is changed. Business roles can be used in conjunction with subscription roles to provide fine-grained access control. Consider the impact on the user's workflow before changing their business role."
  },
  "docs/administration/users/change-password.html": {
    "href": "docs/administration/users/change-password.html",
    "title": "Change Password | XMPro",
    "summary": "Change Password This page explains how to change your password in the XMPro platform. Prerequisites You must have an active XMPro account. You must be logged in to the XMPro platform. You must know your current password. Accessing the Change Password Feature Method 1: From Your Profile Log in to the XMPro platform using your credentials. Click on your profile picture or initials in the top-right corner of the screen. Select \"Profile\" from the dropdown menu. Navigate to the \"Security\" tab. Click the \"Change Password\" button. Method 2: From Account Settings Log in to the XMPro platform using your credentials. Click on your profile picture or initials in the top-right corner of the screen. Select \"Account Settings\" from the dropdown menu. Navigate to the \"Security\" section. Click the \"Change Password\" button. Changing Your Password On the Change Password page, enter your current password in the \"Current Password\" field. Enter your new password in the \"New Password\" field. Re-enter your new password in the \"Confirm New Password\" field to verify it. Click \"Change Password\" to apply the changes. Password Requirements When creating your new password, ensure it meets the following requirements: Minimum length of 8 characters Contains at least one uppercase letter Contains at least one lowercase letter Contains at least one number Contains at least one special character Does not contain your username or common words Is different from your previous passwords After Changing Your Password After successfully changing your password: You will receive a confirmation message. You may be prompted to log in again with your new password. You will receive an email notification confirming that your password has been changed. If You Forgot Your Password If you don't remember your current password, you will need to reset it instead of changing it. For detailed information on resetting your password, see Reset Password. Notes For security reasons, your password is never displayed in plain text. It's recommended to change your password regularly (e.g., every 90 days) to maintain security. Do not share your password with others or store it in an insecure location. If you suspect your account has been compromised, change your password immediately and contact your administrator."
  },
  "docs/administration/users/delete-a-user.html": {
    "href": "docs/administration/users/delete-a-user.html",
    "title": "Delete a User | XMPro",
    "summary": "Delete a User This page explains how to delete a user from the XMPro platform. Prerequisites You must have Global Administrator or Company Administrator privileges to delete users. The user must be a member of your company. Before Deleting a User Before deleting a user, consider the following: Deleting a user is permanent and cannot be undone. All data associated with the user will be removed from the system. Any content created by the user will remain in the system but will be reassigned or marked as created by a deleted user. Consider deactivating the user instead of deleting if you may need to restore access in the future. Accessing User Management Log in to the XMPro platform using your Administrator account. Navigate to the Administration section. Select \"Users\" from the menu. Deleting a User Steps to Delete a User On the Users page, find the user you want to delete in the list. Click the \"Actions\" button for that user. Select \"Delete\" from the dropdown menu. A confirmation dialog will appear, warning you about the consequences of deleting a user. If you want to proceed, enter the user's email address in the confirmation field. Click \"Delete\" to permanently remove the user. Alternative Method: From User Details On the Users page, click on the user's name to open their details. Navigate to the \"Account\" tab. Scroll to the bottom of the page. Click the \"Delete User\" button. A confirmation dialog will appear, warning you about the consequences of deleting a user. If you want to proceed, enter the user's email address in the confirmation field. Click \"Delete\" to permanently remove the user. Deactivating a User Instead of Deleting If you want to temporarily remove a user's access without permanently deleting their account: On the Users page, find the user you want to deactivate in the list. Click the \"Actions\" button for that user. Select \"Deactivate\" from the dropdown menu. A confirmation dialog will appear. Click \"Deactivate\" to temporarily remove the user's access. Reactivating a Deactivated User To restore access for a deactivated user: On the Users page, set the filter to show \"Inactive Users\". Find the user you want to reactivate in the list. Click the \"Actions\" button for that user. Select \"Activate\" from the dropdown menu. A confirmation dialog will appear. Click \"Activate\" to restore the user's access. Notes Deleting a user will revoke all their access to the XMPro platform immediately. Deleted users will receive an email notification informing them that their account has been deleted. If a deleted user needs access again in the future, you will need to invite them as a new user. Consider exporting any important user data before deletion if it may be needed in the future."
  },
  "docs/administration/users/index.html": {
    "href": "docs/administration/users/index.html",
    "title": "Users | XMPro",
    "summary": "Users Users are individuals who have access to the XMPro platform. Each user belongs to a company and has specific roles and permissions within that company. As an administrator, you can: Invite new users Register user accounts Manage user profiles Change user passwords Reset user passwords Delete users Change user business roles User management is an important aspect of maintaining security and ensuring that users have the appropriate access to features and services within the XMPro platform."
  },
  "docs/administration/users/invite-a-user.html": {
    "href": "docs/administration/users/invite-a-user.html",
    "title": "Invite a User | XMPro",
    "summary": "Invite a User This page explains how to invite a new user to the XMPro platform. Prerequisites You must have Global Administrator or Company Administrator privileges to invite users. The company must already be registered in the system. Accessing User Management Log in to the XMPro platform using your Administrator account. Navigate to the Administration section. Select \"Users\" from the menu. Inviting a New User Steps to Invite a User On the Users page, click the \"Invite User\" button. Fill in the user details: First Name: Enter the user's first name. Last Name: Enter the user's last name. Email Address: Enter the user's email address. This will be used as their username. Business Role: Select the appropriate business role for the user. Department: Optionally, specify the user's department. Phone Number: Optionally, enter the user's phone number. Configure user settings: Language: Select the preferred language for the user. Time Zone: Select the appropriate time zone for the user. Other settings as required. Assign subscriptions: Select the subscriptions to grant to the user. For each subscription, assign a role (Viewer, Contributor, Administrator). Click \"Send Invitation\" to invite the user. Customizing the Invitation Email Before sending the invitation, you can customize the invitation email: Click the \"Customize Email\" button. Edit the email subject and body as needed. Use the available placeholders for dynamic content (e.g., {UserName}, {CompanyName}). Click \"Save\" to apply the changes. Tracking Invitations Navigate to the \"Invitations\" tab. This tab displays all pending and completed invitations, including: Recipient email Invitation date Status (Pending, Accepted, Expired) Expiration date Resending an Invitation Find the invitation you want to resend in the list. Click the \"Resend\" button for that invitation. Optionally, update the invitation details. Click \"Resend Invitation\" to send the invitation again. Canceling an Invitation Find the invitation you want to cancel in the list. Click the \"Cancel\" button for that invitation. Confirm the cancellation when prompted. The invitation will be canceled, and the user will no longer be able to accept it. After Invitation Once a user accepts an invitation, they will need to: Click the link in the invitation email. Create a password for their account. Complete any additional registration steps. Log in to the XMPro platform. Managing Existing Users For detailed information on managing existing users, see: Register an Account Profile Change Password Reset Password Delete a User Change Business Role Notes Invitations are valid for a limited time (typically 7 days). Users must accept the invitation and complete the registration process before they can access the platform. If an invitation expires, you can resend it to the user."
  },
  "docs/administration/users/profile.html": {
    "href": "docs/administration/users/profile.html",
    "title": "Profile | XMPro",
    "summary": "Profile This page explains how to manage your user profile in the XMPro platform. Prerequisites You must have an active XMPro account. You must be logged in to the XMPro platform. Accessing Your Profile Log in to the XMPro platform using your credentials. Click on your profile picture or initials in the top-right corner of the screen. Select \"Profile\" from the dropdown menu. Viewing Your Profile Information The Profile page displays your current profile information, including: Personal Information: First Name Last Name Email Address Phone Number Department Job Title Profile Picture Account Settings: Language Preference Time Zone Notification Preferences Subscription Information: Assigned Subscriptions Roles within each subscription Updating Your Profile Update Personal Information On the Profile page, click the \"Edit\" button in the Personal Information section. Update your information as needed: First Name Last Name Phone Number Department Job Title Click \"Save\" to apply the changes. Change Profile Picture On the Profile page, hover over your profile picture. Click the \"Change Picture\" button that appears. Choose one of the following options: Upload a new picture: Click \"Browse\" to select an image from your device. Take a photo: If your device has a camera, you can click \"Take Photo\" to capture a new profile picture. Remove picture: Click \"Remove\" to delete your current profile picture and use your initials instead. If uploading or taking a photo, you may be prompted to crop the image. Click \"Save\" to apply the changes. Update Account Settings On the Profile page, click the \"Edit\" button in the Account Settings section. Update your settings as needed: Language: Select your preferred language for the platform interface. Time Zone: Select your time zone to ensure accurate time displays. Notification Preferences: Configure how and when you receive notifications. Click \"Save\" to apply the changes. Viewing Your Subscriptions On the Profile page, navigate to the \"Subscriptions\" tab. This tab displays all subscriptions you have access to, including: Subscription name Role within the subscription Access status Expiration date (if applicable) Requesting Access to Subscriptions On the Profile page, navigate to the \"Subscriptions\" tab. Click the \"Request Access\" button. Select the subscription you want to request access to. Provide a reason for the request. Click \"Submit Request\" to send the request to the administrators. Managing Your Password For detailed information on managing your password, see: Change Password Reset Password Notes Some profile information may be managed by your administrator and cannot be changed by you. Your email address is used as your username and cannot be changed. Contact your administrator if you need to update your email address. Profile changes may take a few moments to propagate throughout the system."
  },
  "docs/administration/users/register-an-account.html": {
    "href": "docs/administration/users/register-an-account.html",
    "title": "Register an Account | XMPro",
    "summary": "Register an Account This page explains how to register a new account in the XMPro platform. Prerequisites You must have received an invitation to join the XMPro platform. The invitation must not have expired. Registering a New Account Steps to Register an Account Open the invitation email sent to you. Click the \"Register Account\" button or link in the email. You will be redirected to the XMPro registration page. Fill in the registration form: First Name: Your first name (may be pre-filled from the invitation). Last Name: Your last name (may be pre-filled from the invitation). Email Address: Your email address (will be pre-filled from the invitation). Password: Create a strong password for your account. Confirm Password: Re-enter the password to confirm. Review and accept the terms and conditions. Click \"Register\" to create your account. Password Requirements When creating your password, ensure it meets the following requirements: Minimum length of 8 characters Contains at least one uppercase letter Contains at least one lowercase letter Contains at least one number Contains at least one special character Does not contain your username or common words Completing Your Profile After registering your account, you may be prompted to complete your profile: Fill in any additional required information: Phone Number Department Job Title Profile Picture Configure your account settings: Language Preference Time Zone Notification Preferences Click \"Save\" to update your profile. Logging In for the First Time After registering your account, you can log in to the XMPro platform: Navigate to the XMPro login page. Enter your email address and password. Click \"Log In\" to access the platform. You may be prompted to complete a first-time login tutorial or setup process. Managing Your Account For detailed information on managing your account, see: Profile Change Password Reset Password Notes If you encounter any issues during registration, contact your company administrator or XMPro support. Your account will be configured with the subscriptions and roles assigned to you by the administrator who sent the invitation. If your invitation has expired, contact your administrator to request a new invitation."
  },
  "docs/administration/users/reset-password.html": {
    "href": "docs/administration/users/reset-password.html",
    "title": "Reset Password | XMPro",
    "summary": "Reset Password This page explains how to reset your password in the XMPro platform. Prerequisites You must have an active XMPro account. You must have access to the email address associated with your account. When to Reset Your Password You should reset your password in the following situations: You have forgotten your current password. You are unable to log in to your account. You suspect your account has been compromised. You have received a notification that your password needs to be reset. Resetting Your Password Method 1: From the Login Page Navigate to the XMPro login page. Click the \"Forgot Password?\" link below the login form. Enter your email address in the provided field. Click \"Reset Password\" to request a password reset. Check your email for a password reset link. Click the link in the email to open the password reset page. Enter your new password in the \"New Password\" field. Re-enter your new password in the \"Confirm New Password\" field to verify it. Click \"Reset Password\" to apply the changes. Method 2: Contacting an Administrator If you are unable to reset your password using the self-service method, you can contact an administrator: Reach out to your company administrator or the XMPro support team. Provide your email address and verify your identity. The administrator will reset your password and send you a temporary password or a password reset link. Follow the instructions provided by the administrator to set a new password. Password Requirements When creating your new password, ensure it meets the following requirements: Minimum length of 8 characters Contains at least one uppercase letter Contains at least one lowercase letter Contains at least one number Contains at least one special character Does not contain your username or common words Is different from your previous passwords After Resetting Your Password After successfully resetting your password: You will receive a confirmation message. You can log in to the XMPro platform using your new password. You may be prompted to update your security questions or other account settings. If You Still Can't Access Your Account If you are still unable to access your account after resetting your password, try the following: Check if you are using the correct email address. Ensure you are entering the new password correctly. Clear your browser cache and cookies, then try again. Try using a different browser or device. Contact your company administrator or the XMPro support team for further assistance. Notes Password reset links are typically valid for a limited time (e.g., 24 hours). If your password reset link has expired, you will need to request a new one. For security reasons, XMPro will never ask for your password via email or phone. If you receive a password reset email that you did not request, your account may be at risk. Contact your administrator immediately."
  },
  "docs/blocks-toolbox/actions/README.html": {
    "href": "docs/blocks-toolbox/actions/README.html",
    "title": "Actions Blocks | XMPro",
    "summary": "Actions Blocks Actions blocks in XMPro App Designer enable user interactions and trigger functionality within your applications. These blocks allow users to navigate between pages, submit forms, execute operations, and interact with data sources. Available Actions Blocks Box Hyperlink - A container that acts as a clickable hyperlink Button - A standard button control for triggering actions Data Operations - A block for performing operations on data sources Hyperlink - A text-based hyperlink for navigation Best Practices for Using Actions Blocks Use clear and descriptive labels: Ensure that action labels clearly communicate what will happen when the user interacts with them. For example, use \"Save\" instead of \"OK\" for a button that saves a form. Provide visual feedback: Use visual cues to indicate when an action is being processed or has been completed. This helps users understand the status of their interactions. Consider placement and visibility: Position action blocks where users expect to find them and ensure they are easily visible. Follow standard UI patterns, such as placing primary actions on the right and secondary actions on the left. Implement appropriate validation: Before executing actions that modify data or navigate away from a page, ensure that appropriate validation is performed to prevent data loss or invalid operations. Handle errors gracefully: When actions encounter errors, provide clear and helpful error messages that guide users on how to resolve the issue. Optimize for performance: Actions should be responsive and execute quickly. If an action requires significant processing time, provide feedback to the user and consider using asynchronous processing. Consider accessibility: Ensure that action blocks are accessible to all users, including those using screen readers or keyboard navigation. Use appropriate ARIA attributes and ensure keyboard focus is managed correctly. Examples Form Submission Box (Form Container) ├── Field (Name) │ ├── Textbox ├── Field (Email) │ ├── Textbox ├── Field (Message) │ ├── Text Area ├── Box (Buttons) │ ├── Button (Submit) │ │ ├── On Click (Submit Form) │ ├── Button (Cancel) │ │ ├── On Click (Navigate to Home) Data Grid with Actions Box (Data Grid Container) ├── Data Grid │ ├── Column (ID) │ ├── Column (Name) │ ├── Column (Email) │ ├── Column (Actions) │ │ ├── Button (Edit) │ │ │ ├── On Click (Navigate to Edit Page) │ │ ├── Button (Delete) │ │ │ ├── On Click (Delete Record) ├── Box (Toolbar) │ ├── Button (Add New) │ │ ├── On Click (Navigate to Add Page) │ ├── Button (Refresh) │ │ ├── On Click (Refresh Data) Navigation Menu Box (Navigation Container) ├── Menu │ ├── Menu Item (Home) │ │ ├── Hyperlink (Navigate to Home) │ ├── Menu Item (Products) │ │ ├── Hyperlink (Navigate to Products) │ ├── Menu Item (Services) │ │ ├── Hyperlink (Navigate to Services) │ ├── Menu Item (About) │ │ ├── Hyperlink (Navigate to About) │ ├── Menu Item (Contact) │ │ ├── Hyperlink (Navigate to Contact) Dashboard with Action Cards Box (Dashboard Container) ├── Layout Grid │ ├── Box Hyperlink (Navigate to Sales) │ │ ├── Card (Sales) │ │ │ ├── Text (Sales Summary) │ │ │ ├── Chart (Sales Chart) │ ├── Box Hyperlink (Navigate to Inventory) │ │ ├── Card (Inventory) │ │ │ ├── Text (Inventory Summary) │ │ │ ├── Chart (Inventory Chart) │ ├── Box Hyperlink (Navigate to Customers) │ │ ├── Card (Customers) │ │ │ ├── Text (Customer Summary) │ │ │ ├── Chart (Customer Chart) By effectively using actions blocks, you can create interactive and responsive applications that provide a seamless user experience. These blocks enable users to navigate through your application, interact with data, and trigger functionality that meets their needs."
  },
  "docs/blocks-toolbox/actions/box-hyperlink.html": {
    "href": "docs/blocks-toolbox/actions/box-hyperlink.html",
    "title": "Box Hyperlink | XMPro",
    "summary": "Box Hyperlink A Box Hyperlink is a Block where you can link to another location or website. Box Hyperlink Properties Appearance Common Properties You can change the visibility of the Box Hyperlink. See the Common Properties article for more details on common appearance properties. Text This is only available if the mode option in the behavior tab is set to true. Action Common Properties Properties that are common to most Blocks include: Navigate To and Show Confirmation Dialog; See the Common Properties article for more details on common action properties."
  },
  "docs/blocks-toolbox/actions/button.html": {
    "href": "docs/blocks-toolbox/actions/button.html",
    "title": "Button | XMPro",
    "summary": "Button A Button is a Block that the user can click on which can trigger an event such as loading another page or confirming details on a form. Button Properties Appearance Common Properties Properties that are common to most Blocks include visibility, styling mode, tooltip, and icon; See the Common Properties article for more details on common appearance properties. Type The type of the button can be changed depending on its purpose. Options include danger, normal, success, and default. Text The text that shows on top of the Button. Behavior Common Properties The disabled property is common to most Blocks; See the Common Properties article for more details on common behavior properties. Enable Focus This determines if the user can navigate to the Button by using the keyboard. This includes using the tab button to switch between text boxes on a form, and then clicking the tab button at the end to highlight and select the Button. Buttons will also be focussed on when you click on them. If a Button is clicked, and no action occurs, the Button will also remain in focus. Validation Common Properties Properties that are common to most Blocks include: groups to validate; See the Common Properties article for more details on common validation properties. Action Common Properties Properties that are common to most Blocks include: Navigate To and Show Confirmation Dialog; See the Common Properties article for more details on common action properties."
  },
  "docs/blocks-toolbox/actions/data-operations.html": {
    "href": "docs/blocks-toolbox/actions/data-operations.html",
    "title": "Data Operations | XMPro",
    "summary": "Data Operations A Data Operation Block is a button with additional functionality that allows you to Insert or Delete records from a Data Repeater Block that is bound to a Data Source. Data Operation Properties Appearance Common Properties Properties that are common to most Blocks include Visibility, Styling Mode, Tooltip, and Icon. The Styling Mode, Tooltip, and Icon properties are only available for the Data Operations Block if the Display Mode property is set to 'Button'; See the Common Properties article for more details on common appearance properties. Text The text that shows on top of the Button. If the Display Mode is set to 'Hyperlink', the default text will either be 'Add' or 'Delete', depending on what Mode is selected under Behaviors. Display Mode The Display Mode of a Data Operations Block can either be a Button or a Hyperlink. By default, the Block is configured with either an 'Add' or 'Delete' icon if the 'Button' Display Mode is selected. Type The type of the button can be changed depending on its purpose. Options include Danger, Normal, Success, and Default. This is only available if the Display Mode property is set to 'Button'. Behavior Common Properties The Disabled property is common to most Blocks. The Disabled property is only available for the Data Operations Block if the Display Mode property is set to 'Button'; See the Common Properties article for more details on common behavior properties. Mode There are two modes that you can choose from: Insert and Delete. The Mode determines what operation will be performed on the Data Source and records. 'Insert' will add a new row to the list: 'Delete' will delete a record from the list: Note When a record is inserted or deleted, it does not immediately update the Data Source. To do this, you will need to add a regular Button with 'Update Data Sources' selected. See the Common Properties article for more details on common action properties. Data Source The Data Source that you would like to add a new record to. This is only available if the Mode property is set to 'Insert'. If the Mode property is set to 'Insert', the Data Source property is required for the Data Operations Block. Enable Focus This determines if the user can navigate to the Button by using the keyboard. This includes using the tab button to switch between text boxes on a form, and then clicking the tab button at the end to highlight and select the Button. Buttons will also be focused on when you click on them. If a Button is clicked, and no action occurs, the Button will also remain in focus. This is only available if the Display Mode property is set to 'Button'. Show Confirmation Dialog This can either be set to 'True' or 'False'. If True, a dialog box will appear asking the user if they are sure they want to delete that particular record. This is only available if the Mode property is set to 'Delete'."
  },
  "docs/blocks-toolbox/actions/hyperlink.html": {
    "href": "docs/blocks-toolbox/actions/hyperlink.html",
    "title": "Hyperlink | XMPro",
    "summary": "Hyperlink A Hyperlink is a Block where you can link to another location or website. Hyperlink Properties Appearance Common Properties The visibility property is common to most Blocks; See the Common Properties article for more details on common appearance properties. Text This is the text that will be displayed as a link for the user to click on. Action Common Properties Properties that are common to most Blocks include: Navigate To and Show Confirmation Dialog; See the Common Properties article for more details on common action properties."
  },
  "docs/blocks-toolbox/advanced/README.html": {
    "href": "docs/blocks-toolbox/advanced/README.html",
    "title": "Advanced Blocks | XMPro",
    "summary": "Advanced Blocks Advanced blocks in XMPro App Designer provide specialized functionality for complex scenarios and advanced use cases. These blocks offer powerful capabilities that extend beyond the basic building blocks, enabling you to create sophisticated and highly customized applications. Available Advanced Blocks Metablock - A container block that can encapsulate multiple blocks and their functionality into a reusable component Best Practices for Using Advanced Blocks Understand the capabilities: Take time to fully understand the capabilities and limitations of advanced blocks before implementing them in your applications. These blocks often have complex functionality that requires careful consideration. Document your implementations: When using advanced blocks, document your implementation thoroughly, including the purpose, configuration, and any custom code or logic. This will help with maintenance and knowledge transfer. Test thoroughly: Advanced blocks often involve complex interactions and functionality. Test your implementations thoroughly to ensure they work as expected in all scenarios. Consider performance implications: Some advanced blocks may have performance implications, especially when used with large datasets or complex logic. Monitor performance and optimize as needed. Use modular design: When creating Metablocks, follow modular design principles to ensure they are reusable and maintainable. Encapsulate related functionality and provide clear interfaces. Maintain version control: Keep track of versions of your advanced block implementations, especially for Metablocks that may be reused across multiple applications. Provide training and documentation: If other users will be working with your advanced block implementations, provide training and documentation to ensure they understand how to use and maintain them. Examples Reusable Form Component with Metablock Metablock (Form Component) ├── Box (Form Container) │ ├── Field (Name) │ │ ├── Textbox │ ├── Field (Email) │ │ ├── Textbox │ ├── Field (Phone) │ │ ├── Textbox │ ├── Box (Buttons) │ │ ├── Button (Submit) │ │ ├── Button (Cancel) ├── Properties │ ├── Property (Form Title) │ ├── Property (Submit Action) │ ├── Property (Cancel Action) │ ├── Property (Data Source) Dashboard Component with Metablock Metablock (Dashboard Component) ├── Box (Dashboard Container) │ ├── Text (Title) │ ├── Box (Metrics) │ │ ├── Bar Gauge (Metric 1) │ │ ├── Bar Gauge (Metric 2) │ │ ├── Bar Gauge (Metric 3) │ ├── Chart (Trend Chart) │ │ ├── Series (Data Series 1) │ │ ├── Series (Data Series 2) ├── Properties │ ├── Property (Dashboard Title) │ ├── Property (Metric 1 Label) │ ├── Property (Metric 1 Value) │ ├── Property (Metric 2 Label) │ ├── Property (Metric 2 Value) │ ├── Property (Metric 3 Label) │ ├── Property (Metric 3 Value) │ ├── Property (Chart Data) Custom Data Entry Component with Metablock Metablock (Data Entry Component) ├── Box (Data Entry Container) │ ├── Text (Instructions) │ ├── Data Grid (Data Entry Grid) │ │ ├── Column (Field 1) │ │ ├── Column (Field 2) │ │ ├── Column (Field 3) │ ├── Box (Buttons) │ │ ├── Button (Add Row) │ │ ├── Button (Delete Row) │ │ ├── Button (Save) ├── Properties │ ├── Property (Instructions Text) │ ├── Property (Grid Columns) │ ├── Property (Data Source) │ ├── Property (Save Action) By effectively using advanced blocks, you can create sophisticated, reusable components that enhance the functionality and maintainability of your XMPro applications. These blocks enable you to encapsulate complex logic and user interface elements into modular, reusable components that can be shared across your organization."
  },
  "docs/blocks-toolbox/ai/README.html": {
    "href": "docs/blocks-toolbox/ai/README.html",
    "title": "AI Blocks | XMPro",
    "summary": "AI Blocks AI blocks in XMPro App Designer provide powerful artificial intelligence capabilities that can be integrated directly into your applications. These blocks leverage advanced AI technologies to enhance your applications with intelligent features such as natural language processing, content generation, and contextual assistance. Available AI Blocks Azure Copilot - AI assistant powered by Microsoft Azure AI services ChatGPT Copilot - AI assistant powered by OpenAI's ChatGPT technology Best Practices for Using AI Blocks Define clear use cases: Identify specific use cases where AI can add value to your application. AI blocks are most effective when they address well-defined problems or enhance specific user experiences. Set appropriate expectations: Communicate clearly to users about the capabilities and limitations of AI features in your application. This helps manage user expectations and builds trust. Provide context: AI models perform better when given appropriate context. Design your application to provide relevant context to AI blocks, such as user history, current task, or domain-specific information. Implement user feedback mechanisms: Allow users to provide feedback on AI-generated content or suggestions. This feedback can help improve the AI's performance over time and identify areas for improvement. Consider privacy and security: Be mindful of the data being sent to AI services. Ensure that sensitive information is handled appropriately and in compliance with relevant regulations. Design for graceful degradation: Ensure that your application can still function effectively if AI services are unavailable or if responses don't meet expectations. Monitor and evaluate performance: Regularly monitor the performance of AI blocks in your application. Track metrics such as accuracy, relevance, and user satisfaction to identify opportunities for improvement. Examples Customer Support Assistant Box (Support Assistant Container) ├── Text (Welcome message) ├── ChatGPT Copilot │ ├── Configuration │ │ ├── System Prompt (You are a helpful customer support assistant...) │ │ ├── Temperature (0.7) │ │ ├── Max Tokens (1000) │ ├── Chat Interface │ │ ├── Message History │ │ ├── User Input │ │ ├── Send Button Document Analysis and Summarization Box (Document Analysis Container) ├── File Uploader │ ├── Button (Upload Document) ├── Azure Copilot │ ├── Configuration │ │ ├── System Prompt (Analyze and summarize the uploaded document...) │ │ ├── Temperature (0.5) │ │ ├── Max Tokens (2000) │ ├── Output Display │ │ ├── Text (Summary) │ │ ├── Text (Key Points) │ │ ├── Text (Recommendations) Data Insights Assistant Box (Data Insights Container) ├── Data Grid (Data Display) ├── ChatGPT Copilot │ ├── Configuration │ │ ├── System Prompt (You are a data analysis assistant...) │ │ ├── Temperature (0.2) │ │ ├── Max Tokens (1500) │ ├── Chat Interface │ │ ├── Message History │ │ ├── User Input │ │ ├── Send Button ├── Box (Insights Display) │ ├── Text (Generated Insights) │ ├── Chart (Visualization) By effectively integrating AI blocks into your applications, you can provide intelligent, context-aware experiences that enhance user productivity, provide valuable insights, and automate complex tasks. These AI-powered features can significantly improve the value and capabilities of your XMPro applications."
  },
  "docs/blocks-toolbox/ai/azure-copilot.html": {
    "href": "docs/blocks-toolbox/ai/azure-copilot.html",
    "title": "Azure Copilot | XMPro",
    "summary": "Azure Copilot The Azure Copilot block utilizes the Azure OpenAI service to provide advanced AI chat functionality in your application. Azure Copilot Properties Appearance Common Properties The visibility property is common to most Blocks; See the Common Properties article for more details on common appearance properties. Prompt Input Height (px) The height in pixels of the prompt display within the overall block size. Adjust this value to best suit your design: higher for a portrait block and lower for a landscape block - or desktop vs mobile. Response Welcome Message The initial text displayed in the response area. Disclaimer Message The disclaimer message displayed below the prompt input. Behavior Use Variables Tick to select static variables for the Azure OpenAI Endpoint, Azure OpenAI Key, and Azure OpenAI Deployment ID, or manually enter the values. Azure OpenAI Endpoint Specifies the resource endpoint for creating an Azure OpenAI service within the user's block. You can obtain your API Endpoint within the Azure Portal. Azure OpenAI Key This secret key is essential for the Azure OpenAI service, allowing the user to interact with the service. You can obtain your API key within the Azure Portal. Azure OpenAI Deployment ID The model version (e.g., gpt-4, dall-e, gpt-3-turbo) that the designer intends to use in their Copilot block. System Prompt Influence the personality of the AI's response."
  },
  "docs/blocks-toolbox/ai/chatgpt-copilot.html": {
    "href": "docs/blocks-toolbox/ai/chatgpt-copilot.html",
    "title": "ChatGPT Copilot | XMPro",
    "summary": "ChatGPT Copilot The ChatGPT Copilot block utilizes the ChatGPT OpenAI service to provide advanced AI chat functionality in your Application. ChatGPT Copilot Properties Appearance Common Properties The visibility property is common to most Blocks; See the Common Properties article for more details on common appearance properties. Prompt Input Height (px) The height in pixels of the prompt display within the overall block size. Adjust this value to best suit your design: higher for a portrait block and lower for a landscape block - or desktop vs mobile. Response Welcome Message The initial text displayed in the response area. Disclaimer Message The disclaimer message displayed below the prompt input. Behavior Use Variables Tick to select a static variable for the ChatGPT OpenAI Key, or manually enter the value. ChatGPT OpenAI Key The key required to authorize interaction with the ChatGPT OpenAI service. You can obtain your API key by accessing the \"API keys\" section within the OpenAI API platform. ChatGPT Models The desired model version (e.g., gpt-4, dall-e, gpt-3-turbo). System Prompt Influence the personality of the AI's response."
  },
  "docs/blocks-toolbox/ai/index.html": {
    "href": "docs/blocks-toolbox/ai/index.html",
    "title": "AI | XMPro",
    "summary": "AI"
  },
  "docs/blocks-toolbox/basic/README.html": {
    "href": "docs/blocks-toolbox/basic/README.html",
    "title": "Basic Blocks | XMPro",
    "summary": "Basic Blocks Basic blocks are the fundamental building blocks of XMPro applications. They provide essential functionality for user input, data display, and interaction. These blocks are the most commonly used components in application development. Available Basic Blocks Calendar - Date selection and calendar display component Check Box - Toggle control for boolean values Color Selector - Component for selecting colors Data Grid - Tabular data display with sorting, filtering, and editing capabilities Date Selector - Component for selecting dates Dropdown Grid - Dropdown component with grid-based selection Embedded Page - Component for embedding other pages within a page File Library - Component for managing and displaying files File Uploader - Component for uploading files Html Editor - Rich text editor for HTML content Image - Component for displaying images Indicator - Visual indicator for status or state List - Component for displaying lists of items Lookup - Component for looking up values from a data source Number Selector - Component for selecting numeric values Radio Buttons - Selection control for choosing one option from a set Range Slider - Component for selecting a value from a range Select Box - Dropdown selection component Switch - Toggle switch for boolean values Tags - Component for displaying and managing tags Text - Component for displaying text Text Area - Multi-line text input component Textbox - Single-line text input component Tree Grid - Hierarchical data display with grid capabilities Tree List - Hierarchical list component Best Practices for Using Basic Blocks Choose the right input type: Select the appropriate input block based on the type of data you're collecting. For example, use a Date Selector for dates, a Number Selector for numbers, and a Select Box for selecting from a predefined list of options. Provide clear labels: Always include clear labels for input blocks to help users understand what information is being requested. Use validation: Implement validation on input blocks to ensure that users provide valid data. This can prevent errors and improve the user experience. Consider accessibility: Ensure that your application is accessible to all users by providing appropriate ARIA labels, ensuring keyboard navigation works correctly, and maintaining sufficient color contrast. Optimize for performance: Be mindful of the number of blocks you use, especially data-intensive blocks like Data Grid and Tree Grid, as they can impact performance. Use consistent styling: Maintain consistent styling across your application to provide a cohesive user experience. Implement responsive design: Ensure that your application works well on different screen sizes by using responsive design techniques. Examples Form with Various Input Types Box (Form Container) ├── Field (Name) │ ├── Textbox ├── Field (Email) │ ├── Textbox (with email validation) ├── Field (Date of Birth) │ ├── Date Selector ├── Field (Gender) │ ├── Radio Buttons │ ├── Option (Male) │ ├── Option (Female) │ ├── Option (Other) ├── Field (Interests) │ ├── Tags ├── Field (Comments) │ ├── Text Area ├── Box (Buttons) │ ├── Button (Submit) │ ├── Button (Cancel) Data Display Box (Data Display Container) ├── Data Grid │ ├── Column (ID) │ ├── Column (Name) │ ├── Column (Email) │ ├── Column (Status) │ │ ├── Indicator │ ├── Column (Actions) │ │ ├── Button (Edit) │ │ ├── Button (Delete) Search and Filter Box (Search Container) ├── Field (Search) │ ├── Textbox │ ├── Button (Search) ├── Field (Category) │ ├── Select Box ├── Field (Date Range) │ ├── Date Selector (From) │ ├── Date Selector (To) ├── Field (Price Range) │ ├── Range Slider By effectively using basic blocks, you can create intuitive, functional, and user-friendly applications that meet your users' needs."
  },
  "docs/blocks-toolbox/basic/calendar.html": {
    "href": "docs/blocks-toolbox/basic/calendar.html",
    "title": "Calendar | XMPro",
    "summary": "Calendar The Calendar is a Block that displays a Calendar and allows the user to select the required date within a specified date range. This is useful to use on forms where the user needs to enter a date for a particular field. It is also useful for displaying certain important dates to the user. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Calendar Properties Appearance Common Properties You can specify if the Calendar is visible, or if tooltips are enabled. See the Common Properties article for more details on common appearance properties. Show Today Button This specifies if the button that takes the user back to the current date is displayed. Zoom Levels This specifies the time frame of selectable dates. Options include month, year, decade, and century. Min and Max Zoom Levels This specifies the limit on where the user can zoom in and out of the dates. For example, they can zoom until they reach the page that shows the yearly view, and can only zoom out to see decades. Behavior Common Properties The read-only and disabled properties are common to most Blocks; See the Common Properties article for more details on common behavior properties. Min and Max This only lets the user select dates within a limited range. First Day of the Week Changes the day of the week that the Calendar starts on. Value Common Properties The Value property is common to most Blocks; See the Common Properties article for more details on common value properties. The accepted values for the Calendar include the selected date or time that the user clicks on. This can either be a date, number, or sequence of characters. The Date option will accept the date directly. The number option will accept the date using a timestamp. The string option will accept the date as a sequence of characters provided they are in the correct format: \"yyyy-MM-dd\" (for example, \"2017-03-06\") \"yyyy-MM-ddTHH:mm:ss\" (for example, \"2017-03-27T16:54:48\") \"yyyy-MM-ddTHH:mm:ssZ\" (for example, \"2017-03-27T13:55:41Z\") \"yyyy-MM-ddTHH:mm:ssx\" (for example, \"2017-03-27T16:54:10+03\") Disabled Dates Data Source Common Properties If set to the Dynamic Data Source option, additional options include filtering, sorting, showing a number of results, and skipping a number of results. See the Common Properties article for more details on common Data Source properties. Static Items If a Dynamic Data Source is not used, you can enter key dates to display manually under the Data section. Dynamic Data Source This option allows you to connect the control to a specific Data Source such as a database to pull data dynamically. This will give you additional options to sort, filter, show, or skip certain records. Data The data allows you to choose a date based on the connected Data Source. This can be configured when using static items for disabled dates Data Sources. Action Common Properties Properties that are common to most Blocks include: Navigate To and Show Confirmation Dialog; See the Common Properties article for more details on common action properties."
  },
  "docs/blocks-toolbox/basic/checkbox.html": {
    "href": "docs/blocks-toolbox/basic/checkbox.html",
    "title": "Check Box | XMPro",
    "summary": "Check Box A Checkbox is a control that allows the user to tick an option. This is useful to use on forms where the user only needs to select something that requires a 'Yes' or 'No' type of option for a particular field. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Check Box Properties Appearance Common Properties You can change the visibility and tooltip for a Checkbox which is common to most Blocks; See the Common Properties article for more details on common appearance properties. Label The label is the text that shows next to the Checkbox. Behavior Common Properties The disabled and read-only properties are common to most Blocks; See the Common Properties article for more details on common behavior properties. Value Common Properties The value property is common to most Blocks; See the Common Properties article for more details on common value properties. The value dictates whether or not the Checkbox was selected. This can either be true or false. Action Common Properties Properties that are common to most Blocks include: Navigate To and Show Confirmation Dialog; See the Common Properties article for more details on common action properties."
  },
  "docs/blocks-toolbox/basic/color-selector.html": {
    "href": "docs/blocks-toolbox/basic/color-selector.html",
    "title": "Color Selector | XMPro",
    "summary": "Color Selector The Color Selector lets the user select from a range of colors using the selector dropdown. Once the user selects the color, the hex value of the color is displayed in the input box. This is a useful tool for the user to select colors visually. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Color Selector Properties Appearance Common Properties The Color Selector has properties that are common to most Blocks: visible, styling mode, tooltip, placeholder, and clear buttons; See the Common Properties article for more details on common appearance properties. Behavior Common Properties Common options for the behavior include read-only and disabled. See the Common Properties article for more details on common behavior properties. Apply Value Mode When the use buttons option is selected, the user has to select from the OK or Cancel buttons at the bottom of the color picker. Accept Custom Value If this is enabled, the user will be able to type in or copy and paste their own hex value into the input box. If this is disabled, this will not be possible and the user can only select a color from the dropdown. Value Common Properties The value property is common to most Blocks; See the Common Properties article for more details on common value properties. The value determines the starting color for the Color Selector. Only a sequence of characters that are equal to a known hex color will be accepted. Action Common Properties Properties that are common to most Blocks include: Navigate To and Show Confirmation Dialog; See the Common Properties article for more details on common action properties."
  },
  "docs/blocks-toolbox/basic/data-grid.html": {
    "href": "docs/blocks-toolbox/basic/data-grid.html",
    "title": "Data Grid | XMPro",
    "summary": "Data Grid A Data Grid allows you to display important information to the user in a grid format. This is useful for displaying all records from a database, or a selected number of records from a database. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Data Grid Properties Appearance Common Properties The visibility property is common to most Blocks; See the Common Properties article for more details on common appearance properties. Show Borders Will show borders around the grid. Show Headers Will show the headers/column title. Show Column Lines Will show a vertical line between columns. Show Row Lines Will show a horizontal line between rows. Alter Row Color The background color of the odd rows will be grey. Show Column Chooser Column Chooser button will be displayed and the user has the ability to hide/show columns. Enable Paging The default option is to show all the results. The user can specify how many items should be displayed per page and pages will be displayed under the grid. Behavior Common Properties The disabled property is common to most Blocks; See the Common Properties article for more details on common behavior properties. Allow Selection Allows the user to select an item from the grid. Allow Multiple Selection Allows the user to select multiple items from the grid. Allow Adding Add button will be displayed and the user can add/insert rows by clicking on it. Allow Deleting The delete button will be displayed on the right side of the row. Allow Updating This will enable editing the row by clicking the item. Allow Search This will let you search the grid with the search bar. Edit Mode Grid data can be edited in several modes. Set the Edit Mode property to specify the mode. Mode Description Batch A user edits data cell by cell. Changes are not updated until a user clicks the Save button. In this mode, the \"Add\" button is found above the grid rather than in the grid's header row, along with the Save and Reset buttons. Batch With External Save A user edits data cell by cell. Changes are not updated until a user clicks an external Block (e.g. a Button) with Update Data Sources corresponding to the grid's data source. Cell A user edits data cell by cell. Changes are saved once a cell loses focus, or discarded if a user presses Esc. Row A user edits data row by row. When a user clicks an \"Edit\" button in the right-most column, the corresponding row enters the editing state, and the \"Save\" and \"Cancel\" buttons appear in the right-most column. Pressing the \"Save\" button will update your data source immediately. Allow Search This will show a search bar at the top of the Data Grid. Allow Export to Excel The export button will be displayed and by clicking it will export the grid into an excel file. Enable Column Filtering This will let you filter the results per column by clicking the filter icon next to the column name. Enable Row Filtering The search bar will be added for each column and the user can search the results. Enable Filter Panel Create Filter button will be displayed and clicking it will open a Filter Builder. If a user changes the filter expression in the Filter Panel the changes are reflected in the Enable Row Filtering and Enable Column Filtering, and vice versa. Default Filter This defines the default filter selected in the Filter Panel. Anyone visiting the page for the first time will have the same filter applied to the Data Grid. For example, every time this Data Grid has loaded records that start with the letter \"C\" are displayed. The filter will still be applied if the filter panel is disabled, which will prevent the user from changing the filter. Allow Grouping Auto-Adjust Column Widths Will try to adjust the width of the column to show results as much as possible. Store User Customization Changes made to the grid are saved in the cookies on your browser between page refresh and window changes. This can include column reordering, resizing and applied filters. Value Common Properties The value property is common to most Blocks; See the Common Properties article for more details on common value properties. Data Source Common Properties Properties that are common to most Blocks include: filter, sort, show # of results, and skip # of results; ‌See the Common Properties article for more details on common Data Source properties. The Data Source property is required for the Data Grid. Columns List of all columns from the selected Data Source. Users can reorder or change the visibility, name, type, alignment, width, set it as read-only, and set the Editor type. Format If the Type field is set to Number, you have the option to format the field as default (none), currency, or percentage. Currency If the Format field is set to Currency, choose which currency symbol to display. Date Time Format If the Type field is set to Date or Date Time, you can enter a custom format for the Date or the Date Time. By default, the values are displayed in the user's browser's locale format. Editor Type - Lookup The lookup field will only appear when the cell or the row is in edit mode. The Lookup editor type has three configurable properties. The column's value is automatically mapped to the Text property. For more details about Data Source see the Common Properties article for more details on common data source properties. Display Field is the value of what text will be displayed. Value Field selection from the new Data Source needs to match the value that is in the cell. Editor Type - Hyperlink It will show the value in the field as a Hyperlink. The Hyperlink editor type is based on the Hyperlink block. See the Hyperlink article for more details on how to configure the Hyperlink block. The column's value is automatically mapped to the Text property. Editor Type - Indicator The value in the field has to be a valid color format. The indicator editor type is based on the Indicator block with less configurable options. See the Indicator article for more details on how to configure the Indicator block. The column's value is automatically mapped to the Text property. Open in New Tab/Window Tick for the URL to open in a new tab/window, instead of redirecting the current tab. This applies when the Editor Type is set to 'Hyperlink'. Note We recommend opening XMPro URLs in the same tab/window - as users may experience degraded performance when a large number of XMPro tabs are opened. Column Reordering Reordering columns is possible at runtime and is enabled by default on every Grid. Users can change the order by dragging one column to another position. Column Resizing Resizing columns is possible at runtime and is enabled by default on every Grid. Users can resize the columns by dragging the edge of the column. Action Common Properties Properties that are common to most Blocks include: Navigate to and Show Confirmation Dialog; See the Common Properties article for more details on common action properties. Override Grid Values When saving grid rows to a Data Source, you may want to override some values. For instance, if you want to update a column with the current date and time, or replace a column with a Parameter or Variable. To do this follow these instructions: On the Data Grid, enable the Allow Updating property and set the Edit Mode to Batch With External Save. Then, under the Action accordion in the Block Properties of a Button or other Block, click the button with the gear icon of the corresponding Data Source to the Data Grid. Press the + button to the right of Override Values, select the column to override, and press Add. In the Value column of Override Values, choose a static or dynamic value. Press Apply on the Update Data Source page, and Save the App Page."
  },
  "docs/blocks-toolbox/basic/date-selector.html": {
    "href": "docs/blocks-toolbox/basic/date-selector.html",
    "title": "Date Selector | XMPro",
    "summary": "Date Selector The Date Selector is an input field that allows users to select a date. When they open the drop-down arrow, the Date Selector shows a calendar format where they can easily and visually see the dates of the year. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Date Selector Properties Appearance Common Properties You can change the visibility, styling mode, placeholder, and tooltip. See the Common Properties article for more details on common appearance properties. Adaptive Behavior For some screen sizes, the date picker may not fit across the screen. If enabled, this allows the box to be displayed in a different format, for example, the date box is displayed without the large analog clock and uses a digital clock instead. This is useful for smaller devices such as mobile or IPads. Behavior Common Properties The disabled property is common to most Blocks; See the Common Properties article for more details on common behavior properties. Type This specified the format of the date. The options are date, time, or date and time. Picker Type This specifies the way the calendar or clock is displayed to the user. Options for this include default, calendar, list, rollers, or native. List Interval Specifies the intervals between the date or time options on the list. Show Analog Clock Specifies whether or not the analog clock is displayed. Value Common Properties The value property is common to most Blocks; See the Common Properties article for more details on common value properties. Action Common Properties Properties that are common to most Blocks include: Navigate To and Show Confirmation Dialog; See the Common Properties article for more details on common action properties."
  },
  "docs/blocks-toolbox/basic/dropdown-grid.html": {
    "href": "docs/blocks-toolbox/basic/dropdown-grid.html",
    "title": "Dropdown Grid | XMPro",
    "summary": "Dropdown Grid A Dropdown Grid is a dropdown that displays a grid of data to the user. The Data Grid can be connected to a Data Source to retrieve and display specific values. This is useful for dynamically creating the dropdown from items that already exist and may change over time. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Dropdown Grid Properties Appearance Common Properties You can change the visibility, styling mode, placeholder, tooltip, and the visibility of the clear button; See the Common Properties article for more details on common appearance properties. Grid The visibility of the borders, column lines, row lines, and headers of the grid can be specified. By default, the borders, column lines, row lines, and headers are set to true. Options for the grid include showing or hiding the borders, headers, column lines, or column rows. For details on these common grid properties, see the Data Grid article. Behavior Common Properties The Dropdown Grid behavior includes changing the read-only, disabled. See the Common Properties article for more details on common behavior properties. Allow Paging This specifies whether the content on the grid is separated into pages. Enable Column Filtering Allows the user to filter for a specific column in the list. Enable Row Filtering Allows the user to filter for a specific row in the list. Page Size Specifies the number of records that are displayed to the user for each page. This will only work if allow paging is set to true. Allow Grouping This option gives the user the ability to group records together. Store User Selection When enabled, your selection at runtime is saved in your browser's local data, so that it is remembered when the page reloads. This includes re-opening the App and returning from a drill-down. Value Common Properties The value property is common to most Blocks; See the Common Properties article for more details on common value properties. Data Source Common Properties Properties that are common to most Blocks include: filter, sort, show # of results, and skip # of results; See the Common Properties article for more details on common Data Source properties. The Data Source property is required for the Dropdown Grid. Data Display Expression The expression is a user-friendly name for what the user can see. For example, the text that is showing in one of the rows of the dropdown. The Display Expression property is required for the Dropdown Grid. Value Expression This is the actual value stored in the background of the application in the code. For example, instead of true or false, it would be 0 or 1. The Value Expression property is required for the Dropdown Grid. Columns Order The order allows you to specify the format for the columns. This includes the visibility of the columns, the alignment, the captions, or the width. Action Common Properties Properties that are common to most Blocks include: Navigate To and Show Confirmation Dialog; See the Common Properties article for more details on common action properties."
  },
  "docs/blocks-toolbox/basic/embedded-page.html": {
    "href": "docs/blocks-toolbox/basic/embedded-page.html",
    "title": "Embedded Page | XMPro",
    "summary": "Embedded Page Embedded Pages allow the user to see a preview of a page for another website. This is useful if you want the user to visually see content from another website without linking to it and without them leaving the application. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Embedded Page Properties Appearance Common Properties You can change the visibility of the embedded page, which is common to most blocks; See the Common Properties article for more details on common appearance properties. Behavior Enable Scrollbars When selected, the scroll bars will be visible to the user. Deselecting these will disable the scrollbars. URL This is the URL of the web page to display within the embedded page block. This is the most important property of the Embedded Page block. Usage Notes The embedded page block is used to display an external web page on your app page. This can be used to display relevant instructions or information from external or internal sites without needing to navigate away from the application. Links within the embedded web page will navigate the embedded page itself, not the browser. This means that the user won't have the normal browser methods of going back. Therefore, the embedded page should be carefully selected to contain all of the necessary information and prevent scenarios where your users navigate away and cannot return. Tip When selecting a page to embed, consider whether it contains all the necessary information and whether links within that page might lead users to navigate away from the content they need. Additional Resources For more information on how to use Embedded Pages, you can refer to the following resources: How To Use Embedded Pages - XMPRO App Designer Toolbox (Video)"
  },
  "docs/blocks-toolbox/basic/file-library.html": {
    "href": "docs/blocks-toolbox/basic/file-library.html",
    "title": "File Library | XMPro",
    "summary": "File Library The File Library Block allows you to upload and store multiple files in your App. Files can be uploaded, downloaded, or deleted. This can be useful if you want to share certain files with users who have access to your App. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Searching for files in the File Library To search for files in the File Library Block, enter the file name of the file you would like to search for. The File Library Block will automatically filter the files and show you the search results. File Library Properties Appearance Common Properties You can change the visibility of the File Library, which is common to most blocks; See the Common Properties article for more details on common appearance properties. Behavior Show All Users Files This allows you to toggle between seeing only the files that you have uploaded, or the files that other users that have access to the App have uploaded. Allowed File Extensions This allows you to specify the types of files that are allowed to be uploaded. If left blank, any file type can be uploaded. If a file extension is listed, (for example, a .png file), the File Library Block will not allow you to upload any other file except those with a .png extension. You can add file library extensions in the following way: Max File Size (MB) This determines the maximum file size that can be uploaded. If you attempt to upload a file that exceeds the max size, it will not be uploaded. Allow Delete This specifies if each file can be deleted or not. Each file has a checkbox next to it that allows you to delete the file. When Allow Delete is set to false, the checkboxes next to the files are no longer visible, so files cannot be selected and deleted. Allow Upload This specifies if the upload icon in the top-right of the File Library block is visible or not. If visible, the user can click on the icon to upload files. If not visible, they will be unable to upload files."
  },
  "docs/blocks-toolbox/basic/file-uploader.html": {
    "href": "docs/blocks-toolbox/basic/file-uploader.html",
    "title": "File Uploader | XMPro",
    "summary": "File Uploader A File uploader is a Block that allows the user to upload files in an application. The File can be uploaded, downloaded, or deleted. This can be useful if you want to share certain files with users who have access to your App. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. File Uploader Properties Appearance Common Properties Properties that are common to most Blocks include visibility, styling mode, tooltip, and icon; See the Common Properties article for more details on common appearance properties. Label The text that shows up next to the button. Select Button Text The text that shows on top of the Button. Upload Failed Message The text of the message that is shown to the user when a file upload fails. Behaviour Common Properties The disabled property is common to most Blocks; See the Common Properties article for more details on common behavior properties. Allowed File Extensions This allows you to specify the types of files that are allowed to be uploaded. If left blank, any file type can be uploaded. If a file extension is listed, (for example, a .png file), the File Uploader will not allow you to upload any other file except those with a .png extension. You can add file type extensions in the following way: Max File Size (MB) This determines the maximum file size that can be uploaded. If you attempt to upload a file that exceeds the max size, it will not be uploaded. Multiple Upload This allows you to upload multiple files. All selected files are zipped and then uploaded to the application. File Name Prefix This option is available when Multiple Upload is enabled. It allows you to add a prefix to the zip file created by Multiple Upload. Allow Delete This allows the user to delete the uploaded file. Value Common Properties The value property is common to most Blocks; See the Common Properties article for more details on common value properties. Validation Common Properties Properties that are common to most Blocks include: groups to validate; See the Common Properties article for more details on common validation properties. Action Common Properties Properties that are common to most Blocks include: navigate to and show confirmation dialog; See the Common Properties article for more details on common action properties."
  },
  "docs/blocks-toolbox/basic/html-editor.html": {
    "href": "docs/blocks-toolbox/basic/html-editor.html",
    "title": "Html Editor | XMPro",
    "summary": "Html Editor The HTML Editor allows the user to create notes and style them. Styling includes changing the font, size, font-weight, or heading style of the text. The user can also add bullet points, numbered lists, images, quotes, links to websites, and more. This is useful if you would like the user to have an area where they can write and save notes. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. HTML Editor Properties Appearance Common Properties You can change the visibility of the HTML Editor; See the Common Properties article for more details on common appearance properties. Toolbar This specifies if the toolbar at the top can be visible. Behavior Common Properties The behavior includes changing the HTML Editor to be read-only or disabled. See the Common Properties article for more details on common behavior properties. Value Common Properties The value property is common to most Blocks; See the Common Properties article for more details on common value properties. The HTML Editor accepts any sequence of characters as a value and anything else that is entered such as images. Action Common Properties Properties that are common to most Blocks include: Navigate To and Show Confirmation Dialog; See the Common Properties article for more details on common action properties."
  },
  "docs/blocks-toolbox/basic/image.html": {
    "href": "docs/blocks-toolbox/basic/image.html",
    "title": "Image | XMPro",
    "summary": "Image The Image Block allows you to display a specific image to the user. This is useful if you would like to add visuals to the page to style the App, or to present important information to the user in a way that will stand out and attract attention - guiding your visitor's line of sight. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Important Strike a balance between style and performance. Keep in mind that the larger the file size and the more images added to a single App Page, the longer the loading time. This effect will be felt during both design (open and save of an App) and run time. When in doubt, use fewer images and aim for a file size of 200kb or less. Image Properties Appearance Common Properties The visibility property is common to most Blocks; See the Common Properties article for more details on common appearance properties. Image Source Choose whether the image is stored in App Files (default) or Embedded. The App Files option, added in v4.4.17, is the recommended image source - embedded files bloat the app size and cannot be copied/downloaded/reused. Image You can select the Image you want to be displayed. The following image file types are supported: BMP, GIF, JPEG, PNG, SVG, and WEBP. Behavior The Image block doesn't have specific behavior properties beyond the common ones. Action Common Properties Properties that are common to most Blocks include: Navigate To and Show Confirmation Dialog; See the Common Properties article for more details on common action properties. Additional Information SVG file types are supported for all image properties, which was added in v4.1.13. This allows for scalable vector graphics that maintain quality at any size. In v4.4.17, a new \"Image Source\" property was added to the Image Block, allowing images to be stored in App Files. This enhancement allows images to be shared across multiple blocks and improves storage and retrieval efficiency. Previously, images could only be embedded within individual blocks."
  },
  "docs/blocks-toolbox/basic/indicator.html": {
    "href": "docs/blocks-toolbox/basic/indicator.html",
    "title": "Indicator | XMPro",
    "summary": "Indicator The Indicator Block shows a point at a particular location on the page and allows you to indicate something important in a specific area. It is useful for attracting the user's attention to a particular point or spot on the page. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Indicator Properties Appearance Common Properties The visibility property is common to most Blocks; See the Common Properties article for more details on common appearance properties. Text This is the text that is displayed inside the Indicator. Mode The mode indicates the shape of the Indicator. Size This specifies the size of the Indicator. Color This species the color of the Indicator. X-Axis and Y-Axis This specifies where the Indicator is positioned along an X-axis and Y-axis. Label The visibility of the label can be set to never, on hover, or always. The position of the label can be set to either top, bottom, left, or right. The color of the text and the background can be changed. The padding determines how much spacing shows between the text and the edge of the box. The border radius option specifies the outer edge and corners of the block around the label. Action Common Properties Properties that are common to most Blocks include: navigate to and show confirmation dialog; See the Common Properties article for more details on common action properties."
  },
  "docs/blocks-toolbox/basic/list.html": {
    "href": "docs/blocks-toolbox/basic/list.html",
    "title": "List | XMPro",
    "summary": "List A List is a UI component that displays a collection of items in the form of a list. The List is scrollable if there are too many items to fit in its container, or it can also be separated into pages. A List can be connected to a Data Source to retrieve and display specific values. Lists can be useful if you want to display a list of items to the user. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Tip It is recommended that you read the article listed below to improve your understanding of Data Sources. How to Create and Manage Data Sources List Properties Appearance Common Properties Properties that are common to most Blocks include visible and tooltip; See the Common Properties article for more details on common appearance properties. Enable Paging The default option is to show all the results. The user can specify how many items should be displayed per page and pages will be displayed under the List. Behavior Common Properties The disabled property is common to most Blocks; See the Common Properties article for more details on common behavior properties. Search Enabled It will add a search bar where the user can search the items in the List. Value Common Properties This option is used to select the default value and must match a value from the Data Source. See the Common Properties article for more details on common value properties. Data Source ‌Data sources can be Static or Dynamic. Static values have to be entered manually while Dynamic will get the value from the provided Data Source. See the Common Properties article for more details on common Data Source properties. The Data Source property is required for the List. Static Items If a Dynamic Data Source is not used, you can enter key dates to display manually under the Data section. Dynamic Data Source This option allows you to connect the control to a specific Data Source such as a database to pull data dynamically. This will give you additional options to sort, filter, show, or skip certain records. Data Display Expression The expression is a user-friendly name for what the user can see. For example, the text that is showing in the List. The Display Expression property is required for the List. Grouping This is only available if the Data Source is Dynamic. Here we have the option to set how the items in the list will be grouped. If Grouping is enabled, the Group By Expression property is required for the List. Action Common Properties Properties that are common to most Blocks include: Navigate to and Show Confirmation Dialog; See the Common Properties article for more details on common action properties."
  },
  "docs/blocks-toolbox/basic/lookup.html": {
    "href": "docs/blocks-toolbox/basic/lookup.html",
    "title": "Lookup | XMPro",
    "summary": "Lookup The Lookup is a UI component that allows a user to search for an item in a collection shown in a drop-down menu. This is useful when there are many options or items to select from and it may be hard for the user to find one particular item. This UI control allows the user to navigate to the item faster. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Lookup Properties Appearance Common Properties Properties that are common to most Blocks include visible, styling mode, placeholder, tooltip and show clear button; See the Common Properties article for more details on common appearance properties. Styling Mode Placeholder and Show Clear Button Placeholder is the text that will be displayed before a value is selected. Show Clear Button will add a button to clear the selected item. Dropdown Title Title of the Lookup when it's open for selection. Behavior Common Properties The disabled property is common to most Blocks; See the Common Properties article for more details on common behavior properties. Enable Search It will add a search bar where the user can search the items in the Lookup. Value Common Properties This option is used to select the default value and must match a value from the Data Source. See the Common Properties article for more details on common value properties. Data Source ‌Data sources can be Static or Dynamic. Static values have to be entered manually while Dynamic will get the value from the provided Data Source. The Data Source property is required for the Lookup Block. Static Items If a dynamic data source is not used, you can enter key dates to display manually under the Data section. Dynamic Data Source This option allows you to connect the control to a specific data source such as a database to pull data dynamically. This will give you additional options to sort, filter, show, or skip certain records. See the Common Properties article for more details on common data source properties. Data This is only available if the Data Source is Dynamic. Here we have the option to set the values of the buttons as well as what text will be displayed. ‌ See the Common Properties article for more details on common data properties. Display Expression The expression is a user-friendly name for what the user can see. For example, the text that is displayed to the user. The Display Expression property is required for the Lookup Block. Value Expression This is the actual value stored in the background of the application in the code. For example, instead of true or false, it would be 0 or 1. The Value Expression property is required for the Lookup Block. Grouping This is only available if the Data Source is Dynamic. Here we have the option to set how the items in the Lookup will be grouped. If grouping is enabled, the Group By Expression property is required for the Lookup Block. Action Common Properties Properties that are common to most Blocks include: Navigate To and Show Confirmation Dialog; See the Common Properties article for more details on common action properties."
  },
  "docs/blocks-toolbox/basic/number-selector.html": {
    "href": "docs/blocks-toolbox/basic/number-selector.html",
    "title": "Number Selector | XMPro",
    "summary": "Number Selector In scenarios where the user should select a number, the Number Selector is an input field that gives the user the option to do so. When the user enters a value, the field automatically makes sure the value entered is a number. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Important Number Selector automatically converts the entered value into a scientific notation if it is greater than 21 digits for an integer value and greater than 6 digits for a decimal value. Number Selector Properties Appearance Common Properties The Number Selector has the option to change its visibility, styling mode, set a placeholder, show tooltips, and show clear button. See the Common Properties article for more details on common appearance properties. An option that is specific to Number Selector is Show Spin Buttons. Style Show Clear Button The clear button will appear on the right side. Clicking the button will remove the value from the control. Show Spin Buttons The up and down buttons will be shown on the right side of the control which the user can use to increase or decrease the current value. Behavior Common Properties The Number Selector has the option to set the option for read-only and disabled. See the Common Properties article for more details on common behavior properties. Options that are specific to Number Selector are min, max, format. Min and Max Affects the minimum and maximum values that Number Selector can accept. Format This will format the value to the format that was specified. In the example below, it will show it as a currency. Read Only and Disabled This affects whether or not the Number Selector value can be changed and if it can only be read and not manipulated. Value The Number Selector accepts numbers only. The value can be static, dynamic, or an expression. See the Common Properties article for more details on common value properties. Validation Common Properties Properties that are common to most Blocks include: validation Group, required, pattern, and message; See the Common Properties article for more details on common validation properties. Action Common Properties Properties that are common to most Blocks include: Navigate To and Show Confirmation Dialog; See the Common Properties article for more details on common action properties."
  },
  "docs/blocks-toolbox/basic/radio-buttons.html": {
    "href": "docs/blocks-toolbox/basic/radio-buttons.html",
    "title": "Radio Buttons | XMPro",
    "summary": "Radio Buttons The RadioGroup is a UI component that contains a list of options for users to choose from. This is useful if you only want the user to select a single item out of the options in the list. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Radio Buttons Properties Appearance Common Properties The Radio Button has the option to change its visibility. See the Common Properties article for more details on common appearance properties. An option that is specific to Radio Button includes the ability to change its orientation. Orientation The default value is Vertical which means Radio Buttons will be displayed top to bottom. By changing to Horizontal, the Radio Buttons will be displayed left to right. Behavior Common Properties The disabled property is common to most Blocks; ‌See the Common Properties article for more details on common behavior properties. Disable This affects the Radio Buttons group to be shown as read-only. Value Common Properties This option is used to select the default value and must match a value from the Data Source. See the Common Properties article for more details on common value properties. Data Source Common Properties ‌A Data Source can be Static or Dynamic. Static values have to be entered manually while Dynamic will get the value from the provided Data Source. See the Common Properties article for more details on common Data Source properties. The Data Source property is required for Radio Buttons. Static Items If a dynamic Data Source is not used, you can enter key dates to display manually under the Data section. Dynamic Data Source This option allows you to connect the control to a specific Data Source such as a database to pull data dynamically. This will give you additional options to sort, filter, show, or skip certain records. Data Display Expression The expression is a user-friendly name for what the user can see. For example, the text that is displayed to the user. The Display Expression property is required for Radio Buttons. Value Expression This is the actual value stored in the background of the application in the code. For example, instead of true or false, it would be 0 or 1. The Value Expression property is required for Radio Buttons. Action Common Properties Properties that are common to most Blocks include: Navigate To and Show Confirmation Dialog; See the Common Properties article for more details on common action properties."
  },
  "docs/blocks-toolbox/basic/range-selector.html": {
    "href": "docs/blocks-toolbox/basic/range-selector.html",
    "title": "Range Slider | XMPro",
    "summary": "Range Slider A Range Slider is an input field that can be used to select a numeric value within a given range. This is useful when there is a limited number of options the user should select from and eases the validation for the user's input. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Range Slider Properties Appearance Common Properties The Range Slider has the option to change its visibility and to show tooltips. See the Common Properties article for more details on common appearance properties. Options that are specific to Range Sliders include the ability to change the mode, range, or labels. The appearance of the labels such as the positioning and format of the labels can also be configured. Mode In point mode, only the single point that the user selected will be visible. When in range mode, the Range Slider allows you to specify whether or not to show the range line between the two selected values. Show Range When the show range option is set to true, the range that is selected will be highlighted. Show Labels Show labels will add a value on both sides of the slider so the user will know the numeric range. Label Position This gives you the option to add the labels either above the slider or underneath the slider. Label Format The label format option allows you to choose what type of numeric range the slider is. Some of the options that are available include but are not limited to currency, decimal, fixed point, seconds, minutes, days, or hours. Behavior Common Properties Properties that are common to most Blocks include: read-only and disabled. See the Common Properties article for more details on common behavior properties. Min and Max This affects the minimum and maximum values for where the range starts. Step This affects the intervals that are allowed between the values that can be chosen. For example, if the step interval is set to 50, and the user selects 45, the slider value will automatically be set to 50. Value If the point option is selected as the mode, there will only be one value. However, if the range option is selected as the mode, you can choose a start value or an end value. Point Value This determines what the selected value is when the mode is set to the point option. Range Start and End value This determines the range that is selected from the start value to the end value. This is available when the range option is selected for the mode. Action Common Properties Properties that are common to most Blocks include: Navigate To and Show Confirmation Dialog; See the Common Properties article for more details on common action properties."
  },
  "docs/blocks-toolbox/basic/select-box.html": {
    "href": "docs/blocks-toolbox/basic/select-box.html",
    "title": "Select Box | XMPro",
    "summary": "Select Box The Select Box component is an editor that allows a user to select an item from a drop-down list. This is useful when there are many options or items to select from and it may be hard for the user to find one particular item. This UI control allows the user to navigate to the item faster. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Select Box Properties Appearance Common Properties A Select Box has the option to change its visibility, styling mode, placeholder, tooltip and show clear button. See the Common Properties article for more details on common appearance properties. Styling Mode Placeholder and Show Clear Button The placeholder is the text that will be displayed before a value is selected. The show clear button will add a button to clear the selected item. Behavior Common Properties The select box has the option to be disabled and read-only. See the Common Properties article for more details on common behavior properties. Enable Search The user can search the items in the select box. Store User Selection When enabled, your selection at runtime is saved in your browser's local data, so that it is remembered when the page reloads. This includes re-opening the App and returning from a drill-down. Value Common Properties This option is used to select the default value and must match a value from the Data Source. See the Common Properties article for more details on common value properties. Data Source Common Properties ‌The Data source can be Static or Dynamic. Static values have to be entered manually while Dynamic will get the value from the provided Data Source. See the Common Properties article for more details on common Data Source properties. The Data Source property is required for the Select Box. Static Items If a Dynamic Data Source is not used, you can enter key dates to display manually under the Data section. Dynamic Data Source This option allows you to connect the control to a specific Data Source such as a database to pull data dynamically. This will give you additional options to sort, filter, show, or skip certain records. Data This is only available if the Data Source is Dynamic. Here we have the option to set the values of the buttons as well as what text will be displayed. ‌ See the Common Properties article for more details on common data properties. Display Expression The expression is a user-friendly name for what the user can see. For example, the text that is displayed to the user. The Display Expression property is required for the Select Box. Value Expression This is the actual value stored in the background of the application in the code. For example, instead of true or false, it would be 0 or 1. The Value Expression property is required for the Select Box. Grouping This is only available if the Data Source is Dynamic. Here we have the option to set how the items in the select box will be grouped. If Grouping is enabled, the Group By Expressions property is required for the Select Box. Action Common Properties Properties that are common to most Blocks include: Navigate To and Show Confirmation Dialog; See the Common Properties article for more details on common action properties."
  },
  "docs/blocks-toolbox/basic/switch.html": {
    "href": "docs/blocks-toolbox/basic/switch.html",
    "title": "Switch | XMPro",
    "summary": "Switch The Switch is a UI component that can be in two states: \"On\" and \"Off\". This is useful to show the end-user if something is turned on or running. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Switch Properties Appearance Common Properties The switch has the option to change its visibility and show tooltips. See the Common Properties article for more details on common appearance properties. An option that is specific to the Switch is to change the state between ON and OFF. Switch On Text and Switch off Text Ability to change the Switch state text. Default values are ON and OFF. Behavior Common Properties The Switch can be read-only and disabled. See the Common Properties article for more details on common behavior properties. Value Common Properties The value property is common to most Blocks; See the Common Properties article for more details on common value properties. Action Common Properties Properties that are common to most Blocks include: Navigate To and Show Confirmation Dialog; See the Common Properties article for more details on common action properties."
  },
  "docs/blocks-toolbox/basic/tags.html": {
    "href": "docs/blocks-toolbox/basic/tags.html",
    "title": "Tags | XMPro",
    "summary": "Tags Tags can be labels used to group certain contents or topics of the website together. A TagBox is a field that allows the user to select multiple Tags (such as items or categories) from a drop-down menu. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Tags Properties Appearance Common Properties Tags have the option to change their visibility, show tooltips, set placeholder, and change the styling. See the Common Properties article for more details on common appearance properties. Options that are specific to Tags are Multiline, Show Clear Button, Show Dropdown Button, Show Selection Controls. Styling Mode Placeholder Tags, unlike some other controls, have default placeholder text. The default value is \"Select\". Show Clear Button The clear button will appear on the right side. Clicking the button will remove all the Tags from the control. Show Dropdown Button Show Selection Controls Multiline Behavior Common Properties Tags have the option to be disabled and read-only. See the Common Properties article for more details on common behavior properties. Options that are specific to Tags are Enable Search and Apply Value Mode. Readonly Disabled Enable Search Will allow the user to type in box and results will be updated. Apply Value Mode The default option is Use Button which will show the OK and Cancel buttons. Instantly it will add the tag when it's selected. Value Common Properties The value property is common to most Blocks; See the Common Properties article for more details on common value properties. The value should be an array of strings. For example, if we put the following in the Value field [\"Sydney\", \"New York\", \"London\"] the results will be the following. Data Source Common Properties ‌A Data Source allows Dynamic Source only. The Dynamic Source will get the value from the provided Data Source. See the Common Properties article for more details on common Data Source properties. The Data Source property is required for Tags. Data Display Expression The expression is a user-friendly name for what the user can see. For example, the text that is showing in one of the rows of the dropdown. The Display Expression property is required for Tags. Value Expression This is the actual value stored in the background of the application in the code. For example, instead of true or false, it would be 0 or 1. The Value Expression property is required for Tags. Action Common Properties Properties that are common to most Blocks include: Navigate To and Show Confirmation Dialog; See the Common Properties article for more details on common action properties."
  },
  "docs/blocks-toolbox/basic/text-area.html": {
    "href": "docs/blocks-toolbox/basic/text-area.html",
    "title": "Text Area | XMPro",
    "summary": "Text Area A Text Area is an input field that allows the user to input a large amount of text. It is an element of a form that is usually used for comments, descriptions, or any other input that requires multiple sentences to be written. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Text Area Properties Appearance Common Properties The Text Area has the option to change its visibility, styling mode, placeholder, and to show tooltips. See the Common Properties article for more details on common appearance properties. Options that are specific to Text Areas include the ability to change the min and max height of the input field. Max and Min Height This specifies the minimum and maximum height and the way in which the Text Area expands. Behavior Common Properties Common options for the behavior include read-only and disabled. See the Common Properties article for more details on common behavior properties. Max Length Max Length specifies how many characters are allowed in the input area. Spellcheck Spellcheck gives you the ability to enable if the text area is checked for spelling errors. Value Common Properties This specifies the starting value of the text area. If left blank, then the starting value of the text area will be empty. The text area can accept any sequence of letters, numbers, or symbols as a value. See the Common Properties article for more details on common value properties. Action Common Properties Properties that are common to most Blocks include: Navigate To and Show Confirmation Dialog; See the Common Properties article for more details on common action properties."
  },
  "docs/blocks-toolbox/basic/text.html": {
    "href": "docs/blocks-toolbox/basic/text.html",
    "title": "Text | XMPro",
    "summary": "Text A text is a control that allows you to type any text value or sequence of characters. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Text Properties Appearance Common Properties You can change the visibility and tooltip properties for a Text Block; See the Common Properties article for more details on common appearance properties. Style Select the styling option for the text. Text The text displayed on the canvas and at runtime. Behavior The Text block doesn't have specific behavior properties beyond the common ones. Value Common Properties The value property is common to most Blocks; See the Common Properties article for more details on common value properties. Action Common Properties Properties that are common to most Blocks include: Navigate To and Show Confirmation Dialog; See the Common Properties article for more details on common action properties. Additional Information Tooltip Support As mentioned in the v4.1.13 release notes, a tooltip can be added to the Text Block. This is useful to keep the text short and use a tooltip for longer descriptions. Text Wrapping In v4.1.0, text wrapping was improved for the Text control to better handle long text content."
  },
  "docs/blocks-toolbox/basic/textbox.html": {
    "href": "docs/blocks-toolbox/basic/textbox.html",
    "title": "Textbox | XMPro",
    "summary": "Textbox A Textbox is an input field that allows the user to input text. It is an element of a form that is usually used for inputs such as a name, or any other input that requires a small amount of text to be written. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Textbox Properties Appearance Common Properties You can change the visibility, styling mode, and tooltip, placeholder, and clear buttons for a Textbox. See the Common Properties article for more details on common appearance properties. Show Clear Buttons When selected, an additional cancel button can be seen on the side at the end of the Textbox. Behavior Common Properties Properties that are common to most Blocks include: read-only and disabled. See the Common Properties article for more details on common behavior properties. Mode The mode is the type of text that should be entered. The options to choose from are email, password, search, telephone number, text, or URL. Max Length Max Length specifies how many characters are allowed in the input area. Spellcheck Spellcheck gives you the ability to enable if the text box is checked for spelling errors. Masks A masked text box allows the user to enter a value in a specific pattern. The mask determines the input that needs to be entered, for example, a phone number pattern like +1 (X00) 000-0000. The mask character is the character that will show to mask the part that needs to be entered. The Mask Invalid Message is the message that is displayed when the entered text does not match the specified pattern. Value Common Properties A common property for Textbox is the Value. See the Common Properties article for more details on common value properties. Action Common Properties Properties that are common to most Blocks include: navigate to and show confirmation dialog; See the Common Properties article for more details on common action properties."
  },
  "docs/blocks-toolbox/basic/tree-grid.html": {
    "href": "docs/blocks-toolbox/basic/tree-grid.html",
    "title": "Tree Grid | XMPro",
    "summary": "Tree Grid Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Tree Grid Properties Appearance Common Properties You can change the visibility of the Tree Grid. See the Common Properties article for more details on common appearance properties. See the Data Grid article for details on other common grid appearance properties: Show Borders, Show Headers, Show Column Lines, Show Row Lines, and Enable Paging. Define how the scrollbar behaves: Virtual: Rows are loaded when they get into the viewport and removed once they leave it. Infinite: Each next page is loaded once the scrollbar reaches the end of its scale. Standard: The pager informs the main navigation and scrolling is available only if the rows do not fit. None: There is no scrollbar. Instead, the control grows to fit the number of rows. Behavior Common Properties You can disable the Tree Grid. See the Common Properties article for more details on common behavior properties. See the Data Grid article for details on other common behavior properties: Allow Selection, Allow Deleting, Allow Updating, Allow Search, Enable Column Filtering, Enable Row Filtering, and Auto-Adjust Column Widths. Tip When searching a Tree Grid, the search results include their parents: If your result includes a parent, the children will not be available to expand unless they also meet the search criteria. If your result includes a child, the parents are listed for navigation - even if they don't meet the search criteria. Allow Adding When it's enabled, there are two places where the user can add a new record: an Add button appears at the top of the Tree Grid and in the right-most column. The Add button in the top-right corner adds a new record at the root level of the Tree Grid. The Add option in the right-most column adds a new child record. For example, if you select 'Add' under 'Arthur', a new record is added as a child of the 'Arthur' record. Edit Mode The user can edit the data in several modes. The option is only available when Allow Updating is enabled. Mode Description Batch A user edits data in cells across multiple rows - indicated by a green border. Changes are not applied until the user clicks the Save button. Click Reset to revert the changes. Batch With External Save As with Batch mode, the user edits data in cells across multiple rows - indicated by a green border. However, the user must add a separate block (such as a Button) to apply the changes. Manually configure this button's Update Data Sources section in Block Properties: select a Data Source (such as the Tree Grid's Data Source), and the action(s) to be taken (update, refresh, or delete). Cell A user edits the data cell by cell. Changes are saved once a cell loses focus, or discarded if a user presses Esc. Row The user edits data row by row. When a user clicks the Edit button in the right-most column, the corresponding row becomes editable and Save and Cancel buttons appear in the right-most column. Click the Save button to apply the changes. Allow Drag & Drop When it's enabled, it will show the drag icon by default in front of the row. The user can change the location of the row by clicking the icon and dragging the row. A row can be dragged inside another row to nest it underneath. If this is disabled, reordering is not available. The Drag and Drop option is only available when the Edit mode is set to either 'Row' or 'Cell'. Show Drag Icons When it's enabled, it will show the drag icon in front of the row. The user can reorder the row by clicking the icon and dragging the row. If this is disabled, the user can click anywhere on the row and drag the row. Maximum Tree Depth The limit for the Maximum Tree Depth determines how many levels of nested records the user will be allowed to expand and view. For example, if there is no Maximum Tree Depth limit set, the user will be able to expand and view all nested records. If the Maximum Tree Depth limit is set to 2, the user can only expand records until the second level, and they will no longer be able to expand any further. The Maximum Tree Depth limit also applies to adding and updating rows. The option to add a new row is only available for records within the Max Tree Depth limit specified. Store User Selection When enabled, your selected row is saved at runtime in your browser's local storage, so that you return to the same row when the page reloads without having to re-navigate the tree. This includes re-opening the App and returning from a drill-down. Value Common Properties The value property is common to most Blocks. See the Common Properties article for more details on common value properties. If the value is set to the ID of a record in a Data Source, that record will be selected when the application is launched. Data Source Common Properties Properties that are common to most Blocks include data source, filter, sort, show # of results, and skip # of results. See the Common Properties article for more details on common Data Source properties. The Data Source property is required for the Tree Grid. Data Parent Id The Parent Id tells the component how the fields are connected to each other. The Parent Id refers to the Id of the parent record. For example, in the hierarchy of employees, multiple people could report to one manager, so their parent Ids would be the Id of the person they are listed underneath. If the parent Id of a record is set to null or 0, it will automatically be placed as a root or main parent element on the tree. The Parent Id property is required for the Tree Grid. Columns Common Properties See the Data Grid article for details on common column properties: Format, Currency, Date Time Format, Editor Type - Lookup, Editor Type - Hyperlink, and Editor Type - Indicator, and Open in New Tab/Window. Order The order allows you to specify the format for the columns. This includes the visibility of the columns, the alignment, the captions, or the width. Action Common Properties Properties that are common to most Blocks include: Navigate To and Show Confirmation Dialog; See the Common Properties article for more details on common action properties."
  },
  "docs/blocks-toolbox/basic/tree-list.html": {
    "href": "docs/blocks-toolbox/basic/tree-list.html",
    "title": "Tree List | XMPro",
    "summary": "Tree List The Tree List UI component is a tree-like representation of textual data. This component is useful when we want to display something that has a hierarchy. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Tree List Properties Appearance Common Properties Properties that are common to most Blocks include visible and tooltip. See the Common Properties article for more details on common appearance properties. Behavior Common Properties The disabled property is common to most Blocks; See the Common Properties article for more details on common behavior properties. Search Enabled A search bar will be shown on top of the list and the user can search the data. Value Common Properties The value property is common to most Blocks; See the Common Properties article for more details on common value properties. When an Id is entered into the value field, it detects it automatically. Data Source Common Properties Properties that are common to most Blocks include: filter, sort, show # of results, and skip # of results; See the Common Properties article for more details on common Data Source properties. The Data Source property is required for the Tree List. Data Display The Display property is required for the Tree List. Id The Id property is required for the Tree List. Parent Id Properties include the Parent Id so the component knows how the fields are connected to each other. The Parent Id refers to the Id of the parent record. For example, in the hierarchy of employees, multiple people could report to one manager, so their parent Ids would be the Id of the person they are listed underneath. If the parent Id of a record is set to null or 0, it will automatically be placed as a root or main parent element on the tree. See the Common Properties article for more details on common data properties. The Parent Id property is required for the Tree List. Action Common Properties Properties that are common to most Blocks include: Navigate To and Show Confirmation Dialog; See the Common Properties article for more details on common action properties."
  },
  "docs/blocks-toolbox/common-properties.html": {
    "href": "docs/blocks-toolbox/common-properties.html",
    "title": "Common Properties | XMPro",
    "summary": "Common Properties Many blocks in the XMPro App Designer share common properties that control their appearance, behavior, and data binding. This page describes these common properties that you'll encounter across multiple blocks. Basic Properties ID Description: A unique identifier for the block within the page. Usage: Used for referencing the block in expressions, scripts, and for data binding. Type: String Name Description: A human-readable name for the block. Usage: Used for identifying the block in the designer and for accessibility. Type: String Visible Description: Controls whether the block is visible on the page. Usage: Can be set to a static value or bound to an expression to dynamically show/hide the block. Type: Boolean Enabled Description: Controls whether the block is enabled or disabled. Usage: Can be set to a static value or bound to an expression to dynamically enable/disable the block. Type: Boolean Styling Properties Width Description: Controls the width of the block. Usage: Can be set to a specific pixel value, percentage, or auto. Type: String Height Description: Controls the height of the block. Usage: Can be set to a specific pixel value, percentage, or auto. Type: String Margin Description: Controls the external spacing around the block. Usage: Can be set for all sides or individually for top, right, bottom, and left. Type: String Padding Description: Controls the internal spacing within the block. Usage: Can be set for all sides or individually for top, right, bottom, and left. Type: String Background Color Description: Controls the background color of the block. Usage: Can be set to a color name, hex value, or transparent. Type: String Text Color Description: Controls the text color of the block. Usage: Can be set to a color name or hex value. Type: String Border Description: Controls the border of the block. Usage: Can set the border width, style, and color. Type: String Font Description: Controls the font properties of the block. Usage: Can set the font family, size, weight, and style. Type: String Data Binding Properties Data Source Description: Specifies the data source for the block. Usage: Connects the block to a data source defined in the page. Type: Object Data Field Description: Specifies the field from the data source to bind to the block. Usage: Used to display or edit a specific field from the data source. Type: String Value Description: The current value of the block. Usage: Can be set directly or bound to a data source field. Type: Varies based on block type Event Properties On Click Description: Action to perform when the block is clicked. Usage: Can be set to navigate to another page, update a data source, or execute a custom script. Type: Function On Change Description: Action to perform when the value of the block changes. Usage: Can be set to update a data source, validate input, or execute a custom script. Type: Function On Load Description: Action to perform when the block is loaded. Usage: Can be set to initialize the block, load data, or execute a custom script. Type: Function Accessibility Properties Tooltip Description: Text to display when hovering over the block. Usage: Provides additional information about the block's purpose or functionality. Type: String ARIA Label Description: Accessible label for screen readers. Usage: Improves accessibility for users with screen readers. Type: String Tab Index Description: Controls the tab order of the block. Usage: Determines the order in which blocks receive focus when tabbing through the page. Type: Number Conclusion These common properties provide a consistent way to control the appearance, behavior, and data binding of blocks in the XMPro App Designer. While not all blocks support all of these properties, understanding these common properties will help you work more efficiently with the various blocks in the toolbox."
  },
  "docs/blocks-toolbox/device-input/README.html": {
    "href": "docs/blocks-toolbox/device-input/README.html",
    "title": "Device Input Blocks | XMPro",
    "summary": "Device Input Blocks Device Input blocks allow users to interact with your XMPro applications using various device capabilities, such as cameras and location services. These blocks enable the capture of real-world data directly through the user's device. Available Device Input Blocks Location Capture - Captures the user's geographical location using the device's GPS or network-based location services Visual Media Capture - Captures photos or videos using the device's camera Best Practices for Using Device Input Blocks Request permissions appropriately: Device input blocks often require specific permissions from the user (e.g., camera access, location access). Ensure that your application requests these permissions at the appropriate time and provides clear explanations of why they are needed. Handle permission denials gracefully: Users may deny permission for your application to access certain device features. Your application should handle these situations gracefully and provide alternative ways for users to accomplish their tasks. Consider device capabilities: Not all devices have the same capabilities. Some may have multiple cameras, while others may have limited or no location services. Design your application to adapt to the available device capabilities. Optimize for performance: Capturing and processing device input can be resource-intensive. Optimize your application to minimize battery drain and ensure smooth performance. Provide feedback: Give users clear feedback when device input is being captured or processed. This helps users understand what's happening and can improve the overall user experience. Respect privacy: Be transparent about how device input data is used and stored. Only capture the data you need, and ensure that sensitive data is handled securely. Test on multiple devices: Test your application on a variety of devices to ensure that device input blocks work correctly across different hardware and software configurations. Examples Location-Based Asset Tracking Box (Asset Tracking Container) ├── Location Capture │ ├── Button (Capture Location) ├── Field (Asset ID) │ ├── Textbox ├── Field (Asset Name) │ ├── Textbox ├── Field (Asset Type) │ ├── Select Box ├── Field (Notes) │ ├── Text Area ├── Box (Buttons) │ ├── Button (Save) │ ├── Button (Cancel) Field Inspection with Photo Documentation Box (Inspection Container) ├── Field (Inspection ID) │ ├── Textbox ├── Field (Inspector) │ ├── Textbox ├── Field (Date) │ ├── Date Selector ├── Visual Media Capture │ ├── Button (Take Photo) │ ├── Image (Preview) ├── Field (Condition) │ ├── Select Box │ ├── Option (Good) │ ├── Option (Fair) │ ├── Option (Poor) ├── Field (Notes) │ ├── Text Area ├── Box (Buttons) │ ├── Button (Submit) │ ├── Button (Save Draft) Mobile Data Collection Box (Data Collection Container) ├── Location Capture │ ├── Button (Capture Location) │ ├── Map (Location Preview) ├── Visual Media Capture │ ├── Button (Take Photo) │ ├── Image (Preview) ├── Field (Sample ID) │ ├── Textbox ├── Field (Sample Type) │ ├── Select Box ├── Field (Measurements) │ ├── Number Selector ├── Field (Observations) │ ├── Text Area ├── Box (Buttons) │ ├── Button (Submit) │ ├── Button (Save Draft) By effectively using device input blocks, you can create applications that leverage the capabilities of modern devices to capture and process real-world data, enabling a wide range of use cases from field inspections to asset tracking and beyond."
  },
  "docs/blocks-toolbox/device-input/location-capture.html": {
    "href": "docs/blocks-toolbox/device-input/location-capture.html",
    "title": "Location Capture | XMPro",
    "summary": "Location Capture The Location Capture Block allows users to capture their current device location. This is useful when composing Applications such as a mobile inspection app and the user may be required to include the asset location when logging an issue. Location Capture Properties Appearance Common Properties The visibility property is common to most Blocks; See the Common Properties article for more details on common appearance properties. Capture Button Text The text displayed on the capture button. Behavior Common Properties The disabled property is common to most Blocks; See the Common Properties article for more details on common behavior properties. Value Latitude Value The Latitudinal value that is taken from the control such as when the capture button is clicked. Longitude Value The Longitudinal value that is taken from the control such as when the capture button is clicked. Validation Common Properties The Validation Group and Required properties are common to most Blocks; See the Common Properties article for more details on common validation properties. Required Message The text of the error message that is displayed to the user if no location data has been captured. Capture Failed Message The text of the error message that is displayed to the user if the location capture fails for any reason. The reason for the failure is appended to it, e.g. \"User denied the request for Geolocation\"."
  },
  "docs/blocks-toolbox/device-input/visual-media-capture.html": {
    "href": "docs/blocks-toolbox/device-input/visual-media-capture.html",
    "title": "Visual Media Capture | XMPro",
    "summary": "Visual Media Capture The Visual Media Capture Block enables users to capture a photo or a video or upload an existing media file. This is useful when composing Applications such as a mobile inspection app and the user may be required to include an image when logging an issue. Visual Media Capture Properties Appearance Common Properties The visibility property is common to most Blocks; See the Common Properties article for more details on common appearance properties. Capture Button Text The text displayed on the capture button. Behavior Common Properties The disabled property is common to most Blocks; See the Common Properties article for more details on common behavior properties. Allowed File Extensions This allows you to specify the types of files that can be uploaded. If left blank, any file type can be uploaded. If a file extension is listed, (for example, a .png file), the Visual Media Capture will not allow you to upload any other file except those with a .png extension. Max File Size This setting dictates the maximum allowable file size for uploads. If you attempt to upload a file that exceeds the maximum size, it will not be uploaded. Provider The supported external cloud storage providers are Azure Blob or Amazon S3. Use Variables Tick to use variables for the provider-related properties. Blob Connection String The blob connection string (applies to the Azure Blob Provider only). Blob Container Name The blob container name (applies to the Azure Blob Provider only). Access Key The Amazon S3 access key (applies to the Amazon S3 Provider only). Secret Key The Amazon S3 secret key (applies to the Amazon S3 Provider only). Bucket Name The Amazon S3 bucket name (applies to the Amazon S3 Provider only). Region The Amazon S3 region (applies to the Amazon S3 Provider only). Value Common Properties The value property is common to most Blocks; See the Common Properties article for more details on common value properties. We recommend using a dynamic value property so that when you upload media files, the URLs are bound to the value property, with enclosed brackets and comma-separated. You can utilize a stored proc to save the file URLs into a data source and view them by adding the URL as a hyperlink to a Data Grid. // Sample code to populate the file URLs to a SQL table CREATE PROCEDURE [dbo].[sp_MobileSpec_Update] ( @MobileId BIGINT, @MediaArray NVARCHAR(MAX), @UploadDate DATETIME2(7), @UploadUser NVARCHAR(150) ) AS BEGIN SET NOCOUNT ON IF (@MediaArray IS NOT NULL AND @MediaArray != '') BEGIN DECLARE @IDs TABLE (ID NVARCHAR(MAX)) -- Use NVARCHAR(MAX) to store URLs -- Populate @IDs table with the result of dbo.SplitString function INSERT INTO @IDs (ID) SELECT Value FROM dbo.SplitString(@MediaArray, ',') -- Iterate over each URL and insert into the table DECLARE @ID NVARCHAR(MAX) DECLARE cur CURSOR FOR SELECT ID FROM @IDs OPEN cur FETCH NEXT FROM cur INTO @ID WHILE @@FETCH_STATUS = 0 BEGIN DECLARE @LastSlashPosition INT = LEN(@ID) - CHARINDEX('/', REVERSE(@ID)) + 1 DECLARE @FileName NVARCHAR(MAX) = SUBSTRING(@ID, @LastSlashPosition + 1, LEN(@ID) - @LastSlashPosition) SET @FileName = SUBSTRING(@FileName, CHARINDEX('/', @FileName) + 1, LEN(@FileName)) -- Process each URL here INSERT INTO [MobileInspectionApp_AppFile] ([MobileInspectionAppId], [AppFileId], [UploadDate], [UploadUser], [URL]) VALUES (@MobileId, @FileName, @UploadDate, @UploadUser, @ID) FETCH NEXT FROM cur INTO @ID END CLOSE cur DEALLOCATE cur UPDATE [MobileInspectionAppTest] SET [AppFileId] = 'Files attached' WHERE [Id] = @MobileId END END Validation Common Properties The Validation Group and Required properties are common to most Blocks; See the Common Properties article for more details on common validation properties. Required Message The text of the error message that is displayed to the user when no media file has been captured or uploaded. Capture Failed Message The text of the error message that is displayed to the user if the media capture fails for any reason."
  },
  "docs/blocks-toolbox/index.html": {
    "href": "docs/blocks-toolbox/index.html",
    "title": "Blocks Toolbox | XMPro",
    "summary": "Blocks Toolbox The Blocks Toolbox provides a comprehensive set of UI components that you can use to build your XMPro applications. These blocks are organized into different categories based on their functionality. Block Categories Common Properties - Properties that are shared across multiple blocks Layout - Blocks for organizing and structuring your application layout Basic - Fundamental UI components for building applications Device Input - Blocks for capturing input from various devices AI - Artificial Intelligence components Actions - Blocks for performing actions and handling events Recommendations - Components for working with recommendations Visualizations - Blocks for data visualization Advanced - Advanced components for specialized use cases Widgets - Custom, reusable components Each block category contains multiple blocks, each with its own set of properties and functionality. Click on a category to explore the blocks it contains."
  },
  "docs/blocks-toolbox/layout/README.html": {
    "href": "docs/blocks-toolbox/layout/README.html",
    "title": "Layout Blocks | XMPro",
    "summary": "Layout Blocks Layout blocks are used to organize and structure the content of your XMPro applications. They provide the foundation for arranging other blocks in a visually appealing and functional way. Available Layout Blocks Accordion - Collapsible content panels for organizing information in a limited space Box & Data Repeater Box - Container blocks for grouping content and repeating content based on data Card & Content Card - Styled containers for presenting information in a card format Field & Fieldset - Blocks for organizing form fields and their labels Layout Grid - Grid-based layout system for creating responsive designs Menu - Navigation component for creating menus Scroll Box - Container with scrollbars for content that exceeds the available space Stacked Layout Horizontal & Vertical - Layouts for stacking content horizontally or vertically Tabs - Tabbed interface for organizing content into separate views Templated List - List component that uses templates to display data Toolbar - Container for organizing action buttons and controls Best Practices for Using Layout Blocks Start with the right layout structure: Choose the appropriate layout blocks based on the content you need to display and the user experience you want to create. Use nesting effectively: Layout blocks can be nested within each other to create complex layouts. Use this capability judiciously to avoid overly complex structures. Consider responsiveness: Use layout blocks that adapt well to different screen sizes, especially if your application will be used on mobile devices. Maintain consistency: Use similar layout patterns throughout your application to provide a consistent user experience. Use appropriate spacing: Pay attention to margins and padding to ensure that your content is well-spaced and easy to read. Group related content: Use layout blocks to group related content together, making it easier for users to understand the relationships between different pieces of information. Optimize for performance: Avoid excessive nesting of layout blocks, as this can impact performance, especially on mobile devices. Examples Basic Page Layout Layout Grid ├── Box (Header) ├── Box (Sidebar) │ ├── Menu ├── Box (Content) │ ├── Tabs │ │ ├── Tab 1 Content │ │ ├── Tab 2 Content ├── Box (Footer) Form Layout Box (Form Container) ├── Fieldset (Personal Information) │ ├── Field (Name) │ ├── Field (Email) │ ├── Field (Phone) ├── Fieldset (Address) │ ├── Field (Street) │ ├── Field (City) │ ├── Field (State) │ ├── Field (Zip) ├── Box (Buttons) │ ├── Button (Submit) │ ├── Button (Cancel) Dashboard Layout Layout Grid ├── Box (Header) ├── Box (Dashboard Content) │ ├── Card (Summary Metrics) │ ├── Card (Chart 1) │ ├── Card (Chart 2) │ ├── Card (Recent Activity) │ │ ├── Scroll Box │ │ │ ├── Data Repeater Box (Activity Items) By effectively using layout blocks, you can create well-organized, visually appealing, and functional applications that provide a great user experience."
  },
  "docs/blocks-toolbox/layout/accordion.html": {
    "href": "docs/blocks-toolbox/layout/accordion.html",
    "title": "Accordion | XMPro",
    "summary": "Accordion An Accordion is a group of Accordion Items that shows other Blocks and content within them. An Accordion Item can be expanded or collapsed and is therefore useful for grouping content together. They are also useful for hiding content into collapsed sections when there is a lot of content on the page. Adding a new Accordion Item To add a new Accordion Item, you can either click on an Accordion Item or the Accordion itself and click the plus button in the toolbar. Note See the Canvas article for more details on these controls. Cloning Accordions or Accordion Items You can clone both Accordion Items and the whole Accordion itself. To clone an Accordion Item, select an Accordion Item and click on the clone symbol in the top right-hand Block toolbar. This will create a copy of an existing Accordion Item inside the Accordion. Note See the Canvas article for more details on these controls. When the whole Accordion is cloned, two separate Accordions will be created. It will not create an Accordion inside the existing Accordion. Reordering Accordion Items Accordion Items can be reordered within the Accordion. To reorder an Accordion Item, click and drag the move button in the toolbar to place the Accordion Item somewhere else. Accordion Items cannot be dragged outside of the Accordion itself. However, Accordion Items can be dragged into other Accordions. Note See the Canvas article for more details on these controls. Adding Blocks to an Accordion Item To add Blocks inside the Accordion Items, Drag and drop other Blocks into the Accordion from the canvas toolbar or elsewhere in the canvas. Warning The height and width of the Accordion will be determined by the Blocks contained in it. To guarantee consistent behavior, items inside the Accordion must have a set pixel height and width in the style manager, not a percent or auto. Accordion Item Properties Appearance Common Properties The accordion has properties that are common to most Blocks: visibility and icon; See the Common Properties article for more details on common appearance properties. Title This is the title that shows at the top of the Accordion Item. Behavior Common Properties The disabled property is common to most Blocks; See the Common Properties article for more details on common behavior properties. Accordion Properties Appearance Common Properties The Accordion has properties that are common to most Blocks: visibility and tooltip. See the Common Properties article for more details on common appearance properties. Behavior Common Properties The disabled property is common to most Blocks; See the Common Properties article for more details on common behavior properties. Allow Expanding Multiple This determines if it is possible to expand multiple Accordion items at once. Collapsible By default, all Accordion Items can be collapsed except for one which must remain open. If the collapsible option is set to true, the user can collapse all Accordion Items. Selected Index This specifies which Accordion Item is open by default. The starting index is 0, which refers to the first item on the list. Data Source Common Properties The Accordion has properties that are common to most Blocks: filter, sort, show # of results, skip # of results, and show default row. See the Common Properties article for more details on common Data Source properties."
  },
  "docs/blocks-toolbox/layout/box-and-data-repeater-box.html": {
    "href": "docs/blocks-toolbox/layout/box-and-data-repeater-box.html",
    "title": "Box & Data Repeater Box | XMPro",
    "summary": "Box & Data Repeater Box A Box is a simple Block that allows you to add data or other elements inside it. This can be used as a container to store a group of other Blocks. A Data Repeater Box is a Box that allows you to repeat data multiple times, including data that is coming from a Data Source. If a text field is added to the Data Repeater Box, that text Block can be bound to a field coming from the Data Source. The Data Repeater Box will then repeat the data for that field for each record. Warning Take care with repeated elements that also have a Data Source, such as a Lookup, as their Data Source is fetched for every record returned by the Data Repeater's Data Source. A large result set may result in a timeout. You can use the Show # of Results under the Data Source property to limit the repetition of the blocks. Box Properties Appearance Common Properties The visibility property is common to most Blocks; See the Common Properties article for more details on common appearance properties. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Data Source Common Properties The Box has properties that are common to most Blocks: filter, sort, show # of results, and skip # of results; See the Common Properties article for more details on common data source properties. Data Repeater Box Properties Appearance Common Properties The visibility property is common to most Blocks; See the Common Properties article for more details on common appearance properties. Data Source Common Properties The data repeater box has properties that are common to most Blocks: filter, sort, show # of results, skip # of results, and show default row; See the Common Properties article for more details on common Data Source properties."
  },
  "docs/blocks-toolbox/layout/card-and-content-card.html": {
    "href": "docs/blocks-toolbox/layout/card-and-content-card.html",
    "title": "Card & Content Card | XMPro",
    "summary": "Card & Content Card A Card is a Block that allows you to configure certain metrics and values. This is useful if you would like to display data to the user in a particular format. The Content Card acts as a container or wrapper around other Block contents that you put inside it. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Card Properties Appearance Common Properties The visibility property is common to most Blocks; See the Common Properties article for more details on common appearance properties. Data Source Common Properties The Card has properties that are common to most Blocks: filter, sort, show # of results, skip # of results, and show default row; See the Common Properties article for more details on common Data Source properties. Content Card Properties Appearance Common Properties The visibility property is common to most Blocks; See the Common Properties article for more details on common appearance properties. Data Source Common Properties The content card has properties that are common to most Blocks: filter, sort, show # of results, skip # of results, and show default row; See the Common Properties article for more details on common Data Source properties."
  },
  "docs/blocks-toolbox/layout/field-and-fieldset.html": {
    "href": "docs/blocks-toolbox/layout/field-and-fieldset.html",
    "title": "Field & Fieldset | XMPro",
    "summary": "Field & Fieldset A Field is a Block where the user can enter details and is useful for collecting information on a Form. Fieldsets are Blocks that already contain Fields and can be used to group Fields within a larger Form. Fields or groups of Fields can be validated to ensure the user has entered the correct input. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Fieldset Properties Appearance Common Properties The visibility property is common to most Blocks; See the Common Properties article for more details on common appearance properties. Data Source Common Properties The fieldset has properties that are common to most Blocks: data source, filter, sort, show # of results, skip # of results, and show default row; See the Common Properties article for more details on common Data Source properties. Field Label Properties Appearance Common Properties The tooltip property is common to most Blocks; See the Common Properties article for more details on common appearance properties. Label The text of the field label can be specified."
  },
  "docs/blocks-toolbox/layout/horizontal-and-vertical-stacked-layouts.html": {
    "href": "docs/blocks-toolbox/layout/horizontal-and-vertical-stacked-layouts.html",
    "title": "Stacked Layout Horizontal & Vertical | XMPro",
    "summary": "Stacked Layout Horizontal & Vertical Horizontal Stacked Layouts separate a given area into columns. Vertical Stacked Layouts separate a given layout into rows. Columns or rows can be added or reduced to change the layout. This Block can be useful if the position of the page contents needs to be displayed right-to-left or top-to-bottom. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Add a Box to the Horizontal Layout To add a horizontal pane, select a pane and click on the plus symbol in the top-right Block toolbar. Note See the Canvas article for more details on these controls. Delete a Box in the Horizontal Layout To delete a horizontal pane, select a pane and click on the delete 'bin' symbol in the top-right block toolbar. Note See the Canvas article for more details on these controls. Add a Box to the Vertical Layout To add a Box to a Vertical Layout, select a pane and click on the plus symbol in the top-right block toolbar. Note See the Canvas article for more details on these controls. Delete a Box in the Vertical Layout To delete a vertical pane, select a pane and click on the delete 'bin' symbol in the top-right block toolbar. Note See the Canvas article for more details on these controls. Horizontal and Vertical Layout Properties Appearance Common Properties The visibility property is common to most Blocks; See the Common Properties article for more details on common appearance properties. Data Source Common Properties The Vertical and Horizontal Layouts have properties that are common to most Blocks: filter, sort, show # of results, skip # of results, and show default row; See the Common Properties article for more details on common Data Source properties."
  },
  "docs/blocks-toolbox/layout/layout-grid.html": {
    "href": "docs/blocks-toolbox/layout/layout-grid.html",
    "title": "Layout Grid | XMPro",
    "summary": "Layout Grid A Layout Grid is a Block that separates a given area into a grid. Columns and rows can be added or reduced to change the size of the grid. This Block can be useful if the style and position of the page contents need to be displayed in a grid-like format. Adding a new Row To add a new row, select the cell and click on the up symbol in the top-right hand block toolbar. This will then select the parent of the cell, which is the row. Then, click on the plus button in the toolbar. Note See the Canvas article for more details on these controls. Adding a new Column To add a new column, select any cell and click on the plus button in the top-right hand block toolbar. Note See the Canvas article for more details on these controls. Deleting a Row To delete a row, highlight a cell, click on the up symbol in the top-right hand block toolbar to take you to the parent row. Go back to the top-right hand toolbar and click on the delete bin symbol. Note See the Canvas article for more details on these controls. Deleting a Column To delete a column, highlight a cell, go to the top-right hand toolbar and click on the delete bin symbol. Note See the Canvas article for more details on these controls. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Row & Cell Properties Appearance Ratio You can specify the ratio of each column or row of the layout grid. This determines how much space that column takes relative to the other columns. To change the ratio of a column, select the cell, and go to block properties. To change the ratio of a row, select a cell, click on the up icon in the top-right block toolbar, then go to block properties. For example, the following images show the changes made to the ratio of the first column. Here, the first column has a ratio of 3. Here, the first column has a ratio of 5."
  },
  "docs/blocks-toolbox/layout/menu.html": {
    "href": "docs/blocks-toolbox/layout/menu.html",
    "title": "Menu | XMPro",
    "summary": "Menu A Menu is a list of options, commands, or pages presented to the user that they can select. This can be used for navigation purposes, such as separating the app or page into sections that the user can be directed to. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Adding Menu Items Once a Menu Block has been added to the screen, menu items can be added by using the Items property under Behavior in Block Properties. A separate area will open that will allow you to add items to your Menu. Here you can specify the text that will display on the link, and the page the app will navigate to when the user clicks on the link. To edit an item, click the item on the grid, and edit mode will be enabled. Menu items can be reordered and moved inside to create a submenu by clicking the left icon and dragging it into position. Menu Properties Appearance Common Properties The menu has properties that are common to most Blocks: visibility and tooltips, See the Common Properties article for more details on common appearance properties. Options that are specific to the Menu include collapse when space is limited, orientation, and submenu direction. Collapse When Space is Limited If the width of the menu is longer than the screen, it will collapse. Applies only if the orientation is \"horizontal\". Orientation Behavior Common Properties The disabled property is common to most Blocks; See the Common Properties article for more details on common behavior properties. Items The Items section is used to configure the menu items. If Menu is left unconfigured, by default it will display all the pages of the app. Hide Submenu on Mouse Leave Hide submenu on mouse leave is when we have a submenu and if this option is enabled it will collapse the menu, otherwise, a click is required to close the menu."
  },
  "docs/blocks-toolbox/layout/scroll-box.html": {
    "href": "docs/blocks-toolbox/layout/scroll-box.html",
    "title": "Scroll Box | XMPro",
    "summary": "Scroll Box The Scroll Box is a box that will create scrollbars if the content is larger than the box. In order for the scrollbar to work properly, the containing element must not have a display value of flex. This is useful if you have lots of content that cannot fit in one area of the webpage, but you would still like the user to view all the content. Scroll Box Properties Behavior Common Properties The disabled property is common to most Blocks; See the Common Properties article for more details on common behavior properties. Direction This determines the direction of the scrollbar. The scrollbar can be horizontal, vertical, or both. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Swipe to Scroll This setting enables the scrollbar when it is swiped with the finger, for example, on a touch screen device such as a phone or IPad."
  },
  "docs/blocks-toolbox/layout/tabs.html": {
    "href": "docs/blocks-toolbox/layout/tabs.html",
    "title": "Tabs | XMPro",
    "summary": "Tabs Tabs are Blocks that allow you to separate and view related items together. Tabs can be added or removed to the tab Block if needed. They are useful for creating sections that the user can switch between or grouping related groups into one area of the page. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Adding a Tab To add a new Tab, select the whole tab Block or an individual Tab and click on the plus symbol in the top-right hand Block toolbar. Note See the Canvas article for more details on these controls. Deleting a Tab To delete a Tab, select the whole Tab Block or an individual Tab and click on the bin symbol in the top-right hand Block toolbar. Note See the Canvas article for more details on these controls. Navigating Between Tabs You can navigate between the Tabs within the canvas itself. Click on each Tab to open the contents within that particular Tab. Reordering Tabs Tabs can be reordered by clicking and dragging them to change the order. When a Tab is highlighted, hold the move icon and drag the Tab to another location within the Block. Note See the Canvas article for more details on these controls. Tab Properties Appearance Common Properties Tabs have properties that are common to most Blocks: visibility and tooltip. See the Common Properties article for more details on common appearance properties. Selected Index The selected index determines which tab is open by default. The index starts from 0, which means that an index of 0 refers to the first Tab, an index of 1 refers to the second Tab, and so on. Behavior Common Properties The disabled property is common to most Blocks; See the Common Properties article for more details on common behavior properties. Hide Tab Navigation When enabled, a hidden icon appears to the left of the tab icon and name on the Canvas. The user will see the selected tab's content at runtime, but they cannot see the tabs or click on another tab. The navigation between tabs is controlled by the Selected Index property, e.g. by an expression. Note This is useful when a header or footer section is reused across multiple pages. For example, one page is used in three different drill-down scenarios. The top one-third differs based on the drill-down type and the bottom two-thirds is the same for all scenarios. Rather than cloning the page and maintaining all three, place the top one-third in a Tab Block - and set the Selected Index to a parameter passed from the previous page to control which tab's content is shown. Data Source Common Properties Tabs have properties that are common to most Blocks: filter, sort, show # of results, and skip # of results; See the Common Properties article for more details on common Data Source properties. Tab Item Properties Appearance Title and Icon Each Tab has a heading that labels that particular tab section. An icon can also be added for visual purposes."
  },
  "docs/blocks-toolbox/layout/templated-list.html": {
    "href": "docs/blocks-toolbox/layout/templated-list.html",
    "title": "Templated List | XMPro",
    "summary": "Templated List Templated Lists allow you to display a list of data to the users. This is useful for displaying information from a Data Source such as a database. It is also useful to group information so the user can see a list of items underneath their corresponding categories and expand and collapse them as needed. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. How to Add items to a Templated List Columns of data can be added to a Templated List on the canvas. To add an item, select one of the fields and click on the plus symbol to add a new field next to it. Fields can also be cloned by clicking on the clone button from the same top-right hand toolbar. Note See the Canvas article for more details on these controls. To bind a column to a particular field from the Data Source, highlight the text in one of the columns and modify the text property under Appearance in Block Properties How to Delete Templated List items To delete an item in the templated list, highlight one of the boxes and click on the delete button in the toolbar. Note See the Canvas article for more details on these controls. Templated List Properties Appearance Common Properties The templated list has properties that are common to most Blocks: visibility and tooltip. See the Common Properties article for more details on common appearance properties. Enable Paging When paging is enabled, the items in the list will be grouped into pages. Page Size You can specify the number of items that display on the page at any given time. Scrolling Mode If enable paging is off, only virtual, infinite and none scrolling mode options will be available. If enable paging is on, only the standard and none scrolling mode options will be available. Behavior Common Properties The disabled property is common to most Blocks; See the Common Properties article for more details on common behavior properties. Search Enabled The search enabled option shows a search bar at the top of the list. This lets the user search for a particular record. Data Source Common Properties If you bind a data source to the templated list, the text labels for each column will need to bind to the columns coming from the data source. Once this is configured, data from the database will display in the list. The templated list has properties that are common to most Blocks: filter, sort, show # of results, and skip # of results; See the Common Properties article for more details on common Data Source properties. The Data Source property is required for Templated Lists. Grouping Enable Grouping You can choose to group the items on the list by the columns. In the designer mode, when the enable grouping option is selected, a new group field will appear which will act as the area where the grouping headings will be displayed. If Grouping is Enabled, the Group By Expression property is required for Templated Lists. This will categorize related records together, which can be expanded and viewed when the application is launched. Allow Collapsing and Expand by Default Allow collapsing lets the user collapse the contents of the group so they are temporarily hidden. If this is set to false, the user will be unable to collapse the grouped content. Expand by default determines if the list items are collapsed or expanded by default. Action Common Properties The templated list has properties that are common to most Blocks: navigate to, update data sources, and show confirmation dialog; See the Common Properties article for more details on common action properties."
  },
  "docs/blocks-toolbox/layout/toolbar.html": {
    "href": "docs/blocks-toolbox/layout/toolbar.html",
    "title": "Toolbar | XMPro",
    "summary": "Toolbar The Toolbar is a UI component that can contain items that are used to manage screen content. Those items can be plain text or other blocks such as images and are useful for showing options to the user in a more visual form than a regular menu. Note Some images in this document may be missing and need to be migrated from the original GitBook documentation. Toolbar Properties Appearance Common Properties Toolbars have properties that are common to most Blocks: visibility and tooltip. See the Common Properties article for more details on common appearance properties. Behavior Common Properties The disabled property is common to most Blocks; See the Common Properties article for more details on common behavior properties. Toolbar Item Properties Adding Toolbar Items The Toolbar Block has 3 areas, before, center, and after. Adding new items can be done in two ways. The first one is to add it by clicking the plus sign when you have either the toolbar or tollbar item selected. This is will create a new item with default settings. Note See the Canvas article for more details on these controls. If you have the toolbar selected and add a new item it will be added to the before section which is the first one from the left. If you have the toolbar item selected and click the plus sign, the new item will be added to the same section as the selected item. Items can be moved by dragging and dropping them to the desired section. You can also add items by clicking the copy button when one of the items is selected. This will create a new item with the exact same settings as the selected item. Appearance Common Properties You can change the visibility, styling, add a tooltip and change the icon. See the Common Properties article for more details on common appearance properties. Text Tooltip items can have text next to the icon. Behavior Common Properties The disabled property is common to most Blocks; See the Common Properties article for more details on common behavior properties. Locate in overflow menu Auto - It will try to fit items as long as they are fully visible. Always - It will hide the item even if there is enough space to show it. Never - It will show the items at all times. Some items may overlap.\\ Validation Common Properties The groups to validate property is common to most Blocks; See the Common Properties article for more details on common validation properties. Action Common Properties The toolbar has properties that are common to most Blocks: navigate to and show confirmation dialog; See the Common Properties article for more details on common action properties."
  },
  "docs/blocks-toolbox/recommendations/README.html": {
    "href": "docs/blocks-toolbox/recommendations/README.html",
    "title": "Recommendations | XMPro",
    "summary": "Recommendations"
  },
  "docs/blocks-toolbox/recommendations/alert-action.html": {
    "href": "docs/blocks-toolbox/recommendations/alert-action.html",
    "title": "Alert Action | XMPro",
    "summary": "Alert Action Configure a button to perform one of the following actions on an open Recommendation Alert: Mark As Resolved Mark As False Positive Save Share Assign The button text - and in some cases the icon - are defaulted when the action property is selected, but the designer can override these properties. The action button will be disabled at run-time should the alert be archived. Fig 1: Alert Action Alert Action Properties Appearance Common Properties Properties that are common to most Blocks include visible, styling mode, tooltip, and icon; See the Common Properties article for more details on common appearance properties. Type The type of the button can be changed depending on its purpose. Options include danger, normal, success, and default. Text The text that shows on top of the Button. Behavior Common Properties Properties that are common to most Blocks include disabled; See the Common Properties article for more details on common behavior properties. Alert ID Supply an Alert Identifier on which the selected action will be performed. Action The action to be performed on the alert. Action Description Mark as Resolved Someone reviewed the issue, took mitigation steps, and considered the matter resolved. Mark as False Positive Someone reviewed the issue, determined that the asset doesn't have an issue or it was triggered while in Maintenance/Service mode, and considered the matter closed. Save Saves the changes made on the Recommendation Alert. Share Ability to share the Recommendation Alert to users that have run access to the Recommendation. Selected users will receive an email with the note and a link to the Recommendation Alert. Assign Ability to Assign (or Reassign) responsibility for the Recommendation Alert to a user that has run access to the Recommendation. The default selection is the logged-in user. When this action is performed, the action is recorded on the Timeline and in the Discussion - thus notifying the assignee. Enable Focus This determines if the user can navigate to the Button by using the keyboard. This includes using the tab button to switch between text boxes on a form, and then clicking the tab button at the end to highlight and select the Button. Buttons will also be focused on when you click on them. If a Button is clicked, and no action occurs, the Button will also remain in focus. Action Configure actions to be triggered when the user clicks the button. For detailed instructions see the Common Properties article for more details"
  },
  "docs/blocks-toolbox/recommendations/alert-event-data.html": {
    "href": "docs/blocks-toolbox/recommendations/alert-event-data.html",
    "title": "Alert Event Data | XMPro",
    "summary": "Alert Event Data The conditions that gave rise to the alert, i.e. the data received by the Data Stream. If \"Log Data On All Occurrences\" is checked in the Rule, this data will be updated as new data is received. Fig 1: Event Data for an Alert Alert Event Data Properties Appearance Common Properties The visibility property is common to most Blocks; See the Common Properties article for more details on common appearance properties. Title Optional text that shows at the top of the block and defaults to \"Event Data\". Behavior Alert ID Supply an Alert Identifier and its Event Data is displayed when the Page is opened. Allow Export to Excel This determines if the user can export the grid as an Excel file."
  },
  "docs/blocks-toolbox/recommendations/alert-form.html": {
    "href": "docs/blocks-toolbox/recommendations/alert-form.html",
    "title": "Alert Form | XMPro",
    "summary": "Alert Form The Alert Form is a place where relevant information can be entered. It is only available if the Recommendation has a Form configured. Fig 1: Form functionality for an Alert Alert Form Properties Appearance Common Properties The visibility property is common to most Blocks; See the Common Properties article for more details on common appearance properties. Behavior Alert ID Supply an Alert Identifier and the Form associated with its Recommendation Rule is displayed when the Page is opened, with the given Alert's values."
  },
  "docs/blocks-toolbox/recommendations/alert-list.html": {
    "href": "docs/blocks-toolbox/recommendations/alert-list.html",
    "title": "Alert List | XMPro",
    "summary": "Alert List The Alert List will show a list of all the Alerts for the selected Recommendations. Alert List Properties Behavior Recommendations This will display a list of Alerts for Recommendations. Before launching the App, you can select which Recommendations to show the Alerts for. Fig 1: Alert List Recommendations Type The identifier filter options are Asset, Process, KPI, and Entity. ID The Identifier used to filter the Recommendation Alerts. Fig 2: Alert List ID"
  },
  "docs/blocks-toolbox/recommendations/alert-survey.html": {
    "href": "docs/blocks-toolbox/recommendations/alert-survey.html",
    "title": "Alert Survey | XMPro",
    "summary": "Alert Survey The Alert Survey allows the user to provide feedback on whether the triggered Recommendation Alert was helpful or not in preventing unplanned downtime. Fig 1: Alert Survey Alert Survey Properties Appearance Common Properties The visibility property is common to most Blocks; See the Common Properties article for more details on common appearance properties. Behavior Alert ID Supply an Alert Identifier and its survey is displayed when the Page is opened."
  },
  "docs/blocks-toolbox/recommendations/alert-timeline.html": {
    "href": "docs/blocks-toolbox/recommendations/alert-timeline.html",
    "title": "Alert Timeline | XMPro",
    "summary": "Alert Timeline A list of the activities that have occurred on an Alert. If this list includes a hyperlink for an escalated alert, configuration properties allow the designer to determine the behavior of that hyperlink. Perhaps the design is for all alerts to be shown using the same Page, or use a formula to show archived alerts on a different Page in the same or a different App (use the URL option to accomplish this). Fig 1: Alert Timeline Alert Timeline Properties Appearance Common Properties The visibility property is common to most Blocks; See the Common Properties article for more details on common appearance properties. Title Optional text that shows at the top of the block and defaults to \"Alert Timeline\". Behavior Alert ID Supply an Alert Identifier and its timeline is displayed when the Page is opened. Navigate To This configures the page or website that the webpage will navigate to when the user clicks on a linked alert's hyperlink: Page takes you to the specified page of the current App, optionally in a new tab/window URL takes you to the specified URL (any website), optionally in a new tab/window Page The page to which the user is redirected, which is applicable when Navigate To is set to 'Page'. Fig 2: Navigate To and Page properties See the Navigate Between Pages article for more information about navigating between pages. URL The URL to which the user is redirected, which is applicable when Navigate To is set to 'URL'. Fig 3: Navigate To, URL, Open in New Tab/Window, and Alert Paramater Name properties Open in New Tab/Window Tick to open in a new tab/window, instead of redirecting the current tab. Alert Parameter Name Supply the parameter name of the Page/URL that will be used to navigate to the escalated alert. It is used to append the escalated Alert Identifier."
  },
  "docs/blocks-toolbox/recommendations/alert-triage.html": {
    "href": "docs/blocks-toolbox/recommendations/alert-triage.html",
    "title": "Alert Triage | XMPro",
    "summary": "Alert Triage The Alert Triage is an area that provides useful information on suggested actions to resolve the Alert, and links to relevant resources. It is only available if the Recommendation Rule has Triage Instructions enabled. Fig 1: Triage data for an Alert Alert Triage Properties Appearance Common Properties The visibility property is common to most Blocks; See the Common Properties article for more details on common appearance properties. Title Optional text that shows at the top of the block and defaults to \"Triage Instructions\". Behavior Alert ID Supply an Alert Identifier and the the Triage Instructions associated with its Recommendation Rule are displayed when the Page is opened."
  },
  "docs/blocks-toolbox/recommendations/recommendation-alert-discussion.html": {
    "href": "docs/blocks-toolbox/recommendations/recommendation-alert-discussion.html",
    "title": "Alert Discussion | XMPro",
    "summary": "Alert Discussion The Alert Discussion is an area in which messages can be posted to collaborate with members of your team about an Alert. Messages can be sent or read by anyone who has access to the Recommendation Alert. Messages are displayed with the latest message at the bottom of the list. Any messages which have not been read since the last time you visited the page will be below the \"Last Read\" line break. You can search for messages that contain a certain word or phrase by typing in the search bar at the top. You can add a message by typing in the editor at the bottom of the discussion section and clicking the button with a paper plane icon at the bottom right corner. Advanced text editing can be opened by clicking the button with an underlined letter A icon at the bottom left corner. You can mention another user by typing the @ symbol or clicking the button with the @ symbol, which will pop up a list of users. Clicking on a user will mention them in the message and send them a notification email when the message is sent. Fig 1: Discussion functionality. Summary Although each discussion is specific to a particular Recommendation Alert, you have the option to include a summary at the top for easy navigation to all alert discussions related to a specific Identifier. Alert discussions most recently contributed to will appear at the top. At runtime, you can sort by a different column, add a filter, or use the column chooser to change the columns displayed. The discussions on archived alerts are hidden by default. Tick Show Archived to see all alert discussions, for this entity, to which you have access. Fig 2: Summary navigation. Alert Discussion Properties Behavior Type The Identifier options are Asset, Process, KPI, Work Order, Work Request, Entity, and None (default). ID Supply an Identifier to show a filtered summary grid of Alerts that match the given Identifier. It is required when Type is not None. Alert ID The Alert ID is optional. Supply a value to default the alert discussion displayed when the Page is opened. Note If an ID is provided without an Alert ID, the bottom of the block will be empty when the Page is opened. In this case, the user should select a Recommendation Alert from the grid to see its alert discussion. Action Configure actions to be triggered when the user clicks the Alert ID of a row in the summary grid. For example, enable the user to navigate to the alert page. For detailed instructions see the Common Properties article for more details"
  },
  "docs/blocks-toolbox/recommendations/recommendation-analytics.html": {
    "href": "docs/blocks-toolbox/recommendations/recommendation-analytics.html",
    "title": "Alert Analytics | XMPro",
    "summary": "Alert Analytics Alert Analytics is an area in which the number of Alerts for an Identifier over a period of time can be compared - with an optional alert ranking filter. The Identifiers of an Alert is defined in the Run Recommendation Agent of the Recommendation's Data Stream. The Entity Identifier is mandatory, whereas the Asset, KPI, and Process Identifiers are optional. The analytics section compares the currently viewed period of alerts with the previous period and displays the difference as a percentage. The statistics compared are: The number of Alerts generated The number of Alerts that were auto escalated The number of Alerts marked as false positive The number of Alerts resolved. Below the breakdown, there is a stacked bar chart of the number of Alerts for the Identifier over time. Below that is a horizontal bar of the number of Alerts for the Identifier in the selected period, separated by Rule. Fig 1: The recommendation analytics for Entity ID 1 over the last 30 days and for all alert rankings. Alert Analytics Properties Behavior Type The identifier options are Asset, Process, KPI, and Entity. ID The Identifier used to filter and show analytics on all Recommendation Alerts."
  },
  "docs/blocks-toolbox/recommendations/recommendation-chart.html": {
    "href": "docs/blocks-toolbox/recommendations/recommendation-chart.html",
    "title": "Recommendation Chart | XMPro",
    "summary": "Recommendation Chart The Recommendation Chart shows the number and severity of unresolved Alerts for the selected Recommendations. Recommendation Chart Properties Behavior Recommendations This will display the number and severity of unresolved Alerts for the selected Recommendations. Before launching the App, you can select which Recommendations to display the data for. Fig 1: Recommendation Chart showing unresolved alerts"
  },
  "docs/blocks-toolbox/visualizations/README.html": {
    "href": "docs/blocks-toolbox/visualizations/README.html",
    "title": "Visualizations Blocks | XMPro",
    "summary": "Visualizations Blocks Visualization blocks in XMPro App Designer provide powerful tools for displaying data in visual formats. These blocks enable you to create charts, maps, gauges, and other visual representations that help users understand complex data at a glance. Available Visualizations Blocks Autodesk Forge - 3D model viewer powered by Autodesk Forge Azure Digital Twin Hierarchy - Hierarchical view of Azure Digital Twins Bar Gauge - Visual representation of a value within a range using a bar Chart - Versatile charting component for various chart types Circular Gauge - Visual representation of a value within a range using a circular gauge D3 Visualization - Custom visualizations using D3.js Esri Map - Geographic mapping using Esri ArcGIS Image Map - Interactive image with clickable regions Linear Gauge - Visual representation of a value within a range using a linear gauge Live Feed - Real-time data feed visualization Map - Geographic mapping component Pie Chart - Circular chart divided into sectors Pivot Grid - Interactive pivot table for data analysis Polar Chart - Circular chart with values plotted against angle and radius Power BI - Embedded Power BI reports and dashboards Sparkline - Small, word-sized chart Time Series Analysis - Specialized chart for time series data Tree Map - Hierarchical data visualization using nested rectangles Unity - 3D visualization using Unity Unity (Legacy) - Legacy version of Unity visualization Best Practices for Using Visualizations Blocks Choose the right visualization: Select the appropriate visualization type based on the data you want to display and the insights you want to convey. For example, use line charts for trends over time, bar charts for comparisons, and pie charts for showing proportions. Keep it simple: Avoid cluttering visualizations with unnecessary elements. Focus on the key data points and insights you want to communicate. Use consistent styling: Maintain consistent colors, fonts, and styles across your visualizations to provide a cohesive user experience. Provide context: Include titles, labels, and legends to help users understand what they're looking at. Consider adding brief explanations or annotations for complex visualizations. Enable interactivity: Where appropriate, enable interactive features such as tooltips, zooming, and filtering to allow users to explore the data in more detail. Optimize for performance: Be mindful of the amount of data you're visualizing, especially for real-time or large datasets. Consider aggregating or sampling data to maintain performance. Ensure accessibility: Make sure your visualizations are accessible to all users, including those with visual impairments. Provide alternative text descriptions and consider color contrast. Examples Dashboard with Multiple Visualizations Box (Dashboard Container) ├── Layout Grid │ ├── Card (Key Metrics) │ │ ├── Bar Gauge (Metric 1) │ │ ├── Bar Gauge (Metric 2) │ │ ├── Bar Gauge (Metric 3) │ ├── Card (Trend Analysis) │ │ ├── Chart (Line Chart) │ │ ├── Series (Data Series 1) │ │ ├── Series (Data Series 2) │ ├── Card (Distribution) │ │ ├── Pie Chart │ │ ├── Series (Category Distribution) │ ├── Card (Geographic Data) │ │ ├── Map │ │ ├── Layer (Data Points) Asset Monitoring Dashboard Box (Asset Monitoring Container) ├── Layout Grid │ ├── Card (Asset Status) │ │ ├── Circular Gauge (Temperature) │ │ ├── Circular Gauge (Pressure) │ │ ├── Circular Gauge (Vibration) │ ├── Card (Performance Trends) │ │ ├── Time Series Analysis │ │ ├── Series (Temperature Over Time) │ │ ├── Series (Pressure Over Time) │ │ ├── Series (Vibration Over Time) │ ├── Card (Asset Location) │ │ ├── Map │ │ ├── Layer (Asset Locations) │ ├── Card (3D Model) │ │ ├── Unity │ │ ├── Model (Asset 3D Model) Financial Analysis Dashboard Box (Financial Analysis Container) ├── Layout Grid │ ├── Card (Financial Summary) │ │ ├── Bar Gauge (Revenue vs Target) │ │ ├── Bar Gauge (Expenses vs Budget) │ │ ├── Bar Gauge (Profit Margin) │ ├── Card (Revenue Breakdown) │ │ ├── Pie Chart │ │ ├── Series (Revenue by Product) │ ├── Card (Financial Trends) │ │ ├── Chart (Line Chart) │ │ ├── Series (Revenue) │ │ ├── Series (Expenses) │ │ ├── Series (Profit) │ ├── Card (Detailed Analysis) │ │ ├── Pivot Grid │ │ ├── Rows (Product, Region) │ │ ├── Columns (Quarter, Month) │ │ ├── Values (Revenue, Expenses, Profit) By effectively using visualization blocks, you can create powerful dashboards and reports that help users gain insights from complex data. These visualizations can support decision-making, monitoring, and analysis across a wide range of business scenarios."
  },
  "docs/blocks-toolbox/widgets.html": {
    "href": "docs/blocks-toolbox/widgets.html",
    "title": "Widgets | XMPro",
    "summary": "Widgets Widgets in XMPro App Designer are custom, reusable components that can be created by combining existing blocks and functionality. They provide a way to encapsulate complex UI patterns and functionality into reusable components that can be shared across your applications. Understanding Widgets Widgets are similar to Metablocks but are managed at the application level rather than at the page level. They allow you to create custom components that can be reused across multiple pages and applications, promoting consistency and reducing development time. Widgets can contain any combination of blocks, including layout blocks, basic blocks, visualizations, and even other widgets. They can also include custom logic, data bindings, and styling. Creating Widgets To create a widget: Design the component using standard blocks in the App Designer Select all the blocks that make up the component Right-click and select \"Create Widget\" Provide a name and description for the widget Configure the widget properties and data bindings Save the widget Once created, the widget will be available in the Widgets section of the Toolbox and can be used like any other block. Managing Widgets Widgets can be managed through the Widget Manager, which allows you to: View all available widgets Edit existing widgets Create new widgets Delete widgets Export widgets for sharing Import widgets from other sources To access the Widget Manager, click on the \"Widgets\" button in the App Designer toolbar. Best Practices for Using Widgets Design for reusability: When creating widgets, design them to be as reusable as possible. Use parameters and properties to make them configurable for different use cases. Document your widgets: Provide clear documentation for your widgets, including their purpose, configuration options, and usage examples. This will help other users understand how to use them effectively. Use consistent naming conventions: Adopt consistent naming conventions for your widgets to make them easy to find and understand. Group related functionality: Create widgets that encapsulate related functionality to promote modularity and maintainability. Test thoroughly: Test your widgets thoroughly in different contexts to ensure they work as expected in all scenarios. Consider performance: Be mindful of the performance implications of your widgets, especially if they include complex logic or data operations. Maintain version control: Keep track of widget versions and changes to ensure compatibility and facilitate updates. Examples of Common Widgets Address Entry Widget A widget that combines multiple input fields for entering address information, including: Street address City State/Province Postal code Country Data Card Widget A widget that displays a summary of data in a card format, including: Title Key metrics Trend indicator Action buttons Filter Panel Widget A widget that provides a standardized interface for filtering data, including: Text search Date range selection Category filters Apply/Reset buttons Chart with Legend Widget A widget that combines a chart with a customizable legend and additional controls, including: Chart component Interactive legend Time period selector Export options Form Section Widget A widget that provides a standardized form section with consistent styling and behavior, including: Section title Form fields Validation Help text Sharing Widgets Widgets can be shared across your organization to promote consistency and reduce duplication of effort. To share a widget: Export the widget from the Widget Manager Share the exported widget file with other users Other users can import the widget through their Widget Manager By effectively using widgets, you can create a library of reusable components that enhance productivity, maintain consistency, and reduce development time across your XMPro applications."
  },
  "docs/concepts/agent/index.html": {
    "href": "docs/concepts/agent/index.html",
    "title": "Agent | XMPro",
    "summary": "Agent An Agent is a reusable object which forms the building block of a Data Stream. When a number of Agents are connected together, a Data Stream is formed. Each Agent is designed to perform a specific function in the stream. For example, they can be used to retrieve data from a database in real-time, display data, filter, sort the data, or save the data somewhere else, depending on the function of that individual Agent. Agents are needed to connect to specific systems. Since Agents are individual components, new Agents can also be added and integrated into the Data Stream to complete a specific functionality. Video Presentation Discussing Agents and Collections Each Agent consists of code, settings, and other properties that are packaged into a file that can be uploaded to Data Stream Designer. XMPro has a library of Agents available to use. To acquire any of these Agents, please contact your XMPro sales representative or write to us at support@xmpro.com. Alternatively, since Agents can be written by anyone that has some knowledge of programming and has access to the required technologies, you can write your own Agent by following these instructions. Categories In Data Steam Designer, Agents are divided into different categories, depending on the overall function they perform. There are six different categories available: Action Agents, Context Providers, Listeners, Transformations, AI & Machine Learning, Recommendations, and Functions. To be able to distinguish them properly, they have been tagged with a certain color as well as an abbreviation. These categories are separate from the App and Data Stream Categories. Action Agents An Action Agent is an Agent that consumes events in a stream and then performs internal or external (third-party) actions, e.g. sending notifications or performing data warehouse updates. Action Agents output a response after each event has been processed. For example, the Azure SQL Action Agent writes data to an Azure SQL database. Context Providers Context providers are Agents that provide context to a stream by consuming reference or static data and making it available. For example, the SQL Server Context Provider provides static data to the Data Stream by reading the data in a database table and sending it to the next Agent. Listeners Listeners are Agents that listen for data or events from sensors and third-party systems. For example, the MQTT Listener listens for data from sensors as it is posted to MQTT. Transformations Transformation Agents alter the shape or form of data. For example, the Join Transformation joins data it receives from two separate data sources. AI & Machine Learning AI & Machine Learning Agents allow you to run advanced AI to transform the data. For example, Azure ML, IBM Watson, and Jupyter Notebook. Recommendations Recommendation Agents are related to Recommendations and let you complete actions such as running recommendations, updating recommendations, and more. Functions Functions perform specific mathematical or statistical operations on data. For example, the FFT Function performs forward FFT calculations on the data it receives. Settings An Agent consists of code and user settings. The code defines the actions an Agent performs in any Data Stream. The settings are the input for the code that executes, provided by the user when adding the Agent to a Data Stream, such as authentication credentials. For example, consider the SQL Server Writer Agent. The function this Agent performs in a Data Stream is to take the data it receives and write it to a table in a database. The settings a user must define for the Agent so it can do that are as follows: Name of the SQL Server instance SQL Server username Whether SQL Server authentication should be used or not SQL Server password Database to which the data should be written Whether a new table should be created or not Table to which data should be written - if the user wishes to use an existing table Name of the table that should be created if the user wants to write the data to a new table If database triggers should be fired when a record is inserted Endpoints Endpoints provide entry and exit points to the Agent. The input endpoint allows the Agent to receive data from another Agent, whereas the output endpoint enables the Agent to pass data to another Agent. They are represented on the Data Stream canvas as green rectangles. The error endpoint allows an Agent to send any error data further along a part of the stream, designed to handle data records or events that do not meet certain requirements. It is represented on the Data Stream canvas as a red rectangle. Note Image placeholder: Fig 1. Data Stream Canvas and Agent's User Settings Note Image placeholder: Fig 2. Agent properties Finding Agents The search bar can be used to find any specific Agents that you may be looking for. There is a dropdown option where you can specify to search through everything in Data Stream Designer, or only for Agents. Note Image placeholder: Fig 3. Searching for an Agent Versions Agents can keep track of their different versions. Versions of an Agent can be copied, and changes made to it can be created as a new version without affecting previous versions. See the Version article for more details on versions. Publish and Unpublish Data Streams On the Agents page, there will be a number next to the version if the agent has been used in a Data Stream. Click the number to view a list of all Data Streams that are using that Agent version. Here you can directly unpublish or publish a Data Stream. As an Admin, this is useful if you need to unpublish a Data Stream and you don't have access to it. See the How to Admin Unpublish Override article for more details. Note Image placeholder: Fig 4. Data Stream Toolbox Interface Actions on the Agent Action Description Add Adds a new Agent. Select Selects multiple Agents. Delete Deletes the Agent. Save Saves any changes made to the Agent up to this point. Discard Discards any changes made to the Agent up to this point. Delete Versions Deletes selected versions of the Agent. Further Reading Agent FAQs How to Create and Manage Agents How to Run an Integrity Check How to Upgrade a Stream Object Version How to Use Error Endpoints Virtual vs Non-Virtual Agents"
  },
  "docs/concepts/agent/virtual-vs-non-virtual-agents.html": {
    "href": "docs/concepts/agent/virtual-vs-non-virtual-agents.html",
    "title": "Virtual vs Non-Virtual Agents | XMPro",
    "summary": "Virtual vs Non-Virtual Agents There can be two types of Agents in Data Streams; Virtual and Non-Virtual Agents. When packaging an Agent using the Stream Integration Manager, it is important to specify if an Agent is Virtual by making sure that the Virtual checkbox is correctly ticked. Virtual Agents A Virtual Agent is an Agent that is not bound to a certain environment to be able to function, for example, the Azure SQL Listener is an Agent which can be configured anywhere as it will always have access to the globally available Azure SQL Server, which it needs to integrate with. Non-Virtual Agents An Agent can be classified as Non-Virtual if it relies on a specific environment to function. Non-Virtual Agents need to interact with a system that is only available in a specific environment, for example, the SQL Server Listener. The SQL Server Listener is an Agent that can only be configured while it is on the same local area network as the SQL Server instance it needs to connect to. As shown in the diagram above, even though both Virtual and Non-Virtual Agents ultimately run on the Stream Host, there is a considerable difference in how they are handled at design time. A Virtual Agent can be configured even if no Stream Host is online, but this is not possible for a Non-Virtual one. Virtual Agents are also very fast as the engine doesn't have to go all the way to the Stream Host to configure them and this results in a smoother user experience. Note Virtual Agents can be configured even if there is no Stream Host online, but Non-Virtual Agents require a Stream Host to be online."
  },
  "docs/concepts/application/app-files.html": {
    "href": "docs/concepts/application/app-files.html",
    "title": "App Files | XMPro",
    "summary": "App Files App Files are a way to import files into an Application. App Files are managed through the App Files command or through the + button next to related properties in the Block Properties of a Page. Some Blocks allow the selection of App Files for some Block Properties. The App Files explorer allows you to use, rename, delete, and perform more actions on an uploaded file. App Files are useful in many scenarios, for example, if you need specific files to integrate Unity or D3 Visualizations onto the page. ![](/docs/images/image (288).png>) ![](/docs/images/image (1520).png>) List of Blocks that use App Files Unity D3 Visualization Unity Files that can be uploaded for Unity include the Unity web assembly code file, Unity data file, and Unity framework file for a Unity project. See the Unity article to view the files you will need to integrate a Unity project. D3 Visualization Files that can be uploaded for D3 Visualizations include an HTML script that is used to create data visualization which will allow you to display data from a data source in real-time. See the D3 Visualizations article for more information about how to write and upload a script. Actions on the App Files Action Description New directory Creates a new directory under the specified directory or at the root Files directory. Upload files Uploads files into the specified directory or at the root Files directory. Change View Allows the view to be changed from Thumbnails to Details. Refresh Refreshes the App Files. If any changes have been made it also opens the Progress popup, which displays the recent history. Download Downloads the selected App File(s). Move to Moves the selected App File(s) to a different directory. Copy to Copies the selected App File(s) to a different directory. Rename Renames the selected App File. Delete Deletes the selected App File(s). Clear Selection Clears the selected App Files(s). Further Reading How to Manage App Files"
  },
  "docs/concepts/application/block-properties.html": {
    "href": "docs/concepts/application/block-properties.html",
    "title": "Block Properties | XMPro",
    "summary": "Block Properties Block Properties are properties that define the appearance and behavior of the Block. This allows you to customize the block's content such as the heading text, or configure any data you would like to display on your application. Each block type has different properties. To access the Block Properties tab, double-click a Block in the canvas or click on a Block in the canvas and click the Block Properties command. ![](/docs/images/image (1285).png>) Properties are split into several common categories. Click on a category to expand the accordion and see the Properties in that category. The common categories are as follows: Category Description Appearance Properties that define how a Block looks. Behavior Properties that define how a Block behaves. Value Properties that define what value a Block has. Data Source Properties that define what Data Source the Block has (applies to Blocks that allow a Data Source to be attached). Data, Columns, Items Properties that define what the Data Source properties are mapped to, or allow a static data set to be created (applies when relevant to the Block). Grouping Properties that define how the Data Source is grouped (applies when relevant to the Block). Action Properties that define what action, and validation to perform, and what Data Source to update when it or items inside the Block are clicked (applies to Blocks that allow an action). Validation Properties that define what validation should be performed on the value of the Block before allowing an action (applies to Blocks that allow user input). Dynamic and Expression Properties Some properties have the option of being a Dynamic or Expression property. If the property has a button on the left with an A icon and \"Static Value\" when you hover it, it can be a Dynamic or Expression Property. Press the button to cycle through the modes. ![Dynamic Mode](/docs/images/image (844).png>) ![Expression Mode](/docs/images/image (1684).png>) Dynamic properties allow you to select a value for the property from the Page Parameters, Variables, User Details, Device Details, and from a column or expression of the current row of a parent Block's Data Source. User Details Property Description Username Returns the Username of the user that is signed in. First Name Returns the First Name of the user that is signed in. Last Name Returns the Last Name of the user that is signed in. Full Name Returns the Full Name of the user that is signed in, using the format \"[First Name] [Last Name]\". Email Returns the Email of the user that is signed in. Preferred Language Returns the regional language tag of the logged in user's preferred language. Use this to adjust text when an App will be used in multiple languages: Iif(userPreferredLanguage == \"en-US\", \"Hello world\", \"Oi Worldo\") Language Tag English (United States) en-US svenska sv-SE Português (Brasil) pt-BR Русский ru-RU Español es-ES Nederlands nl-NL Deutsch de-DE français fr-FR 中文 (Zhōngwén) zh-CN ελληνικά el-GR Türkçe tr-TR čeština cs-CZ suomi fi-FI Italiano it-IT Magyar hu-HU 日本語 (にほんご) ja-JP slovenski jezik sl-SI Tiếng Việt vi-VN Device Details Property Description IsMobile Returns True boolean value if the App is open on a mobile device. Example: For Mobiles and/or Tablets will be True. IsDesktop Returns True boolean value if the App is open on a desktop device. Example: For Laptops and/or Desktops will be True. Culture Returns the Device culture/language. Example: If the language is set to US English will return en-US. {% hint style=\"info\" %} See the Data Integration article for more information on Data Sources. {% endhint %} Expression properties allow you to create short scripts to create a custom value from all the above options. See the Variable and Expressions article to learn more about Expressions. ![](/docs/images/image (1008).png>) The Dynamic and Expression properties will have all the Block's ancestor's Data Sources displayed in the order of closest ancestor to furthest. For example, in the following images, the bottom-most Text Block in the Page Layers has two ancestors that have a Data Source: the Data Repeater Box and the Templated List. Therefore the two data sources are listed in the Text Block's Dynamic and Expression property dropdowns. ![](/docs/images/image (750).png>) ![](/docs/images/image (1709).png>) Further Reading How to Use Dynamic Properties How to Use Expression Properties How to Use Validation"
  },
  "docs/concepts/application/block-styling.html": {
    "href": "docs/concepts/application/block-styling.html",
    "title": "Block Styling | XMPro",
    "summary": "Block Styling When a Block is selected, it can be styled under the 'Block Styling' tab in the Toolbox. Styling can include changing the text color, background color, borders, typography, or dimensions of the Block. This allows you to customize the look and feel of the application based on themes or color palettes for your specific organization. You can also customize the style of certain actions such as hovering over or clicking a button, or changing the style for every second Block. Style Groups A Block can also be assigned to a style group where a common set of styles can be configured and applied to multiple Blocks at the same time. ![](/docs/images/Style Group 1.png>) Certain Blocks such as content cards or cards that are dragged onto the canvas already have pre-existing style groups, such as grids. These will show up under the 'Style Group'. ![](/docs/images/Style Group 2.png>) If you have a Style Group selected and make changes to any of the styling configurations, the styling will automatically be applied to all the Blocks that are also in that style group. For example, if two grids have a style group called box-card, and you select only one of the grids and change the background to light blue, that change will also be applied to the other grid. ![](/docs/images/Style Group 3.png>) To make changes without affecting other blocks, deselect the style group and make the changes. ![](/docs/images/Style Group 4.png>) You can add multiple style groups at a time. If a Block has multiple style groups and you only want to apply styling to one of them, make sure only that style group is selected when configuring new styles. ![](/docs/images/Style Group 5.png>) If multiple style groups are selected and the styling is changed, the styling will be applied to both of them together. For example, if you have two style groups, box-card, and lightgreen, and you apply styling to both of them, that styling will only be applied to Blocks that have both box-card and lightgreen style groups. ![](/docs/images/Style Group 6.png>) Devices Styles can also be configured for different devices. See the Devices article for more details on devices. Style Sections Dimension Style CSS Property Width width sets the width of an element. Height height sets the height of an element. Min Width min-width defines the minimum width of an element. Min Height min-height defines the minimum height of an element. Max Width max-width defines the maximum width of an element. Max Height max-height defines the maximum height of an element. Margin margin-top sets the top margin of an element. margin-right sets the right margin of an element. margin-bottom sets the bottom margin of an element. margin-left sets the left margin of an element. Padding padding-top sets the top padding (space) of an element. padding-right sets the right padding (space) of an element. padding-bottom sets the right padding (space) of an element. padding-left sets the right padding (space) of an element. {% hint style=\"info\" %} When not using 'auto', the supported css units for the dimension properties are: fixed: px relative: % and vh/vw {% endhint %} Flex Layout {% hint style=\"info\" %} Recommended reading: Flex and How to Use Flex. {% endhint %} Style CSS Property Direction flex-direction specifies the direction of the flexible items: Row, Row reverse, Column, or Column reverse. Justify justify-content aligns the flexible container's items when the items do not use all available space on the main-axis (horizontally): Start, End, Space between, Space around, or Centre. Align Items align-items specifies the default alignment for items inside the flexible container: Start, End, Bottom, Stretch, Centre. Wrap flex-wrap specifies whether the flexible items should wrap or not: No wrap, Wrap, Wrap reverse. Grow flex-grow is an integer that specifies how much the item will grow relative to the rest of the flexible items inside the same container. Align Self align-self specifies the alignment for the selected item inside the flexible container: Auto, Start, End, Bottom, Stretch, Centre. Typography Style CSS Property Font font-family specifies the font for an element: Arial, Arial Black, Brush Script MT, Comic Sans MS, Courier New, Georgia, Helvetica, Impact, Lucida Sans Unicode, Tahoma, Times New Roman, Trebuchet MS, Verdana Font size font-size sets the size of a font. Either string or an integer along with a fixed (px) or relative (%, em, rem, vh, or vw) css unit. Font Weight font-weight sets how thick or thin characters in text should be displayed: Thin, Extra-Light, Light, Normal, Medium, Semi-Bold, Bold, Extra-Bold, or Ultra-Bold. Font Style font-style specifies the font style for a text. Normal, italic, or oblique. Line Height line-height specifies the height of a line using a fixed (px) or relative (%, em, rem, vw, or vh) css unit. Font Color color specifies the color of text by name, RGB, or RGBA. Text Align text-align specifies the horizontal alignment of text in an element: Left (default), Center, Right, or Justify. Text Decoration text-decoration specifies the decoration added to the text: Underline, Strikethrough, or None (default). Word Spacing word-spacing increases or decreases the space in pixels between words in a text (default normal). Letter Spacing letter-spacing increases or decreases the space between characters in a text (default normal). Either string or an integer along with a fixed (px) or relative (%, em, or rem) css unit. Wrap Text Sets whether text should be wrapped: Yes or No. Wrap Text At The options are Spaces, Capitals and Symbols, and Anywhere. Decorations Style CSS Property Background Color background-color sets the background color of an element by name, RGB, or RGBA. The background of an element is the total size of the element, including padding and border (but not the margin). Border Width border-top-width sets the width of an element's top border in px or em. border-right-width sets the width of an element's right border in px or em. border-bottom-width sets the width of an element's bottom border in px or em. border-left-width sets the width of an element's left border in px or em. Border Style border-style sets the style of an element's four borders: Solid, Dotted, Dashed, Double, Groove, Ridge, Inset, Outset, or None. Border Color border-color sets the color of an element's four borders by name, RGB, or RGBA. Border Radius Add rounded borders to the corners of elements: border-top-left-radius defines the radius of the top-left corner. border-top-right-radius defines the radius of the top-right corner. border-bottom-left-radius defines the radius of the bottom-left corner. border-bottom-right-radius defines the radius of the bottom-right corner. Background background adds one or more image layers, each comprising: Image (file or URL) Repeat (repeat, repeat-x, repeat-y, no-repeat) Position (left top, left center, left bottom, right top, right center, right bottom, center top, center center, center bottom, Attachment (scroll, fixed, local) Size (auto, cover, contain) Advanced Styling These styling options are rarely needed, but they provide additional flexibility for expert users. Advanced position and displayed styling Style CSS Property Display display specifies the type of rendering box of an element: block, inline, inline-block, flex (default), or none. Position position specifies the type of positioning used for an element: static (default), relative, absolute, or fixed. Top* top sets the vertical position of a positioned element (default auto). Right* right sets the horizontal position of a positioned element (default auto). Left* left sets the horizontal position of a positioned element (default auto). Bottom* bottom sets the vertical position of a positioned element (default auto). {% hint style=\"info\" %} The supported css units for properties marked with a * are: fixed: px relative: % and vh/vw {% endhint %} Extra advanced styling Style CSS Property Transition transition adds one or more transition effect layers, each comprising: transition-property (all, width, height, background-color, transform, box-shadow, opacity) transition-duration how many seconds the effect lasts Easing (linear, ease, ease-in, ease-out, ease-in-out) Perspective perspective defines how far the object is away from the user. A lower value will result in a more intensive 3D effect than a higher value. Z Index z-index specifies the stack order of a positioned element. Transform Apply 3D mouseover effects to transformable elements: rotateX() rotates an element around its X-axis at a given degree. rotateY() rotates an element around its Y-axis at a given degree. rotateZ() rotates an element around its Z-axis at a given degree. scaleX() defines a 3D scale transformation by giving a value for the X-axis. scaleY() defines a 3D scale transformation by giving a value for the Y-axis. scaleZ() defines a 3D scale transformation by giving a value for the Z-axis. Pointer Events pointer-events defines whether or not an element reacts to pointer events: All (default) or None. Cursor cursor sets the mouse cursor, if any, to show when the mouse pointer is over an element: Auto (default), Allowed, Help, None, Pointer, Progress, Wait, Zoom In, or Zoom Out. Note: Pointer Events must be set to All for the Cursor changes can be applied. Advanced flex layout styling Style CSS Property Flex Container flex-container enables all flexible items be the same length, regardless of content. Shrink flex-shrink is an integer that specifies how the item will shrink relative to the rest of the flexible items inside the same container. Basis flex-grow is an integer that specifies how much the item will grow relative to the rest of the flexible items inside the same container. Order order is an integer that specifies the order of a flexible item relative to the rest of the flexible items inside the same container. Typography advanced styling Style CSS Property Text Shadow text-shadow adds one or more layers to the text, each comprising an X position, Y position, and Blur (px or %) - as well as a Color (name, RGB, or RGBA). Advanced decoration styling Style CSS Property Overflow overflow sets the desired behavior when content does not fit in the parent element box (overflows) in the horizontal and/or vertical direction: visible (default), hidden, scroll, or auto. Opacity opacity sets the transparency of an element, where 1 is not at all transparent, 0.5 is 50% see-through, and 0 is completely transparent. Box Shadow box-shadow adds one or more shadow layers to an element, each comprising an X position, Y position, Blur, and Spread (px or %) - as well as a Color (name, RGB, or RGBA) and Shadow Type (Outside, Inside). Further Reading How to use Block Styling and Devices How to use Flex"
  },
  "docs/concepts/application/block.html": {
    "href": "docs/concepts/application/block.html",
    "title": "Block | XMPro",
    "summary": "Block A Block is an element or control available to be added to the Page. Blocks make up the foundation of the Application's design and content, including any text, headings, images, or data that needs to be displayed to the user of the Application. Blocks are accessed through the Blocks tab in the page designer and can be added to the Page by dragging them onto the Canvas. Blocks can also be searched by typing in the search bar at the top. ![](/docs/images/Block 1.jpg>) List of Blocks Layout Accordion Box & Data Repeater Box Card & Content Card Fieldset & Field Layout Grid Menu Scroll Box Stacked Layout Horizontal & Vertical Tabs Templated List Toolbar Basic Calendar Check Box Color Selector Data Grid Date Selector Dropdown Grid Embedded Page File Library File Uploader Html Editor Image Indicator List Lookup Number Selector Radio Buttons Range Slider Select Box Switch Tags Text Text Area Textbox Tree Grid Tree List Device Input Location Capture Visual Media Capture AI Azure Copilot ChatGPT Copilot Actions Box Hyperlink Button Data Operations Hyperlink Recommendations Alert Action Alert Analytics Alert Discussion Alert Event Data Alert Form Alert List Alert Timeline Alert Triage Alert Survey Recommendation Chart Visualizations Autodesk Forge Azure Digital Twin Hierarchy Bar Gauge Chart Circular Gauge D3 Visualization Esri Map Image Map Linear Gauge Live Feed Map Pie Chart Pivot Grid Polar Chart Power BI Sparkline Time Series Analysis Tree Map Unity Unity (Legacy) Advanced Metablock {% hint style=\"info\" %} Can't find the Block you're looking for? Contact support to submit a feature request. {% endhint %} Further Reading How to Create and Manage Widgets"
  },
  "docs/concepts/application/canvas.html": {
    "href": "docs/concepts/application/canvas.html",
    "title": "Canvas | XMPro",
    "summary": "Canvas The App Designer Canvas is a drag and drop editor for creating customized web pages. UI Blocks are elements or controls such as a textbox that can be dragged from the toolbox to anywhere on the Canvas. See the UI Blocks article for more details on UI Blocks. This is the functionality that allows anyone on your team to create and deploy applications without needing to be a programmer. Controls such as graphs, gauges, or charts can be populated with real-time data from different systems. For example, live data from running systems or machines can be displayed directly to the user. See the Data Integration article for more details on integrating data sources into the page. The Blocks that are added onto the Canvas are what make up the overall design of a Page for the Application. Each Page in the App has its own Canvas that can be customized, and this can be opened by going into the Application and editing the Page. Block Toolbar A Block is outlined in blue on the canvas when it is selected. A blue toolbar will appear on the top-right corner of the Block which will provide options to move, clone, or do other actions on the Block. Note that some of these only apply to certain elements that allow these functions. Action Icon Description Move Allows you to click and drag the Block to move it to a different location on the Canvas. You can also click and drag from anywhere on the Block itself, but the Block may be obscured by its children. Select Parent Selects the parent of the currently selected Block. Clone Clones the selected Block. Add Adds a new section of the Block, for example, a new tab or column. Save As Widget Saves the selected Block and its inner Blocks as a Widget, which allows this group to be re-used later. See the Widgets article for more details on Widgets. Delete Deletes the Block. Canvas Hierarchy Blocks are organized on the Canvas using a hierarchy. The Canvas itself is made up of Blocks nested under other Blocks, and this makes up the overall design of the Page. When each Block is selected, it will be outlined and can help identify what part of the hierarchy it belongs to, including all of its parent Blocks and children Blocks. For example, there could a parent Block that contains multiple Blocks inside it, to form a grid of gauges containing a different value each. The hierarchy can also be viewed as a list in the Page Layouts tab in the Toolbox. See the Page Layers article for more details on Page Layers. Data Sources You can connect Data Sources to blocks on the canvas by going to 'Data Source' under 'Block Properties'. This allows you to view data on the Page based on a connected Data Source. See the Data Integration article for more details on Data Sources. Further Reading How to Design Pages"
  },
  "docs/concepts/application/data-integration.html": {
    "href": "docs/concepts/application/data-integration.html",
    "title": "Data Integration | XMPro",
    "summary": "Data Integration Data Integration refers to connecting an Application to a source of data, be it a database, a Data Stream, a Recommendation, or something else. Data Integration is needed if you want to display any real-time or context data to the user, such as through statistics or graphs. The App has a number of Data Sources, which connect to a source of data through a connector, using the parameters defined in a Connection. App Designer Connector Connectors allow you to connect to third-party sources of data. You can assume that Connectors have already been set up by your Company Administrator. See the Connectors article for more details on Connectors. XMPro has a library of Connectors available to use. To acquire any of these Connectors, please contact your XMPro sales representative or write to us at support@xmpro.com. Alternatively, since Connectors can be written by anyone that has some knowledge of programming and has access to the required technologies, you can write your own Connector by following these instructions. Connection The parameters defined in a Connection allow the App to connect to a source of data like a SQL Database and expose the entities as Data Sources within the Page. Connection parameters can include credentials such as a username, password, path, URL, or location identifier that you can use to make a remote connection to the Data Source. For example, connection parameters to connect to an SQL Database would include the Server Name, Authentication type, Username, and password. A new App will have the Recommendations Connector and the Data Stream Connector by default. Connections are managed in an App by clicking the App Data command. Any Connections that were created with an older version of a Connector will have an upgrade icon. ![](/docs/images/image (478).png>) The properties of a Connection are dependent on the type of Connector. To see the properties of a Connection, click the Connector in the list of Connectors above. Upgrade the Connection If the Connector has a newer version, an upgrade icon will appear on the App Data list indicating that the Connection can be upgraded, and the Upgrade action will become available. This upgrades the Connection to use the latest uploaded version of the Connector. ![](/docs/images/_a (1).png>) Data Source Data Sources are a link to a specific Entity in a Connection's Entities, for example, a table in a SQL database. Data Sources are managed through the Page Data tab of a Page. You can add a new Data Source by clicking on the + button to the right of the Data Sources heading, and edit a Data Source by clicking on the pencil button to the right of the Data Source. If the Connection has the ability to have Live Data Updates, the checkbox will be visible. If checked, the Data Source will automatically update the Blocks when new data becomes available without refreshing the Page. Primary Key A primary key is the column or columns that contain values that uniquely identify each row in a table. Without a primary key, we cannot update, insert or delete data in the table. The Primary Key is a required field and cannot be empty. If the selected Entity has a Primary Key it will be specified automatically in the Primary Key field otherwise it has to be specified manually by selecting one or multiple columns from the list. Data Source Validation ![](/docs/images/Primary Key explained.png>) A Data Source may become invalid due to several issues. The most common ones can be expired passwords, deleted entity or unreachable servers. If a Data Source becomes invalid a count will be visible on the Page Data tab to notify the user. If you click the Page Data tab of the page, an exclamation mark will be visible in front of the Data Source name and hovering over the exclamation mark will show a detailed error. ![](/docs/images/PageValidation2 (1).png>) Data Source in the Page Blocks A Data Source may be attached to many Blocks. Blocks that allow Data Sources to be attached have a section in Block Properties called Data Source. If a Data Source is applied to a Block, depending on what type of Block it is, it will either repeat itself or repeat its child elements per the number of rows of the Data Source or use the data to populate its items or visualization. For example: Repeating Blocks will display a single record in each block, and a new block will be created for every record in the data source. All blocks will be created with the same styling as the first one. Blocks that populate items will display all records in a single Block. ![](/docs/images/image (604).png>) The parameters of the Data Source are then made available to its Action if it allows one, and all its descendants' Dynamic and Quick Expression value bindings. {% hint style=\"info\" %} See the Block Properties article for more information on Dynamic and Quick Expression values. {% endhint %} List of Data Source Blocks The following Blocks populate their items or visualization with rows from the Data Source. Accordion Autodesk Forge Chart D3 Visualization Data Grid Dropdown Grid List Lookup Pie Chart Pivot Grid Polar Chart Radio Buttons Select Box Sparkline Tabs Tags Templated List Tree Grid Tree List Unity The following Blocks repeat themselves/create a new block for each record in the Data Source: Box & Data Repeater Box Card & Content Card Fieldset Horizontal Stacked Layout Vertical Stacked Layout Parameters If a Data Source has Input Parameters, the Parameters option will be available. Here you can add a Static or Dynamic Value to each Input Parameter. ![](/docs/images/image (1151).png>) Filter A Filter may be applied to the Data Source on the Block, which will then only retrieve data that matches the Filter. ![](/docs/images/image (722).png>) You can add new conditions or groups by clicking the + button. Groups can be nested within each other to create advanced logic. In an \"And\" group, all the conditions must be true, and in an \"Or\" group, only one of the conditions must be true to trigger an Alert. As an example, in the following Filter, both of the following must be true for the data: Average must be greater than 50, And V1, V2, or V3 must be greater than 60. ![](/docs/images/image (754).png>) Sort You can sort the data by any field in ascending or descending order. ![](/docs/images/image (994).png>) Show/Skip You can choose to only retrieve a certain number of items and/or skip a certain number of items by adding values to Show # of Results and Skip # of Results. Show Default Row If the control is of the type where the block repeats itself/creates a new block for each record, the Show Default Row option will be available. This defines whether an empty row will be added at the end of the rows received from the Data Source, allowing you to insert a new row into the Data Source. Further Reading How to Create and Manage Connections How to Create and Manage Data Sources How to use Data Sources in the Page"
  },
  "docs/concepts/application/devices.html": {
    "href": "docs/concepts/application/devices.html",
    "title": "Devices | XMPro",
    "summary": "Devices An App can be deployed on multiple devices such as a laptop, tablet, or phone. The style and position of the Blocks can be configured to make sure it is compatible with the screen width of the device. This is needed in order for you to view the way the Application looks on multiple devices, and to ensure users have a good user experience regardless of the screen size they are viewing your Application on. Responsive Design All pre-built page layouts come with predefined styles and media queries to ensure that Blocks are responsive on different devices. When using a pre-built layout, the positions of the Blocks will automatically change when switching between desktop, tablet, and mobile in order to fit everything comfortably on the page regardless of the screen width. Configuring Desktop, Tablets, and Phones Changes such as block styling made to desktop devices are automatically applied to devices of all sizes. Changes made to tablet devices affect screens smaller than 1366 pixels, and changes made to mobile mode affect screens smaller than 896 pixels. If a different style is configured for both desktop and mobile mode, the mobile mode style will override the styles of larger devices when viewing in mobile mode. The same applies to the tablet. The styles applied to devices are applied to both the selected style group and the selected state as well. Further Reading Block Styling How to use Block Styling and Devices How to use Flex"
  },
  "docs/concepts/application/flex.html": {
    "href": "docs/concepts/application/flex.html",
    "title": "Flex | XMPro",
    "summary": "Flex Flex (CSS Flexbox Layout Module) is a way to design flexible responsive layouts for web pages. The main concept behind flex is the flex container. If you set a container to display: flex;, its child items' width or height will be expanded or shrunk to best fill the space available. .png>) The direction the flex items will be stacked in is determined by the flex-direction property. The options include row, row-reverse, column, and column-reverse. .png>) The alignment of the items in the main-axis (horizontal if flex-direction is row or row-reverse, vertical if flex-direction is column or column-reverse) is determined by the justify-content property. The options include flex-start, flex-end, space-between, space-around, and center. .png>) .png>) .png>) .png>) The alignment of the items in the cross-axis (vertical if flex-direction is row or row-reverse, horizontal if flex-direction is column or column-reverse) is determined by the align-items property. The options include flex-start, flex-end, stretch, and center. .png>) .png>) .png>) The child items of a flex container have a say in how they are altered. The flex-grow property on the child of a flex container defines whether the item should be the desired ratio of this item compared to its siblings. .png>) .png>) The flex-shrink property defines how much a flex item will shrink relative to the rest of the flex items. Best used in conjunction with the flex-basis property. The flex-basis property specifies the initial length of an item on the main-axis. .png>) Further Reading How to Use Flex in a Page"
  },
  "docs/concepts/application/index.html": {
    "href": "docs/concepts/application/index.html",
    "title": "Application | XMPro",
    "summary": "Application An XMPro Application or App is an event-driven web application created using XMPro's no-code App Designer. It enables subject matter experts to create and deploy real-time, event intelligence applications without being a programmer. This means that Engineers and subject matter experts can build apps in days or weeks without further overloading IT, enabling your organization to accelerate and scale your digital transformation. Applications are accessed through the App Designer Categories dashboard or the Applications button on the left menu. Finding Applications The search bar can be used to find any specific Apps that you may be looking for. There is a dropdown option where you can specify to search through everything in App Designer, or only for Apps. Overview Applications are grouped into categories, which are selected in the App properties. They are displayed on the App Designer home page dashboard and in the Applications list under their category. The dashboard shows the Apps' name, description, and icon as well as the publish status and date last modified. Applications have a Default Theme, which is the theme selected when any new pages are created. Each page can have its own theme. Page An Application is made up of one or more Pages. A Page is a web page within the Application that can be built using the no-code App Designer. The Application starts with a Landing Page, which acts as the main home page and is the first page to open when a user opens the application. Users can edit Pages or build the content on the webpage by selecting and dragging Blocks such as controls or text onto a Canvas. An application can then be launched in order to see a preview of what the Application would look like during runtime. Pages can also be configured to display real-time or contextual data directly from a source of data, such as a database, Data Stream, or Recommendation. This can be done using Data Integration. Pages can also be configured to navigate to other pages, where data can also be passed between pages using parameters. See the Page article for more information on pages. Versions Different versions of Applications can be made to keep track of any major or minor changes made. Versions of an application can be opened by selecting the 'Versions' tab when editing the application. Viewing a particular version will open the application for that version. Copying a version allows you to continue working and making changes to the application while maintaining a version of it before you made changes, which can also act as a backup mechanism. See the Version article for more information on versions. Access An Application can be shared with other users. The person who originally created the Application will be listed as the owner and can never lose his/her right to access it unless it is deleted. When the application is shared with another user, the person sharing it can decide whether to give them co-owner, read, or write access to the application. See the Manage Access article for more information on managing access. Template An Application can be saved as a Template which can then be used again by you or other users when creating a new Application. The Template will show as a pre-designed application that can be selected when configuring the layout of the new App. See the Templates article for more information on Templates. Company Landing Page A Company Landing Page can be set for what Application Landing Page the user will see when they first open App Designer. You can choose to set a Company Landing Page for both Desktop or Mobile. You can select a different App for Desktop and a different App for Mobile. See the Manage Landing Pages article for more information on Templates. Actions on the Application Action Description Properties Opens the properties for the selected App. Also allows you to Delete the App. App Data Opens the App Data section. See the Data Integration article for more details about Data Integrations. Publish/Unpublish Publishes or unpublishes the Application. Manage Access Allows you to manage which users are allowed to view or modify this Application. Versions Versioning for the Application. Save Template Saves the current Application as a Template to create new Applications from. See the Templates article for more information on templates. Notes Opens the Notes, which allows you to add notes about the Application for collaboration and future-proofing. Export Export the Application. Clone Clone the Application App Files Opens the App Files, which allows you to manage files uploaded to this app. See the App Files article for more details about App Files. Save Saves any changes made to the Application up to this point. Discard Discards any changes made to the Application up to this point. Delete Deletes the Application. Add Page Allows you to add and import a Page to the Application. The Add Page button is located at the top right corner of the App Pages list. See the Page article for more information on Pages. Further Reading How to Create and Manage Applications How to Create and Maintain notes How to Manage Themes How to Set a Company Landing Page"
  },
  "docs/concepts/application/metablocks.html": {
    "href": "docs/concepts/application/metablocks.html",
    "title": "Metablocks | XMPro",
    "summary": "Metablocks New XMPro block release XMPro's Metablock lets users embed custom HTML, JavaScript and CSS directly into an AD App Page. This allows for the creation of highly tailored and interactive components. By connecting these blocks to your own data sources, you can develop solutions that seamlessly integrate with XMPro Data streams and data sources. Metablock offers a way to extend the functionality of XMPro, enabling advanced customization and real-time data interaction. Key Features Custom Embedding Embed custom HTML, JavaScript and CSS to create unique components within your app pages. Data Source Integration Connect Metablock to various data sources to enhance data interaction and visualization. Customizable Key-Value Pairs users can define their own key-value pairs and seamlessly utilize them within their application. Advanced Customization Leverage the flexibility of Metablock to tailor the functionality and appearance of your applications to meet specific requirements. Use Cases for Metablock A small list of potential examples of the use of a Metablock includes: Specialist graphing Rendering 3D images from apps like AutoDesk or Unity Collaboration boards like Excalidraw or Miro Gen AI Interfaces Live vision Monitoring and Visualization tools to offer an alternative to grids Future Capabilities Metablock is set to evolve with exciting new features and enhancements. Our goal is to empower users by integrating familiar tools and providing an enriched custom experience. Summary In essence, Metablocks is a flexible and customizable tool that improves application development with advanced visualizations and integration features. Its evolving capabilities allow users to create and manage custom Metablocks easily, keeping applications adaptable and up-to-date. Focused on user flexibility and smooth integration, Metablocks is a key asset for modern application development."
  },
  "docs/concepts/application/navigation-and-parameters.html": {
    "href": "docs/concepts/application/navigation-and-parameters.html",
    "title": "Navigation and Parameters | XMPro",
    "summary": "Navigation and Parameters It is possible to allow navigation between Pages of an App by configuring the Action of a Block, and pass data to the Page by configuring the Pass Page Parameters option. You can add Parameters to the Page you are navigating to, and configure what value should be passed to them when the button is clicked. You can choose Dynamic or Static values. Parameters may be needed if you want to send particular values onto another page as the user clicks on it. For example, if you have a list of machines, and a user selects one, the application may open a new page that displays information for that particular machine. In that case, you may want to pass the ID of the machine the user clicked on to the page that is being opened. ![](/docs/images/image (1433).png>) ![The page will be navigated to with 14 as the value of the Id Parameter ](/docs/images/image (647).png>) You can add and edit Parameters for the current Page in the Page Data tab. ![](/docs/images/image (176).png>) Parameters have a Type that restricts what type of data can be sent to the Parameter. The options for Type are Boolean, DateTime, Double, Int, Long, and String. The following Blocks allow Action: Box Hyperlink Button Chart Esri Map Data Grid Hyperlink Indicator List Map Templated List Toolbar Tree Grid Tree List Further Reading How to Navigate between Pages How to Pass Parameters Between Pages"
  },
  "docs/concepts/application/page-layers.html": {
    "href": "docs/concepts/application/page-layers.html",
    "title": "Page Layers | XMPro",
    "summary": "Page Layers Blocks are organized on the canvas in a hierarchy. The Page Layers tab in the toolbox displays a tree list representation of the hierarchy. This can make selecting blocks simpler and easier as some applications may have many nested layers and it can be difficult to select them on the canvas itself. This allows you to find the exact block you need to manipulate. Parent Blocks can be expanded to view their nested Child blocks underneath. See the Canvas article for more details on the canvas hierarchy. The visibility of the block and the location of the blocks on the canvas can also be moved using this Page Layers list. A Block on the canvas will be outlined if it is selected. If the Page Layers tab is opened, the list will also update to show where the block is on the hierarchy. The number on the right shows how many children blocks the selected block has. Actions on the Page Layers Action Description Change visibility Changes the visibility of the selected Block. If changed to hidden, the Block will not be seen on the canvas or when the page is launched. Expand Expands a selected Block to see the children. Collapse Collapses a selected Block's children. Select Selects a Block. The selected block will be outlined on the canvas. Drag/Move Moves the Block to a different location. This will also change the Block's location on the canvas. Rename Renames the outline surrounding the Block. The Block's name is not shown anywhere when in view mode, it is purely for the sake of clarity in edit mode. Further Reading How to use Page Layers"
  },
  "docs/concepts/application/page.html": {
    "href": "docs/concepts/application/page.html",
    "title": "Page | XMPro",
    "summary": "Page A Page is a web page built with XMPro's No Code App Designer. Applications can have one or more Pages in them and can navigate and pass data between Pages. One of the Pages is marked as a Landing Page, which means it functions as a home page and is the first page that will be navigated to when opening an App. Pages can be edited by clicking on a page in the App Pages list while editing an Application, or by clicking the edit button in the top right of a page while viewing an Application if you have edit access. When editing a Page you can click and drag the grey header to scroll left and view the list of Pages for the whole App. The Page editor has the following sections: Area Description Blocks and Properties You can switch between the tabs by clicking on the commands at the top. You can drag and drop blocks from the Blocks (Toolbox) tab into the Canvas, customize Block styling and properties from their respective tabs, manage Parameters, Variables, and Data Sources from the Page Data Tab, and see a hierarchical view of the Canvas in the Page Layers tab. For more details on each concept, visit the Page Layers article, Blocks article, Block Styling article, Navigation and Parameters article, Variables and Expressions article, and Data Integration article. Actions Actions can be performed on the page by clicking on commands here. The commands in the middle of the command bar switch between different screen widths and apply a media query to any Block Styles applied while selected Devices The commands in the middle of the command bar switch between different screen widths or Devices and apply a media query to any Block Styles applied while selected. See the Devices article for more details on Devices. Canvas The Canvas is a no-code drag and drop editor for creating customized web pages. UI elements and blocks such as text boxes, graphs, or other various controls can be dragged onto the Canvas from the toolbox to build the contents of the Page. See the Canvas article for more detail on the canvas. Theme You can select from two themes for Pages: Light and Dark. An example of each theme is shown below. Light Dark Page Layouts When a Page is created there is a choice between 12 different Page Layouts. The layouts have built-in responsive styles and reshape into a single column in a smaller screen layout. The layouts are as follows: Page Layout Order of Cards on Mobile Responsive Page Layout Example Desktop Mobile Actions on the Page Action Description Save Saves any changes made to the Page up to this point. Discard Discards any changes made to the Page up to this point. Undo Undo any changes made to the Page up to this point. Redo Redo any changes made to the Page up to this point. Show Borders Switches the Canvas between two modes: Bordered, in which every block has a dotted outline around them (invisible when viewing the Page), and Unbordered, in which blocks do not have outlines and are shown as they will be when viewing the Page. Export Exports the Page layout. Does not include Data Sources. Settings Opens the Page Settings, where you can change the Name and Theme, and delete the page. Launch Launches the page as it will be viewed in the Application. Delete Deletes the Page. Further Reading How to Create and Manage Pages How to Import an App Page How to Design Pages for Mobile How to Navigate Between Pages How to Pass Parameters Between Pages"
  },
  "docs/concepts/application/template.html": {
    "href": "docs/concepts/application/template.html",
    "title": "Template | XMPro",
    "summary": "Template Templates are pre-designed Applications that can be selected when creating a new Application. Templates can be used to save time without having to build a whole new app layout from scratch, and also allow you to create a consistent theme or design that you can use across all your Applications. The base Template that the App Designer starts with is the Blank App. The Blank App creates an Application with one Landing Page and provides a choice of Page Layout the same as creating a new Page. Other templates are designed and saved by a user and have whatever page layouts the Application had when it was saved as a template. Templates can be searched by name, and filtered by category. When you click a Template it will give you a preview with screenshots, Name and Description, and a list of Pages that the Template contains: Templates are created by clicking Save as Template on an App. This will create a Template that has the Pages of the App. The Thumbnail screenshot will be shown on the New App page, and all the screenshots will be visible after clicking on the Template on the New App Page. Note Please note that any Data Sources on the Pages will not be saved into the Template. Existing Templates are managed by clicking the Templates button in the left menu. Finding Templates The search bar can be used to find any specific Templates that you may be looking for. There is a dropdown option where you can specify to search through everything in App Designer, or only for Templates. Actions on the Template Action Description Save Saves any changes made to the Template up to this point. Discard Discards any changes made to the Template up to this point. Delete Deletes the Template. View Navigates to the Template view. Export Exports the Template. Further Reading How to Create and Manage Templates"
  },
  "docs/concepts/application/variables-and-expressions.html": {
    "href": "docs/concepts/application/variables-and-expressions.html",
    "title": "Variables and Expressions | XMPro",
    "summary": "Variables and Expressions Variables Variables are a way to store a value for later use, or, when in Expression mode, to calculate a value from other Variables, Parameters, user input, data from Data Sources, and various Functions, Constants, and Operators. Variables are managed through the Page Data tab in the page editor. They have two modes - Value and Expression. In Value mode, the Variable takes the latest changed value of the Block's value. In Expression mode, the Blocks' values are overridden by the calculated value of the Variable. ![](/docs/images/image (982).png>) Expressions An Expression is an extra column on a Data Source that calculates its value according to the designed expression. ![](/docs/images/image (1867).png>) Quick Expressions A Quick Expression is a quick one-off expression you can assign to a Block Property. To access Quick Expressions, cycle through the property's modes with the button on the left until it becomes an Expression. ![](/docs/images/Cycle property mode.gif>) ![](/docs/images/image (746).png>) Expression Editor The Expression Editor is where an Expression is built. At the top is a text area in which you can type the expression. Below the text area are three sections - the categories, the Expression terms, and the description areas. Clicking on a category will show different items in the Expression terms area, and clicking on an Expression term will show a description of the term in the description area. Double-clicking an Expression Term will enter that term in the text area at the position of the cursor. ![](/docs/images/Untitled Project.gif>) Variables Example In the following example, the Circular Gauges take their value from Variables. There are three variables - A, B, and C. A and B are in Value mode, and C is in Expression mode with the following expression: {Variable.A} + {Variable.B} ![](/docs/images/Variables Example.gif>) Further Reading How to Create and Manage Variables and Expressions"
  },
  "docs/concepts/category.html": {
    "href": "docs/concepts/category.html",
    "title": "Category | XMPro",
    "summary": "Category Overview A Category is a container that groups related Data Streams and Applications. Categories are shared between core XMPro Products to provide a homogenous environment. They are displayed as cards on the home page of the Data Stream Designer and App Designer. Clicking on the card will navigate to a dashboard showing the Data Streams or Apps associated with that Category. Categories are useful as they allow you to group your Data Streams, Applications, or other XMPro Objects into logical areas. Organizations have the flexibility to create Categories based on their specific requirements such as the organizational structure of their asset hierarchy. Finding Categories The search bar can be used to find any specific Categories that you may be looking for. There is a dropdown option where you can specify to search through everything in App Designer, or only for Categories. Note Image placeholder: Search Categories Empty Categories A Category that doesn't have any Apps or Data Streams inside it will not be shown on the dashboard. App and Data Stream Indicators An indication of the state of the items contained within a Category is shown at the bottom of each tile. What the numbers represent is as follows: The number of published Apps or Data Streams. The number of unpublished Apps or Data Streams. Note Image placeholder: App and Data Stream Indicators Default Category The Data Stream Designer or App Designer will have one Category defined after it is initially installed, called \"My Sandbox\". This Category is used as a default Category as there is a requirement for all Data Streams or Apps to belong to a Category. In terms of the Application, no Data Stream or Application is allowed to exist if it doesn't belong to a Category. After installing Data Stream Designer or App Designer, you can create any Category you like. However, when the default Category is left empty (not containing any Data Streams or Apps), it will not display on the landing page after at least one other Category is created. You will still be able to see it on the Categories page and create Data Streams or Apps in it. Once you've created a Data Stream or App in it, it will show up on the landing page once again. Note Image placeholder: Default Category Deleting a Category If you delete a Category, all of the Apps or Data Streams inside of that Category get put into My Sandbox. Order The way Categories are displayed on the list can be reordered. This can be done by selecting the reorder option and selecting and dragging the Categories to reorder them. Note Image placeholder: Category Reordering 1 Note Image placeholder: Category Reordering 2 Further Reading How to Create and Manage Categories"
  },
  "docs/concepts/collection.html": {
    "href": "docs/concepts/collection.html",
    "title": "Collection and Stream Host | XMPro",
    "summary": "Collection and Stream Host Note It is recommended that you read this documentation along with the articles listed below to improve your understanding of how Stream Hosts and Collections work. Agents Data Stream Processing Overview Watch the video below for an introduction to Stream Hosts and Collections, and their role in enabling scalable real-time data processing. Whether you're overseeing a few assets or thousands, stream hosts and collections provide the flexibility, efficiency, and scalability needed to transform your data operations. Video 1: How XMPro Stream Hosts and Collections Enable Scalable, Real-Time Data Processing Stream Hosts A Stream Host is an application that can be installed as a Docker container, a Windows Service, or a Console Application. Stream Hosts enable Data Streams to run and execute actions and are also responsible for getting the configurations of Non-Virtual Agents. In other words, the Stream Host application needs to be running for Agents in a Data Stream to process the data, according to the design of the Stream. Both Virtual and Non-Virtual Agents essentially run on a Stream Host, but how they are handled at design time differs greatly. Non-Virtual Agents require a Stream Host to be online for Data Stream Designer to be able to get their configurations, but Virtual Agents don't need a Stream Host to be running. Collections Stream Hosts are grouped into different Collections, which are created and maintained in Data Stream Designer. A Collection can be defined as a Category that contains a set of Stream Hosts that run the same Data Streams. Each Stream Host is associated with a Collection by keeping the ID of the Collection in the appsettings.json file of the Stream Host, which can be found in the location where the Stream Host has been installed. Thus, all the Stream Hosts that have the same Collection ID stored in this file, will automatically fall under the Collection that has that ID. After creating a Collection within Data Stream Designer, you can associate a Data Stream with a default Collection. Unless changes are made for individual Agents in your Data Stream, each Agent will use this Collection to perform the actions it was designed to perform. However, Data Stream Designer allows you to build a stream in which some Agents use one Collection and others use a different Collection. For more information on how to configure this, click here. Along with the Collection ID, the appsettings.json file contains other configurations for the Stream Host. Each of these configurations are listed and described in the table below. Property Description ID Uniquely identifies the Stream Host. Name Name of the Stream Host. Collection ID ID of the Collection associated with the Stream Host. Rank Indicator of the preference given to the Stream Host. This especially applies when more than one Stream Host with the same Collection ID is running. The Stream Host with the lowest rank will be used to get the configurations of Non-Virtual Agents. The rank has a default value of 0. See Stream Host Rank. Secret Used to verify the connection between the Stream Host and the Collection. This field needs to correspond to the Key that is stored for a Collection in Data Stream Designer. If this key is revoked/replaced, all Stream Hosts will be disconnected. Server URL URL that the Stream Host needs to use to connect to Data Stream Designer. Cryptography key Key that the Stream Host will use to encrypt or decrypt secure user settings, for example, a SQL Server Password. The table below contains a list of the configurations stored for a Collection along with a description for each. This information can be found by opening the Collections page from the left-hand menu in Data Stream Designer and selecting your Collection from the list. Please note that the only field you will not be able to change is the ID of the Collection. Property Description ID Uniquely identifies the Collection. Key Used to verify the connection between the host and Data Stream Designer. If this key is revoked/replaced, all Stream Hosts will be disconnected. Name Name of the Collection. Remote Receiver See Remote Receivers and Publishers. Remote Publisher See Remote Receivers and Publishers. Metadata Tags that have been added to the Collection. Note Image placeholder: Collection Configuration Publish and Unpublish Data Streams On the Collections page, click More and Data Streams to view a list of all Data Streams that are using a given Collection. Here you can directly unpublish or publish a Data Stream. As an Admin, this is useful if you need to unpublish a Data Stream and you don't have access to it. See the How to Admin Unpublish Override article for more details. Note Image placeholder: Publish/Unpublish Data Streams Finding Collections The search bar can be used to find any specific Collections that you may be looking for. There is a dropdown option where you can specify to search through everything in Data Stream Designer, or only for Collections. Note Image placeholder: Search Collections Actions on the Collection Action Description New Adds a new Collection. Select Selects multiple Collections. Delete Deletes the selected Collections. Save Saves any changes made to the Collection up to this point. Discard Discards any changes made to the Collection up to this point. Stream Hosts Opens a new page with a list of hosts that have installed this Collection. Connection Profile Download the Connection Profile used in the installation of a Stream Host. Download Host Download the installation file for the selected platform. Variables Manage the Collection's Variables. Data Streams Opens a new page with a list of all Data Streams in the selected Collection. Revoke Key Revokes the current key and will generate a new one. Hosts that are using the revoked key will be unable to connect until their configuration is manually updated. Delete Deletes the selected Collection. Remote Receivers and Publishers The Remote Receiver and Remote Publisher properties are used to bridge the gap between Agents in a stream that is associated with different Collections, for example: Consider having a stream that has two Agents: Listener_A, which is associated with Collection_A, and Action_Agent_B, which is associated with Collection_B Listener_A needs to communicate to Action_Agent_B, but they belong to different Collections. As a result, the engine will automatically use the Remote Receiver and Publisher to pass data from one Collection to another. Please note that only MQTT is currently supported for this functionality. Example: Consider a stream that has the following Agents: Event Simulator Event Printer For the purpose of this example, the Event Simulator will be named ES_1 and the Event Printer EP_1, where ES_1 is using Collection_A and EP_1 is using Collection_B. Since these two Agents are using two different Collections, data flow from ES_1 to EP_1 is not possible as there is no physical connection (the Stream Hosts in these Collections can be on different machines in different parts of the world). To solve this problem and allow data flow from ES_1 to EP_1, the Collection EP_1 is using needs to be set up as follows: The Remote Receiver for Collection_B needs to be set to MQTT and the broker address needs to be specified. The Remote Publisher for Collection_B needs to be set to MQTT and the broker address needs to be specified. When the stream is published now, the data simulated by ES_1 will be published to MQTT, which will enable the engine to get the data EP_1 expects by listening for it on MQTT. Thus, data flow between ES_1 and EP_1 has been established. Note Image placeholder: Remote Receivers and Publishers Example 1 The below image is an example of the configuration for Collection_B. Note Image placeholder: Remote Receivers and Publishers Example 2 Connection Profiles Overview A Connection Profile is a file, containing certain details about a Collection, that can be downloaded from Data Stream Designer. The purpose of this file is to make the installation process of Stream Hosts easier by providing you with an option to upload this file to the Stream Host installer instead of manually specifying the Collection information. This file contains the values described in the table below. Property Description Device Name Name of the Stream Host. This name will be the same as the device name that needs to be specified when downloading a Collection Profile. If you install the Stream Host as a Windows Service, the service name and device name will be the same. Collection ID The ID of the Collection the Stream Host will be associated with. Secret Used to verify the connection between the stream host and the Collection. If this key is revoked/replaced, all stream hosts will be disconnected. Server URL URL that the Stream Host needs to use to connect to Data Stream Designer. This is the URL of the instance of Data Stream Designer that has been installed on the server. Key Key used to encrypt or decrypt user settings. Download a connection profile To download an installation profile, follow the steps below: Open the Collections page from the left-hand menu. Select the Collection you would like to install the Stream Host for. Click Connection Profile. Enter a Device Name. Click OK. The download will start automatically. Note Image placeholder: Download Connection Profile Stream Host Installation To install a Stream Host, please refer to these instructions. Set Log Level The Log Level determines the type of information or level of detail that will be logged. The two Log Levels are Trace and Info. Info The Info Log level is the default Log Level. This will only log Info level messages or logs, which include details on when a Stream Host is starting or stopping. Any errors related to Connections or the Stream Host itself will also be logged at this level. Trace The Trace Log level will log more detailed messages and errors about the Data Stream or specific Agents themselves. This is very useful for debugging as it can log information about what is happening inside the code itself, such as which methods are being called and when. Using the Trace Log Level for a long period of time can take up a lot of space on the hard drive. Selecting the Trace Log Level will log both Trace and Info Level Logs. Note Image placeholder: Set Log Level Stream Host Rank All the Stream Hosts contained in a Collection will be used to run the Data Streams associated with that Collection. However, you can choose which Stream Host should be responsible for getting the configurations of Non-Virtual agents. The rank given to a Stream Host defines what preference the Stream Host will be given in such a scenario. The Stream Host with the lowest rank will be picked by the system. All Stream Hosts will always have a default rank value of 0. Example Consider having a Data Stream with the following Agents: MQTT listener Filter transformation SQL Server Writer The purpose of this Data Stream is to get temperature readings from a sensor on a machine, filter the readings that are higher than 120°C and write these values to a table in a SQL Server database. All three agents are configured to use Stream Hosts in the \"Mine 09\" Collection to run (see image below). However, to get the user settings for the SQL Server Writer Action Agent, which is a Non-Virtual Agent, Data Stream Designer will pick and use the first Stream Host available. This can be changed by setting the rank of the Stream Host you want to use to be lower than the other Stream Hosts that are available. Note Image placeholder: Stream Host Rank Example 1 Note Image placeholder: Stream Host Rank Example 2 Further Reading How to Create and Manage Collections"
  },
  "docs/concepts/connector.html": {
    "href": "docs/concepts/connector.html",
    "title": "Connector | XMPro",
    "summary": "Connector Overview A Connector is a pre-built integration plug-in for the XMPro App Designer that allows you to connect to third-party data sources without writing code. They can be used when integrating data from data sources such as a database into an Application. This can include real-time data from machines, websites, other streaming data, or contextual data such as the make or model of a certain item. Connectors are useful if you want to use the data within an Application or display the data to the user on a page of an Application. Data Sources can include databases, Data Streams, or Recommendations. See the Data Integration article for more details. You can assume that Connectors have already been set up by your Administrator. Each Connector consists of code, settings, and other properties that are packaged into a file that can be uploaded to App Designer. XMPro has a library of Connectors available to use. To acquire any of these Connectors, please contact your XMPro sales representative or write to us at support@xmpro.com. Alternatively, since Connectors can be written by anyone that has some knowledge of programming and has access to the required technologies, you can write your own Connector by following these instructions. Category Connectors can be grouped into categories. This category is separate from the App and Data Stream Categories. Settings A Connector consists of code and user settings. The code defines the actions a Connector performs in App Designer. The settings are the input for the code that executes, provided by the user when adding the Connector to App Designer, such as authentication credentials. For example, consider the SQL Server Connector, which retrieves data from a SQL database. The settings a user must define for the Connector so it can do that are as follows: Name of the SQL Server instance SQL Server username Whether SQL Server authentication should be used or not SQL Server password Database to which the data should be written Finding Connectors The search bar can be used to find any specific Connectors that you may be looking for. There is a dropdown option where you can specify to search through everything in App Designer, or only for Connectors. Note Image placeholder: Search Connectors Adding a Connector Connectors can be added by uploading their XMP file on the connectors page. Each version of a Connector has its own XMP file. When uploading an app, any missing Connectors that the app needs must be updated or uploaded for the app to be imported successfully. Note Image placeholder: Adding a Connector 1 Note Image placeholder: Adding a Connector 2 Versions Connectors can have multiple versions. For example, a new version of a Connector can be created if there are any changes or updates made to it. Changes or updates made to new versions of a Connector will not affect the previous versions. Each version of a Connector has its own XMP file. When uploading a Connector, a specific version of that Connector will be uploaded. To upload a different version, you will need to upload the XMP file for that specific version you want to use. When you view a Connector, you can see the list of specific versions for that Connector. The number of apps using each version is shown next to the version number. Apps that you do not have access to will not show on the list. When you upload a new version, the new version will be displayed in this list. When selecting a Connector to use in an app, the application will automatically choose the latest version available. Note When a new version of a Connector is uploaded, any apps using older versions may need to be upgraded. See the Data Integration article for more information on how to upgrade an app's connection. Note Image placeholder: Connector Versions View and Edit Applications There is an alternate way to view or edit Applications directly from the Connectors page. When viewing a Connector, you will see the number of Applications using each version of the Connector. Expand to see the full list of Applications, which can be viewed by clicking on the Application or edited by clicking the Edit button. Note Image placeholder: View and Edit Applications Publish and Unpublish Apps You can also publish or unpublish an Application that uses that version of the Connector. As an Admin, this is useful if you need to unpublish an Application and you don't have access to it. See the How to Admin Unpublish Override article for more details. Note Image placeholder: Publish and Unpublish Apps Actions on the Connector Action Description Add Adds a new Connector. Select Selects multiple Connectors. Delete Deletes the Connector. Manage Categories Creates and edits categories to organize the Connectors. These categories are separate from the App and Data Stream Categories. Save Saves any changes made to the Connector up to this point. Discard Discards any changes made to the Connector up to this point. Delete Versions Deletes a selected version of a Connector. You can only delete versions that don't have any apps using that version of the Connector. If you don't have access to an app that a version is using, you still cannot delete the version. Further Reading How to Create and Manage Connectors"
  },
  "docs/concepts/data-stream/index.html": {
    "href": "docs/concepts/data-stream/index.html",
    "title": "Data Stream | XMPro",
    "summary": "Data Stream A Data Stream is a visual display of data flow, letting you integrate and connect various systems or data sources to view your information. This includes real-time data from machines, websites, or other streaming sources, as well as contextual data like an item's make or model. The Data Stream Designer lets you see all connected data in one place and manipulate it in different ways, such as aggregating, filtering, displaying, or saving to another database. You can monitor data to spot critical events like machine failures, without needing coding skills. Video Presentation Discussing Data Stream Designer Data Stream's flow is represented by agents connected by arrows, enabling data processing at each agent based on its function, allowing you to view and perform actions on data from multiple sources in one place. Note We recommend reading this documentation in conjunction with the Agent article to enhance your understanding of data streams. Finding Data Streams Use the search bar to find specific Data Streams, and select from the dropdown menu to search either throughout Data Stream Designer or only for Data Streams. Data Stream Type The Type selected in the Properties affects your choice of how often and when polling-based Agents run. Streaming (Default): This allows you to specify a Polling Interval in seconds on polling-based Agents in the Stream Object configuration. Polling starts immediately and continues indefinitely. Recurring: This allows you to specify granular options to schedule polling-based Agents in the Stream Object configuration. Options allow you to control the start time, repetition interval, and end time or number of repetitions. There is also the option to repeat indefinitely. Agent Event Queue Capacity v4.4.17: Replaced 'Event Buffer' for clarity in understanding how many events are buffered. The Data Stream includes an advanced option to manage high event volumes, or complex stream configurations. The Agent Event Queue Capacity property allows for a higher number of events to be queued for each Stream Object within the Data Stream reducing the risk of event loss. By default, the Agent Event Queue Capacity is set to 128 events per Stream Object. Warning Increasing the Agent Event Queue Capacity may result in the Stream Host consuming more memory. The Canvas Data streams are crafted in an interactive canvas, allowing you to drag agents from the toolbox to the drawing area. In the same environment, configure agent settings, manage versions, define business cases, and more. Stream Objects on the canvas can be duplicated or deleted, and for easy identification and access, they're marked with a unique color and abbreviation. The Toolbox The toolbox is a component in the Data Stream Designer that allows you to choose an Agent from a library of Agents that have been uploaded to the system and drag the agent from the toolbox to the Canvas when building a stream. All agents in the toolbox will always be the latest version and will all be available for all users, regardless of their role, unless a user does not have permission to view any of the Agents. Agents are grouped by category in the toolbox. To expand a category, click on the arrow next to the category name. Please note that if no agents in a particular category are uploaded, the entire category will be hidden. Thus, if you are missing some of the categories in the toolbox, it is likely that there aren't any Agents uploaded belonging to that category. Building a stream Note See the Manage Data Streams article for detailed instructions and an example of how to build your first stream. Actions on the Data Stream Action Description Save Saves any changes made to the Data Stream up to this point. Discard Discards any changes made to the Data Stream up to this point. Properties Opens the properties for the Data Stream. Also allows you to Delete the Data Stream. Business Case Opens the Business Case for the Data Stream. Notes Opens the Notes, which allows you to add notes about the Application for collaboration and future-proofing. Versions Versioning for the Data Stream. Timeline Opens the Timeline, which shows a timeline of the history of the Data Stream. Manage Access Allows you to manage which users are allowed to view or modify this Data Stream. Integrity Check Verifies if the agents in your stream are configured correctly. Logs Allows you to check for log messages for the Stream Host being used to run the Data Stream. Error Flow Toggles between being able to view and configure the default error Agent for the Stream, which collects all the errors from all agents with an Error endpoint. Publish/Unpublish Publishes or unpublishes the Recommendation, which makes it start listening for data from the Data Stream. Configure (Agent) Opens the configuration page for the selected Stream Object. Upgrade (Agent) Upgrades the selected Stream Object to the latest Agent Version. Only available if the selected Stream Object's Agent is not the latest version. Delete (Agent) Deletes the selected Stream Object from the canvas and Data Stream. Help (Agent) Opens the help page for the selected Stream Object's Agent. Delete Deletes the Data Stream. Export Export the Data Stream as an encrypted file. Clone Clones the Data Stream as a new Data Stream. Further Reading How to Create and Manage Data Streams How to Manage Recurrent Data Streams How to Troubleshoot a Data Stream"
  },
  "docs/concepts/data-stream/running-data-streams.html": {
    "href": "docs/concepts/data-stream/running-data-streams.html",
    "title": "Running Data Streams | XMPro",
    "summary": "Running Data Streams Running Data Streams Data Streams need to be running to start performing the functions they have been designed to do. When you click on the \"Publish\" button, your Stream will start running. As soon as this button is clicked, the engine will look at which Collection each Agent in the Data Stream is associated with. It will then look at which Stream Hosts are available for use in each Collection and use those Stream Hosts to allow Agents in that Collection to execute actions. To read more about how Stream Hosts and Collections work, click here. To verify which collection an Agent will use, click on the \"Configure\" button. Viewing Live Data The Live View functionality in Data Stream Designer allows you to view data as it is processed by the Agents in your Stream. For each Agent in your Stream, you can let the data display in either a grid, gauge, or chart. To view this data, select the \"Live View\" button after publishing your Stream. Next, select the Agents you would like to view the data for. Note Pre v4.3.7, users should close the Live View before navigating away to signal to the Stream Host to stop sending the data. Further Reading How to Use Live View"
  },
  "docs/concepts/data-stream/stream-object-configuration.html": {
    "href": "docs/concepts/data-stream/stream-object-configuration.html",
    "title": "Stream Object Configuration | XMPro",
    "summary": "Stream Object Configuration Data Stream and Agent Configuration User Settings All Agents will individually be associated with a Collection. This Collection may or may not be different from the default Collection set for your Data Stream. This setting will always be listed among the rest of the user settings. There are a number of Agents that do not require any settings, for example, the Event Printer Agent. This Agent simply prints events and you do not need to specify settings such as a server URL, username, password, or upload a file. Other Agents, however, require settings to be filled in before you can successfully run the stream. For example, consider having a CSV listener Agent in your Data Stream. The CSV listener Agent will require you to specify the following values: Specify a polling interval (seconds) Upload a CSV file Specify the CSV definition (name of each column in the CSV file along with what data type the values in each column are) If these values have been provided correctly, the data will be read from the CSV file you specified when you publish your stream. Input Mapping and Arrow Configuration Some Agents allow inputs to be mapped if the Require Input Map property has been set to true during packaging. What Input Mapping allows you to do is to specify that a specific Agent receives its input in a specific structure. This causes the arrows leading to an Agent to be made configurable and will allow the user to map the inputs of an agent to incoming attributes, for example: Consider having the following Agents in a stream: CSV Listener SQL Server Writer The CSV Listener is configured to get data from a file that contains the following headings: Timestamp (of type DateTime) ReadingNo (of type Long) Temperature_A01 (of type Double) Vibration_A01 (of type Double) Result (of type Double) The task the SQL Server Writer needs to perform is to write the data it receives to a SQL Server database, but it expects the structure of the data to be in a specific format. The table we need to write the data to has the following columns: ID (bigint, identity column) ReadingNo (bigint) Temperature (float) Vibration (float) Results (float) Timestamp (datetime) For the data to be written to the database in a specific format, you need to map the correct columns in the CSV file to the correct SQL Server table columns. To do this, both the CSV Listener and the SQL Server Writer Agents need to be configured first. To configure an Agent, click on the Agent and then on the “Configure” button. Fill in all the details required, for example, the SQL Server instance name and credentials. Next, you can go ahead and configure the Input Mapping by clicking on the arrow that connects the CSV Listener and the SQL Server Writer Agents. Then, click on “Configure“. Choose which column should be mapped to which column by selecting the correct value from the drop-down menu for each row. Please note that the data types of the items being mapped to each other need to be the same. If not, the value in the left column will be disabled and you will not be able to select it. Remember that, even though the same principle applies to all Agents, input mapping might be done differently for different Agents. Mapping Functions In some cases, you might have to map a large number of inputs for an Agent. Some functions have been implemented to make the process of mapping a large number of fields easier, such as Match by Expression, AutoMap, and Show Unmapped. AutoMap By clicking on the “AutoMap” button, Data Stream Designer will match all the fields that are common between the Agents involved, for example, if you look at the stream in the image below, you will notice that both the SQL Server Writer Agent and the CSV Listener has the fields listed below in common, which will automatically be mapped if they have the same data type. ReadingNo Timestamp Match by Expression The Match by Expression function allows for an expression to be used to make mapping a large number of fields easier and quicker. The fields can be mapped by using any of the following options: Prefix Postfix Expression The Prefix option allows you to specify that columns should be matched based on the first part of a column name, for example: In the CSV listener Agent, there is a column named “A01_Temperature“ In the SQL Server Writer Agent, there is a column named “Temperature“ In the images below, “A01” is specified as the postfix. Based on the prefix given, the column “A01_Temperature” in the CSV Listener can be matched to the column “Temperature” in the SQL Server Writer Agent. The Postfix option allows you to specify that columns should be matched based on the last part of a column name, for example, In the file uploaded to the CSV listener Agent, there are columns named “Temperature_A01“ and “Vibration_A01“ In the table referenced in the SQL Server Writer Agent configurations, there are columns named “Temperature“ and “Vibration“ In the images below, “_A01” is specified as the postfix. Based on the postfix given, the columns “Temperature_A01“ and “Vibration_A01“ in the CSV Listener can be matched to the columns “Temperature“ and “Vibration“ in the SQL Server Writer Agent. The Expression option allows you to use a regular expression to match the columns, for example, In the file uploaded to the CSV Listener Agent, there is a column named “Device_Temperature_Fahrenheit“ In the table referenced in the SQL Server Writer Agent configurations, there is a column named “Temperature“ In the images below, “Device_$1_Fahrenheit” is used as the regular expression. Based on this expression, the column “Device_Temperature_Fahrenheit” in the CSV listener is mapped to the column “Temperature” in the SQL Server Writer Agent. Show Unmapped The Show Unmapped function allows you to filter the rows displayed, based on if the columns have been mapped. If you chose to filter items based on if they are mapped or not, all the records that haven’t been mapped yet will be listed, for example, In the image below, “ReadingNo” and “Timestamp” have been mapped for both Agents using the AutoMap function. However, there are three records that remain that need to be mapped. In some scenarios, there might be a lot more records with some being mapped and others not. Further Reading How to Manage Input Mappings How to Configure a Stream Object"
  },
  "docs/concepts/data-stream/timeline.html": {
    "href": "docs/concepts/data-stream/timeline.html",
    "title": "Timeline | XMPro",
    "summary": "Timeline History: Timeline Changes to a Data Stream are recorded and stored in the database so that the user can view a history of any changes made by themselves or others. This can therefore be used as a collaboration tool to see the changes users make (even if it's only a single user), as well as notes about things that need to be addressed. These changes can be viewed from the canvas by clicking the \"Timeline\" button. Every time an event occurs, it will be added to a timeline in the form of a block, displaying the following: The time and date the change occurred. The name of the person who made the change. The area where the change was made, for example, \"Tags\". The description of the change. Adding Notes Most of the items added to the timeline are added by the system itself, for example, if the version of the Stream increased or if someone changed a setting or configuration of one of the Agents. However, occasionally you may want to add an explanatory notes to the timeline. Filtering The timeline can be filtered in two ways: the type of event logged and the context. Examples of event types are changes made to Agents or version numbers that have been increased. Each type is named after the element on which the change was made. The types of events that can be filtered are as follows: Agent Attributes Canvas Configure Note Publish Share Tag Unpublish Version You can also filter by context, i.e. you can view all changes, or only the events that apply to a specific version. Canvas changes apply to a version, whereas sharing access or changing attributes apply to the data stream as a whole. Further Reading How to Manage the Timeline"
  },
  "docs/concepts/data-stream/verifying-stream-integrity.html": {
    "href": "docs/concepts/data-stream/verifying-stream-integrity.html",
    "title": "Verifying Stream Integrity | XMPro",
    "summary": "Verifying Stream Integrity Verifying Stream Integrity Data Stream Designer offers the capability to verify if the Agents in your Stream are configured correctly and warns you if there are any issues with them. This is done to ensure the integrity of your Data Stream and to make sure all input fields are valid and accurate. To verify the integrity of your Stream, you can simply click on the “Integrity Check” button. When clicking this button, each agent will call the Validate method in its code. This method contains a set of rules that needs to be satisfied, for example, the SQL Server Agent needs to have the following values specified: SQL Server instance name Username Password (if SQL authentication is used) Database name Table name If any of these values are incorrect or not specified, the Agent will be marked with red, and its code will return a list of errors that it found, which you will see in the form of a list when you hover over the Agent with your mouse cursor. To read more about how the code works, see the Building Agents article. Further Reading How to Run an Integrity Check"
  },
  "docs/concepts/index.html": {
    "href": "docs/concepts/index.html",
    "title": "Concepts | XMPro",
    "summary": "Concepts This section provides detailed explanations of the core concepts in the XMPro platform. Understanding these concepts will help you build effective real-time applications. XMPro AI XMPro AI provides AI capabilities to enhance your applications, including the XMPro Notebook for data analysis and machine learning. Data Stream Data Streams are the backbone of real-time data processing in XMPro. They allow you to ingest, transform, analyze, and act on data from various sources. Collection and Stream Host Collections and Stream Hosts manage the execution of Data Streams and provide scalability and reliability. Agent Agents are the building blocks of Data Streams, providing specific functionality for data ingestion, transformation, analysis, and actions. Application Applications provide the user interface for your real-time solutions, allowing users to visualize data and take actions. Recommendation Recommendations enable you to create rules that trigger alerts and actions based on conditions in your data. Connector Connectors provide integration with external systems and data sources. Landing Pages & Favorites Landing Pages & Favorites help users quickly access the information and applications they need. Version Versioning allows you to manage changes to your Data Streams, Applications, and Recommendations over time. Manage Access Access Management controls who can view and edit your Data Streams, Applications, and Recommendations. Category Categories help organize your Data Streams, Applications, and Recommendations. Variable Variables store configuration values that can be reused across your Data Streams, Applications, and Recommendations. Insights Insights provide analytics and monitoring capabilities for your Data Streams and Applications."
  },
  "docs/concepts/insights/data-delivery-insights.html": {
    "href": "docs/concepts/insights/data-delivery-insights.html",
    "title": "Data Delivery Insights | XMPro",
    "summary": "Data Delivery Insights Stream Hosts Foundational Premise: Streaming Signals for Event Intelligence Back to Basics The premise of a Stream Host is to ingest streaming signals as messages into the Data Stream. The premise for a Data Stream is the capability to ingest a fire hose and achieve the desired outcome even if not all the data elements are delivered. Consider how this contrasts with an ETL system. Neither has a primary goal of guaranteed delivery. What this means If a value (e.g. a temperature reading) in the Data Stream is measured against a rule value and a call to action is created when a threshold is reached, there is no impact if the Data Stream misses some readings. These misses could occur for several reasons, one of which is buffering. There are buffers in the Data Stream, but if they overflow and values are missed, the premise is intact. In scenarios where multiple transactions occur concurrently, the Stream Host processes the first transaction and buffers the next. However, if the buffer reaches maximum capacity and another transaction is received, the Stream Host will drop the subsequent transaction to prevent potential bottlenecks. Guaranteed Delivery – Processing Files There are circumstances where there needs to be a guarantee for the delivery of the data and the ‘dropping’ of data is unsuitable for the use case. In these circumstances, the design should adopt a pattern that ensures auditable delivery. Explanation If there is a single batch, data is split into 80 separate messages by an agent within the Data Stream, however, the next agent processes the data significantly more slowly than the previous agent which causes buffering to occur, with 80 messages coming through almost immediately the SH is unable to buffer some messages and therefore drops them. Recent Improvement The v4.4.0 release includes a Stream Host with superior \"buffer\" functionality. The new Window Agent supports a delay in transmitting events to the next Agent in a Data Stream. This enhancement aims to mitigate the risk of excessive event data buffering within the Stream Host. The flow of events can be regulated to prevent potential overload scenarios, ensuring smoother data processing and system stability. Data Stream Design Considerations and Patterns When discussing the Data Stream design there are other considerations that, by way of example, point to how patterns could be applied as the design depends on the data profile and orchestrating the data. Not all ingested messages are processed Introduce a mechanism to allocate batch IDs to ensure a message is processed. This relies on tracking individual messages outside the Data Stream (e.g. in a SQL table). If configured correctly, this can avoid duplication if a message is re-processed. Another consideration could be to split the Data Stream and use storage (e.g. SQL) to process each message/record individually: Data Stream 1 ingests the message and saves it as a record in a table. Data Stream 2 then processes each record row by row. Introduce a delay Introduce a delay in the Data Stream by including a “delay” Agent during processing to ensure messages are completely processed before a new message is received. Small polling interval When the polling interval of the listener is small (e.g. 5 or 10 seconds), the short time frame usually results in a Data Stream that is less performant. Consider a longer polling interval. A general observation is that when event intelligence monitors for asset failure, subject to certain use cases, a polling interval of 3600 seconds (i.e. one hour) is sufficient to timeously perceive degradation. Meta tags: Data Stream."
  },
  "docs/concepts/insights/index.html": {
    "href": "docs/concepts/insights/index.html",
    "title": "Insights | XMPro",
    "summary": "Insights"
  },
  "docs/concepts/landing-pages.html": {
    "href": "docs/concepts/landing-pages.html",
    "title": "Landing Pages & Favorites | XMPro",
    "summary": "Landing Pages & Favorites Overview A list of categories is displayed by default when you open App Designer or Data Stream Designer. In App Designer, this can be changed to display a custom landing page instead. When a custom landing page is set for a company, any user from that specific company will see that landing page when they first open App Designer. An App can be used as a landing page as long as it is published. This is useful if a company has a main App that they access and prevents them from continuously navigating to that App. Apps and Data Streams can be favorited. This is useful for Apps or Data Streams that are frequently used, and favoriting them will allow them to appear on the main page of Subscription Manager for quick and easy access. Blocks can also be favorited. This is useful for Blocks that are frequently used by a designer, and favoriting them will allow them to appear on the Favorites category of App Designer's Blocks for quick and easy access. Note It is recommended that you read this documentation along with the articles listed below to improve your understanding of how Apps and Data Streams work. Application Data Stream Default Landing Page When you load Data Stream Designer or App Designer, the default landing page lists the categories that contain Data Streams or Apps respectively. Indicators at the bottom of each category card inform how many Data Streams/Apps are: Published Draft Errors (published Data Streams with errors) Click the category card to view all the contents, or on the counter for a filtered view. Note Image placeholder: Default Landing Page Favoriting Apps and Data Streams Apps and Data Streams can be favorited for quick and easy access. When you favorite an App or Data Stream, they will appear on the main page in Subscription Manager. Note Image placeholder: Favoriting Apps and Data Streams 1 If there are no favorited Apps or Data Streams, Subscription Manager will display a list of published Apps and Data Streams instead, if there are any. Note Image placeholder: Favoriting Apps and Data Streams 2 Note Image placeholder: Favoriting Apps and Data Streams 3 If there are no Favorited or published Apps or Data Streams, no Apps or Data Streams will show. Note Image placeholder: Favoriting Apps and Data Streams 4 There are also direct links to allow you to create an App, Data Stream, or Recommendation by clicking on the 'Create New' button. Note Image placeholder: Favoriting Apps and Data Streams 5 Note To learn more about how to Favourite Apps or Data Streams, visit the How to Manage Landing Pages article. Company Landing Page You can choose to set an App as the Landing Page, which overrides the default landing page. The landing page for the App selected will be the first page all users in the company see when opening App Designer. You can select different Apps for Desktop and Mobile. Note Image placeholder: Company Landing Page Note You can only choose from published Apps to be a landing page. To learn more about how to set a Company Landing Page, visit the How to Manage Landing Pages article. Desktop To view the Landing Page for Desktop, open App Designer on a Desktop computer. You can also click on the company icon in the top-left corner. Note Image placeholder: Desktop Landing Page Mobile To view the Landing Page for Mobile, open App Designer on a Mobile device. You can also click on the company icon in the top-left corner. Note Image placeholder: Mobile Landing Page Further Reading How to Manage Landing Pages & Favorites"
  },
  "docs/concepts/manage-access.html": {
    "href": "docs/concepts/manage-access.html",
    "title": "Manage Access | XMPro",
    "summary": "Manage Access Managing the access of users is important as it can improve the security of XMPro Objects. Permissions are given to users only based on what they need to do, for example, someone who only needs to view something can be given read access. This can hide additional functionality that users do not need and also protect against unintentional tampering. Whether or not you can design a particular data stream, application, or other XMPro Objects is determined by the permissions that you have on it. The person that originally created the XMPro Object will be listed as the owner and can never lose his/her right to access it unless it is deleted. Other users can then be assigned read or read-and-write access. Grant Permissions Granting permissions makes it easier for multiple users to collaborate on one particular XMPro Object. It also makes it easier to grant permissions to multiple users who are only allowed to view or read it without making changes, possibly to just give feedback or to discuss part of the XMPro Object. Permissions: Owner The Owner has full permission to make any changes required to the XMPro Object. The owner is also allowed to give other users access or change the permissions of existing users. Permissions: Co-Owner The Co-Owner has full permission to make any changes required to the XMPro Object. The Co-Owner is also allowed to give other users or change the permissions of existing users, except removing or changing the Owner's permissions. Permissions: Write A person who has Write access will be allowed to view, edit, publish and unpublish the XMPro Object. Permissions: Read The Read permission will allow a person to view the XMPro Object, but making any changes to the XMPro Object will be prohibited. Permission Matrix Permission/Operation Owner Co-Owner Write Read View ✓ ✓ ✓ ✓ Publish/Unpublish ✓ ✓ ✓ ✗ Edit ✓ ✓ ✓ ✗ Delete ✓ ✓ ✗ ✗ Manage Versions • View ✓ ✓ ✓ ✓ • Create/Copy ✓ ✓ ✓ ✗ • Delete ✓ ✓ ✗ ✗ Manage Access ✓ ✓ ✗ ✗ Note Image placeholder: Manage Access 1 Note Image placeholder: Manage Access 2 Note Image placeholder: Manage Access 3 Actions on the Manage Access page Action Description Add Adds a user. Select Selects multiple users. Edit Edits the permissions of a particular user. Cancel Discards any changes made to the user's permissions up to that point. Delete Deletes a user. Manage Run Access Warning For Apps, if a User has Design Access they will also automatically have access to run the App in view mode. However, for Recommendations, a User can only view Recommendation Alerts if they or their Business Role has Run Access, regardless of whether they have Design Access to the Recommendation. Note Image placeholder: Run Access Example Business Roles (Company Administrator) Business Roles are a hierarchical representation of the different areas of an organization. When managing access for XMPro Products, the Business Roles defined in Subscription Manager are used. Business Roles are managed by the Administrator of a Company through the Users page. Note Image placeholder: Business Roles Management All new users will automatically be added under the default 'All Employees' Business Role. Users can be moved, but not deleted from this list. A User can only be listed once underneath one Business Role. Note Image placeholder: All Employees Business Role Business Role of a User Alternatively, a user's business role can also be managed from the user blade. See Change Business Role. Note Image placeholder: Change Business Role Sync Business Roles from Azure AD If Azure AD has been linked as your External Identity Provider, you can specify a claim name that Azure AD or the graph API will pass to Subscription Manager. When a user logs in, Subscription Manager will look at the value specified in this Claim and assign them to the Business Role with the same name. See the Sync Azure AD Role to SM's Business Role article for information on how to configure the claim name. Note If a Business Role with the same name doesn't exist, it will be created as a child under the default Business Role, 'All Employees'. Actions on the Manage Run Access page Action Description Save Saves any changes made to the Manage Run Access up to this point. Discard Discards any changes made to the Manage Run Access up to this point. Further Reading How to Manage Access"
  },
  "docs/concepts/recommendation/action-requests.html": {
    "href": "docs/concepts/recommendation/action-requests.html",
    "title": "Action Requests | XMPro",
    "summary": "Action Requests An Action Request is a mechanism to trigger actions in another system while attending to an Alert, such as updating data, sending notifications, or additional processing. This is made possible with the Action Request Agents in the Data Stream Designer. {% hint style=\"info\" %} It is recommended that you read this documentation along with the articles listed below to improve your understanding of how action requests work. Recommendation Alert Form Data Stream Stream Object Configuration {% endhint %} Open an Action Request An Action Request is opened when a user clicks a button on a user-defined Form in a Recommendation Alert. The Action Request includes data derived from the Alert, Form, and Button clicked. {% hint style=\"info\" %} If you add a button to a Form, ensure there is a corresponding Data Stream to process the Action Requests - or they will remain open and unprocessed. {% endhint %} ![](/docs/images/image (681).png>) Process an Action Request The Read Action Request Agent polls App Designer for new Action Requests for the specified Recommendation and passes that data to the next Stream Object in the Data Stream for processing. The Read Action Request Agent also sends all open Action Requests when the Data Stream is first published - so that any Action Requests opened while the stream was not running can be processed. The Close Action Request Agent closes Action Requests from upstream. Processed Action Requests must be closed, or they will be reprocessed if the Data Stream is restarted. ![](/docs/images/image (1070).png>) Outputs of the Read Action Request Stream Object Output Description RequestId The Id of the specific button press. This should be passed to the Close Action Request Stream Object's Action Id. AlertId The Id of the Recommendation Alert. AlertDescription The Headline of the Recommendation Alert. AlertComments The current Notes of the Recommendation Alert. ActionRequested The time that the button was pressed. ActionParameters A JSON string of the Recommendation Alert's Form's values at the time the button was pressed."
  },
  "docs/concepts/recommendation/auto-escalate.html": {
    "href": "docs/concepts/recommendation/auto-escalate.html",
    "title": "Auto Escalate | XMPro",
    "summary": "Auto Escalate Auto Escalate is only available if Enable Execution Order is checked. Auto Escalate prevents more than one pending Recommendation Alert from existing at once for a single business event. When an Alert is created, if Auto Escalate is checked, any previously created Recommendation Alert that has not yet been Resolved or Marked as False Positive in the Recommendation is marked as Resolved. Example In the following Recommendation, there are three Rules. In each Rule, there is Rule Logic that evaluates whether the Average field from the received data is greater than 50, 70, and 90. ![](/docs/images/image (86).png>) A Recommendation Alert has been created because the Recommendation received a data row with 66.8 as the Average value: ![](/docs/images/image (898).png>) Then the Recommendation receives a data row from the Data Stream with greater than 90 Average value. It will: Evaluate the data against the first rule: Exceeded 90°C. The average value is greater than 90, so it will evaluate as true. Create a Recommendation Alert based on the Exceeded 90°C Rule, and resolve the pending Recommendation Alert. ![](/docs/images/image (1441).png>) ![](/docs/images/image (1105).png>)"
  },
  "docs/concepts/recommendation/deleted-items.html": {
    "href": "docs/concepts/recommendation/deleted-items.html",
    "title": "Deleted Items | XMPro",
    "summary": "Deleted Items When you delete a Recommendation, there may be archived Recommendation Alerts created by that Recommendation that you don't want to delete. In order to facilitate this, deleting a Recommendation, Recommendation Version, or Rule will move the item to the Deleted Items. Items that are deleted will not generate new Recommendation Alerts, and will not be visible in other areas of the App Designer and Data Stream Designer. For example, deleted items will not show when selecting a Recommendation to filter on in the Recommendations Block or Stream Objects. Deleting the Recommendation itself will also delete all versions and rules within that Recommendation. Deleting a version will also delete all rules within the version. The Recommendation itself will not be deleted, and only the deleted version and rules will show in the deleted items list. ![](/docs/images/image (1850).png>) Deleted items can be accessed through the Deleted Items button on the Recommendations page. ![](/docs/images/image (78).png>) {% hint style=\"info\" %} Items that have not been deleted don't have a select box, and cannot be restored or permanently deleted. {% endhint %} Alert Count Column When you view deleted items, an Alert Count Column will be displayed next to the items. This will show the number of Alerts that were triggered previously for the deleted Rule. ![](/docs/images/image (1337).png>) Clicking on an Alert Count for a deleted Rule will display the list of Alerts in the Recommendation Alerts table. ![](/docs/images/image (60).png>) Selection Selecting a Recommendation will select and expand all Versions and Rules for that Recommendation. ![](/docs/images/image (812).png>) Selecting a Version will select and expand all the Rules for that Version. ![](/docs/images/image (1230).png>) When an item is deselected, only that item will be deselected. For example, if you deselect a Recommendation, only the recommendation will be deselected. The Version and Rules for that Recommendation will remain selected. Similarly, if you deselect a Version, only the Version will be deselected. Any rules for that Version will remain selected. ![](/docs/images/image (1604).png>) If all Versions are deselected in a Recommendation, that Recommendation will be deselected, as you cannot have a Recommendation without a Version. Restore Items To restore items, select them and press the Restore button. Restored items will return to their original state before being deleted. Restored items will start generating Recommendation Alerts if the Recommendation is published. It is possible to restore an item while its parent item is still deleted. In this case, the restored item will not be visible anywhere but will become visible when the parent item is restored. ![](/docs/images/image (304).png>) If you restore an item without selecting the parents of the item, the parents will be restored as well. For example, if you select a Rule (and do not select its version or Recommendation), the Rule will be restored and the version and recommendation will still be automatically restored as well, even if they were not selected. ![](/docs/images/image (969).png>) Permanently Delete Items {% hint style=\"danger\" %} Warning! Recommendation Alerts will also be permanently deleted when permanently deleting items. {% endhint %} To permanently delete items, select them and press Delete. The items, any child items, and any Recommendation Alerts generated from them will be deleted forever. Actions on the Deleted Items Action Description Restore Restores the Deleted item. Delete Permanently deletes the item. Select All Toggles between selecting and deselecting all the items in the deleted items list. Expand All Expands all Recommendations and Versions to see all Versions and Rules underneath. Collapse All Collapses any already expanded Recommendations. Further Reading How to Manage Deleted Recommendation Items"
  },
  "docs/concepts/recommendation/execution-order.html": {
    "href": "docs/concepts/recommendation/execution-order.html",
    "title": "Execution Order | XMPro",
    "summary": "Execution Order If Execution Order is enabled, the Rules will be evaluated one by one in ascending order when data is received from the Data Stream. If the data doesn't match the Rule Logic, it will move on to the next Rule and evaluate again. If the data does match the Rule Logic, it will create a Recommendation Alert and stop evaluating against any subsequent Rules. Example In the following Recommendation, there are three Rules. In each Rule, there is Rule Logic that evaluates whether the Average field from the received data is greater than 50, 70, and 90. In the case that the Recommendation receives a data row in which the Average value is 86, it will: Evaluate the data against the first rule: Exceeded 90°C. 86 is not greater than 90, so it will evaluate as false and move on. Evaluate the data against the second rule: Exceeded 70°C. 86 is greater than 70, so it will evaluate as true. Create a Recommendation Alert based on the Exceeded 70°C Rule. ![](/docs/images/image (379).png>)"
  },
  "docs/concepts/recommendation/form.html": {
    "href": "docs/concepts/recommendation/form.html",
    "title": "Form | XMPro",
    "summary": "Form A Form is a collection of fields that appear on Recommendation Alerts. Forms are how relevant information can be entered and changed over the course of resolving an Alert. Forms can also contain buttons that allow specific actions to be performed in other business systems, like creating a work order in your EAM system. The Action Request Agent in the Data Stream Designer will pass the data from your form to other systems. {% hint style=\"info\" %} See the Action Requests article for more details on Action Requests. {% endhint %} When a Recommendation Alert is created from the Rule, the Rule's selected Form is used to determine the fields (Blocks) that will appear in the alert. Forms are managed through the Recommendations page. ![](/docs/images/image (1614).png>) The same form can be seen as created in a Recommendation Alert: ![](/docs/images/image (898).png>) Category Forms can be organized into different categories. This refers to the category under which the Form is found in the Form list. This category is separate from the App and Data Stream Categories. Actions on the Form The following Actions can be taken on a Recommendation Form: Action Description Blocks Opens the list of Blocks that are available to the Form. Properties Opens the properties for the selected Block. Save Saves any changes made to the Form up to this point. Discard Discards any changes made to the Form up to this point. Settings Opens the Form Settings, where you can modify the Form's Name and Category or delete the Form. Manage Versions Versioning for the Form. Manage Access Allows you to manage which users are allowed to view or modify this Recommendation and the Recommendation Alerts created by this Recommendation. Clone Clones the Form as a new Form. Export Export the Form as an encrypted file. Delete Deletes the Form. Blocks The following Blocks can be added to a Recommendation Form: Name Description Text A control that displays text. Textbox A control for the user to input text. Text Area A control for the user to input a large amount of text (multiline). Numberbox A control for the user to input a numeric value. Datebox A control for the user to input a date. Checkbox A control that allows the user to tick an option. Dropdown A control that allows the user to select from a predefined list. Grid A control that displays tabular data, updated as a JSON Array by an Update Recommendation Agent in a Data Stream. It is read-only when viewed in App Designer. It is used to add information for users reviewing or resolving Recommendation Alerts, such as a Data Stream configured to listen for changes in an external work order system and update the Alert. Button A control that can be clicked to trigger an event or action. Blocks are added to the Form by dragging from the Blocks tab. ![](/docs/images/image (1748).png>) Blocks can be re-ordered by dragging them up and down ![](/docs/images/image (583).png>) The properties of a Block are available in the Properties tab after clicking on a Block to select it. A Block can also be deleted by clicking on the delete button in the selection toolbar: ![](/docs/images/image (607).png>) Block Properties Property Description Property Description Name Name of the recommendation. This is usually one or two words that describe the form. Name is how the Block is differentiated for Data Stream Agents. Caption The caption is displayed above the Block. (Text) Caption Style The style of the text Block. Options include Heading 1 - 4, Body, Metric, and Small Text. Read Only A flag that determines whether the field will allow a new value to be added on the Alert. Required A flag that determines whether the Block must have a value before the Alert can be saved. (Dropdown) Items The items that are available for selection in the dropdown. Text is shown in the dropdown, and Value is what will be selected and saved. (Button) Button Style The style of the button in the Recommendation Alert: Text: The button has no borders. The text color depends on the Button Type. Outlined: The button has a colored outline. The border color depends on the Button Type. Contained: The button has a colored background. The color depends on the Button Type. If the Button Type is Normal, it has a black outline. (Button) Button Type The color of the button in the Recommendation Alert: Danger: Red. Default: Blue. Normal: White or Black. Success: Green. {% hint style=\"info\" %} Number Selector automatically converts the entered value into a scientific notation if it is greater than 21 digits for an integer value and greater than 6 digits for a decimal value. {% endhint %} Further Reading How to Create and Manage Forms Action Requests: creating work orders in Data Streams with a Form button"
  },
  "docs/concepts/recommendation/index.html": {
    "href": "docs/concepts/recommendation/index.html",
    "title": "Recommendation | XMPro",
    "summary": "Recommendation Recommendations enable engineers or employees of an organization to respond to critical events based on expert knowledge in the organization before the opportunity expires, while managers can close the loop by monitoring that it is done in a timely and appropriate manner. Recommendation Alerts are advanced alerts that are triggered when a critical event occurs. Alerts get created when real-time data meets the conditions of recommendation rules. The Alerts are then discovered by employees through email or SMS notifications and the monitoring of systems. Discussions allow users to message each other about the particular event that triggered the alert. Other people who view the alert can also view those discussions to be up-to-date with any new information. Triage Instructions are instructions given to help whoever is resolving the Recommendation Alert. Custom Forms can also be shown to the user when they view an alert. Users can use the forms to enter relevant information over the course of resolving the Alert. In practical terms, Recommendations observe live data from a Data Stream with a Run Recommendation Agent and evaluate the data against conditions defined in their Rules. If a Rule's condition is met, a Recommendation Alert is created from the Rule. Recommendations are created and managed in the App Designer. To manage Recommendations, click the Recommendations button in the menu on the left, and then click on the Manage Recommendations button. Note Image placeholder: Recommendations Menu Note Image placeholder: Manage Recommendations Finding Recommendations The search bar can be used to find any specific Recommendations that you may be looking for. There is a dropdown option where you can specify to search through everything in App Designer, or only for Recommendations. Note Image placeholder: Search Recommendations Category Recommendations can be grouped into categories. This refers to the category under which the Recommendation is found in the Recommendations list. Data Stream This is Data Stream from which the Recommendation will receive data from. Enable Execution Order A flag that determines whether the Rules will be evaluated in ascending order or all at once. See the Execution Order article to read more about the Execution Order. Auto Escalate A flag that determines whether an existing pending Recommentaion Alert should be resolved and escalated to a newly created Alert. This would occur when new data is received that meets a Rule's Logic further up the Execution Order of the current Recommendation Alert's Rule. See the Auto Escalate article to read more about the Auto Escalate. Rules Rules determine the conditions for creating Recommendation Alerts, and what created alerts should look like. Variables Variables are a way to transform the data received from the Data Stream before applying it to the Rule Logic. Actions on the Recommendation Action Description Save Saves any changes made to the Recommendation up to this point. Discard Discards any changes made to the Recommendation up to this point. Publish/Unpublish Publishes or unpublishes the Recommendation, which makes it start listening for data from the Data Stream. Manage Access Allows you to manage which users are allowed to view or modify this Recommendation and the Recommendation Alerts created by this Recommendation. Timeline Upcoming feature. Versions Versioning for the Recommendation. Delete Deletes the Recommendation. Clone Clones the Recommendation as a new Recommendation. Export Export the Recommendation as an encrypted file. Further Reading How to Create and Manage Recommendations How to Create and Manage Variables"
  },
  "docs/concepts/recommendation/notification.html": {
    "href": "docs/concepts/recommendation/notification.html",
    "title": "Notification | XMPro",
    "summary": "Notification A Notification defines how users will be notified when a Recommendation Alert occurs. A Recommendation Alert occurs when a Rule condition is met and triggers the Alert to happen. Notifications are set for a recommendation Rule. Each Rule can have multiple Notifications, each based on different conditions that can trigger an Alert. You can add a new Notification by pressing the plus button on the top right of the notifications. Triggers Notifications have several triggers. When they happen, will notify everyone who is subscribed to the notification. The triggers are as follows: New Recommendation When \"Send Recommendation when a new recommendation is generated\" is checked, subscribers will be notified when a new Recommendation Alert is created from the same Rule. Status Change When \"Resolved\" is checked, subscribers will be notified when a Recommendation Alert created by the same Rule is resolved. When \"False Positive\" is checked, subscribers will be notified when a Recommendation Alert created by the same Rule is marked as False Positive. Notes When \"Send notification when notes are added to a recommendation\" is checked, subscribers will be notified when Notes are added or changed on a Recommendation Alert created by the same Rule. Time Pending When \"Send a notification after\" is checked, subscribers will be notified the specified amount of time after a Recommendation Alert is created from the same Rule if the Alert has not been resolved. When \"And repeat notification\" is checked, subscribers will be notified the specified number of times, waiting the specified amount of time between each notification, if a Recommendation Alert has still not been resolved. For example, in the image below, subscribed users will be notified once every hour for two hours that a Recommendation Alert has not been created and hasn't been resolved. Channels Email When a trigger occurs, everyone who has subscribed and has entered a correct email address in their profile will receive an email. SMS When a trigger occurs, everyone who has subscribed and has entered a correct mobile phone number (in international format) in their profile will receive an email. {% hint style=\"info\" %} The email and mobile number that you use to receive notifications can be changed when updating the user settings. {% endhint %} Notification Template When a Recommendation Alert is triggered by a critical event, the user can receive a notification via text message or email. The notification contains a message that notifies the user of the Recommendation Alert. You can choose to create a custom message template for when a notification is triggered, or use a default template provided. {% hint style=\"info\" %} To learn more about Notification Templates, visit the How to Manage Notification Templates article. {% endhint %} Subscribe Notification Settings are available only if the user has run access. Notification Settings are managed individually by each User. To manage your subscriptions, hover over your user profile in the top-right of the page in App Designer, and click \"Notification Settings\". By default, you will not be subscribed to any Notifications. You can subscribe to an individual Notification, Rule, Recommendation, or Category by checking it in the list and saving. If an item is checked, you are subscribed to all Notifications — as well as Notifications that may be created in the future — within the item, and you will be notified when Notifications are triggered. ![](/docs/images/image (731).png>) Further Reading How to Create and Manage Notifications How to Manage Notification Templates How to Subscribe to Notifications"
  },
  "docs/concepts/recommendation/recommendation-alert.html": {
    "href": "docs/concepts/recommendation/recommendation-alert.html",
    "title": "Recommendation Alert | XMPro",
    "summary": "Recommendation Alert Recommendation Alerts are advanced alerts that are triggered when real-time data meets the criteria defined in a Recommendation Rule. They notify you when certain conditions occur in your data and provide decision support for how to take action. The Recommendation Alerts are found by clicking on the Recommendations button in the menu on the left of the App Designer. ![Fig 1: Access the recommendations menu](/docs/images/image (682).png>) The grid provides an overview of all the Alerts that you have access to. The rightmost Status column shows whether the Alert is Pending or Resolved. You can order, filter by specific values, search the alerts by any of the columns, and search in all columns in the Search bar. To see Resolved as well as Pending Alerts, check the \"Show Archived\" checkbox. Click on an Alert in the grid to navigate to the Recommendation Alert details. ![Fig 2: The recommendation alerts grid](/docs/images/image (912).png>) Finding Recommendation Alerts The search bar can be used to find any specific Recommendation Alerts that you may be looking for. There is a dropdown option where you can specify to search through everything in App Designer, or only for Recommendation Alerts. Detailed View The Recommendation Alert page provides details of the alert. It allows you to input information into the Form, view Triage Instructions, have a Discussion with your team, and see the alert Timeline and Analytics. Fig 4: Recommendation Alert page Headline The headline of the Alert. Generated by the Rule Headline. Recommendation Details A paragraph on the details of the Alert. Generated by the Rule Description. Event Data The data received by the Data Stream. If \"Log Data On All Occurrences\" is checked in the Rule, this data will be updated as new data is received. Notes An area to write notes and observations. Actions This area contains the actions that can be performed on a Recommendation Alert. Assign and Reassign Reassign added in v4.4.4 Ability to Assign (or Reassign) responsibility for the Recommendation Alert to a user that has run access to the Recommendation. The default selection is the logged-in user. When this action is performed, the action is recorded on the Timeline and in the Discussion - thus notifying the assignee. Fig 4: Assign a recommendation alert Share Ability to share the Recommendation Alert to users that have run access to the Recommendation. Selected users will receive an email with the note and a link to the Recommendation Alert. Save Saves the changes made on the Recommendation Alert. Mark as False Positive Someone reviewed the issue, determined that the asset doesn't have an issue or it was triggered while in Maintenance/Service mode, and considered the matter closed. Mark as Resolved Someone reviewed the issue, took mitigation steps, and considered the matter resolved. Form A form where relevant information can be entered. It is only available if the Recommendation has an attached Form. Accessed by clicking on the Form tab. For more details on how to add actions to a form button, see Recommendation Actions. Triage Instructions An area that provides useful information on actions to take to resolve the Alert and links to relevant resources. It is only available if the Recommendation has Triage Instructions enabled. Accessed by clicking on the Triage Instructions tab. Discussion An area in which messages can be posted to collaborate with members of your team. Each discussion is specific to a particular Recommendation Alert. A more detailed explanation can be found below. Timeline All previous alerts created by the same Rule and a list of all the events to happen on this Alert. Analytics An area in which the number of Alerts for the Asset (the Entity Identifier specified in the Run Recommendation Agent) can be compared, as well as a breakdown by Generated, Auto Escalated, False Positive, and Resolved alerts over a period of time. A more detailed explanation can be found below. Discussion The discussion section is an area in which messages can be sent by anyone who has access to the Recommendation Alert. Messages are displayed with the latest message at the bottom of the list. Any messages which have not been read since the last time you visited the page will be below the \"Last Read\" line break. You can search for messages that contain a certain word or phrase by typing in the search bar at the top. You can add a message by typing in the editor at the bottom of the discussion section and clicking the button with a paper plane icon at the bottom right corner. Advanced text editing can be opened by clicking the button with an underlined letter A icon at the bottom left corner. You can mention another user by typing the @ symbol or clicking the button with the @ symbol, which will pop up a list of users. Clicking on a user will mention them in the message and send an email to them when the message is sent. Analytics The analytics section compares the currently viewed period of alerts with the previous period and displays the difference as a percentage. The statistics compared are: The number of Alerts generated The number of Alerts that were auto-escalated The number of Alerts marked as false positive The number of Alerts resolved. Below the breakdown, there are two charts: The number of all Recommendation Alerts for the Asset (the Entity Identifier specified in the Run Recommendation Agent). A breakdown of all Recommendation Alerts by Rule for the Asset in the selected period. Further Reading How to Manage Alerts Manage Alerts on Mobile"
  },
  "docs/concepts/recommendation/recommendation-scoring.html": {
    "href": "docs/concepts/recommendation/recommendation-scoring.html",
    "title": "Scoring | XMPro",
    "summary": "Scoring What is Alert Ranking and Alert Scoring? When setting up a Recommendation, authors are able to influence the order in which Alerts will be shown. Ranking and Scoring help the alert recipient prioritize recommendations according to their importance. What is the difference between Alert Ranking and Alert Scoring? Alert Ranking allows users to rank a recommendation as High/Medium/Low. Alert Scoring is an alternate fine-tuning of the alert ranking by specific factors such as recommendation, category, rule, and an optional value from the Data Stream. By allowing the author to assign a calculated score to an Alert instead of a ranking, you can have even more detailed control over its importance level. This Score also helps the alert recipient to understand its relative importance. {% hint style=\"info\" %} Authors still have the option to use Alert Ranking instead of Alert Scoring if they prefer. {% endhint %} How is the scoring calculated? This feature allows recommendation authors to assign numerical values (1-10) to various aspects during configuration. These values are then multiplied, resulting in an alert score. score = recommendation factor * category factor * rule factor * optional factor The score is calculated at the time the alert is generated and is not recalculated should any of the factors be updated. Where are scoring values added? The values used to score an Alert could be configured in these different areas: Recommendation – assesses the significance of the recommendation itself Fig 1. Recommendation Factor Recommendation Category – evaluates the importance of the recommendation's category Fig 2. Category Factor Recommendation Rule - when managing a rule within a recommendation Fig 3. Rule Factor Recommendation Optional - an Optional Rule Factor value retrieved from the Data Stream. Fig 4. Optional Factor Viewing the recommendation scoring When viewing the Recommendation Alerts list, The Recommendation Scoring is displayed on the Score Column, and the Alerts are arranged in a descending order based on their Scoring. Fig 5. Recommendation Alerts List You can also view the Scores using the Score Factor Matrix. To open the Score Factor Matrix: Open Manage Recommendations Click on “More” ellipses Click the \"Score Factor Matrix\" button Fig 6. Step 1 of 3 - Where to find \"Open Manage Recommendations\" Fig 7. Step 2 of 3 - Where to find \"...\" and Step 3 of 3 - Where to find \"Score Factor Matrix\" Fig 8. Score Factor Matrix {% hint style=\"info\" %} This table only shows an estimate as the Optional Factor of the Recommendation could not be determined until the Alert is generated. {% endhint %} Viewing the Score Factor history on a timeline The changes made on a Score Factor Recommendation can be viewed on a timeline in the following areas: Category Timeline – when observing score factor changes within this timeline. Fig 9. Category Timeline Recommendation & Rule Timeline – when viewing score factor changes within this timeline. Fig 10. Recommendation & Rule Timeline"
  },
  "docs/concepts/recommendation/rule.html": {
    "href": "docs/concepts/recommendation/rule.html",
    "title": "Rule | XMPro",
    "summary": "Rule A Rule defines the conditions for triggering a Recommendation Alert and what the created Recommendation Alert should look like. Multiple Rules can also be grouped into one Recommendation. ![](/docs/images/image (20).png>) Rule Logic Data sent from the selected Data Stream is passed through the Rule Logic and if the conditions created are met by the data, a new Recommendation Alert will be created. You can add new conditions or groups by clicking the + button. Groups can be nested within each other to create advanced logic. In an \"And\" group, all the conditions must be true, and in an \"Or\" group, only one of the conditions must be true to trigger an Alert. As an example, in the following Rule Logic, both of the following must be true to trigger an Alert: Average must be greater than 50, And V1, V2, or V3 must be greater than 60. ![](/docs/images/image (309).png>) Alert Headline & Description This refers to the headline and description that the Recommendation Alert will be created with. Any tag (starting with @) will be replaced with the value output from the Data Stream. Alert Ranking The priority level that the Recommendation Alert will be created with. Priority level determines the order in which the Alerts will be displayed. Impact Metric The measurable impact of the business event detected by the Recommendation Rule. This will be shown on Alerts in the Recommendation Block in App Pages. Enable Form A flag that determines whether the Recommendation Alert will be created with a Form. Select Form The Form that the Recommendation Alert will be created with. Form Version The Version of the Form that the Recommendation Alert will be created with. Additional Recommendation Management Column An additional column in the Recommendation Alerts grid. Any tag (starting with @) will be replaced with the value entered in the corresponding field in the Form. Resolution Resolution determines whether new data from the Data Stream will automatically resolve the Recommendation Alert if the Rule Logic is no longer true. Manual Resolution: A user must manually resolve each Recommendation. Automatic Resolution: Recommendation auto resolves when trigger conditions are no longer true. This may impact performance, as it might continuously trigger and resolve a large number of Recommendations at a time if the data fluctuates frequently. Recurrence Recurrence determines whether new data from the Data Stream will create new Recommendation Alerts if a Pending Recommendation Alert already exists and the Rule Logic is true. If the conditions created by the Rule Logic are met by the data, and if Recurrence is set to All Occurrences or First Occurrence and no Pending Alert exists, a new Recommendation Alert will be created. First Occurrence: The current recommendation must be resolved before others can be triggered for the same rule. All Occurrences: A new recommendation will be triggered every time the rule conditions are true. Recurrence will be disabled and be considered to be the First Occurrence if the Recommendation has Execution Order enabled. Log Data On Determines whether new data from the Data Stream will log new data in the Event Data grid if the Rule Logic is true. First Occurrence: Current recommendation will only log data on the first occurrence. All Occurrences: New alert data will be triggered every time the rule conditions are true. Triage Instructions Instructions to help whoever is resolving the Recommendation Alert. Enable Triage Instructions is a flag that determines whether the Recommendation Alert will be created with Triage Instructions. Resources A list of links on the Recommendation Alert to help whoever is resolving the Recommendation Alert. Enable Resources is a flag that determines whether the Recommendation Alert will be created with Resources. Notifications This refers to the list of Notifications that will be sent out to everyone who is subscribed. View Related Alerts You can view Alerts that are directly related to a Rule. To do this, open the Rule's page and click on Alerts at the top. This will take you directly to the Alerts list table, which will be filtered to only display the Alerts related to the specific rule you are viewing. ![](/docs/images/image (111).png>) ![](/docs/images/image (1559).png>) Actions on the Rule Action Description Save Saves any changes made to the Rule up to this point. Discard Discards any changes made to the Rule up to this point. Clone Clones the Rule as a new Rule in this Recommendation. Delete Deletes the Rule from this Recommendation. Further Reading How to Create and Manage Rules"
  },
  "docs/concepts/variable.html": {
    "href": "docs/concepts/variable.html",
    "title": "Variable | XMPro",
    "summary": "Variable Variables are placeholders used to maintain integration keys and secrets. They can be used by IT administrators, engineers, or other users to store and use credentials or passwords in a safe and secure manner. They provide a central point to maintain credentials that can improve security. Engineers and other users can reference and use these credentials without knowing the actual value they contain. More secure credentials such as passwords can be encrypted to add an extra layer of security. Variables can be added in both the App Designer and the Data Stream Designer. The value of the Variable is only available in the environment it is created in. For example, if the Variable is created in the Data Stream Designer, both the name and value of the Variable will be available in the Data Stream Designer. However, in the App Designer, only the name will be available and the value will be left empty. Finding Variables The search bar can be used to find any specific Variables that you may be looking for. There is a dropdown option where you can specify to search through everything in App Designer, or only for Variables. Note Image placeholder: Search Variables Category Variables can be grouped into categories. This category is separate from the App and Data Stream Categories. Encrypt Value If selected, the value of a Variable will be encrypted. This way the value cannot be seen by any user. If not selected, the value can be seen when the Variable is selected and configured. If a value is not encrypted, and you select encrypt value, the value that is already in the input field will be cleared automatically. Similarly, if the value is already encrypted, and you deselect encrypt value, the value already entered will be cleared. Note Image placeholder: Encrypt Value Value The value of the Variable, for example, the username, server name, or password. Using a Variable Once a Variable has been saved, the Variable can be selected from a dropdown list when you are required to fill in forms or credentials. For example, in order to connect to a database to display data on your App, you will need to connect to it using a server name, username, and password. Note Image placeholder: Using a Variable 1 Note Image placeholder: Using a Variable 2 Actions on the Variable Action Description Add Adds a new Variable. Select Selects multiple Variables. Manage Categories Creates and edits categories to organize the variables. These categories are separate from the App and Data Stream Categories. Save and Close Saves any changes made to the Variable up to this point. Discard Discards any changes made to the Variable up to this point. Delete Deletes the Variable. Further Reading How to Create and Manage Variables"
  },
  "docs/concepts/version.html": {
    "href": "docs/concepts/version.html",
    "title": "Version | XMPro",
    "summary": "Version Version Management Versioning provides traceability, which is the ability to view all revisions and changes made throughout an XMPro Object's history, including changes made during each development stage. Adding new versions also allows different team members to work on and make changes to an XMPro Object that is currently in use by other people. This allows others to continue using it without any downtime. Making changes on a new version of an XMPro Object also allows you to maintain a version of it before you made changes, which can also act as a backup mechanism. If major issues are found in the new version, you are easily able to switch back to an older one if needed. Each XMPro Object keeps track of different versions, including: Data Streams Agents and Connectors Applications Recommendations Forms Minor version increases are automatically handled by the system every time you make changes to the particular XMPro Object and save those changes. However, control over how major version increases are handled is given, to some extent, to the user as versions can easily be copied or removed. Copying a version allows you to continue working and making changes to the XMPro Object while maintaining a version of it before you made changes, which can also act as a backup mechanism. To view the available versions for a particular XMPro Object, click on the \"Versions\" button. Note Image placeholder: Versions Button When you open an XMPro Object, the latest version is automatically opened. Even if there is an older version published, the latest version will still be opened. Note Image placeholder: Latest Version Published Versions A published version is a version of the XMPro Object that is published and viewable and usable to others. See the Publish article for more details on published XMPro Objects. Versions that are published will display a green play symbol next to the version number. Note Image placeholder: Published Versions Copying Versions When copying a version, all properties and inner components of the XMPro Object's version will be duplicated as a new version. Note Image placeholder: Copying Versions 1 Note Image placeholder: Copying Versions 2 Deleting Versions Published versions cannot be deleted. When an XMPro Object only has a single version or one last remaining version, that version also cannot be deleted. Note Image placeholder: Deleting Versions Actions on the Version Action Description View/Open Opens that particular version of the XMPro Object. Copy Copies the selected version as a new version, with a higher version number. Delete Deletes the Version. Further Reading How to manage Versions"
  },
  "docs/concepts/xmpro-ai/index.html": {
    "href": "docs/concepts/xmpro-ai/index.html",
    "title": "XMPro AI | XMPro",
    "summary": "XMPro AI Unlock a world of possibilities XMPro AI provides an end-to-end solution to operationalize AI in your existing business processes. Augment your business processes with XMPro AI to gain a competitive edge through automated decision-making. Easily embed developed AI models into XMPro data streams to analyze vast amounts of data and generate real-time insights, enabling you to make informed decisions quickly. XMPro AI is baked into our modules, from the agents, reading and contextualizing your sensor data, to the rapid development of AI Models within our XMPro Notebook, and dashboards in our Application Designer - all of which help to make sense of the data and to create intelligent digital twins. XMPro can be deployed on the cloud, On-Premise, or the edge. With XMPro AI, you can make data-driven decisions with confidence, maximizing efficiency and driving business growth. Fig 2: XMPro AI - How It Works Embedded AI Leverage Machine Learning (ML) to extract actionable insights from vast data volumes, uncover hidden patterns, predict outcomes, and drive business growth and efficiency. Advanced Analytics and Predictive Insights Visit our Blueprints, Accelerators, and Patterns repository to see how you can embed AI: Remaining Use of Life (RUL) Data Stream, which uses our Python Agent, in the Smart Manufacturing Accelerator. The Asset Monitoring - Binary Classification Pattern utilizes our Binary Classification Agent. The Liner Wear Prediction - Regression Pattern utilizes our Regression Agent. The Vertical Travel - Forecasting Pattern utilizes our Forecasting Agent. Innovation AI Utilize XMPro Intelligent Digital Twins and XMPro Notebooks for scalable and cost-effective innovation with AI. Simulation and Optimization Conduct simulations, real-time visualizations, explore different configurations, predict outcomes, and identify parameters for improving efficiency, productivity, or energy consumption. Augmented AI Interact with systems, applications, and proprietary data using natural language commands, fostering effortless communication, enhanced user experiences, and increased adoption of intelligent automation solutions. Co-Pilot Capability in App Designer Build a widget to harness OpenAI from within your Application. Ask for advice from your co-pilot and paste the results into the related recommendation alert. Note This example is running on the public version of ChatGPT. Contact us for more information on how to do this using your Azure OpenAI Service, trained on your own IP. ChatGPT Capability in XMPro Notebook Ask ChatGPT for help from within XMPro Notebook. Model Governance Support As AI scales within the organization, corporate guardrails require AI to be modeled within an MLOps framework. Our MLflow Agent is the first in a series, that enables effective model governance using a popular MLOps toolset. This empowers data scientists to promote new model versions within MLflow without going back to edit the XMPro Data Stream. In the example below, version 1 of a model is configured and the Data Stream is published. Observe that the first event printed confirms model version one was used to make a prediction. We then switch to MLflow to promote version 2 to production. Observe that when we switch back to the Data Stream output, model version two is seamlessly used to make the next prediction. Fig 7: A new version promoted in MLflow is seamlessly picked up and processed in the Data Stream."
  },
  "docs/concepts/xmpro-ai/xmpro-notebook.html": {
    "href": "docs/concepts/xmpro-ai/xmpro-notebook.html",
    "title": "XMPro Notebook | XMPro",
    "summary": "XMPro Notebook [!Video https://www.youtube.com/watch?v=7D1IpFmA-CQ] Overview XMPro Notebook provides an intuitive and flexible interface for data analysis, scientific computing, machine learning, and more. Users can write code and execute cells independently, which facilitates step-by-step exploration and experimentation with real-time data. Getting Started XMPro Notebook is an embedded version of Jupyter and can be accessed from the waffle menu on the top left navigation, and by navigating to \"AI\". Quickstart Guide Once opened there is a handy quick-start guide that acts as an introduction on how to use XMPro Notebook. Note The first time an XMPro notebook user session loads, the infrastructure is provisioned in real-time, meaning that the application will take a few seconds to load. ChatGPT XMPro AI has built-in ChatGPT functionality through the use of Python Magics. Once you've provided your ChatGPT API Key, you can use line magic to provide a single line of input, or cell magic to provide multiple lines of input. Set your ChatGPT API Key # Set your ChatGPT API Key %chatgpt -k \"<your ChatGPT api-key>\" Line magic: %chatgpt Example input: %chatgpt \"show a correlation plot for the iris dataset\" Example output: import seaborn as sns import matplotlib.pyplot as plt iris = sns.load_dataset('iris') corr = iris.corr() sns.heatmap(corr, annot=True, cmap='coolwarm') plt.show() Cell magic: %%chatgpt Example input: %%chatgpt \"improve this plot example\" import matplotlib.pyplot as plt x = [1, 2, 3, 4, 5] y = [5, 4, 3, 2, 1] plt.plot(x, y, 'ro') plt.xlabel('X axis') plt.ylabel('Y axis') plt.title('Example Plot') plt.show() Example output: import matplotlib.pyplot as plt import numpy as np # Generate some random data x = np.random.randint(0, 10, size=50) y = np.random.randint(0, 10, size=50) colors = np.random.rand(50) # Create a scatter plot with different marker sizes and colors plt.scatter(x, y, s=50*x, c=colors, alpha=0.5) # Set axis labels, limits, and title plt.xlabel('X axis') plt.ylabel('Y axis') plt.xlim(0, 10) plt.ylim(0, 10) plt.title('Random Data Scatter Plot') # Add a colorbar legend plt.colorbar() # Display the plot plt.show() MLops MLflow is a well-known open-source MLops platform that streamlines the machine learning lifecycle: to create instances of models and run them in a structured and organized manner. This example illustrates how to leverage the MLflow Python library to create an instance of a machine learning model and execute it within your MLflow environment. Example input: # Importing necessary libraries from mlflow.models.signature import ModelSignature from mlflow.types.schema import ColSpec, Schema import mlflow from sklearn.linear_model import LinearRegression import numpy as np # Generate some sample data for training X = np.array([[1], [2], [3], [4], [5]]) y = np.array([2, 4, 6, 8, 10]) # Create an instance of Linear Regression model model_lr = LinearRegression() # Fit the model to the training data model_lr.fit(X, y) # Define a schema for the input and output of your model # In this case, both input and output are single columns of string type sig = ModelSignature( inputs=Schema([ColSpec(name=\"input\", type=\"string\")]), outputs=Schema([ColSpec(name=\"output\", type=\"string\")]), ) # Set the tracking URI for MLflow # This is the address where the MLflow server is running # Replace the IP address and port number with the ones for your server mlflow.set_tracking_uri('http://<URI>:<PORT>') # Start a new MLflow run # This represents a single execution of the model training code mlflow.start_run() # Log the trained model with MLflow mlflow.sklearn.log_model( sk_model=model_lr, artifact_path=\"model\", registered_model_name=\"LinearRegression\", signature=sig ) # End the MLflow run mlflow.end_run() Example output: Successfully registered model 'LinearRegression'. Created version '1' of model 'LinearRegression'. Libraries Libraries are a collection of pre-written code and functions that can be imported and used in programs to simplify development and add additional functionality. The following Python libraries are pre-installed in XMPro Notebook: altair beautifulsoup4 bokeh bottleneck cloudpickle conda-forge::blas=*=openblas cython dask dill h5py ipympl ipywidgets matplotlib-base mlflow numba numexpr numpy openai opencv python pandas patsy protobuf pytables scikit-image scikit-learn scipy seaborn sqlalchemy statsmodels sympy widgetsnbextension xlrd If any additional libraries are needed, the installation can be performed in the Notebook Cell. Below is an example command for a Python library: pip install <your library name> Please contact XMPro if you would like to propose another library added to the set of defaults. Warning Any library you load is only valid for the session and will need to be reinstalled when a new session is created. Licensing Unlike other XMPro products, two product licenses are required: one for the core AI product, and a second for XMPro Notebook. For more information on how to request a license, please view the instructions on how to Request a License."
  },
  "docs/faqs.html": {
    "href": "docs/faqs.html",
    "title": "FAQs | XMPro",
    "summary": "FAQs This page has been moved to Resources > FAQs."
  },
  "docs/getting-started.html": {
    "href": "docs/getting-started.html",
    "title": "Getting Started | XMPro",
    "summary": "Getting Started"
  },
  "docs/getting-started/browser-requirements.html": {
    "href": "docs/getting-started/browser-requirements.html",
    "title": "Browser Requirements | XMPro",
    "summary": "Browser Requirements Supported Browsers The XMPro platform can be run on a variety of browsers and operating systems. The latest two major releases of the following browsers are supported on the indicated operating systems: Browser Windows macOS iOS Android Google Chrome ✓ ✓ ✓ ✓ Apple Safari ✗ ✓ ✓ ✗ Microsoft Edge ✓ ✓ ✗ ✗ Mozilla Firefox ✓ ✓ ✗ ✗ Opera ✓ ✓ ✗ ✗ Supported Operating Systems The following operating systems are supported for browsers running the XMPro platform: Operating System Supported versions Windows Windows 10 or later macOS 10.13 or later iOS iOS 10 or later Android 5 or later Third-Party Cookies The XMPro Platform requires third-party cookies on web browsers to be allowed/enabled for it to function properly. Follow the steps below to enable cookies on the different browsers. Note Enabling third-party cookies is essential for the XMPro platform to function correctly. Without this setting, you may experience issues with authentication and certain features. Google Chrome On your computer, open Chrome. At the top right, click More, then Settings. Under \"Privacy and security,\" click Site settings. Click Cookies. Next to \"Blocked,\" turn on the switch to turn on cookies. For more information, you can visit the Official Google Chrome Documentation. Microsoft Edge Open Microsoft Edge, select Menu (3 dots icon on the top right corner of the browser) > Settings > Site permissions > Cookies and site data Turn on \"Allow sites to save and read cookie data (recommended)\" to unblock cookies Turn on \"Block third-party cookies\" or add desired sites in the \"Block\" section to block the cookies. For more information, you can visit the Official Microsoft Documentation. Apple Safari In the Safari app on your Mac, choose Safari > Preferences Click Privacy Unselect \"Block all cookies\". For more information, you can visit the Official Apple Documentation."
  },
  "docs/getting-started/browser-requirements.sample.html": {
    "href": "docs/getting-started/browser-requirements.sample.html",
    "title": "Browser Requirements | XMPro",
    "summary": "Browser Requirements This sample page demonstrates how to format content in DocFX markdown. Supported Browsers The XMPro platform can be run on a variety of browsers and operating systems. The latest two major releases of the following browsers are supported on the indicated operating systems: Browser Windows macOS iOS Android Google Chrome ✓ ✓ ✓ ✓ Apple Safari ✗ ✓ ✓ ✗ Microsoft Edge ✓ ✓ ✗ ✗ Mozilla Firefox ✓ ✓ ✗ ✗ Opera ✓ ✓ ✗ ✗ Alerts DocFX supports various types of alerts: Note This is a note alert. Use it for general information. Tip This is a tip alert. Use it for helpful tips. Warning This is a warning alert. Use it for important warnings. Caution This is a caution alert. Use it for critical information. Important This is an important alert. Use it for essential information. Code Blocks DocFX supports code blocks with syntax highlighting: public class Program { public static void Main(string[] args) { Console.WriteLine(\"Hello, World!\"); } } function sayHello() { console.log(\"Hello, World!\"); } { \"name\": \"John Doe\", \"age\": 30, \"isActive\": true } Tabs DocFX supports tabs: Windows macOS Linux This content is for Windows users. This content is for macOS users. This content is for Linux users. Images DocFX supports images: Links DocFX supports various types of links: Internal link to Introduction Internal link to a section External link to XMPro website Link to a section on the same page Lists DocFX supports ordered and unordered lists: Unordered list: Item 1 Item 2 Item 3 Nested item 1 Nested item 2 Ordered list: First item Second item Third item Nested item 1 Nested item 2 Tables DocFX supports tables: Name Description Required id The unique identifier Yes name The display name Yes description The description No type The data type Yes Blockquotes DocFX supports blockquotes: This is a blockquote. It can span multiple lines. Horizontal Rules DocFX supports horizontal rules: Emphasis DocFX supports emphasis: Italic text Bold text Bold and italic text Conclusion This sample page demonstrates the various formatting options available in DocFX markdown. Use this as a reference when migrating content from GitBook to DocFX."
  },
  "docs/getting-started/end-to-end-use-case.html": {
    "href": "docs/getting-started/end-to-end-use-case.html",
    "title": "End-To-End Use Case | XMPro",
    "summary": "End-To-End Use Case Note This Use Case assumes the XMPro platform is installed and configured, or you are using the Free Trial that has everything set up for you. This step-by-step tutorial is meant to be an introduction to using the XMPro platform. Completing it will give you a solid foundation to understand the more advanced concepts and detailed how-to guides. This tutorial will explain how to create and design a Data Stream, configure Stream Objects to ingest, analyze, transform, and perform actions on data. You will also learn how to set up a Recommendation to generate alerts based on rule logic, create and design an App, create Data Sources and Connections, and configure a simple Data Grid and Chart. Warning Please note that the XMPro platform requires third-party cookies to be enabled on your browser. Use Case Let's assume there is a power plant that uses a heat exchanger to keep the turbine cool and at the optimum temperature. The heat exchanger circulates water between the cooling tower and the heat exchanger to dissipate heat. To keep a proper circulation of liquid, there are three pumps [A, B, C] installed. Each Pump has a sensor that provides live data for Flow Rate (L/m) and Temperature (°C) using MQTT. Unless the Pump is under maintenance the Flow Rate should be above 15000 L/m and Water temperature should be below 130°C. Engineers should be alerted if the average flow rate falls below 250 L/s. If the average temperature starts to rise above 130°C then a critical level alert should be raised. Engineers should be provided a view to check the history of pump telemetry, maintenance records, and reservoir level to enable them to take necessary action. 1. Design Data Streams with Real-Time Data Sources The Use Case requires that we gather the Flow Rate and Temperature data from three pumps constantly, and pass it on to be analyzed and have actions performed on the data. We will achieve this with the use of Data Streams. A Data Stream is a visual representation of a flow of data. It is created through the Data Stream Designer. To access the Data Stream Designer, log into your XMPro Account and press the button in the top-left corner of the screen and click on the Data Stream Designer item. A Data Stream has four components: Ingesting data through Listener Agents Contextualizing sensor data/telemetry through Context Provider Agents Analyzing and transforming data through Transformation and AI Agents Performing actions or outputting data to other integrations through Recommendation and Action Agents We will follow those four steps below. Read & Wrangle Live and Context Data In this section, we will simulate reading data from pump sensors and a metadata store, and combine the data together into a single flow. Note See the Data Stream Concept article for more information on Data Streams. To begin, we will need to create a new Data Stream. To create a Data Stream, follow the steps below: Open the New Data Stream page from the left-hand menu. Give the Data Stream a name. For example, \"Pump Condition Monitoring\" Select the Type \"Streaming\". Data Streams of the Streaming type will run polling Agents at a set interval, for instance, every 10 seconds, whereas Recurrent Data Streams run on a customizable schedule, for instance, once a day at 12am. The recurring type only applies to polling-based Stream Objects, which we won't use in this example. Select the category under which the Data Stream is to be added. Feel free to load a suitable icon. If you do not, the default icon will be used. Sample icons can be found in the Icon Library. Select a collection that will be used to publish Data Stream. Enter a description to best describe the Data Stream. Click on \"Save\". In a production environment, Data Streams would integrate with external data emitters through Agents like OSIsoft PI or MQTT Listeners. However, for the sake of keeping the example simple, we won't be using any Agents that require an environment to be set up. Instead, we will be simulating the data with the Event Simulator, Calculated Field, and CSV Context Provider Agents. To simulate the telemetry from the pumps, follow the steps below: Drag into the canvas one of each of the following Agents: Event Simulator (Listener) Calculated Field (Transformation) CSV (Context Provider) Join (Transformation) Note Refer to How to Upload an Agent to Data Stream Designer if you are not able to find the Agents in the toolbox or the correct versions. You can search for the Agent in the search bar, and click and drag the Agent into the canvas to add it. An instance of an Agent added to the canvas is referred to as a Stream Object. Once you have all four Stream Objects in the Data Stream canvas, rename them as follows: Event Simulator as \"Simulate Pump Data\" Calculated Field as \"Add Pump Identifier\" CSV as \"Simulate Context Data for Assets\" Join as \"Contextualize Data\" To change the name of a Stream Object, click the text and edit it. Your stream should end up looking like this: Once you have renamed all four Stream Objects, connect them with arrows as follows: \"Simulate Pump Data\" to \"Add Pump Identifier\" \"Add Pump Identifier\" to \"Contextualize Data\" (first input) \"Simulate Context Data for Assets\" to \"Contextualize Data\" (second input) To connect two Stream Objects, click and drag the green rectangle (Output) at the bottom edge of the first Stream Object, move the cursor to the green rectangle on the left edge of the second Stream Object (Input). Your connected Stream Objects should look like this: Now we will configure the added Stream Objects. Save your Data Stream now and after every change to propagate the changes throughout the Data Stream. Note See the article on how to configure Stream Objects for more information. Simulate Pump Data Note Image: Simulate Pump Data configuration (Image not available) We will need to simulate ingesting data about flow rate and temperature from sensors in the pumps. We can achieve this with the Event Simulator Agent. The \"Simulate Pump Data\" Event Simulator will constantly generate data defined by the Event Definitions at a rate defined by the Events per Second property. Note To edit the configuration of a Stream Object, either double-click it or click it once to select it and click the \"Configure\" button on the canvas header. Edit the \"Simulate Pump Data\" Stream Object and click the + button to the right of the Event Definition grid to add event definitions. Add two event definitions as follows: Name: WaterTemperature Type: Range Minimum Value: 100 Maximum Value: 160 Spike Value: 0 Name: FlowRate Type: Range Minimum Value: 14000 Maximum Value: 16000 Spike Value: 0 Ignore the Spike Value and Generate Spike options, as they are not relevant to the current scenario. Change the Events per Second to 1. Click \"Apply\" on the Simulate Pump Data configuration page. Then click \"Save\" on the Data Stream page. Note Image: Event Simulator configuration (Image not available) Add Pump Identifier Note Image: Add Pump Identifier configuration (Image not available) We need to add a way to simulate having three different pumps. At the moment the data is not identified, so we will need to add a range of identifiers to the data. This can be achieved with the Calculated Field Agent. The \"Add Pump Identifier\" Calculated Field will add a \"PumpId\" field to the data generated with values \"A\", \"B\", and \"C\" for each subsequent row. To configure the Stream Object, double click on \"Add Pump Identifier\" to open its configuration. Or, you can also highlight the Stream Object and click on the \"Configure\" option at the top of the Data Stream. Keep \"Append to Current\" as the \"Results Returned As\" value. This will add the value calculated by the expression to each row instead of creating a new row with the identifier. Click the + button to the right of the Expressions grid to add the following expression: Calculated Field: \"PumpId\" - The field won't exist yet in the dropdown, so you must enter it yourself. Expression: ReadingNo % 2 == 0 ? \"A\" : ReadingNo % 3 == 0 ? \"B\" : \"C\" Data Type: String Note Image: Calculated Field configuration (Image not available) Press \"Apply\" on the PumpId expression and the Add Pump Identifier configuration pages, and press \"Save\" on the Data Stream page. Simulate Context Data for Assets Note Image: Simulate Context Data for Assets configuration (Image not available) There is often metadata associated with assets that is not part of the live data from the sensors. In this case, metadata includes whether the pump is currently under maintenance, the manufacturer, and the last service date. We must retrieve this data from elsewhere. In a production environment, this might be an SAP EAM system, but for this example, we can achieve this through the CSV Context Provider Agent. Double-click on the \"Simulate Context Data for Assets\" Stream Object to open the configuration menu. You can also highlight the Stream Object and click on the \"Configure\" option at the top of the Data Stream. Download the provided file. The contents of the file are below: PumpId,UnderMaintenance,Manufacturer,ServiceDate A,FALSE,Bosch,2020-10-12 B,FALSE,Bosch,2020-08-06 C,FALSE,Bosch,2020-01-04 Then under Data check the Use Uploaded File? checkbox and upload the file into the CSV Context Provider. The CSV Definition will be automatically detected and filled. Change UnderMaintenance from a String to a Boolean, using the options from the dropdown. Also change ServiceDate from a String to a DateTime, using the dropdown. Leave the Limit Rows, Filter Criteria, and Sort by properties as their default values. When completed, press the \"Apply\" button at the top of the configuration, and then save the Data Stream. Note Image: CSV Context Provider configuration (Image not available) Contextualize Data Note Image: Contextualize Data configuration (Image not available) The metadata about each pump needs to be appended to each row of sensor data received from the pumps. This can be achieved with the Join Agent. The \"Contextualize Data\" Join will join together the data from the CSV Context Provider and the Calculated Field using the PumpId as the common field. Configure it as follows: Behavior: Context - we want to join some context data to our row. Context Endpoint: Right - we must tell the Stream Object which input has Context data. The Context Data is received by the Right endpoint, as shown in the image below. Note Image: Join configuration - Context Endpoint (Image not available) Select List: all fields except R_PumpId (as the same data will be in L_PumpId). Join Type: Inner Join. On: L_PumpId = R_PumpId Note You may need to maximize the page to see the grid properly. You can do this by pressing the \"Maximize\" button in the top-right corner of the page. Press the \"Restore\" button in the top-right corner to return it to the regular size. Press \"Apply\" on the Contextualize Data configuration page, and press \"Save\" on the Data Stream page. Note Image: Join configuration - completed (Image not available) Create Analytics and Calculations In this section, we will add some analytics and calculations that will find exceptions, transform the units of the data and find the average level across 5 seconds. Ignore Pumps Under Maintenance Note Image: Ignore Pumps Under Maintenance configuration (Image not available) We want to only pass data onward in the Stream if the current pump is not under maintenance. This can be achieved with the Filter Agent. To do this, drag in a Filter Agent and connect the \"Contextualize Data\" Join endpoint to the Filter. Rename the Filter to \"Ignore Pumps Under Maintenance\", and save. Double-click on the Stream Object to open the configuration menu. Click on the + symbol to add a new rule for the filter. Select \"Add Condition\", and configure the Filter to have the logic R_UnderMaintenance Equals false. The configuration and Data Stream should look like this: Note Image: Filter configuration and Data Stream (Image not available) There are two green outputs to the Filter Stream Object, the left output is where the data is output to when the filter is true. The right output is where the data is output to when the filter is false. The left True Output should be the output that you connect to the next Stream Object. Note Image: Filter outputs (Image not available) Press \"Apply\" on the Ignore Pumps Under Maintenance Data configuration page, and press \"Save\" on the Data Stream page. Change Unit to L/s Note Image: Change Unit to L/s configuration (Image not available) The data from the Pump Data has different units than what we want to use - it is measured in L/m and we want the units to be in L/s. This can be solved with the Calculated Field Agent. To transform the data, drag in a Calculated Field Agent, and rename it to \"Change Unit to L/s\". Connect the \"Ignore Pumps Under maintenance\" Filter endpoint to the Calculated Field and Save. Make sure you connect the left True Output of the \"Ignore Pumps Under maintenance\" Stream Object to the Calculated Field's input. Note Image: Connecting Stream Objects (Animation not available) Configure the Calculated Field as follows: Calculated Field: L_FlowRate Expression: L_FlowRate / 60 Data Type: Double This will divide the flow rate by 60 to make the value in L/s instead of L/m. Press \"Apply\" on the Change Unit to L/S configuration page, and press \"Save\" on the Data Stream page. Note Image: Calculated Field configuration (Image not available) Average across 5 seconds Note Image: Average across 5 seconds configuration (Image not available) The Use Case requires that engineers should be alerted if the flow rate averaged over 5 seconds falls below 250 L/s, and if the temperature averaged over 5 seconds also starts to rise above 130°C then a critical level alert should be raised. This can be achieved with the Aggregate Agent. To calculate the average temperature and flow rate over 5 seconds, drag in the Aggregate Agent and name it \"Average across 5 seconds\". Connect the \"Change Unit to L/S\" Calculated Field endpoint to the Aggregate Agent and save. Note You may need to maximize the page to see the grid properly. You can do this by pressing the \"Maximize\" button in the top-right corner of the page. Press the \"Restore\" button in the top-right corner to return it to the regular size. Configure the Aggregate Agent as follows: Attributes to group on: L_PumpId Aggregate: Average (of) L_FlowRate (as) FlowRateAvg Average (of) L_WaterTemperature (as) CoolantTemperatureAvg Unit: Second Size: 5 Press \"Apply\" on the Average across 5 seconds configuration page, and press \"Save\" on the Data Stream page. Note Image: Aggregate Agent configuration (Image not available) Note Image: Aggregate Agent configuration details (Image not available) Data Conversion Note Image: Data Conversion configuration (Image not available) We want the data for the average flow rate to be in integer format to display it more easily. This can be achieved through the Data Conversion Agent. To do this, drag in a Data Conversion Agent and rename it to \"Data Conversion\". Connect the \"Average across 5 seconds\" endpoint to the Data Conversion Agent and press \"Save\" on the Data Stream page. Configure the Data Conversion Agent with the following two rows (You may need to maximize the page again). Click on the + symbol to add each row: First row: Input Column: \"FlowRateAvg\" Output Alias: \"FlowRateAvg\" Data Type: \"Int64\" Second row: Input Column: \"CoolantTemperatureAvg\" Output Alias: \"CoolantTemperatureAvg\" Data Type: \"Int64\" The input columns should already have FlowRateAvg and CoolantTemperatureAvg as options listed in the dropdown menu. This will replace the FlowRateAvg and CoolantTemperatureAvg with values converted to Integer format. Press \"Apply\" on the Data Conversion configuration page, and press \"Save\" on the Data Stream page. Note Image: Data Conversion configuration details (Image not available) Note Image: Data Conversion completed (Image not available) Output Formatting and Action Integrations In this section, we will integrate our Data Stream with the App Designer to trigger Recommendations and send data to Apps. Run Recommendation Note Image: Run Recommendation configuration (Image not available) First, we want to trigger Recommendations with the data from the Data Stream. This can be achieved with the Run Recommendation Agent. To do this, drag in a Run Recommendation Agent and rename it to \"Run Recommendation\". Connect the \"Data Conversion\" endpoint to the Run Recommendation Agent and press \"Save\" on the Data Stream page Configure the Run Recommendation Stream Object as follows: Url: the URL of the App Designer site Key: the App Designer Key. Output on first occurrence Only?: true Entity Identifier: L_PumpId - this is for the Recommendation to create separate Alerts for each Entity Columns To Return: Leave empty (Return all columns) Note It is highly recommended that you use any variables that you already have that store the URL or key. You may use the variables that have already been set up if you are using the Free Trial. Otherwise, the App Designer URL and Key can be found by following these steps: Open the App Designer in a new tab by clicking the \"Waffle\" button (a.ka. \"App Launcher\") in the top left corner of the page and clicking \"App Designer\". Copy the App Designer URL from the browser's address bar and paste it into the Url field in the Run Recommendation configuration. Click the \"Settings\" button in the top bar and click the \"Copy\" button to the right of the Integration Key and paste it in the Key field in the Run Recommendation configuration. You will only be able to see this if you have Admin access. If you do not have Admin access, you can ask an Admin to share the key with you. Note If you are configuring the URL and Integration Key without using variables, make sure you uncheck the \"Use Connection Variables\" checkbox option first. Note Image: App Designer menu (Image not available) Note Image: Run Recommendation configuration details (Image not available) This is how your Data Stream and configuration should look: Note Image: Data Stream with Run Recommendation (Image not available) Press \"Apply\" on the Run Recommendation configuration page, and press \"Save\" on the Data Stream page. XMPro App Note Image: XMPro App configuration (Image not available) We want to send data to an App to be displayed as a decision support dashboard for the engineers. This can be achieved through the XMPro App Agent. Drag two XMPro App Agents onto the Data Stream and name them \"Post Pump Overview\" and \"Post Pump Specifics\". One will send an overview of the data for all pumps, and the other will send a large cached amount of data for each pump. Now we run into a problem; we want to connect multiple agents to the same data. To solve this, drag a Broadcast Agent into the Stream, and rename it to \"Broadcast\". Disconnect the \"Ignore Pumps Under Maintenance\" input arrow and connect it to the new Broadcast Stream Object. You can disconnect the arrow by highlighting the arrow itself and clicking on the \"Delete\" button at the top of the Data Stream. Alternatively, you can click on the green rectangle (input) on the \"Ignore Pumps Under Maintenance\" Stream Object, and drag the arrow to the green rectangle (input) of the \"Broadcast\" Stream Object. Connect the Broadcast endpoints to the two XMPro App Stream Objects and the \"Ignore Pumps Under maintenance\" Filter, as shown in the video below: Note Image: Connecting Stream Objects with Broadcast (Animation not available) Press the \"Save\" button at the top of the Data Stream. Your Data Stream should now look like this: Note Image: Data Stream with Broadcast and XMPro App (Image not available) Configure the \"Post Pump Overview\" Stream Object to store in cache and output only one row per pump as follows: (Follow the steps given for the Run Recommendation Agent above to get the Url and Key.) Url: the URL of the App Designer site Key: the App Designer Key. Cache Size: 1 Replace Cache: false Cache Per Entity: true Entity Identifier: L_PumpId Primary Key: L_PumpId Note It is highly recommended that you use any variables that you already have that store the URL or key. You may use the variables that have already been set up if you are using the Free Trial. Note If you are configuring the URL and Integration Key without using variables, make sure you uncheck the \"Use Connection Variables\" checkbox option first. Press \"Apply\" on the Post Pump Overview configuration page and press \"Save\" on the Data Stream page. Note Image: Post Pump Overview configuration (Image not available) Configure the \"Post Pump Specifics\" Stream Object to cache and output 20 rows per pump as follows: Url: the URL of the App Designer site Key: the App Designer Key. For more detail on how to find the key in the Site Settings, see this article. Cache Size: 20 Replace Cache: false Cache Per Entity: true Entity Identifier: L_PumpId Primary Key: L_PumpId and L_ReadingNo Note It is highly recommended that you use any variables that you already have that store the URL or key. You may use the variables that have already been set up if you are using the Free Trial. Note If you are configuring the URL and Integration Key without using variables, make sure you uncheck the \"Use Connection Variables\" checkbox option first. Press \"Apply\" on the Post Pump Specifics configuration page and press \"Save\" on the Data Stream page. Note Image: Post Pump Specifics configuration (Image not available) Your Data Stream is now complete. To start the stream, click on the \"Publish\" button. To see the data flow at each Stream Object, press the \"Live View\" button and select all the Stream Objects. Alternatively, you can also select specific Stream Objects. For example, if you just want to see the data flowing through to the XMPro App, select the XMPro App Stream Object. Troubleshooting the Data Stream To see if data is flowing properly within the Data Stream, you will need to Publish the Data Stream. Before publishing, you want to make sure there are no errors in the configurations of the Stream Objects. Click on the \"Integrity Check\" option at the top of the Data Stream. If any errors are present, the Stream Object with the errors will turn red. Hovering over the Stream Object will show you the list of errors. Once these errors are fixed, you will need to run the Integrity Check again. Note To read more about Integrity Checks, read the Verifying Stream Integrity article. Once all Stream Objects have passed the Integrity Check, you can click on \"Publish\", then \"Live View\", on the top of the Data Stream. The Live Data will open on the side, and you can then click on \"Select Views\" to click on the Stream Objects you want to troubleshoot. If data is displaying for the Stream Object, that means the Stream Object should be working correctly. If not, you can recheck your configuration values for the Stream Object. You can also check if you have a Stream Host running. There are also other ways that you can troubleshoot Data Streams. Note For more ways on how you can troubleshoot a Data Stream, read the Troubleshoot a Data Stream article. 2. Create Event Rules & Recommendations The Use Case requires that engineers should be alerted if the flow rate averaged over 5 seconds falls below 250 L/s, furthermore, if the temperature averaged over 5 seconds also starts to rise above 130°C then a critical level alert should be raised. To achieve this we will use Recommendations. Recommendations can be found in the App Designer. Open the App Designer in a new tab by clicking the button in the top left corner of the page and clicking \"App Designer\". To access the Recommendation management section of the App Designer, click on the \"Recommendations\" button in the left menu and press the \"Manage Recommendations\" button on the page. Create Event Rule and Rule Logic To trigger the required Alerts, we will be creating a Recommendation with two Rules. First, create a Recommendation called \"Pump Flow Threshold\". The Data Stream should be the same \"Pump Condition Monitoring\" stream we created previously. To create a new Recommendation, follow the steps below after navigating to the Recommendation management page: Click \"New\". Specify a name and category for your new Recommendation. Choose a Data Stream to receive data from. Click \"Save\". Warning Make sure to click the \"Manage Access\" command and give at least yourself Run Access, otherwise, you won't be able to see any Recommendation Alerts that will be generated by this Recommendation. This Rule will notify Engineers when the Flow Rate is lower than 250 L/s and give them instructions and resources to help resolve the issue. Select the Enable Execution Order checkbox, since we want the more critical rule to override the medium rule. Create a Rule by pressing the + button to the right of the Rules list. Give the Rule the following properties: Note The tags for the Alert Heading and Alert Description fields will not work if they are copied and pasted into the field. You will need to select the tags yourself by adding an @ symbol and selecting from the tags in the list. Name Value Description Rule Name Medium - Flow rate falling The Rule Name is for identification only and will not be shown to the user in the Recommendation Alerts grid or detailed view. Alert Headline Warning @L_PumpId: Flow rate is falling This refers to the headline that the Recommendation Alert will be created with. Any tag (starting with @) will be replaced with the value output from the Data Stream. Add tags by typing @ and selecting the item. Alert Description Flow rate is reported to be falling, danger of plant overheating and shutdown. Flow Rate: @FlowRateAvg Coolant Temperature: @CoolantTemperatureAvg This refers to the description that the Recommendation Alert will be created with. Any tag (starting with @) will be replaced with the value output from the Data Stream. Alert Ranking Medium The priority level that the Recommendation Alert will be created with. Priority level determines the order in which the Alerts will be displayed. Icon An icon of your choice (hover your mouse over the default icon to upload a different one - sample icons can be found in the Icon Library) The icon that will be displayed on the Recommendation Alert in the grid and in detailed view. Impact Metric Prefix: $ Value: 15 Unit of Measurement: K The impact that the Recommendation Alert will have. For example, if the value for this was $15K, that means that the cost of the condition causing the Alert would be $15K (or $15000). This will be shown on Alerts in the Recommendation Block in App Pages. Rule Logic FlowRateAvg Is less than or equal to 250 Data sent from the selected Data Stream is passed through the Rule Logic and if the conditions created are met by the data (and if Recurrence is set to All Occurrences or First Occurrence and no Pending Alert exists), a new Recommendation Alert will be created. You can add new conditions or groups by clicking the + button. Groups can be nested within each other to create advanced logic. In an \"And\" group, all the conditions must be true, and in an \"Or\" group, only one of the conditions must be true to trigger an alert. The Rule Logic determines whether a Recommendation Alert will be created on receiving data from the Data Stream. Note For more details on Rule Logic, see this article. Create recommendations & Triage Instructions There should be some instructions for the engineers to follow to help resolve the issue when it occurs. This can be provided through the Triage Instructions. Continue creating the Rule with the following properties: Name Value Description Enable Form false A flag that determines whether the Recommendation Alert will be created with a Form. Additional Recommendation Management Column - An additional column in the Recommendation Alerts grid. Resolution Manual Resolution determines whether new data from the Data Stream will automatically resolve the Recommendation Alert if the Rule Logic is no longer true. Manual Resolution: A user must manually resolve each recommendation. Automatic Resolution: Recommendation auto resolves when trigger conditions are no longer true. This may impact performance. Recurrence First Occurrence Recurrence determines whether new data from the Data Stream will create new Recommendation Alerts if there already exists a Pending Recommendation Alert and the Rule Logic is true. Recurrence will create an Alert for each unique Entity selected by the Data Stream. For example, since there are three pumps (A, B, and C), each pump will generate its own Alert when something goes wrong, and it will need to be resolved before new Alerts for that pump are created. First Occurrence: The current recommendation must be resolved before others can be triggered for the same rule. All Occurrences: A new recommendation will be triggered every time the rule conditions are true. Recurrence will be disabled and be considered to be First Occurrence if the Recommendation has Execution Order enabled. Log Data On First Occurrence Determines whether new events from the Data Stream that satisfy the Rule Logic, after a recommendation has been triggered, will be logged or not. First Occurrence: Current recommendation will only log the initial event which triggered the recommendation. All Occurrences: Event data will be stored for every instance that satisfies the rule logic. Enable Triage Instructions true A flag that determines whether the Recommendation Alert will be created with Triage Instructions. Triage Instructions Instructions to help whoever is resolving the Recommendation Alert. Find the Triage Instructions below to copy for this use case. Possible problems causing a discharge pressure drop: Blocked Suction Pipe A partial obstruction can be caused by a piece of foreign material being drawn across the bottom of the suction pipe during the operation of the pump. Such an obstruction may not be sufficient to stop operation completely, but will result in a reduced output from the pump. It will also cause a drop in discharge pressure and amps, and will increase the vacuum reading on the pump suction. Rough running and vibration of the pump may also occur due to cavitation within the pump. Blocked Impeller Impellers are capable of passing a certain size particle. If a particle larger in size enters the suction pipe, it may become lodged in the eye of the impeller, restricting the output of the pump. Such an obstruction will usually result in a drop of amperes and a drop in both discharge pressure and suction vacuum readings. The out-of-balance effects resulting from this condition may cause pump vibration. SHUTTING DOWN PROCEDURE Before you shut down the pump, it should be allowed to operate for a short period on only clean water to clear the system. Then proceed as follows: Depress the 'STOP PUMP' push-button on the control panel. Gland seal water (if any) must be left on during all subsequent operations, namely: start-up, running, shutdown, runback and system drain. Gland water may only then be turned off. Provide additional resources for decision support You can provide the Engineers with helpful links to videos, PDFs, or websites to help resolve the issue with the Resources. Continue creating the Rule by checking Enable Resources and adding the following Resources: Resource Url Test Procedure http://xmdocsdownload.s3.amazonaws.com/Technical/WarmanETP.pdf You can see this rule in action by clicking the \"Publish\" command in the Recommendation and in the Pump Condition Monitoring Data Stream. If the average flow rate coming from the Data Stream is lower than 250 L/s the Rule will generate an Alert for each Pump that can be viewed in the Recommendation Alerts grid. Click on the \"Recommendations\" button to go back to the Recommendation Alerts grid. To see more details about the alert, click on a row in the \"Recommendation Alerts\" grid. The Recommendation Alert page provides details of the alert and allows you to monitor, discuss, and take action. Multiple Rules and Rule escalation We want to trigger new Alerts that override the Medium level alert raised before if the temperature averaged over 5 seconds also starts to rise above 130°C. To do this we will create the second Rule and configure the escalation settings. This second Rule will notify Engineers when the Coolant Temperature is higher than 130°C and Flow Rate is lower than 250 L/s and give them instructions and resources to help resolve the issue. To create another Rule, click on the Recommendations page from the left-hand menu, and click on \"Manage Recommendations\". Select the \"Pump Flow Threshold\" Recommendation that was created previously. To make amendments to the Recommendation, you will first need to unpublish the Recommendation, by clicking on the \"Unpublish\" button at the top of the Recommendation. Finally, click on the \"plus\" symbol under \"Rules\" to add the new Rule. Create the new Rule with the following properties: Name Value Rule Name Critical - Plant is overheating Alert Headline Alert @L_PumpId: Plant has started to overheat due to low flow. Alert Description Plant is overheating due to low flow rate, immediate action is required to avoid damage. Temperature: \uFEFF@CoolantTemperatureAvg\uFEFF Flow Rate: \uFEFF@FlowRateAvg\uFEFF Alert Ranking High Icon Feel free to load a suitable icon or use the default. Sample icons can be found in the Icon Library. Impact Metric Prefix: $ Value: 25 Unit of Measurement: K Rule Logic FlowRateAvg Is less than or equal to 250 CoolantTemperatureAvg\uFEFF Is greater than 130 Enable Form false Additional Recommendation Management Column - Resolution Manual Recurrence First Occurrence Log Data On First Occurrence Enable Triage Instructions false Enable Resources false We also want the Critical Rule to override the Medium Rule. To do this, ensure that the Enable Execution Order and Auto-Escalate checkboxes are ticked in the Recommendation, and reorder the Rules to put the Critical Rule at the top. You can see the full Recommendation in action by clicking the \"Publish\" command in the Recommendation. Ensure that your Data Stream is also published and running. If the average coolant temperature is higher than 130°C and the average flow rate coming from the Data Stream is lower than 250 L/s the Rule will generate an Alert for each Pump that can be viewed in the Recommendation Alerts grid. Any pending Alerts from the Medium rule will be resolved and escalated to the new Alert. 3. Create Event Boards & Apps The Use Case requires that the Engineers should be provided a view to check the history of Pump telemetry, maintenance records, and Reservoir Level in order to enable them to take necessary action. This requirement can be met through the use of an App with a couple of Pages. Layout Event Boards First, create an Application (or App) by pressing the \"New Application\" button in the left menu and clicking the \"Blank App\" template. Give the App the following properties: Name: \"Power Plant THG Event Board\" Description: \"An App to monitor assets at a power plant.\" Category: any Icon: Feel free to load a suitable icon. If you do not, the default icon will be used. Sample icons can be found in the Icon Library. Default Theme: Dark Landing Page Layout: the first item Click on the \"Save\" button. This will take you to a page where you can view the list of pages in the App or edit the App itself. Click on the Landing Page to edit it. App Layout We will want two sections on the landing page, so follow the steps to duplicate the Vertical Stacked Layout as shown in the screenshot below. Click \"Page Layers\". Click on the carets to expand the layers until you reach the second Vertical Stacked Layout. Click on the Vertical Stacked Layout. A blue toolbar will appear at the top-right corner of the block. Press the third button with a \"Clone\" symbol to duplicate the Vertical Stacked Layout and everything it contains. Note See the article on Page Layers for more information. The result should look as below: You can rename both Vertical Stacked Layouts to identify them. In the Page Layers tab, double click the Left Vertical Stacked Layout to change the text to 'Left Vertical Stacked Layout'. Double click on the text of the other Right Vertical Stacked Layout and rename that to 'Right Vertical Stacked Layout'. The left side of the page should be twice as wide as the right side, so follow the steps to change the Left Vertical Stacked Layout's Flex Grow property in the Block Styling to 2 as shown in the screenshot below. Make sure the Left Vertical Stacked Layout is still selected in the Page Layers. Click the \"Block Styling\" button to open the style manager. Click the \"Flex Layout\" accordion item to expand it. Change the Grow field to 2. Press the \"Save\" button at the top of the Application designer. Note See the articles on How to use the Style Manager and How to use Flex for more information. The result should look like this: We want to display the properties of the pumps in regularly spaced and sized cards on an App page. To do this, create one card with a Data Source, which will repeat the card for each pump. Change the Left Vertical Stacked Layout's Display property in the Block Styling to block, so that the cards will overflow to however many pumps are present. Press the \"Save\" button at the top of the Application designer. Follow the steps below to select the left Card and delete it. We will replace the default Card with a useful layout. Open the Page Layers. Expand to the Left Vertical Stacked Layout's Card. Click the Card item to select the Card element. Click the last button on the blue toolbar to delete the Card. Replace the deleted Card by dragging in a Card from the Blocks, which will include some extra items. Note See the article on Blocks for more information. We want the card to take up half the width of the left section, and overflow to the next line when there are more than 2 cards. Select the new Card using the Page Layers tab. Click on the \"Block Styling\" tab. At the top, in the \"Style Group\" field, click on the checkbox on the left of the \"card-gutter\" tag to unselect the card-gutter style group. This is because we only want to apply styles to this specific element. Under the \"General\" subsection, change \"Display\" to \"inline-block\". Under the \"Dimensions\" subsection, change the Width to 50. Under the \"Dimensions\" subsection, change the unit to %. Rename the Metric text items to \"Status\", \"Flow Rate\", and \"Coolant Temperature\". You can double-click the text items and change the text. The Status should show an Indicator (a colored circle) to display the status of the pump. Delete the top Value. Right-align the remaining two Values by expanding the Typography section and changing the Text Align property to Right. Drag two Indicator Blocks from the Blocks tab into the deleted Value text item's box. Click the Box and change the Flex Justify to End. This will align the Indicators to the right side of the box. The \"Status\" indicator should be red or green, depending on whether the pump is under maintenance or not. Change the left Indicator's Color in the Block Properties to \"#ad6363\" (red), and the right to \"#398a33\" (green). Data Source for Pump Data Add the Data Source that will be used to display our pump data. Click on the \"Page Data\" button. Click the + button next to Data Sources. Add a Data Source with Name as Pump Data, Connection as Data Streams Connector, and Entity as Post Pump Overview. Check Live Data Updates as well to keep the data constantly updated. Click on the \"Save\" button. Click the Card and set the Data Source in the Block properties tab to Pump Data. Set Show Default Row to Never, as we don't want an empty card at the end. You can also open the Page Layers tab and look at the highlighted Block to confirm that the correct Block is selected. When the data comes in for each pump, we want the cards to display the values for each pump. To do this, we will give the values, indicators, and heading dynamic or expression values. To change a property to Expression mode, follow the steps below: Select the block (in this case the Heading of the card) through the canvas or Page Layers. Click the \"Block Properties\" button. When adding an expression for the \"PumpId\", press the button on the left of the Text property field to toggle between Static, Dynamic and Expression value modes. Change the mode to Expression so you can enter an expression to display the Pump Id that is selected. Give the Heading's Text property an expression value of \"Pump \" + {L_PumpId} by clicking the button on the left side of the field twice. Note See the Dynamic and Expression Properties section of the Block Properties article for more information. Select the Value Block next to 'Flow Rate'. Click on the \"Block Properties\" tab and give the Text an expression value of ToStr(Round({L_FlowRate})) + \" L/m\". Give the Coolant Temperature Value Text Block an expression value of ToStr(Round({L_WaterTemperature})) + \"°C\". The circle Indicators' Visible property should be changed to an expression value of {R_UnderMaintenance} and !{R_UnderMaintenance} respectively. Change the text of the Title to \"Pumps at Power Plant THG\". To launch the App to see how it looks, press the \"Save\" button at the top of the Page and then press the \"Launch\" button. Each unique pump entity will have its own card, and data will be updated constantly for the Flow Rate and Coolant Temperature. To get back to editing the App, press the \"Edit\" button at the top-right of the App. This button will only be shown for users who have been granted access to the App. Note See the How to Manage Access article for more information. Drilldown Page The Engineers want to be able to see a detailed chart and history of each pump when they click on a pump's card on the landing page. To make a new drilldown Page, close the Landing Page, and add a new Page with the + button. Name the Page \"Pump Details\". To link to the newly created Pump Details Page when clicking a card, close the page and open the Landing Page again. You can click and drag the grey header of the page to the right to see the list of pages. Click on the \"Page Layers\" tab. Expand the carets until you get to \"Box Hyperlink\", and select it. Click on the \"Block Properties\" tab. Change the Navigate to property to \"Page\", the Page property to \"Pump Details\", and click the \"Edit\" button to the right of Pass Page Parameters. Press the + button on the Pass Page Parameters page to add a new Parameter to the Pump Details Page. Name the Parameter \"PumpId\" and give it the Type \"String\". Click on \"Add\". As mentioned previously, clicking on the icon on the left of the textbox field can toggle between Static, Dynamic, and Expression modes. Click on the button in the Value column once to change it to 'Dynamic' mode, which is indicated by the Database symbol. Select the value \"L_PumpId\" as this is the value we want to pass to the drill-down page. Click \"Apply\" to apply your changes to the Pump Details page's Parameters and the Box Hyperlink. Note If you are unable to see the parameters in the drop-down list, double-check that the Data Source has been applied to the correct Card block, as shown above. To see the navigation in action, launch the Landing Page and click on one of the cards. You will be navigated to the Pump Details page. Press the \"Edit\" button at the top right of the Pump Details page to edit it. We want a similar layout for this page, so follow the steps from the Landing Page to duplicate the Vertical Stacked Layout, and make the left side twice as wide as the right side. See the instructions that previously showed how to do this. We want two cards inside the Left Vertical Stacked Layout, so select the left Card and duplicate it. We want the top left card to be twice as tall as the bottom left card, so change the top left Card's Flex Grow property to 2 in the Block Styling. Remember to uncheck the card-gutter style group to make the style only apply to the selected element. We want the title of the page to be the Pump's name, so change the Title's Text property to an expression value \"Pump \" + {Parameter.PumpId}. We also want the Headings of the page to explain what is being shown, so change the top-left heading's text to \"Details\" and the bottom-left heading's text to \"Maintenance History\". We need to get the data from the Data Stream. Go back to the Page Data tab, and next to 'Data Sources', click on the \"plus\" symbol to add another Data Source. Name the new Data Source \"Pump Live Data\", with the Connection \"Data Streams Connector\", Entity as \"Post Pump Specifics\", and Live Data Updates checked. Drag a Chart into the Details card. Highlight the Chart, and in the Block Properties tab, set the Data Source of the Chart to \"Pump Live Data\". Add a Filter to the Data Source of L_PumpId Equals {Parameter.PumpId}. Click on the \"plus\" button to start adding the filter. When selecting the \"PumpId\", press the button on the left of the field to toggle between Static and Dynamic value modes. Change the mode to Dynamic so you can select a dynamic value from the dropdown. Here you can select the \"PumpId\" Parameter. This will make sure the chart is only showing data from the pump that we are looking at. Note See the Chart article for more information. We want the chart to look great and display a line series in a separate panel for flow rate and temperature. To do this, edit the Block Properties of the Chart as follows: In the Appearance accordion item set the Legend Alignment to \"Align Bottom Center\". In the Axes accordion item, change Type to \"Date Time\", Enable Pan and Zoom to \"False\" and Display Grid Lines to \"False\". In the Data accordion item, add two series with the + button to the right of the Series list as follows (leave the default value if it is not specified): Property Flow Rate Series Temperature Series Name Flow Rate Temperature Color #7ee2b5 #c46565 Type Line Line Pane Default Temperature X Axis Data L_Timestamp L_Timestamp Y Axis Data L_FlowRate L_WaterTemperature When making the 'Temperature' series, you will need to add a new Pane called 'Temperature.' To add a new Pane for the chart, click the 'Edit' button next to 'Pane' in the series property page. You can see how this looks by launching the page with the PumpId parameter \"A\", \"B\", or \"C\" On this page we want to also show the Engineers the pump maintenance history. This can be achieved through the use of a grid and a SQL Data Source. Below is a script that can be used to create the table used: CREATE TABLE [dbo].[PumpMaintenance]( [Id] [bigint] IDENTITY(1,1) NOT NULL, [PumpId] [nvarchar](10) NOT NULL, [Timestamp] [date] NOT NULL, [Comments] [nvarchar](max) NOT NULL, CONSTRAINT [PK_PumpMaintenance] PRIMARY KEY CLUSTERED ( [Id] ASC )) INSERT INTO [dbo].[PumpMaintenance] ([PumpId], [Timestamp], [Comments]) VALUES (N'A', CAST(N'2021-05-26' AS Date), N'Routine Inspection, No issues reported.') INSERT INTO [dbo].[PumpMaintenance] ([PumpId], [Timestamp], [Comments]) VALUES (N'B', CAST(N'2021-05-25' AS Date), N'Routine Inspection, No issues reported.') INSERT INTO [dbo].[PumpMaintenance] ([PumpId], [Timestamp], [Comments]) VALUES (N'C', CAST(N'2021-05-24' AS Date), N'Routine Inspection, No issues reported.') INSERT INTO [dbo].[PumpMaintenance] ([PumpId], [Timestamp], [Comments]) VALUES (N'A', CAST(N'2021-03-15' AS Date), N'High Vibration detected, main shaft replaced.') INSERT INTO [dbo].[PumpMaintenance] ([PumpId], [Timestamp], [Comments]) VALUES (N'B', CAST(N'2021-03-14' AS Date), N'Routine Inspection, bearing replaced.') INSERT INTO [dbo].[PumpMaintenance] ([PumpId], [Timestamp], [Comments]) VALUES (N'C', CAST(N'2021-03-13' AS Date), N'Routine Inspection, bearing replaced.') INSERT INTO [dbo].[PumpMaintenance] ([PumpId], [Timestamp], [Comments]) VALUES (N'A', CAST(N'2021-01-04' AS Date), N'Routine Inspection, No issues reported.') INSERT INTO [dbo].[PumpMaintenance] ([PumpId], [Timestamp], [Comments]) VALUES (N'A', CAST(N'2020-07-25' AS Date), N'Routine Inspection, bearing replaced.') Drag a Data Grid into the Maintenance History card and edit its Block Properties. Press the + button next to the Data Source select box to add a new Data Source, and name it \"Pump Maintenance.\" We want to add a SQL Data Source, but we don't have a SQL Connection yet. Press the + button next to the Connection select box and click on \"SQL Connector.\" Give the new SQL Connector a Name, enter your own connection string, User Name, and Password, and choose a Database. If you are using the Free Trial, you can get these details in your welcome email. XMPro sets up a 2GB database for you to use during the 120-day trial period. Save the Connection and select it in the New Data Source. Select the \"PumpMaintenance\" table and give the Data Source the name \"Pump Maintenance\". Save the new Data Source and select it in the Data Grid's Block Properties. This grid will show all the Pump Maintenance rows, but we want to show just the rows that this page is looking at. To do that, edit the Filter and add the filter logic: PumpId Equals {Parameter.PumpId} We want to change the Data Grid's columns around to make them look good. Open the Columns accordion item, and reorder the columns by dragging the dotted handles to the left of the list. Order them Id, Comments, Timestamp, PumpId. We don't want to show the PumpId, as it will always be the same. To hide it, click the \"PumpId\" row in the list and change the Visible property to \"False\". Apply the changes to the column. The Timestamp column should be in Date format, not Date Time. Edit the Timestamp column and change Type to \"Date\". Launch the page with the PumpId parameter \"A\", \"B\", or \"C\" to see this in action. It should look like this: Easy Access to Recommendations We want engineers to be able to see and respond to Recommendation Alerts from this App, so we should drag a Recommendations Block into the right card of both Pages. Also, rename the heading to \"Recommendations\". To filter the Recommendation Alerts to the relevant ones only, select the \"Pump Flow Threshold\" in the Block Properties of the Recommendations Block on both pages. On the Pump Details Page, we only want to show the Recommendation Alerts for the specific pump we're looking at. To do this, Change the Entity ID property to Equals Parameter.PumpId. We have now completed the requirements of the Use Case. To see the App running live, click on the \"Launch\" button. You can also publish the App by dragging the canvas to the left to open the page list, and clicking on \"Publish\". Here is how the final drill-down page looks:"
  },
  "docs/getting-started/free-trial.html": {
    "href": "docs/getting-started/free-trial.html",
    "title": "Free Trial | XMPro",
    "summary": "Free Trial How To Sign Up Are you interested in taking the XMPro Application Development Platform for a test drive? Visit this link to request a 90-day free account with no credit card required to get started. After completing the registration form, you will receive a confirmation email with your login details. What's Included in the Free Trial During the free trial, you will get access to the full suite of XMPro Products along with: Unlimited Data Streams Unlimited Applications Unlimited Recommendations XMPro Notebook Selection of XMPro Agents and Connectors Basic 2GB Azure SQL database for master data storage Please get in touch with support if you require additional Agents and Connectors unavailable in your trial account. Note You will need to upgrade your XMPro Free Account to a paid subscription to continue using the services after your trial period expires. Read the free trial terms and conditions for more details. Explore The Demo Use Case Your free trial account includes a pre-built demo use case, which includes a Data Stream, App and Recommendation focused on solving a specific business problem. In this scenario, a renewable energy company wants to move from a planned maintenance schedule to condition-based maintenance for their assets across multiple plants. There is a Data Stream to: Simulate real-time telemetry data for the assets Combine it with contextual data from a SQL database Check whether the data exceeds certain thresholds Run recommendation rules Publish data to an XMPro App We've also created an App to provide the energy company's engineers with decision support for their new condition-based maintenance program. The App provides them with: A map view of all their assets at their various plants The remaining useful life for assets that require maintenance in the next 14 days Recommendation alerts when specific assets exceed certain thresholds If you drill down into a specific asset, you will see: Live telemetry data such as bearing vibration and temperature for different components Live wind speed and gearbox oil level visualizations Operational safety intelligence information to prevent safety incidents during maintenance The effective utilization score for this specific asset In this scenario, XMPro will generate a recommendation alert when: Gearbox oil viscosity for a wind turbine goes above 75 Oil level reaches a low threshold Next Steps This demo use case demonstrates how you can use the XMPro Platform to build a real-time application in 3 simple steps: Create Data Streams to integrate your data sources & orchestrate the data flow Design visualizations for a real-time view of your operations Create prescriptive recommendations that trigger when critical events happen Once you're done exploring the demo, you can start building your first End-To-End Use Case by following this detailed tutorial."
  },
  "docs/how-tos/agents/README.html": {
    "href": "docs/how-tos/agents/README.html",
    "title": "Agents | XMPro",
    "summary": "Agents An Agent is a reusable object which forms the building block of a Data Stream. When a number of Agents are connected together, a Data Stream is formed. Each Agent is designed to perform a specific function in the stream. For example, they can be used to retrieve data from a database in real-time, display data, filter, sort the data, or save the data somewhere else, depending on the function of that individual Agent. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Agents. Agent {% endhint %} Articles {% content-ref url=\"manage-agents.md\" %} manage-agents.md {% endcontent-ref %} {% content-ref url=\"building-agents.md\" %} building-agents.md {% endcontent-ref %} {% content-ref url=\"packaging-agents.md\" %} packaging-agents.md {% endcontent-ref %} {% content-ref url=\"debugging-an-agent.md\" %} debugging-an-agent.md {% endcontent-ref %}"
  },
  "docs/how-tos/agents/building-agents.html": {
    "href": "docs/how-tos/agents/building-agents.html",
    "title": "Building Agents | XMPro",
    "summary": "Building Agents Overview To get started with developing a new Agent, create a new C# library project in Visual Studio and import the XMPro.IoT.Framework NuGet package. When writing the code for an Agent, you will have to implement a number of interfaces. Which interfaces to implement depends on the category under which your Agent will fall: Listeners Listeners are created by implementing IAgent and IPollingAgent interfaces. To push the events to the next receiver, the OnPublish event should be invoked and the events should be passed as arguments. Action Agents/ Functions Action Agents are created by implementing the IAgent and IReceivingAgent interfaces. The Receive method will be called every time events are received by this Agent. To publish these events again, the same logic as per the Listener Agent can be used. Context Providers Context Providers are created by implementing the IAgent, IPollingAgent interfaces. They are very similar to Listeners; however, Context Providers publish all available records/events when polled instead of only publishing the newer/changed ones. Transformations Transformations are implemented in a similar way as Action Agents, except that all Transformations should have the Require Input Map flag set to false and must not implement the GetInputAttributes method, hence it should be: public IEnumerable<XMIoT.Framework.Attribute> GetIntputAttributes(string endpoint, IDictionary<string, string> parameters) { throw new NotImplementedException(); } The interfaces that can be implemented are as follows: IAgent IPollingAgent IReceivingAgent IPublishesError The matrix below shows which interface needs to be implemented for which category Agent: Agent Category IAgent IPollingAgent IReceivingAgent IPublishesError Listener Required Recommended Optional Optional Context Provider Required Recommended Optional Optional Transformation Required Optional Required Optional Action Agent/ Function Required Optional Required Optional AI & Machine Learning Required Optional Required Optional Note The IPollingAgent interface is not strictly required for Listeners or Context Providers, however, it is generally used in most cases. Not implementing IPollingAgent for a Listener or Context Provider should be considered an advanced option. IAgent IAgent is the primary interface that must be implemented by all Agents as it provides the structure for the workings of the Agent. After implementing this interface, there are several methods you have to add to your project that forms part of this predefined structure. Settings/Configurations Some Agents need to be provided with configurations by the user, for example, for a CSV listener Agent to get records from a CSV file, it needs the following: Polling interval (in seconds) CSV file CSV Definition Each of these settings should be referenced in the code and must correspond to the settings template created when packaging your Agent. Note A template is a JSON representation of all the controls and their layout that will be used to capture the settings from a user. An example of the settings template (generated using the XMPro Package Manager) is shown in the image below. The settings in this example consist of the following controls: Group (Data) File Upload Group (Payload) Grid Each control has a Key, which uniquely identifies it in the template and allows the Agent code to access its value at any time. To get the value contained in a setting, use the following code: string mySetting = parameters[\"myUniqueKey\"]; Before a template is rendered on the screen, or if a postback occurs on any control in the template, the method below would be called to allow the Agent an opportunity to make any necessary runtime changes to the template, for example, verifying user credentials, displaying all tables of a selected database in a drop-down list, etc. In this example, no changes are being made to the template but, if needed, they can be added to the todo section. Note For a postback to occur after a user navigates out of a setting field, the Postback property needs to be set to true when packaging the Agent. public string GetConfigurationTemplate(string template, IDictionary<string, string> parameters) { //parse settings JSON into Settings object var settings = Settings.Parse(template); //populate the settings/configuration controls with the user selected values new Populator(parameters).Populate(settings); // ToDo: update the controls, values or the data sources here //return the updated settings xml return settings.ToString(); } Validate If a user tries to run an Integrity Check on a Data Stream in Data Stream Designer, all Agents will be requested to validate the configurations they have been provided. An Agent has to use this opportunity to inform the user about any configurations that are incorrect, for example, credentials that have expired, required values that are missing, etc. To validate the configurations/ settings in an Agent, the Validate method needs to be implemented. This method returns an array of errors that occurred. If validation was successful, an empty array would be returned. The example code below verifies if a user has specified a broker address, topic, and payload definition for an MQTT Agent: public string[] Validate(IDictionary<string, string> parameters) { int i = 1; var errors = new List<string>(); this.config = new Configuration() { Parameters = parameters }; if (String.IsNullOrWhiteSpace(this.Broker)) errors.Add($\"Error {i++}: Broker is not specified.\"); if (String.IsNullOrWhiteSpace(this.Topic)) errors.Add($\"Error {i++}: Topic is not specified.\"); var grid = new Grid(); grid.Value = this.config[\"PayloadDefinition\"]; if (grid.Rows.Any() == false) errors.Add($\"Error {i++}: Payload Definition is not specified.\"); return errors.ToArray(); } Output Payload Each Agent has the responsibility to inform the Engine about the structure of the payload that will be produced by the Agent. To do this, implement the following method: IEnumerable<Attribute> GetOutputAttributes(string endpoint, IDictionary<string, string> parameters) This method returns a collection that has an Attribute type, which is a type that represents the name and type of a given attribute in the outgoing payload. As from XMPro.IOT.Framework version 3.0.2, comparison/ equality operations are also supported in Attribute, for example: new XMIoT.Framework.Attribute(\"Name1\", Types.DateTime).Equals(new XMIoT.Framework.Attribute(\"Name2\", Types.String)); Create Each Agent needs to implement a method called Create, which will be invoked when your Agent is being hosted. User-defined configuration is passed as a parameter to this method and should be stored in a class variable as far as possible for later use. This is a good point to provide any resources needed for the working of your Agent. void Create(Configuration configuration) { this.config = configuration; // ToDo: Provision any resources or write Startup logic. } Start The Start method needs to be implemented by all Agents. This method will be invoked when your Agent is hosted and starts to work. void Start() Destroy Each Agent needs to implement a Destroy method, which will be invoked if the Create method was called successfully, when a data stream is either being unpublished or it encounters an error and fails to start. Use this method to release any resources or memory that your Agent may have acquired during its creation and lifetime. void Destroy() Publishing Events To push the events to the next Agent, your Agent should invoke the OnPublish event with the events passed as arguments: this.OnPublish?.Invoke(this, new OnPublishArgs(new JArray(), \"EndpointName\")); Note Events are represented as JSON Objects and have to be pushed as a collection, i.e. JArray. Caution Please note that OnPublishArgs(Array rtr) is obsolete from XMPro.IOT.Framework 3.0.2 onwards. You are now required to specify the endpoint name on which you would like to publish (i.e. OnPublishArgs(Array rtr, string Endpoint)) Decrypting Values If an Agent's configuration contains a Secure/Password Textbox, its value will automatically be encrypted. To decrypt the value, use the following set of instructions: var request = new OnDecryptRequestArgs(value); this.OnDecryptRequestArgs?.Invoke(this, request); var decryptedVal = request.DecryptedValue; Custom Events While building your Agent, you may need to use external libraries or third-party event subscriptions to handle custom events. If these are used, you must catch any exceptions from the event handlers yourself, to prevent uncaught exceptions that could possibly crash the Data Stream if they get through. IPolling Agent The IPollingAgent interface allows time-based operations. Implementing this interface, and opting in to Polling by returning true from the RequiresPolling method, will automatically add a PollingInterval setting to the configuration template of your Agent, which can be used by the user to specify the interval for polling. The Poll method will be invoked every time the poll interval elapses. void Poll() This method will be called at regular intervals according to the Configuration settings, and can be used to perform any work or logic you wish, for example, querying a third-party system for changes. bool RequiresPolling(IDictionary<string, string> parameters) The RequiresPolling method is an advanced option. It is expected that in most cases, this method should simply return a true value, which will not change the behaviour of the Agent. The PollingInterval setting will display as normal, and the Poll method will be called at that interval, as normal. Advanced users, however, can use this method to decide to opt-out of Polling settings, by returning false. The parameters method parameter will contain the Stream Object's Configuration, allowing you to determine whether to return true to opt-in, or false to opt out, depending on what settings the user has selected. Opting out will cause the PollingInterval setting to not appear in the configuration tab, and the Poll method to never be called when the Stream is published. This may be useful when the agent you are building can be configured to either actively query its configured third-party system for data at regular intervals, or set up a persistent connection to the third-party service and passively wait for that connection to deliver data. If the Agent does not need to query for data at regular intervals, or perform other work or logic on a specific schedule, it is recommended to not implement IPollingAgent rather than always returning false from the RequiresPolling method. IReceivingAgent If your Agent is required to receive inputs from other Agents, you should implement the IReceivingAgent interface. Input Payload Each Agent is responsible to inform the Engine about the structure of the payload it consumes. To achieve this in your Agent, implement the following method: IEnumerable<Attribute> GetInputAttributes(string endpoint, IDictionary<string, string> parameters) This method returns a collection consisting of Attribute, which is a type that represents the name and type of a given attribute in the incoming payload. Input Mapping In most cases, if an incoming payload structure is supposed to be different from what the parent is sending, i.e. the Input Payload has been specified above, the user will have to map parent outputs to the current Agent's inputs. To enable this, mark the Require Input Map flag as true in the Stream Integration Manager when packaging the Agent. Endpoint Each Agent can have a number of input and output endpoints. Endpoints are the points where incoming or outgoings arrows are connected. Each endpoint consists of a Name<String> attribute. You will be passed an endpoint name when queried for an Input payload definition. Be sure to specify the endpoint name when querying the parent's output payload definition. Parent Outputs All receiving Agents can query the structure of parent Agent outputs connected at a given endpoint by invoking an event, as demonstrated in the example below: var args = new OnRequestParentOutputAttributesArgs(this.UniqueId, \"Input\"); this.OnRequestParentOutputAttributes.Invoke(this, args); var pOuts = args.ParentOutputs; Receiving Events Events published to a receiving Agent can be received by implementing the following method: void Receive(string endpointName, JArray events) The endpointName parameter will identify which endpoint the events have been received at. Note It is not guaranteed that the Start method will be invoked before the Receive method. Use the Create method to execute any logic that needs to be executed before the Receive method is called. IPublishError An Agent can publish messages to an error endpoint by implementing the IPublishesError interface. An unhandled error in an Agent will be captured and error information will be published to the error endpoint. Implement the interface member: public event EventHandler<OnErrorArgs> OnPublishError; To push the error to the next Agent, the OnPublishError event should be invoked, and the error information should be passed as arguments: this.OnPublishError?.Invoke(this, new OnErrorArgs(AgentId, Timestamp, Source, Error, DetailedError, Data)); Note Error endpoints should be enabled in XMPro Stream Integration Manager when packaging the Agent. This can be done by selecting the \"Add On Error Endpoint?\" checkbox. See the image on the right for an example. Example The code below is an example of a basic MQTT Listener Agent. Take note of how the interfaces and methods have been implemented. Note Please note that this example uses the M2MqttDotnetCore 1.0.7 NuGet package. using Newtonsoft.Json.Linq; using System; using System.Collections.Generic; using System.Linq; using System.Text; using uPLibrary.Networking.M2Mqtt; using uPLibrary.Networking.M2Mqtt.Messages; using XMIoT.Framework; using XMIoT.Framework.Settings; using XMIoT.Framework.Settings.Enums;namespace XMPro.MQTTAgents { public class Listener : IAgent { private Configuration config; private MqttClient client; private string Broker => this.config[\"Broker\"]; private string Topic => this.config[\"Topic\"]; public long UniqueId { get; set; } public event EventHandler<OnPublishArgs> OnPublish; public event EventHandler<OnDecryptRequestArgs> OnDecryptRequest; public void Create(Configuration configuration) { this.config = configuration; this.client = new MqttClient(this.Broker); this.client.MqttMsgPublishReceived += Client_MqttMsgPublishReceived; } public void Start() { if (this.client.IsConnected == false) { this.client.Connect(Guid.NewGuid().ToString()); this.client.Subscribe(new string[] { this.Topic }, new byte[] { MqttMsgBase.QOS_LEVEL_EXACTLY_ONCE }); } } private void Client_MqttMsgPublishReceived(object sender, uPLibrary.Networking.M2Mqtt.Messages.MqttMsgPublishEventArgs e) { try { var message = Encoding.UTF8.GetString(e.Message); this.OnPublish?.Invoke(this, new OnPublishArgs(JArray.Parse(message), \"Output\")); } catch (Exception ex) { Console.WriteLine($\"{DateTime.UtcNow}|ERROR|XMPro.MQTTAgents.Listener|{ex.ToString()}\"); } } public void Destroy() { if (this.client?.IsConnected == true) this.client.Disconnect(); } public string GetConfigurationTemplate(string template, IDictionary<string, string> parameters) { var settings = Settings.Parse(template); new Populator(parameters).Populate(settings); return settings.ToString(); } public string[] Validate(IDictionary<string, string> parameters) { int i = 1; var errors = new List<string>(); this.config = new Configuration() { Parameters = parameters }; if (String.IsNullOrWhiteSpace(this.Broker)) errors.Add($\"Error {i++}: Broker is not specified.\"); if (String.IsNullOrWhiteSpace(this.Topic)) errors.Add($\"Error {i++}: Topic is not specified.\"); var grid = new Grid(); grid.Value = this.config[\"PayloadDefinition\"]; if (grid.Rows.Any() == false) errors.Add($\"Error {i++}: Payload Definition is not specified.\"); return errors.ToArray(); } public IEnumerable<XMIoT.Framework.Attribute> GetOutputAttributes(string endpoint, IDictionary<string, string> parameters) { var grid = new Grid(); grid.Value = parameters[\"PayloadDefinition\"]; foreach (var row in grid.Rows) { yield return new XMIoT.Framework.Attribute(row[\"Name\"].ToString(), (Types)Enum.Parse(typeof(Types), row[\"Type\"].ToString())); } } } } Further Reading Packaging Agents"
  },
  "docs/how-tos/agents/debugging-an-agent.html": {
    "href": "docs/how-tos/agents/debugging-an-agent.html",
    "title": "Debugging an Agent | XMPro",
    "summary": "Debugging an Agent Agents can be traced and debugged once they are added and used in a Data Stream. This is particularly useful when testing if an agent is working as intended and troubleshooting agent issues along the way. Setup Before Debugging Package the agent using this guide. Make sure that the Agent is packaged as non-virtual. If the Agent is required to be virtual, temporarily set it to non-virtual during development and just repackage it to virtual when releasing. Note Only non-virtual agents could trigger all breakpoints inside the Agent's code. This will also allow you to access any local environment (database, server, etc.) that you will use for testing. Add the Packaged Agent in the Data Stream Designer by following the Adding an Agent article. Install the XMPro Stream Host by following the Install a Stream Host article. Within the installation wizard, select Console Application as the Host Type. Create a new Data Stream for testing the agent and select the collection profile you used during Installation of Stream Host. Add the created Agent in the Data Stream and Save. Run the Stream Host as Admin and Publish the Data Stream for the first time. A cache folder will be created inside the Stream Host folder which will be used later on. Steps to Debug an Agent Build the Agent Project to generate a dll. If built using the Debug configuration, the dll usually would be found inside the folder [Project folder]/bin/Debug/netstandard2.1. Replace the Agent dll on the Stream Host installation folder under Cache/[Agent Id]/[Agent Version] with the one generated. To determine an Agent's id, export a JSON from the XMP file of the agent via Package Manager and determine the value from the Id property. Run Stream Host as Admin. Run Visual Studio as Admin and open Agent solution. Add a breakpoint in the Agent's code. Attach XMPro.StreamHost.Console.exe to process. By default, Visual Studio shortcut is ctrl + alt + p. Publish the test Data Stream on the Data Stream Designer to initiate debugging. Common Issues Issue: 'Access to Path is denied' when publishing a Data Stream Make sure that Stream Host is ran in Administrator Mode Issue: Breakpoint doesn't fire Make sure the Agent is packaged as Non-Virtual. This can also be an issue with testing out multiple versions of the agent. Ensure that the agent dll is copied to the folder with the correct agent id and version. Alternatively, one can refresh the Cache folder by: Deleting the Cache folder Running Stream Host Publishing the Data Stream Copying the Agent dll to the appropriate folder Potential issue can lie in dll not containing the expected changes. Cleaning and rebuilding the agent may solve this issue. Issue: 'Sorry, something went wrong.' error when configuring an agent Make sure that stream host is running for non-virtual agents. If the agent is supposedly virtual, repackage the agent as non-virtual temporarily to be able to access your local environment settings Ensure that agent settings are configured in line with the agent's code. Things to note for: Ensure that the key when packaging agents match the key used in code (case-sensitive) Ensure that all key names are unique for both group and variables. For value fields (eg. Token Box, Dropdown), ensure that default/configured values match the expected values in code. Take note that this is also case-sensitive Alternatively, adding a breakpoint and debugging in GetConfigurationTemplate can help determine the setting/property that is causing the error Issue: 'Could not complete Integrity check. undefined' when running integrity check This can be caused by missing properties within the settings when packaging an agent. Ensure that the expected configurations within the agent's code are properly configured within the settings in package manager."
  },
  "docs/how-tos/agents/index.html": {
    "href": "docs/how-tos/agents/index.html",
    "title": "Agents | XMPro",
    "summary": "Agents An Agent is a reusable object which forms the building block of a Data Stream. When a number of Agents are connected together, a Data Stream is formed. Each Agent is designed to perform a specific function in the stream. For example, they can be used to retrieve data from a database in real-time, display data, filter, sort the data, or save the data somewhere else, depending on the function of that individual Agent. Note It is recommended that you read the article listed below to improve your understanding of Agents. Agent Articles Manage Agents Building Agents Packaging Agents Debugging an Agent"
  },
  "docs/how-tos/agents/manage-agents.html": {
    "href": "docs/how-tos/agents/manage-agents.html",
    "title": "Manage Agents | XMPro",
    "summary": "Manage Agents Agents create the foundation for Data Streams, and they can be connected to other Agents to create the flow of data. Each Agent performs a specific function. They are useful as they can be used to either retrieve data in real-time, display data, filter or sort the data, or save them to another database, depending on the function of that individual Agent. Note It is recommended that you read the article listed below to improve your understanding of Agents. Agent Creating Agents Creating an Agent can be divided into two parts: Writing the code for an Agent Agents are generally written in C# as library projects that make use of the XMPro.IoT.Framework NuGet package. XMPro.IoT.Framework requires your project to be written using a predefined structure. This structure requires you to implement certain interfaces, depending on the type of Agent you are creating. To learn more about how to use this framework, refer to these instructions. Note Code for some Agents has been made available on GitHub. It might be useful to use these resources as an example when writing your own Agents. Packaging the Agent After writing your code, you need to use the XMPro Package Manager Windows 10 desktop application to package your Agent. This application allows you to specify all the properties your Agent requires, add the user settings in the form of controls, and allows you to upload the DLL of the Agent you've written. Finally, it will create a file with a \".xmp\" extension, which you can upload to Data Stream Designer and start to use to build Streams. To package the Agent, refer to these instructions. Adding an Agent After writing a new Agent and packaging it, you can upload it to Data Stream Designer by following the steps below: Open the Agents page from the left-hand menu. Click Add. Click the Select File button and browse to the .xmp file you've packaged. If the .xmp file is valid, some of the details contained in the file, such as the name of the Agent, will automatically be listed on the form. Select the category of the Agent (prepopulated if contained in the file) Click Save. Note The Metadata field allows you to add tags for the Agent. You can either select a value from the drop-down that appears when you click in the field or type a new value and press Enter. Additional information that forms part of the Agent's details will be displayed on the form, such as the version and Metadata. The newly uploaded Agent will now be available in the toolbox on the Use Case canvas page. To add an Agent to the canvas, follow the steps below: Open the Data Streams page from the left-hand menu. Select your Data Stream. Expand the category in the toolbox where your Agent is located. Click on your Agent and drag the Agent to the canvas. Click Save. Bulk Adding Agents Uploading multiple Agents begins like a single Agent, except a compressed (.zip) file is selected. Note A 100 MB limit applies to the decompressed file size, not the compressed one. If the .zip file is valid, a data grid is populated with the Agent name, version, ID, category, and file size. Complete the upload by following the steps below: Select the category of the Agents (prepopulated if contained in the respective .xmp file) Click on Save If any Agents fail the initial validation due to a missing category, a status column appears to identify them. To fix this, (1) select the category and (2) click on Save. Or Discard to exit the blade. The status column will advise which Agents were uploaded successfully, and which were ignored as the version already exists. Upgrading Agents To upgrade Agents in the Data Stream, visit How To Upgrade a Stream Object Version. Deleting Agent Versions To remove one or more versions of a specific Agent, first, make sure that the versions of the Agent that you're planning to remove are not being used anymore. Then, open the Agents page from the left-hand menu and follow the steps below: On the Agents page, select the Agent. Click on \"Delete Versions\". Select the versions you would like to delete. Click Delete. Deleting Agents When planning to remove multiple Agents completely at the same time, make sure they are not being used anymore. Open the Agents page from the left-hand menu and follow the steps below: Click on \"Select\". Select all the Agents you would like to remove. Click on \"Delete\". Note To cancel the selection, click on \"Select\" again. Confirm that you would like to delete all versions of the selected Agents. Finding Help for Agents Help documentation is available for every Agent. These pages provide context, configuration definitions, an example, and release notes to help if you are unsure of anything related to the Agent you are configuring. See the Integrations article for the list of Agent documentation links."
  },
  "docs/how-tos/agents/packaging-agents.html": {
    "href": "docs/how-tos/agents/packaging-agents.html",
    "title": "Packaging Agents | XMPro",
    "summary": "Packaging Agents Getting Started The XMPro Package Manager is a Windows 11 desktop application that enables you to package a new Agent or update details for an existing Agent. See the Agent article for more information on Agents. This application takes you through the process of specifying all the properties your Agent requires, adding or changing the controls for each of the user settings, and uploading the DLL files of the Agent code. It will provide you, upon completion, with a file that can be uploaded to Data Stream Designer after which you can build Data Streams using the Agent. You can download the software from the Microsoft Windows 10 Store or by clicking here. After installing the XMPro Package Manager, launch the application from the Microsoft Store or search for \"XMPro Package Manager\" in the Start menu and then click on \"XMPro Package Manager\". Note You can run multiple instances of Package Manager at the same time. This side-by-side comparison is helpful when developing a new Agent that is similar to another; or comparing different versions of the same Agent. New / Import On the first screen of the application, you can either create a new Agent package or import and update an existing one. Note Use the arrows at the bottom of the page section to move forward or backward in the application. When you import an existing package, you have the option to export the package as a JSON file. This is useful either to compare packages or for source control and version management. You can also import the JSON file from an existing package, which is particularly useful if you need to modify translations added through the Include Multilingual Support feature. Details The Details form allows you to configure the properties of an Agent. These properties are listed and explained below. Name The name of the Agent is what the Agent will be known as once it is uploaded to the Data Stream Designer platform, for example, \"Jupyter Notebook\". Category The category is selected based on the function the Agent performs, for example, \"AI & Machine Learning\". Description The description is a brief explanation of what the Agent does, for example, \"This Action Agent allows you to create and load Jupyter Notebook files\". Version The version of the Agent. Any real number is acceptable, for example, \"1.02\". Caution If you make a change to an existing Agent, make sure you increment the version number as Data Stream Designer will not allow you to upload two of the same Agents with the same version. Virtual Agents can be classified as either Virtual or Non-Virtual. An Agent is Virtual if it is not specific to a certain environment and can be configured remotely. Non-Virtual Agents have to be in their respective environments to be able to function correctly, e.g. the SQL Server Agent, which has to connect to SQL Server via the intranet. Entry Point The entry point is the namespace and class name of the actual Agent's DLL file. For example, if an Agent with the class name \"ActionAgent\" is located in the XMPro.JupyterNotebookAgents namespace, the Entry Endpoint for it would be \"XMPro.JupyterNotebookAgents.ActionAgent\". Isolated Loading When loading Agents to use in a Stream Host, all the libraries are put in a separate Load Context. Tick Isolated Loading to keep Agent files separate and reduce the risk of libraries clashing or conflicting together. In most cases, this option should be enabled. Icon File The icon used to represent your Agent. Click the Browse button, navigate to where you've stored the file via the Explorer and select the new image file. Note It is recommended that you upload either a JPG or PNG file with a size of 64×64 pixels to accommodate for retina displays. Require Input Map Tick Require Input Map to specify that your Agent will be receiving events in a defined structure. The arrow leading to your Agent will be configurable to allow the user to map the inputs of your Agent to incoming attributes. Note If left unticked, parent outputs will be published to this Agent as they are. Add On-Error Endpoint? Tick to add an additional Output Endpoint, called an Error Endpoint. An Error Endpoint will output error information when your stream is running and something goes wrong, making debugging easier. You can also define actions that will be executed after error data is sent to the next Agent in the stream by your Agent, for example when a certain record is not valid. Endpoints The Endpoints form allows you to specify any Input or Output Endpoints. Input Endpoints These Endpoints represent entry points to the Agent, which will allow the Agent to receive data or input from another Agent. To add an Input Endpoint, type it's name in the text field and click Add. The new Endpoint will appear in the list below the Add button. You may need to hover over the list and scroll down to see it. As per the image, an Input Endpoint named \"Input\" has been added. Caution The name of the Input Endpoints has to match what has been defined in the Agent's code. To remove an existing Endpoint, scroll down in the list until you see it, select the Endpoint and click on the Remove button. Output Endpoints Output Endpoints represent exit points from an Agent and allow you to connect your Agent to another Agent, making it possible to pass data from your Agent to another Agent. To add an Output Endpoint, type it's name in the text field, change the Endpoint Type to \"Output\", and click Add. The new Endpoint will appear in the list below the Add button. You may need to hover over the list underneath the text field and scroll down before being able to see it. In the image, an Output Endpoint named \"Output\" has been added. Caution The name of the Output Endpoints should match what has been defined in the Agent's code. To remove an existing Endpoint, scroll down in the list until you see it, select the Endpoint and click on the Delete button. References The References form is where you upload the file(s) required for the Agent to execute. Only files in the Selected File(s) list will be included in the package, and any DLLs must be created in .NET. To upload a file, click the Browse button next to the DLL File(s) field and navigate to where the files are located. Select all the files needed and click the Add button to add them to the Selected File(s) list. To remove a file from the list, click the Delete button next to the file name in the Selected File(s) list. Type Description Agent The DLL file that was generated when you built the project containing your Agent source code. Reference Additional DLL file(s) referenced by the Agent File, such as Newtonsoft.Json. You do not need to upload the XMIoT.Framework.dll file as this DLL is automatically included. Resource Additional DLL file(s) needed by the Reference File. Zip The Stream Host decompresses the file, while maintaining the folder structure, so that Agents such as the Meta Agent can run external source code and self package. Settings Depending on what your Agent does, it might require that the user provide certain information, such as a server URL, username, or password. For each of these information fields (or settings), you need to specify which control should be used and what each control represents, for example, the Jupyter Notebook Agent will require the user to add a server URL. The user should provide this value using a text-box control. Thus, you need to create a control with a type of \"TextBox\" and a caption that reads \"Server URL\" in the XMPro Package Manager application. The following controls are available to be used to capture user input: Button CheckBox CheckList DropDown EditList FileUpload Filter Grid Group HTML Editor NumberBox ScriptBox TextBox Title TokenBox VariableBox Each control has several properties that have to be set and not all properties apply to all controls. For example, options apply to a drop-down control and not a text-box control. The table below contains a list of all the available properties, their description, and to which controls they are applicable. Property Name Control Type Description Allow Custom Text Drop-Down Allows the user to type custom text in the drop-down field if checked. Allow Custom Tokens Token Box Allows the user to add custom tokens if checked. Caption All Text that will be displayed with the group or setting. The caption is usually one or two words, describing the value that should be provided by the user, for example, \"Server URL\". Default Value Title The default value of the title. Font Size Script Box Size of the font in the Script Box. Group Type Groups Appears as either a sub heading (Default) or a hyperlink (SubPage) on the Agent's configuration page. Help Text All, excluding Groups If you need to provide the user with any additional information about the purpose of the setting or helpful instructions, specify it in this field. Key All Uniquely identifies the group or setting. Keywords Script Box Define your variables or other custom keywords here, so that they will be available in the editor's IntelliSense. Options (Drop Down) Drop-Down Use the Options-area to add values to the drop-down menu by specifying the Text and Value fields and then clicking Save. You may also choose an option to be used as the default option by checking the \"Set as Default Value\" box. Options (HTML Editor) HTML Editor Allows you to specify placeholders that can be mapped to input fields in the input received by the Agent. Postback All If checked, will cause the form to do a postback to retrieve values from the server when the field loses focus (when the user clicks out of the field). Required All, excluding Groups The control will be validated to make sure that a value has been specified if this box is checked. ScriptBox Height Script Box Height of Script Box. ScriptBox Mode Script Box Language in which script has to be written. ScriptBox Theme Script Box The theme of the Script Box. Themes available include: Ambiance, Chaos, Chrome, Clouds, Clouds_midnight, Cobalt, Cromson_editor, Dawn, Dreamweaver ScriptBox Width Script Box Width of Script Box. Secure All The value of the control will be treated as a secure value if this box is checked (encrypted and not displayed on the form in plain text). An example of a secure value is a SQL Server password. Show Grid Lines Grid The grid lines of the grid will be shown if checked. Show Header Grid The header of the grid will be displayed, if checked. Sort Index All This is used to determine the group or setting's position and works with increments of 10. Adjust this value to move the group or setting up or down on the form. Unique Key Grid Mark a specific column as being unique, for example, an identity column. Visible All This field sets the initial visibility of the group or setting. Adding Settings Settings are grouped logically into one or more groups, such as authentication, criteria, and output. Create a group first, then add controls for settings to the group. To do this, follow the steps below: Click on the plus-icon (top right, next to the Settings header). A form section will open, allowing you to specify a group for the settings. Specify a unique value that can be used as the key for the group. Add the caption you would like to use. Click Save. Next, we are going to add a setting. Click on the plus-icon next to the group you've created. Choose the type of control you would like to use. Add a unique key for your control. This key must correspond to what you defined in your code. Add a caption for your control. If needed, add a default value. If required, add help text. Select the options that apply from the list of check-boxes. Click Save. Output Export as JSON file Tick the checkbox Export as JSON file too? if you would like to export the file as JSON too. It will later be saved to the same directory as the XMP file with the file name category_name_version.json. Include Multilingual Support Tick the checkbox Include Multilingual Support? if you would like to add support for languages other than English. Uncheck languages you don't want to include. This feature leverages generative AI to provide language translation. It is available only if the following requirements are met: You are connected to the internet. Open AI is configured (Click here for instructions on how to do so). Note Only the Agent Description, Properties and Static Helptext are translated. Internal messages and dynamic Helptext added when building the Agent are not included. Multilingual support requires XMPro Data Stream Designer and Stream Host v4.4.16+. Override the automated translations by editing the JSON file and repackaging it with an incremented version number. Configure OpenAI Click Configure OpenAI. A form will open for you to add or modify the OpenAI Endpoint and Api Key. Click Save. Review: Details Lastly, you can navigate back through the steps to review the details that you've specified. If you are satisfied, complete the wizard by clicking the Save button below before navigating to the folder where you would like the package to be exported. Your package will be created with the file name category_name_version.xmp. Note If you imported an existing file, take care to: either click 'Save as new Agent' to generate a new Agent, or click 'Save' to generate a new version of the original Agent. ensure you select a different location folder or increment the version to avoid overwriting the original. Further Reading How to upload your new Agent to Data Stream Designer Note You need to have the correct permissions set against your user to be able to edit and upload Agents. This is a role not typically given to all users."
  },
  "docs/how-tos/apps/README.html": {
    "href": "docs/how-tos/apps/README.html",
    "title": "Application | XMPro",
    "summary": "Application An Application or App enables you to create Apps using a low-or-no-code canvas environment. This allows anyone including Engineers and subject matter experts to build an App in a matter of days or weeks without having to be a programmer. Note It is recommended that you read the article listed below to improve your understanding of Applications. Application Articles Manage Apps Manage Templates Manage Pages Design Pages for Mobile Navigate Between Pages Pass Parameters Between Pages Manage Connections Check Connector Logs Manage Data Sources Use Data Sources in the Page Use Dynamic Properties Use Expression Properties Use Page Layers Use Block Styling and Devices Use Flex Use Validation Use Variables and Expressions Create and Maintain Notes Manage Widgets Manage App Files Manage Themes Manage Landing Pages Page Data Import an App Page"
  },
  "docs/how-tos/apps/check-connector-logs.html": {
    "href": "docs/how-tos/apps/check-connector-logs.html",
    "title": "Check Connector Logs | XMPro",
    "summary": "Check Connector Logs Logs allow the composer of an App, the developer writing a new Connector, or an Administrator to view messages generated by a Connector when it is in use in an App. Log entry limits are configured per Connector but applied per App Connection. The Connection's log is cleared if it is upgraded to a newer version of the Connector or it is removed from the App. Logs are saved for Connectors that implement the logging functionality. Note It is recommended that you read the articles listed below to improve your understanding of Applications, Connections, and Connectors. Applications Connectors Connections Configuring Connector Log Limits The log entries are not stored indefinitely. The oldest will be cleared according to the combination of the number of log entries and the duration. The log entry limits apply to all versions of the Connector. A Connector that is in development or experiencing errors may need a higher number and duration during troubleshooting. Logs Allowed The number of log entries retained. The maximum is 1000, the minimum is 1, and the default is 100. Logs Duration The duration, in days, of how long the logs are retained. The maximum is 90, the minimum is 1, and the default is 7. Warning Only the Administrator can change the limits. Indicator An icon in front of the Connector indicates if there are errors logged: Icon Description Red indicates there are new logs for that Connector that no one has opened yet. Yellow indicates there are logs for that Connector that have been viewed. View Connector Logs To view the logs for a Connector, follow the steps below: Open the Connectors page from the left-hand menu. Select a Connector to view its information. Select the version that has an indicator for logs. Edit the Application that you want to see the logs. Click on App Data. Select a Connection that has logs. Open the Logs page. View Connection Logs To view the logs for a Connection, follow the steps below: Click on App Data Select the Connection Click on Logs The Logs page is automatically maximized. Adjust the timeline slider above the grid to view logs within a specific time range. Refresh Connection Logs To refresh Connection logs, follow the steps below: Click on Refresh. Logs in the grid will clear and refresh. Filter Connection Logs To filter Connection logs, follow the steps below: Click on the ‘Source’ filter button. Select the log source. Click on OK. Export Connection Logs To export Connection logs, follow the steps below: Click on the export icon. Export all data or export selected rows."
  },
  "docs/how-tos/apps/create-and-maintain-notes.html": {
    "href": "docs/how-tos/apps/create-and-maintain-notes.html",
    "title": "Create and Maintain Notes | XMPro",
    "summary": "Create and Maintain Notes Notes are a way to share information about an Application between creators of the App and for future reference. They are useful when you want to communicate and record technical or non-technical information about the Application. The Notes are for the Application, so no matter how many Pages the Application has, the Notes will always remain the same. Notes are also saved across versions. Note It is recommended that you read the article listed below to improve your understanding of Applications. Application How to Manage Apps Adding Notes To add Notes to the Notes page, follow the steps below: Open the Application page from the left-hand menu. Click on the edit button to open the Application editor page. Click on Notes. Add Notes to the page. Click on Save. Styling Notes Notes can be styled using the HTML editor and the options at the top of the note. They include the following styles: Bold Italics Underline Strikethrough Subscripts and superscripts Quotes Code blocks Numbered lists Bullet Lists Indentation Heading and paragraph styles Font size Font family Font color Font background-color Left, right, centered, and justified alignment Links to websites Images"
  },
  "docs/how-tos/apps/design-pages-for-mobile.html": {
    "href": "docs/how-tos/apps/design-pages-for-mobile.html",
    "title": "Design Pages for Mobile | XMPro",
    "summary": "Design Pages for Mobile When designing Pages, you may want to use responsive web design principles to support different screen sizes with the same App. This is easily accomplished with Block Styling and Devices. The default Page Layouts that you can choose from when creating a page have built-in responsive styling applied to the cards. For an example of this, see the Responsive Page Layout Example. Note It is recommended that you read the article listed below to improve your understanding of how to design pages for mobile. How to Use Block Styling and Devices How to Manage Pages How to Use Flex How to Make Responsive Card Layout To create a responsive card layout just like the default page layouts, follow the steps below: Create a Page with the first, single-card layout. Clone the Horizontal Stacked Layout, Vertical Stacked Layout, and Card until you have the layout that you desire. Please note that the order in the Page Layers will also be the order, from top to bottom, that items appear in mobile. If you want rows and columns to take up a different ratio on the page, adjust the Flex Grow of the relevant items. Make sure to uncheck Style Groups when applying styles to a specific element if you do not want the style to apply to the Style Group. More info on Flex can be found here. Switch to Mobile view with the middle Devices section of the command buttons. Adjust the heights of the elements. Any styles added in Mobile mode will apply only when the device's screen width is smaller than a threshold. In this case, change the first Horizontal Stacked Layout's Min height to 200%."
  },
  "docs/how-tos/apps/import-an-app-page.html": {
    "href": "docs/how-tos/apps/import-an-app-page.html",
    "title": "Import an App Page | XMPro",
    "summary": "Import an App Page App Pages can be exported as a file and imported as a template to other companies or instances of XMPro Products. This allows you to continue working on the same App Page in a different environment without having to start from scratch. The layout of the template is preserved - data sources must be configured. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Pages. Page Import, Export, and Clone How to Manage Pages {% endhint %} How to Import an App Page To import an App Page, follow the steps below: 1. Open the Applications page from the left-hand menu. 2. Click on the edit button to edit an Application. ![](/docs/images/image (244).png>) 3. Click the plus button to add a page. 4. Select “Upload Layout” from the list of options under the “Layout” dropdown. ![](/docs/images/image (343).png>) 5. Select the file containing the Page that you would like to upload. 6. Enter the File Key. 7. Modify any remaining details if necessary. 8. Click on Save. ![](/docs/images/image (182).png>)"
  },
  "docs/how-tos/apps/manage-app-files.html": {
    "href": "docs/how-tos/apps/manage-app-files.html",
    "title": "Manage App Files | XMPro",
    "summary": "Manage App Files Files can be uploaded and used within the Application. The interface for App Files allows you to use, rename, delete, and perform more actions on a file that has been uploaded to the Application. App Files are useful in many scenarios, for example, if you need specific files to integrate Unity or D3 Visualizations onto the Page. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of App Files. App Files How to Manage Apps {% endhint %} Uploading App Files To upload new App Files, follow the steps below: Click on the Applications page from the left-hand menu. Click on the edit button for the App. 3. Click on App Files. 4. Click on N_ew Directory._ 5_._ Enter the name of the folder. 6. Click on Create. 7. Click on the folder you want to store files in to enter it. 8. Drag and Drop Files into the file area or Click Upload Files. Copying/Moving App Files around To copy or move App Files to different folders, follow the steps below: From the edit application page, click on App Files. Navigate to the file you want to copy or move. Highlight the file. Click on Copy to or Move to. 5. Select the new location you want to move or copy the file to. 6. Click on Move/Copy. Renaming App Files To rename App Files, follow the steps below: From the edit application page, click on App Files. Navigate to the file you want to rename. Highlight the file. Click on rename. 5. Enter a new name. 6. Click on Save. Deleting App Files To delete App Files, follow the steps below: From the edit application page, click on App Files. Navigate to the file you want to delete. Highlight the file. Click on delete. 5. Confirm you want to delete the file. Downloading App Files To download App Files, follow the steps below: From the edit application page, click on App Files. Navigate to the file you want to download. Highlight the file. Click on download. The file will appear in your downloads. Using App Files To use App Files in the application itself, follow the steps below: Add a block on the page which uses a file selector, such as Unity or D3 Visualization. Highlight the block. Click on Block Properties. Use the file selector to select the file you want to use in the application. To add more files to the file manager directly from here, click on the plus sign. Upload files, download, move, copy, rename or delete files directly from here."
  },
  "docs/how-tos/apps/manage-apps.html": {
    "href": "docs/how-tos/apps/manage-apps.html",
    "title": "Manage Apps | XMPro",
    "summary": "Manage Apps An Application or App enables you to create Apps using a drag-and-drop canvas environment. This allows anyone including Engineers and subject matter experts to build an App in a matter of days or weeks without having to be a programmer. Note It is recommended that you read the article listed below to improve your understanding of Applications. Application How To Manage Apps Each card on the Categories dashboard shows the Name, Description, and Icon of the Category, as well as the number of Unpublished and Published Apps in the category. Clicking on the Applications button in the menu will take you to the Applications list. The Applications list is grouped by Category. Apps that have the play icon are Published. Click on a row to view an App, or click on an edit button to edit an App. The edit button will only be shown for Apps that you have edit access to. Clicking on a Category on the App Designer Categories dashboard will take you to the Applications dashboard for that Category. Each card shows the Icon, Name, and Description of the App as well as how many users have access, the version, the time and date it was last modified, the owner's profile picture and name, and whether the App is Published - if so it will have \"Active\" in green, and if not it will have \"Draft\" in yellow. Clicking a row on the Applications list or a card on the Applications dashboard will let you view the App. An example of an App can is shown below: You can go back to the previous page by clicking the X button in the top right of the page. If you have edit access to the App there will also be an edit icon at the top right that you can click to edit the App. You can create an App by clicking the New Application button in the left menu. After navigating to the edit page for an Application, you can edit the properties and delete the App by clicking the Properties command button. You can also edit a page by clicking on it in the pages list or add a new page by clicking the plus button at the top right of the list of pages. The App Pages list is ordered with the Landing Page at the top, and the rest of the pages in alphabetical order. How to Create an App To create an App, follow the steps below: Click the New Application button to create an App. Click a Template to create an Application from that Template. Enter the name and description of the new App. Choose the category the app belongs to. Upload the icon. Sample icons can be found in the Icon Library. Choose between Light and Dark themes. You may also include tags from the drop-down that opens when you click on the field - or type a new value and press Enter. Choose the layout of the Landing (Main) page. Click on Save. How to Delete an App To delete an App, follow the steps below: Open the applications page from the left-hand menu. Click on the edit button of the application you want to delete. Click on Properties. Click on Delete. Confirm that you want to delete the Application. Versions of an App To read more about managing the versions of an App, visit How To Manage Versions. Importing, Exporting, and Cloning an App To read more about exporting, importing, and cloning an App, visit How to Import, Export, and Clone. Importing an App with Data Stream Connections When importing an App with Data Stream Connections, there is an option to map all of the App's connections to their corresponding Data Stream version and Agent. This step can be skipped during the import, but each App Page's data source will have to be updated individually. Sharing Apps To read more about sharing Apps and their managing access, visit How to Manage Access. Further Reading How to Manage Templates How to Manage Pages How to Create and Maintain Notes How to Manage App Files How to Manage Themes"
  },
  "docs/how-tos/apps/manage-connections.html": {
    "href": "docs/how-tos/apps/manage-connections.html",
    "title": "Manage Connections | XMPro",
    "summary": "Manage Connections The parameters defined in a Connection allow the App to connect to a source of data like a SQL Database and expose the entities as Data Sources within the Page. Connection parameters can include credentials such as a username, password, path, URL, or location identifier that you can use to make a remote connection to the Data Source. For example, Connection parameters to connect to an SQL Database would include the Server Name, Authentication type, Username, and password. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Data Integration. Data Integration How to Manage Pages {% endhint %} Adding a Connection to the Application Connections can be added directly to an Application by adding it to the App Data. To add a Connection to an Application, follow the steps below: Open the Applications page from the left-hand menu. Click on the edit button to edit an application from the list. ![](/docs/images/image (936).png>) 3. Click on App Data. 4. Click on add. 5. Select a connector, for example, the SQL connector. 6. Enter the details of the Connection, including it's name. 7. Click on Save. ![](/docs/images/conn_2 (1).png>) The new Connection should appear in the list of connectors, and can now be used in the application to add Data Sources. The Connection can be used for all pages within the application it was added to. Deleting a Connection Single Connection To delete a single Connection, follow the steps below: Open the Applications page from the left-hand menu. Click on the edit button to edit an application. 3. Click on App Data. 4. Select the Connection. 5. Click on Delete. 6_._ Confirm that you want to delete the Connection. Multiple Connections To delete multiple Connections, follow the steps below: Open the Applications page from the left-hand menu. Click on the edit button to edit an application. 3. Click on App Data. 4. Click on Select. 5. Select multiple Connections to be deleted. 6. Click on Delete. ![](/docs/images/conn_7 (1).png>) 7. Confirm that you want to delete the Connection. {% hint style=\"info\" %} You can cancel the multi-select by clicking on the select button again. {% endhint %} Data Stream Connections Navigate to the Data Stream Connector to access a comprehensive inventory of the Data Stream Connections for the Application. The connections are categorized based on the respective pages where they are utilized. You can use this list to verify the current version of the Data Stream in use and make any necessary updates if required. Further Reading How to Manage Data Sources"
  },
  "docs/how-tos/apps/manage-data-sources.html": {
    "href": "docs/how-tos/apps/manage-data-sources.html",
    "title": "Manage Data Sources | XMPro",
    "summary": "Manage Data Sources Data Sources can be created for a Page in the Application. They are a way to link to a specific Entity in a Connection's Entities, for example, a table in a SQL database. Data Sources are managed through the Page Data tab of a Page. Data Sources allow you to display, use, manipulate or store data from a connected source of data such as an SQL Server Database. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Data Integration. Data Integration How to Manage Connections {% endhint %} Creating a Data Source To create a Data Source on the Page of an Application, follow the steps below: Open the Editor for the Application. ![](/docs/images/image (636).png>) 2. Open the Page that will have the Data Source. 3. Click on Page Data. 4. Click on the plus symbol to add a new Data Source. 5. Enter a name. 6. Add the Connection and Entity where data will be retrieved. 7. Specify the Primary Key. 8. Click on Save. ![](/docs/images/Primary Key.png>) ![](/docs/images/image (1407).png>) Edit a Data Source To edit a Data Source, follow the steps below: Open the Editor for the Application. ![](/docs/images/image (3).png>) 2. Open the Page that has the Data Source. ![](/docs/images/image (1825).png>) 3. Click on Page Layers. 4. Click on the edit button. 5. Edit the details of the Data Source. 6. Click on Save. Delete a Data Source To delete a Data Source, follow the steps below: Open the Editor for the Application. ![](/docs/images/image (496).png>) 2. Open the Page that has the Data Source. ![](/docs/images/image (458).png>) 3. Click on Page Layers. 4. Click on the edit button. 5. Click on Delete. 6. Confirm that you want to delete the Data Source. Further Reading How to use Data Sources in the Page"
  },
  "docs/how-tos/apps/manage-pages.html": {
    "href": "docs/how-tos/apps/manage-pages.html",
    "title": "Manage Pages | XMPro",
    "summary": "Manage Pages A Page is a web page built with XMPro's App Designer. Pages allow you to separate the Application into sections, navigate between the Pages, and pass data between Pages using Parameters. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Pages. Page Canvas How to Manage Apps {% endhint %} Creating a Page To create a Page within an existing application, follow the steps below: Click on Applications from the left-hand menu. Click on the edit button of the Application from the list. ![](/docs/images/image (445).png>) 3. Click on the plus button to add a new Page. 4. Enter the details of the new Page. 5. Choose the layout of the new Page. 6. Click on Save. ![](/docs/images/image (25).png>) The new Page will show in the list of pages for the Application. Designing a Page Once a Page has been created, you can design your Page to meet your specifications. There are many Blocks or controls that can be added from the Blocks tab such as Text, Number, and Date Boxes, Grids, Charts, Gauges, Accordions, and Tabs. The full list of Blocks can be found here. Adding New Blocks Blocks are accessed through the Blocks tab in the page designer and can be added to the page by dragging them into the Canvas. Blocks can also be searched by typing in the search bar at the top. Moving Blocks in the Canvas Blocks can be rearranged within the Canvas in a few ways: Moving Blocks using the Canvas Click and drag the Block you want to move. Hover over where you want to move the Block to. A green line and an orange outline will appear to show where the Block will end up. Release the mouse to drop the Block. ![](/docs/images/Drag in the Canvas.gif>) Moving Blocks using the Toolbar Sometimes the Block you want to move may be hard to click or behind other Blocks. In this case, you can move the Block by its drag handle button in the blue toolbar. Select the Block by clicking one of its children and clicking the Select Parent button in the blue toolbar or by selecting it in the Page Layers. Click and drag the drag handle button in the blue toolbar. Hover over where you want to move the Block to. A green line and an orange outline will appear to show where the Block will end up. Release the mouse to drop the Block. ![](/docs/images/Drag by handle.gif>) Moving Blocks by the Page Layers Sometimes you may need to be more precise in dragging a Block. In this case, you can move the Block by rearranging it in the Page Layers. See the Page Layers article for more information on how to use Page Layers. Expand the Block you want to move and the Block you want to put in the Page Layers. Click and drag the drag handle in the Page Layers. Hover over where you want to move the block to. A green line and an orange outline will appear to show where the block will end up. Release the mouse to drop the block. ![](/docs/images/Drag by Page Layers.gif>) Launching a Page Once a Page has been designed, you can view the finished product of the designed Page by launching it. Click on the page you want to preview. Click on Launch. Deleting a Page To delete the Page, open the Page itself and delete it via the settings menu. Follow the steps below to delete the Page: Open the Page you would like to delete. 2. Click on Settings. 3. Click on Delete. 4. Confirm that you want to delete the Page. Further Reading How to Import an App Page How to Design Pages for Mobile How to Navigate between Pages How to Manage Connections How to Use Dynamic Properties How to Use Expression Properties How to Use the Page Layers How to Use Block Styling and Devices How to Use Validation How to Use Variables & Expressions How to Manage Widgets"
  },
  "docs/how-tos/apps/manage-templates.html": {
    "href": "docs/how-tos/apps/manage-templates.html",
    "title": "Manage Templates | XMPro",
    "summary": "Manage Templates Templates are pre-designed Applications that can be selected when creating a new Application. Templates can be used to save time without having to build a whole new App layout from scratch, and also allow you to create a consistent theme or design that you can use across all your Applications. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Templates. Template How to Manage Apps {% endhint %} Creating a Template To create a Template, make sure you already have an App that you want to use to save as a Template. Follow the steps below to create a template out of an existing App. Click on Applications from the left-hand menu. Click on the edit button of the application from the list. ![](/docs/images/image (599).png>) 3. Click on Save Template. 4_._ Enter the name, description, and category of the Template. 5. Upload screenshots of what the Application looks like. 6. Click on Save. ![](/docs/images/template_2 (1).png>) {% hint style=\"info\" %} Adding screenshots helps other users when they want to have a look at the Template to understand what the Template contains. {% endhint %} Using a Template When creating an App, you can choose a Template as a base instead of starting the App from scratch. To use a Template: Click on the plus from the left-hand menu to create a new Application. Choose the template you would like to use. ![](/docs/images/image (495).png>) 3. Click on Select Template. 4. Continue creating the new Application. 5. Click Save. ![](/docs/images/image (1252).png>) Editing a Template A list of existing Templates can be viewed on the Templates page from the left-hand menu. This is where you can make changes to Templates, delete Templates, view, import, or export them. Click on Templates from the left-hand menu. Select the Template you would like to edit. Make the necessary changes. Click Save. Deleting Templates Single Template To remove a single Template, follow the steps below: Open the Templates page from the left-hand menu. Select the Template from the list. Click Delete. Confirm that you would like to delete the Template. Multiple Templates To remove multiple Templates at the same time, follow the steps below: Open the Templates page from the left-hand menu. Click Select. Select the Templates that you would like to remove. Click Delete. 5. Confirm that you would like to delete the Templates selected."
  },
  "docs/how-tos/apps/manage-themes.html": {
    "href": "docs/how-tos/apps/manage-themes.html",
    "title": "Manage Themes | XMPro",
    "summary": "Manage Themes The Application can either be Light-Themed or Dark-Themed. This depends on the overall design of the Application and can be configured when creating the App, and can also be edited after the app has been created. This is useful as some users or developers prefer to have a certain consistent light or dark-colored theme across their app, and this option allows you to customize the app to fit that preference. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Applications. Application How to Manage Apps {% endhint %} Default Theme when creating a new App The Default Theme is the theme that will be automatically selected when you create a new Page within the Application. For example, if the Default Theme for the overall Application is dark, when you create a new Page, the Default Theme for that Page will always be set to dark. To choose the Default Theme when creating a new Application, follow the steps below: Click on the plus button from the left-hand menu to add a new application. Select the Default Theme when creating an App. ![](/docs/images/image (1403).png>) The background will either be light or dark depending on the theme. Changing the Default Theme of an App To edit the Default Theme for the overall App, follow the steps below: Open the edit page for the Application. ![](/docs/images/image (1765).png>) 2. Click on Properties. 3_._ Change the Default Theme. 4. Click on Save. ![](/docs/images/image (1153).png>) {% hint style=\"info\" %} Changing the Default Theme of the App does not change the Themes of the individual Pages that are already configured. {% endhint %} Choosing a Theme when creating a page When creating a new page, follow the steps below: Click on the plus to add a new page. Enter the details of the new page. The theme of the new page will automatically be set to the app's default theme. This can still be changed here. Click on Save. Changing the theme for a page To edit the Theme of an existing Page, follow the steps below: Open the page from the application editor. 2. Click on Settings. 3. Change the Theme of the Page. 4. Click on Save."
  },
  "docs/how-tos/apps/manage-widgets.html": {
    "href": "docs/how-tos/apps/manage-widgets.html",
    "title": "Manage Widgets | XMPro",
    "summary": "Manage Widgets Widgets are collections of Blocks or layouts that you can group together and re-use for other Apps and Pages. For example, if there is a common group of Blocks that is repetitive to continuously recreate, these Blocks can be selected and turned into a Widget. When a Widget is created, it is added as a Block in the Blocks toolbox and can then be dragged onto the canvas and re-used as many times as needed on any Page. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Widgets. UI Block (Toolbox) How to Manage Pages {% endhint %} Creating a Widget When a Widget is created, you can specify if the Widget can only be seen and used by you, or everyone else in the company. If everyone has access to it, anyone can drag and drop your Widget into their own apps. The owner of the Widget is the only one who can edit or delete it. ![](/docs/images/image (842).png>) To create a Widget, follow the steps below: Select the Blocks you would like to save as a Widget. Click the save button to create a new Widget. Enter the name. The visibility determines who will be able to see and use your Widget. If set to everyone, anyone in your company or organization can drag and drop your Widget into their own Apps. Choose an icon. Sample icons can be found in the Icon Library. Click Create. ![](/docs/images/image (1785).png>) ![](/docs/images/New Widget.png>) When a Widget is created, it will show in the toolbox, under 'Blocks' in the 'Widget' category. Using Widgets The Widget can be dragged and dropped anywhere on the canvas, and this will create the same set of blocks as before. Deleting a Widget When a Widget is deleted, it will be removed from the sidebar and you will no longer be able to drag it onto the canvas. Deleting a Widget will not affect anything previously dropped onto the canvas on any Page. Select Edit. Note that you will not be able to edit the Widget unless you are the owner of it. Select Delete. 3. Confirm that you would like to delete the Widget. Importing Widgets Widgets that were built in one Application can be imported to another Application. A single file can contain multiple Widgets that can be uploaded. To import Widgets, follow the steps below: 1. Click on Blocks. 2. Click on Import Widgets. 3. Select a file. 4. Enter a File Key. 5. Click on upload. ![](/docs/images/ImportWidgets1 (1).png>) 6. Select the Widgets you would like to upload. 7. Click on Save. {% hint style=\"info\" %} New Widgets will appear in the list of Widgets. {% endhint %} ![](/docs/images/ImportWidgets3 (1).png>) Exporting Widgets If you want certain Widgets that were built in your Application to be used in another Application, you can export those Widgets. 1. Click on Blocks. 2. Click on export Widgets. 3. Select the Widgets you would like to export. 4. Click on Export. {% hint style=\"info\" %} You can choose multiple Widgets to export at once. They will all be exported into one file. {% endhint %} 5. Enter a File Key. 6. Click OK."
  },
  "docs/how-tos/apps/navigate-between-pages.html": {
    "href": "docs/how-tos/apps/navigate-between-pages.html",
    "title": "Navigate Between Pages | XMPro",
    "summary": "Navigate Between Pages It is possible to allow navigation between Pages of an App by configuring the Action of a Block. This allows you to separate your content within the application into sections that can be easily navigated by the user. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Navigating between Pages. Navigation and Parameters How to Manage Pages {% endhint %} Configuring Navigation between Pages To add Navigation between Pages, make sure the Application has more than one Page. See the Manage Pages article to read more about adding Pages. When a new Page is created, they are automatically configured to have links going back to the landing page, which can be seen at the top of the screen. Some Pages do not automatically have links to other Pages. For example, the Landing page does not have links going to other pages. To add Navigation between Pages, follow these steps: Click on Applications from the left-hand menu. Click on the edit button of the Application from the list. 3. Select a Page from the Application’s Edit menu. 4. Click on Blocks. 5. Under ‘Actions’, select an action such as a button. 6. Drag and drop it onto the Page of the application. 7. Select the button. 8. Click on Block Properties. 9. Under ‘Action,’ click on the Navigate To Dropdown and select Page. 10. Select the page you want to navigate to. 11. Under ‘Appearance,’ give the button a name. 12. Click on Save. Navigating between Pages at Runtime After the Navigation between Pages has been added, you can Launch the App to see what it will look like at runtime. Follow the steps below to Launch and view the App: Click on Launch. The button is visible on the Page. Click on the button to navigate to the other selected Page. 3. The page will open. Deleting Navigation between Pages You can delete the navigation functionality by either deleting the Block itself or by deleting the settings under Action in the Block Properties tab. Navigating Using Back URL You can include a back URL to the current page when configuring a Navigate To URL, so that the user can return to the original page. For example, you want to open an Alert from a custom templated list and the alert drill-down needs a back URL. To append a back URL, follow these steps: Click on Block Properties. Under 'Action', click on the Navigate To Dropdown and select URL. Toggle the URL mode to Expression Mode. Click the dropdown to edit the expression value. 5. Note the current page parameters under Constants. 6. Configure the URL with back parameters, similar to the below: \"https://xmad-sample.azurewebsites.net/viewalert;id=\"+\"12345\"+\";backUrl=render;backParams=\\{\\\"appId\\\":\\\"\"+appId+\"\\\",\\\"pageId\\\":\\\"\"+pageId+\"\\\",\\\"categoryName\\\":\\\"\"+categoryName+\"\\\",\\\"appVersion\\\":\\\"\"+appVersion+\"\\\"\\}\" Copying App Link You can copy the App link if you want to share it. This creates a link to the published app version - or to the latest version if there is no published version. Click More. Click Copy App Link. Copying Page Link You can copy a specific App Page link if you want to share it. This will create a link to the specific app version and page. Open the Page. Click Copy Page Link. Further Reading How to Pass Parameters between Pages"
  },
  "docs/how-tos/apps/page-data.html": {
    "href": "docs/how-tos/apps/page-data.html",
    "title": "Page Data | XMPro",
    "summary": "Page Data You can use Page Data to update the data on the current page when an Action is performed. In the example shown below, each button is configured to update the value of the variable shown in the text control with a different value. ![](/docs/images/Page Data Example.gif>) Adding a Page Data to Control To add Page Data follow the steps below: Select the control. Click to edit Page Data. Click the Plus sign. Select a field. Enter a value or select from Dynamic values. Click Apply. ![](/docs/images/Page Data.png>) Removing a Page Data from Control To remove Page Data follow the steps below: Select the control. Click to edit Page Data. Find the row that you want to be deleted and click Delete. Confirm your action. Click Apply. ![](/docs/images/Page Data remove.png>)"
  },
  "docs/how-tos/apps/pass-parameters-between-pages.html": {
    "href": "docs/how-tos/apps/pass-parameters-between-pages.html",
    "title": "Pass Parameters Between Pages | XMPro",
    "summary": "Pass Parameters Between Pages You can use Parameters if you want to send particular values to another Page. For example, if you have a list of machines, and a user selects one, the Application may open a new Page that displays information for that particular machine. In that case, you may want to pass the ID of the machine the user clicked on to the Page that is being opened. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Navigating between Pages. Navigation and Parameters How to Navigate Between Pages {% endhint %} Adding a Parameter to the Page A Parameter needs to be added to the Page that is receiving the data. In this example, there are two Pages that exist in the Application: the main Landing Page and the secondary Page. The second page is the Page where the Parameter is made. To add a Parameter to a page, follow the steps below: Click on Applications from the left-hand menu. Click on the edit button of the Application from the list. ![](/docs/images/image (531).png>) 3. Click on the page where you want to data to be sent to. 4. Click on Page Data. 5. Click on the plus to add a Parameter. 6. Add the name and type of the new Parameter. 7. Click on Create. 8. Click on Save. ![](/docs/images/image (499).png>) Now that a Parameter has been added, create a textbox or a way to display the value when it is passed to the Page during runtime. Add a textbox to the Page so you will be able to display the value when it is passed from the main page to this page. 2. Click on Block Properties. 3. Click on the ‘A’ button to toggle between static and dynamic text. 4. Select the Parameter from the dropdown. 5. Click on Save. The data has to be sent from the main page to the secondary page. To pass data from the main Landing Page using the Parameter you just created, follow the steps below: Go to the page you will be sending data from. Select the button/link that navigates to the second page. Click on Block Properties. Click on the Edit button under Pass Page Parameters. Configure if the data being passed is dynamic or static. Add/Select a value that you would like to pass to the parameter on the second page. Click on Apply. Passing Dynamic Data to the Page When navigating between pages, you can also pass dynamic data - such as the Chart value - to the next page using page parameters. First, configure the Chart block then follow the steps below: Click on Block Properties. Under _'_Appearance' set Show Drilldown to True. 3. Under ‘Action’, click on the Navigate To Dropdown and select Page. 4. Select the page you want to navigate to. 5. Click Pass Page Parameters. 6. Pass Page Parameter blade will open. 7. Type in the name of the parameter and select the type. 8. Click Add. 9. Change the field type to Dynamic Value. 10. Select the Chart value you want to pass on the next page. 11. Click Apply. View values passed to other pages at runtime Once the Parameter has been configured, launch the application to see how the data will be passed between Pages at runtime. Click on Launch. 2. Click on the link that goes to the second page. 3. The data sent from the first page should reach the second page. Edit a Parameter To edit a Parameter, follow the steps below: Click on Page Data. Click on the pencil/edit button for the Parameter. 3. Make changes to the Parameter. 4. Click on Save. Delete a Parameter To delete a Parameter, follow the steps below: Click on Page Data. Click on the pencil/edit button for the Parameter. 3. Click on Delete. 4. Confirm that you would like to delete the Parameter."
  },
  "docs/how-tos/apps/use-block-styling-and-devices.html": {
    "href": "docs/how-tos/apps/use-block-styling-and-devices.html",
    "title": "Use Block Styling and Devices | XMPro",
    "summary": "Use Block Styling and Devices Block Styling includes options that allow you to change the text color, background color, borders, typography, dimensions, or other styling options of the Block. You can use it to customize the look and feel of the Application based on themes or color palettes for your specific organization. The style and position of the Blocks can also be customized for different devices, which is important in ensuring that users have a good user experience regardless of the screen size they are viewing your Application on. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Devices and Block Styling. Device Block Styling How to Manage Pages {% endhint %} Adding a Style Group Style Groups can be created and customized by the user and applied to Blocks on the Canvas. To style a Block, make sure the Block you want to style is selected. Click on the Block Styling tab in the Toolbox. Add a Style Group by typing a name in the field under 'Style Group' and pressing enter. Expand any category to change styles. Add your custom style. The style of the group will change as a result. The created style group can then be applied to other Blocks. Select a different Block and enter the name of the new style Block in the field under 'Style Group'. Adding Style to States States include events such as hovering over a Block, clicking a Block, or changing the style for every second Block. To change the styling of a state, select the Block you want to style. Click on the Block Styling tab in the Toolbox. Change the state. Add your custom styling. The styling will be applied to the Block for that state. In this example, if the user hovers over the Home text, the background will change from light blue to light green. To see the hovering effect in action, no state should be selected. The background color will remain light blue, and will only change to light green when the user hovers over it. Configure styling between Desktop, Tablet, and Mobile Phones To configure the styling between devices, follow the steps below: Select Desktop. Create a style group. Change the styling. This styling will be applied to all devices automatically. Change the device to tablet. Change the styling for tablets. Changes will apply to screen sizes smaller than 1366 pixels. Change the device to mobile. Change the styling for phones. Changes will apply to screen sizes smaller than 896 pixels. ![](/docs/images/_2 (1).png>) ![](/docs/images/_3 (1).png>) Further Reading How to Design Pages for Mobile How to Use Flex"
  },
  "docs/how-tos/apps/use-data-sources-in-the-page.html": {
    "href": "docs/how-tos/apps/use-data-sources-in-the-page.html",
    "title": "Use Data Sources in the Page | XMPro",
    "summary": "Use Data Sources in the Page Once a Data Source has been added to a Page, it can be used on a number of Blocks. Data Sources can be bound to certain Blocks that allow you to store data or display data, such as a data grid. These can be used if you want to display data in a grid view to the user, or if you want the user to enter some details and have it stored directly into the connected Data Source, such as an SQL Server Database. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Data Integration. Data Integration How to Manage Data Sources {% endhint %} Adding a Data Source to a page To add a Data Source onto the Page of an Application, follow the steps below: Open the Editor for the Application. ![](/docs/images/image (835).png>) 2. Drag a Block that can display data, such as a Data Grid. ![](/docs/images/image (399).png>) 3. Highlight the Block that you want to bind the Data Source to. 4. Select Block Properties. 5. Select a Data Source from the list. 6. Click on Save. ![](/docs/images/image (977).png>) The block highlight color will change to yellow to show it has a Data Source. Click on Launch to launch the Application and view the data. {% hint style=\"info\" %} If the Data Source is properly configured, the data will display and can be visible when the app is launched. {% endhint %} Filtering records from a Data Source To filter and limit the number of records the Data Source displays, follow the steps below: Open the Editor for the Application. ![](/docs/images/image (1594).png>) 2. Highlight the block of the Data Source you want to filter. 3. Click on the edit button to Filter. 4. Add a filtering condition or group. 5. Click on Apply. 6. Click on Save. Sorting records from a Data Source To sort the records the Data Source displays, follow the steps below: Open the Editor for the Application. ![](/docs/images/image (447).png>) 2. Highlight the block of the Data Source you want to sort. 3. Click on the plus button to add a new field to sort by. 4. Sort the field in ascending or descending order. 5. Click on Save. Showing specific records from a Data Source Show # of Results To show a limited number of the records the Data Source displays, follow the steps below: Highlight the block of the Data Source. Click on Block Properties. Show the number of results. Click on Save. Skip # of Results To skip certain rows, follow the steps below: Highlight the block of the Data Source. Click on Block Properties. Skip a number of results. Click on Save. Show Default Row To change the settings for the default row, follow the steps below: Highlight the block of the Data Source. Click on Block Properties. Change the default row. Click on Save."
  },
  "docs/how-tos/apps/use-dynamic-properties.html": {
    "href": "docs/how-tos/apps/use-dynamic-properties.html",
    "title": "Use Dynamic Properties | XMPro",
    "summary": "Use Dynamic Properties Dynamic Properties allow you to select a dynamic value for a property from the Page Parameters, Variables, User Details, and from a column or expression of the current row of a parent Block's Data Source. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Properties. Block Properties How to Manage Pages {% endhint %} How to Enable Dynamic Properties To enable Dynamic Properties, follow the steps below: Select the Block on the Canvas on which you want to add Dynamic Properties. Click the Block Properties tab or double-click the Block on the Canvas. If the property has a button on the left with an A icon, click on that icon to switch to Dynamic Property. Select the value to which you want to bind that property. In this example, it is binding to the device type so this Block will be visible if the Device is Mobile. ![](/docs/images/image (455).png>) ![](/docs/images/image (586).png>)"
  },
  "docs/how-tos/apps/use-expression-properties.html": {
    "href": "docs/how-tos/apps/use-expression-properties.html",
    "title": "Use Expression Properties | XMPro",
    "summary": "Use Expression Properties Expression Properties allow you to create short scripts to create a custom value. This custom value can be calculated from other Variables, Parameters, user input, data from Data Sources, and various Functions, Constants, and Operators. See the Variable and Expressions article to learn more about Expressions. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Properties. Block Properties How to Manage Pages {% endhint %} How to Enable Expression Properties To enable Expression Properties, follow the steps below: Select the Block on the Canvas on which you want to add Expression Properties. Click the Block Properties tab or double-click the Block on the Canvas. If the property has a button on the left with an A icon, click on that icon to switch to Dynamic Property. Click again to switch to the Expression Property. ![](/docs/images/image (743).png>) 4. Build the Expression. ![](/docs/images/image (1715).png>)"
  },
  "docs/how-tos/apps/use-flex.html": {
    "href": "docs/how-tos/apps/use-flex.html",
    "title": "Use Flex | XMPro",
    "summary": "Use Flex Flex styles are a way of changing the layout of the page so it is responsive. When using Flex, the layout and position of the Blocks will respond to fit the screen, which is important in ensuring that users have a good user experience regardless of the screen size they are viewing your Application on. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Devices and the Style Manager. Device Style Manager How to Use Style Manager and Devices {% endhint %} Enabling Flex Styles To enable Flex styles, follow the steps below: Select the parent Block of the Blocks you want to configure the position for. Click on Block Styling. Choose Flex for the display or enable the flex container. Flex Container Direction The direction determines which direction the content will go. It can either be: Row - Left to right Reverse row - right to left Column – Top to Bottom Reverse column – Bottom to Top Justify Justify determines the way the contents are laid out. It can either be: Start End Space Between (puts spaces between the Blocks) Space Around (puts an equal amount of space around each Block) Center Align-Items Determines the vertical alignment of the Blocks. It can either be: Start End Stretch Center Blocks inside the Flex Container Grow Grow will grow the item to fit the container. If multiple Blocks have a grow value greater than 0, they will take up a ratio of the available space. Shrink Shrink determines whether an item is allowed to shrink if the screen is too small or if other Blocks take up too much space. Shrink will not work if the Block has a minimum width or height. Basis This determines the default size of the object along its direction axis. Order The order will change the order of the Blocks. Blocks with no order will be displayed first, followed by the ordered Blocks going in ascending order. Align-Item Aligns the individual Blocks. They can either be: Start End Stretch Center Further Reading How to Design Pages for Mobile"
  },
  "docs/how-tos/apps/use-page-layers.html": {
    "href": "docs/how-tos/apps/use-page-layers.html",
    "title": "Use Page Layers | XMPro",
    "summary": "Use Page Layers Blocks are organized on the Canvas in a hierarchy. A list view of this hierarchy is visible by selecting the Page Layers tab in the toolbox. This can make selecting Blocks simpler and easier as some Applications may have many nested layers and it can be difficult to select them on the Canvas itself. Each Block that is selected also displays a list of any children Blocks if applicable. This allows you to find the exact Block you need to manipulate. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Page Layers. Page Layers How to Manage Pages {% endhint %} Selecting a block Individual Blocks can be selected from the Page Layers view. To select a block on the Page using the Page Layers list, follow the steps below: Click on Page Layers. Expand layers to get to the element. Click on the Block you want to select. The Block selected will be highlighted on the Page. ![](/docs/images/_1 (1).png>) Moving blocks around Clicking and dragging the handle on the right of the Block will re-arrange itself in the list as well as the page. To move Blocks around, follow the steps below: Click on Page Layers. Expand layers to get to the element. Click and drag the handle on the right. Drop the Block in another place to move it around. The selected new location will be highlighted in orange on the Page. ![](/docs/images/_2 (1).png>) ![](/docs/images/_3 (1).png>) The Block will be moved on the Page. Renaming blocks Renaming Blocks will not affect what the block looks like at runtime, it will only change the text or label that will appear when you hover over the Block. To rename Blocks, follow the steps below: Click on Page Layers. Expand layers to get to the element. Double Click on the text to highlight it and change it. 4. Enter the new name of the Block. {% hint style=\"info\" %} Changing the name will not change the content of the Block, it will only change what the Block is called. {% endhint %} Hide and Show blocks You can press the hide on the left to hide elements on the page. This will hide the elements on the page and also in runtime view as well. Click on Page Layers. Expand layers to get to the element. Click on the eye symbol on the left to toggle the visibility of the Block. ![](/docs/images/_8 (1).png>)"
  },
  "docs/how-tos/apps/use-validation.html": {
    "href": "docs/how-tos/apps/use-validation.html",
    "title": "Use Validation | XMPro",
    "summary": "Use Validation Validation is a way of making sure the user has correctly entered data when they are filling out a form. The Validation occurs when they press the submit button. This prevents the user from submitting data that is inaccurate or in the wrong format. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Applications. Block Properties How to Manage Pages {% endhint %} Adding Validation to Forms To add Validation to forms, follow the steps below: Add a form control such as a Fieldset. Create your form. Add input blocks such as textboxes, number selectors, or text areas, which allow validation to be configured. Highlight the field you want to validate. Click on Block Properties. Enter if the field is required. Required means the user must enter a value for the field. If required, enter the error message the user will see if they do not enter a value. If applicable, enter a regex if the value needs to have a specific pattern. For example, if the field needs at least two words, or must have an @ symbol for an email. If applicable, enter the error message if the pattern is not provided. 10. Highlight the submit or confirm button. 11. Click on Block Properties. 12. Select the Validation Groups to validate. Viewing Validation at runtime At runtime, if the 'submit' or 'confirm' button is pressed without valid inputs, the fields will be highlighted in red and the corresponding error warning will show. Once all fields are valid, all errors will disappear from the screen and the user will be able to press the submit or confirm button. Validation Groups Validation Groups can be used when there are multiple forms on the page that need to be separated. To organize validation groups, follow the steps below: Select the field for the second form. Click on Block Properties. Expand Validation. Click on the plus to create a new validation group. 5. Enter the name of the validation group. 6. Click on Create. 7. Highlight each of the fields on the new form. 8. Click on their Block Properties. 9. Add them to the new validation group. 10. Highlight the submit button for the new form. 11. Click on Block Properties. 12. Select the new validation group. The validation will only be applied to that validation group."
  },
  "docs/how-tos/apps/use-variables-and-expressions.html": {
    "href": "docs/how-tos/apps/use-variables-and-expressions.html",
    "title": "Use Variables & Expressions | XMPro",
    "summary": "Use Variables & Expressions Variables are placeholders used to hold and maintain certain values. In some cases, it is possible to not know some of the values that you might want to display or use within the Application. In this case, you can use Variables where the real value can be substituted in later. Expressions can also be configured and are useful for doing certain calculations and returning results which can also be used in the Application. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Variables and Expressions. Variables and Expressions How to Manage Pages {% endhint %} Adding a Variable To add a Variable to the Application, follow the steps below: Open the editor for the Application. ![](/docs/images/image (1758).png>) 2. Open the page the Variable will be stored in. 3. Click on Page Data. 4. Click on the plus symbol to add a new Variable. 5. Type the name of the Variable. 6. Enter the type and whether it is a value or expression. 7. Click on Save. The Variable should show in the list of Variables. Using a Variable To use a Variable, follow the steps below: Highlight the Block you want to bind the Variable to. In this case, it is a textbox for the user’s input. Click on Block Properties. Expand Value. Select the Variable Click on Save. Adding Expressions When adding a Variable, there is an option to build an expression. The example below shows an expression that multiplies the values of two variables together. To build an expression, follow the steps below: Change the mode to expression. Select the expression box. Select from a range of parameters, Variables, and other functions to build an expression. When a value is selected it will appear in the expression box. {% hint style=\"info\" %} Numbers are identified as integers by default. Convert to other data types using: a method e.g.ToLong(0)for the value 0 as a long 2.0for the value 2 as a double {% endhint %} 4. Click on Save. Deleting a Variable To delete a Variable, follow the steps below: Click on Page Data. Click on Edit to edit the Variable. Click on Delete. 4. Confirm that you want to delete the Variable."
  },
  "docs/how-tos/connectors/README.html": {
    "href": "docs/how-tos/connectors/README.html",
    "title": "Connectors | XMPro",
    "summary": "Connectors A Connector is a pre-built integration plug-in for the XMPro App Designer that facilitates a no-code connection to third-party data sources. Note It is recommended that you read the article listed below to improve your understanding of Connectors. Connector Articles Manage Connectors Configure Connectors Manage Connector Instances Manage Connector Logs Manage Connector Versions Manage Connector Packages Manage Connector Subscriptions"
  },
  "docs/how-tos/connectors/building-connectors.html": {
    "href": "docs/how-tos/connectors/building-connectors.html",
    "title": "Building Connectors | XMPro",
    "summary": "Building Connectors Overview To start developing a new Connector, create a new C# library project in Visual Studio and import the XMPro.Integration.Framework NuGet package. When writing the code for a Connector, you will have to implement one or more interfaces: Interface Necessity Description IConnector Required Provides the structure implemented by all Connectors. ILiveConnector Optional Allows the Connector to send notifications to the App Page to notify the change of entity. IConnectorError Optional Allows the Connector to publish error messages to the Connector Logs. ITSAConnector Optional Allows the Connector to advise the Time Series Analysis that the data is pre-processed and returned in buckets. ITSCConnector Optional Deprecated from v4.4.12, upgrade to the ITSAConnector. IConnector IConnector is the primary interface that all Connectors must implement as it provides the structure for the workings of the Connector. There are several methods required to implement this interface. Settings/Configurations Some Connectors need to be provided with configurations by the user. For example, for a SQL Connector to get records from a SQL Database, it needs the following: Server Instance User Name Password Database Each of these settings should be referenced in the code and must correspond to the settings template created when packaging your Connector. {% hint style=\"info\" %} A template is a JSON representation of all the controls and their layout that will be used to capture the settings from a user. {% endhint %} An example of the settings template (generated using the XMPro Package Manager) is shown in the image below. The settings in this example consist of the following controls: Group (Server) Textbox Checkbox Group (Database) DropDown ScriptBox Each control has a Key, which uniquely identifies it in the template and allows the Connector code to access its value at any time. To get the value contained in a setting, use the following code: string mySetting = parameters[“myUniqueKey”]; Before a template is rendered on the screen, or if a postback occurs on any control in the template, the method below would be called to allow the Connector an opportunity to make any necessary runtime changes to the template, for example, verifying user credentials, displaying all tables of a selected database in a drop-down list, etc. In this example, no changes are being made to the template, but they can be added to the todo section if needed. {% hint style=\"info\" %} For a postback to occur after a user navigates out of a setting field, the Postback property needs to be set to true when packaging the Connector. {% endhint %} public string GetConfigurationTemplate(string template, IDictionary<string, string> parameters) { //parse settings JSON into Settings object var settings = Settings.Parse(template); //populate the settings/configuration controls with the user selected values new Populator(parameters).Populate(settings); // ToDo: update the controls, values or the data sources here //return the updated settings xml return settings.ToString(); } Entities Each Connector must inform the Engine about the Entities that will be produced by the Connector. To do this, implement the following method: public IEnumerable<Entity> GetEntities(IDictionary<string, string> parameters) This method returns a collection of Entities, which represent an object or a function of the Connector. For example, an Entity can be a Table within the configured Database in the SQL Connector. Each Entity contains the following: Name Description EntityId A unique identifier for the Entity. Name The name of the entity. IsLive Indicate if the Entity supports Live Updates. Operations The operations which the Entity supports. Read Insert Update Delete {% code overflow=\"wrap\" %} new Entity(\"OpenRecommendation\") { IsLive = True, Name = \"Open Recommendation\", Operations = Operation.Read }; {% endcode %} Entity Properties The Connector must provide a list of the Entity's properties, which describe its key, output schema, and optional inputs. To achieve this, implement the following method: public Entity GetEntity(string entityId, IDictionary<string, string> parameters) This method returns the Entity with a collection of properties for the selected Entity's entityId, which is passed as a parameter to this method. Most properties will be Output properties, which describe the Entity's data, and are indicated by setting key and isParameter to false. For example, this String property named OutputString: {% code overflow=\"wrap\" %} new Property(\"OutputString\", Settings.Enums.Types.String, key: false, isParameter: false) {% endcode %} At least one property should be marked as the Entity's Key, indicated by setting key to true. The key uniquely identifies records when performing Update or Delete tasks, and for Application Blocks such as Grids that require a unique key per record. For example, this Long property named Id: new Property(\"Id\", Settings.Enums.Types.String, key: true, isParameter: false) Finally, we have the optional Parameter properties, indicated by setting isParameter to true. They are included in the output schema, but also exposed to the App Page as optional input to the Read operation - retrieved from its options parameter to modify the results. For example, this Int property named Input: new Property(\"Input\", Settings.Enums.Types.Int, key: false, isParameter: true); {% hint style=\"info\" %} Few Connectors need Parameter properties - they are used when the desired outcome is not achievable through the regular Configuration settings or the data source filter. For example, the Azure Digital Twins Connector's Time Series entity includes an optional input parameter, \"$ts\", which shows up in the output as well as a Timestamp for that specific record/event. {% endhint %} Create The Connector must implement a method called Create, which is invoked when your Connector is hosted. User-defined configuration is passed as a parameter to this method and should be stored in a class variable for later use. This is a good point to provide any resources needed for the working of your Connector. void Create(Configuration configuration) { this.config = configuration; // ToDo: Provision any resources or write Startup logic. } Read The Read method is one of the Entities' operations and is expected to return a JToken back to the Engine. This method is invoked when a Read/Refresh Action is called from a Block within an App Page. {% code overflow=\"wrap\" %} public IQueryable<JToken> Read(string entityId, OperationOptions options, out long count, JObject extraOptions = null) {% endcode %} This method contains a list of parameters being passed from the Engine. Name Description entityId A unique identifier for the Entity OperationOptions The operation options as configured by a user: Parameters: a JObject containing the input parameters' value Filter: the data filter criteria TransactionName: the name of the transaction count The number of records extraOptions A JObject contains the following: Sort: the data sorting criteria Skip: the number of records to skip Take: the number of records to return Insert The Insert method is one of the Entities' operations and is expected to return a JObject back to the Engine with the inserted record Id. This method will be invoked when an Insert Action is called from a Block within an App Page. public JObject Insert(string entityId, JObject values, OperationOptions options) Name Description entityId A unique identifier for the Entity values A JObject of values to be inserted OperationOptions The operation options as configured by a user: Parameters: a JObject containing the input parameters' value Filter: the data filter criteria TransactionName: the name of the transaction Update The Update method is one of the Entities' operations and is expected to return a JObject back to the Engine with the updated record Id. This method will be invoked when an Update Action is called from a Block within an App Page. {% code overflow=\"wrap\" %} public JObject Update(string entityName, JObject key, JObject values, OperationOptions options) {% endcode %} Name Description entityId A unique identifier for the Entity key A JObject containing the primary key of the record to be updated values A JObject of values to be updated OperationOptions The operation options as configured by a user: Parameter: a JObject containing the input parameters' value Filter: the data filter criteria. TransactionName: the name of the transaction Delete The Delete method is one of the Entities' operations and is expected to return back to the Engine with the number of records deleted. This method will be invoked when a Delete Action is called from a Block within an App Page. public int Delete(string entityId, JObject key, OperationOptions options) Name Description entityId A unique identifier for the Entity key A JObject containing the primary key of the record to be deleted OperationOptions The operation options as configured by a user: Parameters: a JObject containing the input parameters' value Filter: the data filter criteria TransactionName: the name of the transaction Destroy Each Connector must implement a Destroy method, which will be invoked when an App Page is closed. Use this method to release any resources or memory that your Connector may have acquired during its lifetime. void Destroy() Decrypting Values If a Connector’s configuration contains a Secure/Password Textbox, its value will automatically be encrypted. To decrypt the value, use the following set of instructions: var request = new OnDecryptRequestArgs(value); this.OnDecryptRequestArgs?.Invoke(this, request); var decryptedVal = request.DecryptedValue; Custom Events While building your Connector, you may need to use external libraries or third-party event subscriptions to handle custom events. If these are used, you must catch any exceptions from the event handlers yourself, to prevent uncaught exceptions that could possibly crash the App Page if they get through. ILiveConnector The ILiveConnector interface allows the Connector to send notifications to an App Page to notify of a change to the entity. There are several methods required to implement this interface. Subscribe Subscribe is called by the Engine to inform the Connector that an App Page has been opened that uses Live Data from a given Entity (IsLive property set to true), and should be used to begin listening for changes to that entity. The Subscribe method has two overloads and can be used in the following ways: Use the first overload if you only want to pass the entity ID to the method. public void Subscribe(string entityId) Use the second overload if you want to use filtering. public void Subscribe(string entityId, OperationOptions options, JObject extraOptions) {% hint style=\"warning\" %} The second overload is supported on App Designer v4.3.5 and XMPro.Integration.Framework v4.2 and above. {% endhint %} UnSubscribe Unsubscribe is called by the Engine to inform the Connector that all AppPages that use Live Data from a given Entity have been closed, and should be used to stop listening for changes to that entity. public void UnSubscribe(string entityId) Publish This method can be used to allow external changes to be passed to the Connector's internal entity tracking. public void Publish(string entityId, Change[] changes, JObject options) OnChange To push the changes of entities to an App Page that subscribed to the live update, your Connector should invoke the OnChange event with the values of changes as arguments: {% code overflow=\"wrap\" %} this.OnChange?.Invoke(this, new OnChangeArgs() { EntityId = entityId, Changes = changes.ToArray() }); {% endcode %} IConnectorError A Connector can publish messages to the Connector Logs by implementing the IConnectorError interface. To log the error, your Connector should invoke the OnConnectorError event with the error information passed as arguments: {% code overflow=\"wrap\" %} this.OnConnectorError?.Invoke(this, new OnErrorArgs(ConnectionId, Timestamp, Source, Error, DetailedError, Data)); {% endcode %} ITSAConnector v4.4.12: replaced the ITSCConnector interface, which is now deprecated. The ITSAConnector interface notifies the Time Series Analysis to use optimized client-side querying to increase its performance. The Connector will pre-process the large volumes of data and return it in buckets. When a Connector that implements the ITSAConnector interface is used with a Time Series Analysis Block, the Block expects the Connector to implement a specialized structure for data inputs and outputs: Implement Date Buckets by organizing the data into separate partitions based on specific time intervals, such as days, weeks, or months. Below is a of sample bucketed data with an interval of 2 hours: Bucketed Timestamp Sensor1 Sensor2 Sensor3 Original Timestamp 2024-03-28 08:00:00 25.4 18.2 32.7 2024-03-28 05:36:17 2024-03-28 08:00:00 24.8 18.5 33.2 2024-03-28 07:12:45 2024-03-28 08:00:00 25.1 18.9 34.0 2024-03-28 08:58:21 2024-03-28 10:00:00 25.6 19.2 34.5 2024-03-28 09:27:54 2024-03-28 10:00:00 26.2 19.6 34.8 2024-03-28 10:44:30 2024-03-28 10:00:00 26.5 19.8 35.1 2024-03-28 11:15:01 2024-03-28 12:00:00 27.0 20.1 35.3 2024-03-28 12:21:59 2024-03-28 12:00:00 27.3 20.5 35.7 2024-03-28 13:33:42 2024-03-28 12:00:00 27.7 20.8 36.0 2024-03-28 14:07:29 2024-03-28 14:00:00 27.9 21.2 36.3 2024-03-28 15:08:11 Replace the Original Timestamp with the Bucketed Timestamp value for final results. For example: Original Timestamp Sensor1 Sensor2 Sensor3 2024-03-28 08:00:00 25.4 18.2 32.7 2024-03-28 08:00:00 24.8 18.5 33.2 2024-03-28 08:00:00 25.1 18.9 34.0 2024-03-28 10:00:00 25.6 19.2 34.5 2024-03-28 10:00:00 26.2 19.6 34.8 2024-03-28 10:00:00 26.5 19.8 35.1 2024-03-28 12:00:00 27.0 20.1 35.3 2024-03-28 12:00:00 27.3 20.5 35.7 2024-03-28 12:00:00 27.7 20.8 36.0 2024-03-28 14:00:00 27.9 21.2 36.3 {% hint style=\"info\" %} SQL and ADX have a native function to achieve this, DATE_BUCKET() for SQL and bin() for ADX. {% endhint %} Use the interface and implement buckets on any Connector to access the Time Series Analysis Block optimizations for your Data Source. Example The code below is an example of an empty connector. Take note of how the interfaces and methods have been implemented. using Newtonsoft.Json.Linq; using System; using System.Collections.Generic; using System.Linq; using System.Text; using XMPro.Integration.Framework; using XMPro.Integration.Framework.Connector; using XMPro.Integration.Settings; namespace XMPro.Integration.NewConnector { public class NewConnector: ILiveConnector, IUsesVariable, IConnectorError, ITSAConnector { public long UniqueId { get; set; } public event EventHandler<OnChangeArgs> OnChange; public event EventHandler<OnErrorArgs> OnConnectorError; public event EventHandler<OnDecryptRequestArgs> OnDecryptRequest; public event EventHandler<OnVariableRequestArgs> OnVariableRequest; private Configuration _config; public void Subscribe(string entityId) { // Implement script for Subscribe method } public void UnSubscribe(string entityId) { // Implement script for UnSubscribe method } public void Publish(string entityId, Change[] changes, JObject options) { // Implement script for Publish method } public string GetConfigurationTemplate(string template, IDictionary<string, string> parameters) { var settings = Settings.Settings.Parse(template); new Populator(parameters).Populate(settings); return settings.ToString(); } public IEnumerable<Entity> GetEntities(IDictionary<string, string> parameters) { this._config = new Configuration(parameters); //Implement script for GetEntities method return new List<Entity>() { { new Entity(\"0\") { Operations = Operation.Read, IsLive = false, Name = \"Name\" } } }; } public Entity GetEntity(string entityId, IDictionary<string, string> parameters) { return new Entity(entityId) { Properties = new List<Property>().ToArray(), Operations = Operation.Read, IsLive = false }; } public void Create(Configuration config) { //Implement script for Create method } public IQueryable<JToken> Read(string entityId, OperationOptions options, JObject extraOptions = null) { try { //Implement script for Read method return Enumerable.Empty<JToken>().AsQueryable(); } catch (Exception e) { this.OnConnectorError?.Invoke(this, new OnErrorArgs(123, DateTime.UtcNow, nameof(Read), e.Message, e.ToString())); return Enumerable.Empty<JToken>().AsQueryable(); } } public JObject Insert(string entityId, JObject values, OperationOptions options) { //Implement script for Insert method } public JObject Update(string entityId, JObject key, JObject values, OperationOptions options) { //Implement script for Update method } public int Delete(string entityId, JObject key, OperationOptions options) { //Implement script for Delete method } public void CommitTransaction(string transactionName) { //Implement script for CommitTransaction method } public void Destroy() { //Implement script for Destroy method } } } Further Reading Packaging Connectors"
  },
  "docs/how-tos/connectors/manage-connectors.html": {
    "href": "docs/how-tos/connectors/manage-connectors.html",
    "title": "Manage Connectors | XMPro",
    "summary": "Manage Connectors Connectors allow you to connect to third-party sources of data. Examples of these Data Sources include databases, Data Streams, or Recommendations, which can be integrated into the Application and are needed if you want to display any real-time or context data to the user on a Page of an Application. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Connectors. Connector {% endhint %} Creating Connectors Creating a Connector can be divided into two parts: Writing the code for a Connector Connectors are generally written in C# as library projects that make use of the XMPro.Integration.Framework NuGet package. XMPro.Integration.Framework requires your project to be written using a predefined structure. This structure requires you to implement certain interfaces. To learn more about how to use this framework, refer to these instructions. {% hint style=\"info\" %} Code for some Connectors has been made available on GitHub. It might be useful to use these resources as an example when writing your own Connectors. {% endhint %} Packaging the Connector After writing your code, you need to use the XMPro Package Manager Windows 10 desktop application to package your Connector. This application allows you to specify all the properties your Connector requires, add the user settings in the form of controls, and allows you to upload the DLL of the Connector you’ve written. Finally, it will create a file with a “.xmp” extension, which you can upload to App Designer and start to use to build Applications. To package the Connector, refer to these instructions. Adding a Connector Connectors can be added via the Connectors page before being used in any of the Applications. Click on the Connectors page from the left-hand menu. Click on Add. Upload the xmp file of the Connector. The unique ID, name, version, and description are re-filled based on details from the file uploaded. Add the category. Click on Save. ![](/docs/images/image (1247).png>) Selecting an existing Connector opens the configuration panel where details for the Connector can be viewed. The category can also be changed here. ![](/docs/images/image (1824).png>) Deleting Connectors Connectors can be removed via the Connectors page. To remove Connectors: Open the Connectors page from the left-hand menu. Click Select. Select the Connectors that you would like to remove. Click Delete. ![](/docs/images/Connectors_5 (1).png>) 5. Confirm that you would like to delete the Connectors selected. Versions of a Connector When a new version of a Connector is added, the Connector will be updated to the new version. The old version will remain and Applications using the old version of the Connector will continue to use that version until it is upgraded manually. Open the Connectors page from the left-hand menu. Select a Connector to view its information. Each version is shown along with the number of Applications still using that version. Select the version to view the complete list of Applications still using that version. {% hint style=\"info\" %} Users with DeleteConnector rights and Admins see the usage count for all Applications, whereas other users see the usage count of Applications to which they have access. {% endhint %} ![](/docs/images/image (246).png>) Categories Connectors can be organized into categories. These categories are separate from the App and Data Stream Categories. Click on the Connectors page from the left-hand menu. Click on Manage Categories. The list of existing categories is shown. Click on Add. 5. Name the category. 6. Click on Add. Categories for Connectors can also be edited or removed. To edit or remove a category for Connectors, select the category from the list of existing categories to enable the edit and remove options. Finding Help for Connectors Help documentation is available for every Connector. These pages provide context, configuration definitions, an example, and release notes to help if you are unsure of anything related to the Connector you are configuring. See the Integrations article for the list of Connector documentation links."
  },
  "docs/how-tos/connectors/packaging-connectors.html": {
    "href": "docs/how-tos/connectors/packaging-connectors.html",
    "title": "Packaging Connectors | XMPro",
    "summary": "Packaging Connectors Getting Started The XMPro Package Manager is a Windows 11 desktop application that enables you to package a new Connector or update details for an existing Connector. See the Connector article for more information on Connectors. This application takes you through the process of specifying all the properties your Connector requires, adding or changing the controls for each user setting, and uploading the DLL files of the Connector code. It will provide you, upon completion, with a file that can be uploaded to Application Designer after which you can use the Connector in App Pages. You can download the software from the Microsoft Windows 10 Store or clicking here. ![](/docs/images/image (1163).png>) After installing the XMPro Package Manager, launch the application from the Microsoft Store or search for “XMPro Package Manager” in the Start menu and then click on “XMPro Package Manager”. ![](/docs/images/image (1409).png>) New / Import On the first screen of the application, you can either create a new Connector package or import and update an existing one. {% hint style=\"info\" %} Use the arrows at the bottom of the page section to move forward or backward in the application. When you import an existing package, you have the option to export the package as a JSON file. This is useful either to compare packages or for source control and version management. You can also import the JSON file from an existing package, which is particularly useful if you need to modify translations added through the Include Multilingual Support feature. {% endhint %} ![](/docs/images/image (1002).png>) Details The Details form allows you to specify or edit the properties of a Connector. These properties are listed and explained below. ![](/docs/images/image (1443).png>) Name The name of the Connector is what the Connector will be known as once it is uploaded to the Application Designer platform, for example, “SQL Connector”. Description The description is a brief explanation of what the Connector does, for example, “This Connector allows you to read/update a SQL Database table“. Version The version of the Connector. Any real number is acceptable, for example, \"1.02\". {% hint style=\"danger\" %} If you make a change to an existing Connector, make sure you increment the version number as Application Designer will not allow you to upload two of the same Connectors with the same version. {% endhint %} Entry Point The entry point is the namespace and class name of the actual Connector’s DLL file. For example, if a Connector with the class name “Connector” is located in the XMPro.AzureSQLConnectors namespace, the Entry Endpoint for it would be “XMPro.AzureSQLConnectors.Connector”. Icon File The icon used to represent your Connector. Click the Browse button, navigate to where you’ve stored the file via the Explorer and select the new image file. {% hint style=\"info\" %} It is recommended that you upload either a JPG or PNG file with a size of 64×64 pixels to accommodate for retina displays. {% endhint %} References The References form is where you upload the file(s) required for the Connector to execute. Only files in the Selected File(s) list will be included in the package, and any DLLs must be created in .NET. To upload a file, click the Browse button next to the DLL File(s) field and navigate to where the files are located. Select all the files needed and click the Add button to add them to the Selected File(s) list. To remove a file from the list, click the Delete button next to the file name in the Selected File(s) list. Type Description Plugin The DLL file that was generated when you built the project containing your Connector source code. Reference Additional DLL file(s) referenced by the Plugin File, such as Newtonsoft.Json. You do not need to upload the XMIoT.Framework.dll file as this DLL is automatically included. Resource Additional DLL file(s) needed by the Reference File. ![](/docs/images/image (1331).png>) Settings Depending on what your Connector does, it might require that the user provide certain information, such as a server URL, username, or password. For each of these information fields (or settings), you need to specify which control should be used and what each control represents, for example, the SQL Connector will require the user to add a server URL. The user should provide this value using a text-box control. Thus, you need to create a control with a type of “TextBox” and a caption that reads “Server URL” in the XMPro Package Manager application. The following controls are available to be used to capture user input: Button CheckBox CheckList DropDown EditList FileUpload Filter Grid Group HTML Editor NumberBox ScriptBox TextBox Title TokenBox VariableBox Each control has several properties that have to be set and not all properties apply to all controls. For example, options apply to a drop-down control and not a text-box control. The table below contains a list of all the available properties, their description, and to which controls they are applicable. Property Name Control Type Description Allow Custom Text Drop-Down Allows the user to type custom text in the drop-down field if checked. Allow Custom Tokens Token Box Allows the user to add custom tokens if checked. Caption All Text that will be displayed with the group or setting. The caption is usually one or two words, describing the value that should be provided by the user, for example, “Server URL”. Default Value Title The default value of the title. Font Size Script Box Size of the font in the Script Box. Help Text All, excluding Groups If you need to provide the user with any additional information about the purpose of the setting or helpful instructions, specify it in this field. Key All Uniquely identifies the group or setting. Keywords Script Box Define your variables or other custom keywords here, so that they will be available in the editor’s IntelliSense. Options (Drop Down) Drop-Down Use the Options-area to add values to the drop-down menu by specifying the Text and Value fields and then clicking Save. You may also choose an option to be used as the default option by checking the “Set as Default Value” box. Options (HTML Editor) HTML Editor Allows you to specify placeholders that can be mapped to input fields in the input received by the Agent. Postback All If checked, will cause the form to do a postback to retrieve values from the server when the field loses focus (when the user clicks out of the field). Required All, excluding Groups The control will be validated to make sure that a value has been specified if this box is checked. ScriptBox Height Script Box Height of Script Box. ScriptBox Mode Script Box Language in which script has to be written. ScriptBox Theme Script Box The theme of the Script Box. Themes available include the following: Ambiance Chaos Chrome Clouds Clouds_midnight Cobalt Cromson_editor Dawn Dreamweaver ScriptBox Width Script Box Width of Script Box. Secure All The value of the control will be treated as a secure value if this box is checked (encrypted and not displayed on the form in plain text). An example of a secure value is a SQL Server password. Show Grid Lines Grid The grid lines of the grid will be shown if checked. Show Header Grid The header of the grid will be displayed, if checked. Sort Index All This is used to determine the group or setting’s position and works with increments of 10. Adjust this value to move the group or setting up or down on the form. Unique Key Grid Mark a specific column as being unique, for example, an identity column. Visible All This field sets the initial visibility of the group or setting. ![](/docs/images/image (69).png>) Adding Settings Settings are grouped logically into one or more groups, such as authentication, criteria, and output. Create a group first, then add controls for settings to the group. To do this, follow the steps below: Click on the plus-icon (top right, next to the Settings header). A form section will open, allowing you to specify a group for the settings. Specify a unique value that can be used as the key for the group. Add the caption you would like to use. Click Save. Next, we are going to add a setting. Click on the plus-icon next to the group you’ve created. Choose the type of control you would like to use. Add a unique key for your control. Please note that this key needs to correspond to what you defined in your code. Add a caption for your control. If needed, add a default value. If required, add help text. Select the options that apply from the list of check-boxes. Click Save. ![](/docs/images/image (306).png>) ![](/docs/images/image (1823).png>) Output Export as JSON file Tick the checkbox Export as JSON file too? if you would like to export the file as JSON too. It will later be saved to the same directory as the XMP file with the file name category_name_version.json. Include Multilingual Support Tick the checkbox Include Multilingual Support? if you would like to add support for languages other than English. Uncheck languages you don't want to include. This feature leverages generative AI to provide language translation. It is available only if the following requirements are met: You are connected to the internet. Open AI is configured (Click here for instructions on how to do so). {% hint style=\"info\" %} Only the Connector Description, Properties and Static Helptext are translated. Internal messages and dynamic Helptext added when building the Connector are not included. Multilingual support requires XMPro App Designer v4.4.16+. Override the automated translations by editing the JSON file and repackaging it with an incremented version number. {% endhint %} ![](/docs/images/PM Output (1).png>) Configure OpenAI API Click Configure OpenAI. A form will open for you to add or modify the OpenAI Endpoint and Api Key. Click Save. Review: Details Lastly, you can navigate back through the steps to review the details that you’ve specified. If you are satisfied, complete the wizard by clicking the Save button below before navigating to the folder where you would like the package to be exported. Your package will be created with the file name category_name_version.xmp. {% hint style=\"info\" %} If you imported an existing file, take care to: either click 'Save as new Connector' to generate a new Connector, or click 'Save' to generate a new version of the original Connector. ensure you select a different location folder or increment the version to avoid overwriting the original. {% endhint %} ![](/docs/images/PM Success Connector.png>) Further Reading How to upload your new Connector to Application Designer {% hint style=\"info\" %} You need to have the correct permissions set against your user to be able to edit and upload Connectors. This is a role not typically given to all users. {% endhint %}"
  },
  "docs/how-tos/data-streams/README.html": {
    "href": "docs/how-tos/data-streams/README.html",
    "title": "Data Streams | XMPro",
    "summary": "Data Streams A Data Stream allows you to view the flow of data between Agents, which are connected using arrows that allow data to flow from one Agent to the next. This allows you to view data from multiple Data Sources in one place, and complete certain actions on the data such as aggregating, filtering, displaying, or re-saving the data into another database. Note It is recommended that you read the article listed below to improve your understanding of Data Streams. Data Stream Articles Manage Data Streams Manage Collections Use Remote Receivers and Publishers Manage Recurrent Data Streams Use Business Case and Notes Run an Integrity Check Check Data Stream Logs Use Live View Use Stream Metrics Troubleshoot a Data Stream Upgrade a Stream Object Version Setup Input Mappings Use Error Endpoints Use the Timeline Context Menu"
  },
  "docs/how-tos/data-streams/check-data-stream-logs.html": {
    "href": "docs/how-tos/data-streams/check-data-stream-logs.html",
    "title": "Check Data Stream Logs | XMPro",
    "summary": "Check Data Stream Logs A Stream Host is an application that can either be installed as a Windows Service or as a Console Application. Stream Hosts enable Data Streams to run, and you can check any status updates, messages, or errors from the Stream Host directly from the Data Stream you are running. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Stream Hosts and Data Streams. Data Stream Collection and Stream Host How to Manage Data Streams {% endhint %} Stream Host logs can be checked by going to the 'Collection' page of Data Stream Designer and viewing the logs there. However, you can also view Logs that are specific to a Data Stream directly from the Data Stream canvas. {% hint style=\"info\" %} To find out more about viewing Stream Host logs, visit the How to Manage Stream Hosts article. {% endhint %} View Data Stream Logs To view Data Stream logs from the canvas, follow the steps below: Publish a Data Stream. Click on Logs. Select the Stream Host Device to view logs for. Logs will display in the grid. ![](/docs/images/Logs - View Logs.png>) You can also maximize the grid to view the data more clearly. ![](/docs/images/Logs - max1.png>) ![](/docs/images/Logs - max2.png>) To view logs within a specific time range, adjust the timeline slider above the grid. ![](/docs/images/Logs - timeline (2).png>) Refresh Data Stream Logs To refresh Stream Host logs, follow the steps below: Publish a Data Stream. Click on Logs. Click on refresh. Logs in the grid will clear and refresh. ![](/docs/images/Logs - refresh.png>) Filter Data Stream Logs To filter Data Stream logs, follow the steps below: Publish a Data Stream Click on Logs. Click on the ‘Level’ filter button. Select the log level. Click on OK. ![](/docs/images/Logs - filter.png>) Export Data Stream Logs To export Data Stream logs, follow the steps below: Publish a Data Stream Click on Logs. Click on the three dots. Export all data or export selected rows. ![](/docs/images/Logs - export2.png>)"
  },
  "docs/how-tos/data-streams/context-menu.html": {
    "href": "docs/how-tos/data-streams/context-menu.html",
    "title": "Context Menu | XMPro",
    "summary": "Context Menu The Right Click Menu or the Context Menu is the menu, which appears when you right-click on the Stream Object in Data Stream. This menu gives you added functionality by offering you actions you can take with the Stream Object. Configure You can configure the Stream Object through the Context Menu. See the Stream Object Configuration article for more details on Agent Configuration. Integrity Check You can run an Integrity Check on the agent through the Context Menu. See Verifying Stream Integrity article for more details on Integrity Check. Disable/Enable You can disable/enable a Stream Object through the Context Menu. Disable a Stream Object Stream Objects can be disabled in a Data Stream. Disabled Stream Objects will be excluded from the output when the Data Stream is published. ![](/docs/images/image (1470).png>) Observe that the Disabled Stream Object is now greyed out. {% hint style=\"warning\" %} Agents with no entry points and multiple entry points cannot be disabled. {% endhint %} Enable a Stream Object Disabled Stream Objects can again be enabled in a Data Stream. Enabled Stream Objects will be Included in the output when the Data Stream is published. Observe that the Stream Object is not greyed out. ![](/docs/images/Included (1).png>) Copy You can copy a Stream Object and Paste it on the canvas. Click anywhere on the canvas and right-click. ![](/docs/images/image (80).png>) Delete You can Delete a Stream Object using the Context Menu."
  },
  "docs/how-tos/data-streams/manage-collections.html": {
    "href": "docs/how-tos/data-streams/manage-collections.html",
    "title": "Manage Collections | XMPro",
    "summary": "Manage Collections Stream Hosts are grouped into different Collections, which are created and maintained in Data Stream Designer. A Collection can be defined as a category that contains a set of Stream Hosts that run the same Data Streams. Collections are used to prevent you from having to redeploy a Data Stream to multiple Stream Hosts, devices, and assets. When a Data Stream is published for a Collection, the devices themselves can just subscribe to that Collection. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Collections. Collection and Stream Host {% endhint %} Video 1: How To Use Collections {% endembed %} Create a Collection Collections are an important concept in the Data Stream Designer. You might have to create a Collection soon after starting to use the Application as they are crucial to the workings of use cases and Streams. A Collection also has to be created before installing a Stream Host as the Stream host is dependent on the information contained in a Collection. To create a Collection, follow the steps below: Open the Collections page from the left-hand menu. Click on New. Choose a name for the new Collection. You may also include tags if needed by choosing tags from the drop-down that will open as soon as you click on the field or by typing a new value and pressing Enter. Click Ok. ![](/docs/images/image (271).png>) Change a Collection key It might be needed to replace or change the key that is associated with a Collection from time to time. To replace a key with a new one, follow the steps below: Open the Collections page from the left-hand menu. Select the Collection from the list. Click on Revoke Key. Confirm that you would like to revoke the key. After clicking Revoke, the key will be replaced with a new key. ![](/docs/images/image (946).png>) Delete a Collection To remove a Collection, follow the steps below: Open the Collections page from the left-hand menu. Select the Collection you would like to remove. Click on the Delete button. ![](/docs/images/image (1323).png>) 4. Confirm that you would like to delete the selected Collection. Further Reading How to Use Remote Receivers and Publishers"
  },
  "docs/how-tos/data-streams/manage-data-streams.html": {
    "href": "docs/how-tos/data-streams/manage-data-streams.html",
    "title": "Manage Data Streams | XMPro",
    "summary": "Manage Data Streams A Data Stream is a visual representation of a flow of data, which is depicted by the use of Agents that are connected by arrows that allow data to flow from one Agent to the next. Data streams are built in an interactive canvas environment that allows you to drag Agents from the toolbox to a drawing area. These Agents allow you to complete certain actions on the data. This includes aggregating, filtering, displaying, or re-saving the data into another database. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Data Streams. Data Stream {% endhint %} Creating a Data Stream To create a Data Stream, follow the steps below: Open the New Data Streams page from the left-hand menu. Give the Data Stream a name. Enter a description. Enter the type (streaming or recurring). Enter the category under which the Data Stream is found in the Data Stream list. Give the Data Stream an icon. Sample icons can be found in the Icon Library. Enter the Collection the Data Stream will have. Click Save. Opening a Data Stream Data Streams can be opened via the Data Streams Page on the left-hand menu, or via the main page that contains the list of Categories. To open Data Streams from the left-hand menu: Open the Data Streams page from the left-hand menu. Select the Data Stream you want to open. To open Data Streams from the list of categories: Click the Logo to open the landing page. Click the Category of the Data Stream. ![Fig 4: Finding a Data Stream from the landing page of categories](/docs/images/_HowToOpen2 (1).png>) 3. Click the Data Stream you want to open. Adding an Agent to the Canvas To add Agents to the canvas, drag and drop Agents from the left-hand toolbox to make up the data flow for your Data Stream. An Agent that has been added to the canvas is called a Stream Object. See the Agents article for more information on Agents. Enter part of the Agent name to filter the toolbox (or expand a category to view the list of Agents). Click and drag an agent onto the Canvas. Click the output endpoint and drag it to an input endpoint to connect two Stream Objects. Copying and Pasting Stream Objects Stream Objects that are in the Data Stream can be copied and pasted using keyboard shortcuts. To copy a Stream object: Select a Stream Object to highlight it. To highlight multiple Stream Objects, hold the ctrl key while you are selecting them. Once the Stream Object(s) are highlighted in yellow, press and hold ctrl + C. To paste the Stream Object that was just copied, press and hold ctrl + P. {% hint style=\"info\" %} You can also copy a Stream Object from one Data Stream and paste it into a different Data Stream. {% endhint %} Deleting a Stream Object To delete a Stream Object on the canvas, follow the steps below: Click a Stream Object to select it. Click Delete. You can also delete Stream Objects that are on the Data Stream canvas by using the 'delete' keyboard shortcut. Select an Agent to highlight it. To highlight multiple Agents, hold the ctrl key while you are selecting them. Once the Stream Object/s are highlighted in yellow, click on the delete key on the keyboard. ![Fig 10: Deleting a Stream Object using the keyboard](/docs/images/Delete Canvas Object.gif>) Deleting a Data Stream To delete a Data Stream, follow the steps below: Click Properties. Click Delete. Fig 11: Deleting a Data Stream Sharing Access to a Data Stream Data Streams can be shared between users with differing permisson. See the Manage Access article to read more about managing access to users. To share a Data Stream, follow the steps below: Click Manage Access. Click Add. Enter the user to which you want to grant access. Choose between read, write, or co-owner permissions. Click Ok. Changing Access to a Data Stream To change permissions of existing users, follow the steps below: Click on Manage Access. Select the user. 3. Change their permissions. 4. Or, delete permissions for the user. Removing Access to a Data Stream To remove the permissions for multiple users, follow the steps below: Click Manage Access. Click Select. Select multiple users. Click Delete. Cloning a Data Stream To clone a Data Stream, follow the steps below: From within the Data Stream canvas, click Properties. Click Clone. This button may not initially be visible on the Properties page but can be found in the menu that appears if you hover with your mouse cursor over the “More” button. Specify the name for the cloned Data Stream. Choose a category to copy the Data Steam to. Please note that this should not be the same as the category of the original Data Stream. Click Save. Further Reading How to Manage Recurrent Data Streams How to Use Business Cases and Notes How to Run an Integrity Check How to Manage Live View How to Troubleshoot a Data Stream How to Upgrade a Stream Object Version How to Setup Input Mappings How to Use Error Endpoints How to Use the Timeline"
  },
  "docs/how-tos/data-streams/manage-recurrent-data-streams.html": {
    "href": "docs/how-tos/data-streams/manage-recurrent-data-streams.html",
    "title": "Manage Recurrent Data Streams | XMPro",
    "summary": "Manage Recurrent Data Streams Data Streams of the Streaming type will run polling Agents at a set interval, for instance, every 10 seconds, whereas Recurrent Data Streams run on a customizable schedule, for instance, once a day at 12am. This may be useful if you only want to read data or perform an action with the data at certain points during the day, or if you want to perform actions on the data once a week, month or year. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Data Streams. Data Stream How to Manage Data Streams {% endhint %} Creating Recurrent Data Streams The streaming type of the Data Stream can be configured at the time of the Data Stream's creation. Click on add Data Stream from the left-hand menu. Change the type to be recurring. Click on Save. ![](/docs/images/image (1607).png>) To change an existing Data Stream to recurring, go into the properties menu and change the type to be recurring. Click on Properties. Change the type to be recurring. Click on Save. Configuring Recurrence for Agents When a Data Stream is set to be recurring, opening the configuration menu for listeners or context providers will allow you to make changes to the schedule for when they occur. To configure recurrence for Agents, follow the steps below: Add Agents to the Data Stream Canvas. Click on Configure for an Agent. Instead of polling intervals, the configuration menu will ask you to configure recurrence. Configure the schedule for the Agent. Start Repeat - set the time the Agent can start listening for data. Repeat - How often the action will be repeated. For example, daily. Repeat Every, Repeat On, and Repeat At - How many times it will be repeated. For example, every day, every second day, or on certain weekdays, and at what time. End repeat - Specifies when to end the recurrence."
  },
  "docs/how-tos/data-streams/remote-receivers-and-publishers.html": {
    "href": "docs/how-tos/data-streams/remote-receivers-and-publishers.html",
    "title": "Use Remote Receivers and Publishers | XMPro",
    "summary": "Use Remote Receivers and Publishers Sometimes it is necessary to run the same Data Stream on two or more different systems. It may be that one system is low-powered and does not have enough resources to handle the integrations or analytical tasks of the Data Stream, or some integrations may not even be accessible on a system that is outside a corporate network or behind a firewall. The solution to these problems is found by using Remote Receivers and Publishers. Two Collections are set up, and half of the Data Stream is run on one Collection while the other half runs on the other Collection. The Stream Host can automatically detect where data has to flow from one Collection to the other (a Collection Hop). Note It is recommended that you read the article listed below to improve your understanding of Live Data. Remote Receivers and Publishers How to Manage Data Streams How to Manage Collections Each Collection allows you to configure a Remote Publisher and a Remote Receiver. Every time a Collection Hop is detected the Stream Hosts will automatically set up the configured Remote Publisher, which will put the data on a central store. The receiving Collection will also automatically set up a Remote Receiver which will receive data from the store and pass it on to the Data Stream on the other side. It is also possible to set up multiple Stream Hosts to funnel data from tens or hundreds of devices into the data store, which will then be redirected to a single Stream Host with a Remote Receiver. Set Up Remote Receivers and Publishers To set up a Remote Receiver and a Remote Publisher, follow the steps below: Create two collections, one for each system. Click the Collection that will be sending data. Choose a Remote Publisher Agent and Version. Configure the Remote Publisher. Each Agent has specific configuration settings. For the MQTT Agent as a Remote Publisher, set up a broker. Click on Apply. Save the Collection. Click the Collection that will be receiving data. Choose a Remote Receiver Agent and Version. Configure the Remote Receiver. Each Agent has specific configuration settings. For the MQTT Agent as a Remote Publisher, Set up a broker and payload definition. In your Data Stream, set the Stream Objects' Collections to the desired Stream Host's Collection."
  },
  "docs/how-tos/data-streams/run-an-integrity-check.html": {
    "href": "docs/how-tos/data-streams/run-an-integrity-check.html",
    "title": "Run an Integrity Check | XMPro",
    "summary": "Run an Integrity Check When running an Integrity Check on a Data Stream, each Stream Object is checked to verify that it is configured correctly. Errors for the Agent will be displayed if there are any issues found with their configurations. This is done to ensure the integrity of your Data Stream and to make sure all input fields are valid and accurate. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of an Integrity Check. Agent Verifying Stream Integrity How to Manage Data Streams {% endhint %} Running an Integrity Check To run an Integrity Check on a Data Stream, follow the steps below: Click on Integrity Check. Wait for the Integrity Check to complete. This can be seen by watching the loading bar on each Agent. When the Integrity Check is completed, Agents will show their errors if they exist. {% hint style=\"success\" %} Agents with a blank background have passed their Integrity Check with no errors. {% endhint %} {% hint style=\"danger\" %} Agents with a red background have reported back some errors in their configuration. {% endhint %} 4. Hover over the Agent with errors to view a list of errors. {% hint style=\"info\" %} An Integrity Check cannot be run on Agents that have unsaved changes. To run an Integrity Check, discard or save all changes made. {% endhint %} Fixing Integrity Check errors The errors are saved to the Stream Object and are not removed until another Integrity Check is performed. Open the configuration panel for the Agents that are showing errors, and fix any errors in the inputted values. In this case, the text file Agent did not have a file path specified. Double click on the Agent with the errors to open its configuration menu. Fix any errors in configuration. Click on Apply. Click on Save. 5. Click on Integrity Check to run it again. 6. Check if the background is no longer red."
  },
  "docs/how-tos/data-streams/setup-input-mappings.html": {
    "href": "docs/how-tos/data-streams/setup-input-mappings.html",
    "title": "Setup Input Mappings | XMPro",
    "summary": "Setup Input Mappings Input Mappings allow you to specify that an Agent receives its input in a specific structure. This is possible by configuring the arrows between the two Agents, which allow the user to map the inputs of the Agent to incoming attributes. This functionality is beneficial if you want to map any incoming data from a preceding Agent to specific attributes that can be saved in rows or columns in a database using an Action Agent, such as an SQL Server Writer. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Input Mappings. Stream Object Configuration How to Manage Data Streams {% endhint %} Adding Input Mappings To add Input Mappings between two Agents, follow the steps below: Click on the arrow between the two Agents you want to map. Click on Configure. The left side lists all the properties the first Agent is sending. The right side is listing all the inputs the receiving Agent is expecting. Select the property you want to map for each field. If the value is greyed out, it means it does not match the type (number, text, etc) that the receiving Agent is expecting for that field. 4. Click on Apply. Automap To map via Automap, follow the steps below: Click on the arrow between the two Agents you want to map. Click on Configure. Click on Automap. Click on Apply. Match by Expression To map via Match by Expression, follow the steps below: Click on the arrow between the two Agents you want to map. Click on Configure. Click on Match by Expression. Enter a prefix, postfix, or Expression. For example, ‘num’ as a prefix will match ‘numApples’ with ‘Apples’. 5. Click on Apply. Show Unmapped To show Unmapped fields, follow the steps below: Click on the arrow between the two Agents you want to map. Click on Configure. Click on Show Unmapped. ![](/docs/images/_1 (1).png>) The list will change to only show the list of fields that have not yet been mapped. ![](/docs/images/_2 (1).png>)"
  },
  "docs/how-tos/data-streams/troubleshoot-a-data-stream.html": {
    "href": "docs/how-tos/data-streams/troubleshoot-a-data-stream.html",
    "title": "Troubleshoot a Data Stream | XMPro",
    "summary": "Troubleshoot a Data Stream When creating and configuring a Data Stream, there is a chance it may not be working as expected, and you may have to find out more information as to why it is not behaving the way it should. There are a few options available on how to troubleshoot a Data Stream. Troubleshooting is required if you want to make sure the flow of data is accurate. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Data Streams. Data Stream How to Manage Data Streams {% endhint %} Troubleshoot using Live View To troubleshoot using the Live View, follow the steps below: Click on Publish. ![](/docs/images/_1 (1).png>) 2. Click on Live View. 3. Select the Agent/s to view the Live Data for. 4. Click Save. ![](/docs/images/_2 (1).png>) {% hint style=\"info\" %} If data is not being displayed when it should be, or if the values are not being displayed as expected, something may be going wrong with the Agent. {% endhint %} Troubleshooting using Error Endpoints To Troubleshoot using the Error Endpoints, follow the steps below: Drag an Event Printer agent onto the canvas. They can be found under ‘Action Agents.’ Add an arrow from the error endpoint connected to the Event Printer. Click on Publish. ![](/docs/images/_5 (1).png>) 4. Click on Live View. 5. Select the Event Printer. 6. Click Save. Troubleshooting when there is no data visible In some cases, the Event Printer does not show any data when trying to troubleshoot the Data Stream. If this is the case, the Collections Stream Host may be able to give some information. Open the Collections page from the left-hand menu. Click on the Collection. Click on Stream Hosts. Select the Stream Host. View the logs from the Stream Host. {% hint style=\"info\" %} Any errors that are generated from an Agent which are printed from the error endpoint are also printed in the Stream Host logs. {% endhint %} ![](/docs/images/image (1005).png>) Troubleshooting when there are no Stream Hosts If the Stream Host is not running at all, you can view the logs from the install directory of the Stream Host on your computer. the install folder is named XMPro Stream Host and is usually found in the Program Files in the C Drive. ![](/docs/images/image (1333).png>)"
  },
  "docs/how-tos/data-streams/upgrade-a-stream-object-version.html": {
    "href": "docs/how-tos/data-streams/upgrade-a-stream-object-version.html",
    "title": "Upgrade a Stream Object Version | XMPro",
    "summary": "Upgrade a Stream Object Version If an Agent has been updated and you want to use the latest version, you can upgrade the Agent using the top menu to get the latest version. It is beneficial to upgrade the Agent especially if there are known errors for that Agent or if the Agent has been improved and provides more functionalities or settings. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Agents. Agent How to Manage Data Streams {% endhint %} Upgrading Agents To upload a new version of an Agent, you should follow the same steps as when uploading an Agent for the first time. For instructions on how to upload an Agent, see the Manage Agents article. The new version of the Agent will appear in the Versions section of your Agent’s detail page. The Agents in the toolbox will always be the latest version available; however, the versions of the Agents in your existing Streams will have to be upgraded as this is not done automatically. On the canvas, any Agent that is not the latest version will display its version underneath its icon. To upgrade the version of an Agent in a stream, open your and select the Agent. Select the Agent Click “Upgrade” Click \"Apply\" Click “Save“ The latest version is selected by default and this is the most common action. You can opt to select a different version. This is useful during agent development if you wish to roll back from the current version. ![Fig 2: upgrade the agent version](/docs/images/Upgrade Agent Version Steps.png>)"
  },
  "docs/how-tos/data-streams/use-business-case-and-notes.html": {
    "href": "docs/how-tos/data-streams/use-business-case-and-notes.html",
    "title": "Use Business Case and Notes | XMPro",
    "summary": "Use Business Case and Notes A Business Case is usually written before a Data Stream is created, and is used to communicate why the users are using that particular Data Stream. They are used to quantify the financial impact, explain why the Data Stream was created and the value that was gained from using it. Business cases allow you to provide descriptions and non-technical explanations that can be communicated to other users in the organization. Notes are another area where you can communicate and record technical information about the Data Stream. From v4.4.7 onwards, Notes are per Version rather than per Data Stream - allowing the use of notes to track the differences between versions. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Business Cases. How to Manage Data Streams {% endhint %} Adding Notes To add a Note to the Data Stream, follow the steps below: Click on Notes. Enter the Notes in the available text field. Customize the text using the text editor options. This includes adding headings, font formats, or images. Click on Save. ![](/docs/images/_1 (1).png>) Adding a Business Case To add a Business Case to the Data Stream, follow the steps below: Click on Business Case. Enter the details for the Business Case. Click on Save. Viewing or Editing a Business Case To view or edit a Business Case, follow the steps below: Click on Business Case. View the date the Business Case was last created or modified. Edit any details. Click on Save."
  },
  "docs/how-tos/data-streams/use-error-endpoints.html": {
    "href": "docs/how-tos/data-streams/use-error-endpoints.html",
    "title": "Use Error Endpoints | XMPro",
    "summary": "Use Error Endpoints When data flows from one Agent to another, a particular Agent may fail to process a certain data point during runtime. When this happens, the data point that failed and the reason why it failed will be passed through the error endpoint instead of going forward to the output endpoint. Each Agent displays its own errors, and therefore this can be a useful tool for debugging particular Agents. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Agents. Agent How to Manage Data Streams {% endhint %} How to Use Error Endpoints To use an error endpoint, follow the steps below: Click on the red endpoint and drag the arrow to where you want to error data to flow. In this example, the error endpoints are going to be printed using an Event Printer Agent. 2. Click on Save. 3. To view the error data, click on Publish. 4. Click on Live View. 5_._ Click on the Event Printer to display errors that occur during runtime. 6. Click on Save. The data printed for the Error Endpoint includes the AgentID of the Agent that had the error, as well as the timestamp of when the error occurred. The message of the error is also printed. The exact data from the data points are also printed. Instead of using the Event Printer to print errors, Error Endpoints can also connect to actions that trigger at the time of the error. For example, the following will send an email to a configured email address if an error occurs. Handling multiple errors from Error Endpoints A Union can be used to handle multiple errors at a time. To use the union Agent to do this, follow the steps below: Under the _Transformation A_gents, drag and drop a Union Agent onto the Data Stream. 2. Connect the error endpoints of multiple Agents to the union. 3. Add another Agent to deal with all the errors at the same time."
  },
  "docs/how-tos/data-streams/use-live-view.html": {
    "href": "docs/how-tos/data-streams/use-live-view.html",
    "title": "Use Live View | XMPro",
    "summary": "Use Live View When a Data Stream is published, the Live View option becomes visible to allow you to view the data that is being processed by the Agents in the Stream. The way the data is displayed can be changed from a grid view to a gauge or a chart. This is useful for viewing the flow of data in real time and is also, therefore, a good tool to use when debugging. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Live Data. Running Data Streams How to Manage Data Streams {% endhint %} Viewing Live Data To view this data, publish your Stream first, then select the “Live View” button. Follow the steps below: Select Live View after publishing your Stream. Select the Agents you would like to view data for. {% hint style=\"info\" %} Make sure your Data Stream is published before trying to open the Live View. Prior to v4.3.7, it was important to always close the Live View before navigating away. Otherwise, connections were left open. If these build up over time and affect performance, an admin can reset them here. {% endhint %} Change Live Data Display To change the type of control the live data is displayed in, follow the steps below: Select the settings icon from the Live View page. Choose the type of control you would like to use. Configure the values of the control. Click Save. ![](/docs/images/image (1678).png>)"
  },
  "docs/how-tos/data-streams/use-stream-metrics.html": {
    "href": "docs/how-tos/data-streams/use-stream-metrics.html",
    "title": "Use Stream Metrics | XMPro",
    "summary": "Use Stream Metrics When a Data Stream is published, the Stream Metrics become visible and allow you to view the number of Stream Hosts on which the Data Stream successfully started, the amount of data and errors that are being processed per minute in the Data Stream, and the number of active Stream Hosts per Collection. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Stream Metrics. Running Data Streams How to Manage Data Streams {% endhint %} Stream Metrics Components The Stream Metrics has four parts: Started On, Stream Load, Stream Errors, and Stream Hosts per Collection. {% hint style=\"warning\" %} Stream metric components (Stream Load and Stream Errors) do affect performance. See the Data Stream Designer Settings article for more information on how to toggle this feature on and off. {% endhint %} Started On Started On shows the number of Stream Hosts on which the Data Stream started vs the total number of Stream Hosts online across all Collections. The indicator is: green if the Data Stream started on all Stream Hosts amber if some started (indicates an error on the Stream Host) red if the Data Stream failed to start on all Stream Hosts (likely an error on the Data Stream) {% hint style=\"info\" %} This is useful to alert the composer when the Data Stream is first published that the Data Stream failed to start. {% endhint %} Stream Load Stream Load represents the amount of data that is being processed by the agents in the Data Stream. The left side of the Stream Load card shows the amount of data that has been processed in the last minute and the right side has a sparkline with a 5-minute history. Hovering over the sparkline will display the amount of data that was processed for the selected minute. ![](/docs/images/Stream Load.PNG>) Stream Errors Stream Errors represent the number of errors that are being generated by the agents in the Data Stream. The left side of the Stream Errors card shows the amount of errors that are generated in the last minute and the right side has a sparkline with a 5-minute history. Hovering over the sparkline will display the number of errors that were generated for the selected minute. ![](/docs/images/Stream Error.PNG>) Stream Hosts per Collection The Stream Hosts per Collection card shows the name and the number of active Stream Hosts in each Collection. If the Data Stream has objects that are in different Collections it will show a card for each Collection."
  },
  "docs/how-tos/data-streams/use-the-timeline.html": {
    "href": "docs/how-tos/data-streams/use-the-timeline.html",
    "title": "Use the Timeline | XMPro",
    "summary": "Use the Timeline The Timeline displays a record of all the changes users have made to the Data Stream, including any notes made about particular issues. This can therefore be used as a collaboration tool to see the changes users make (even if it's only a single user), as well as notes about things that need to be addressed. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Timelines. Timeline How to Manage Data Streams {% endhint %} Viewing the Timeline To open the Timeline, click \"Timeline\". Adding a note To add Notes to the Timeline, follow the steps below: Click \"Timeline\". Click the \"Note\" button. Type the notes you would like to add. Click \"Ok\". Filtering the Timeline To apply filtering on the Timeline, follow the steps below: Click on Filter. Select the type of items you would like to display. Click OK. Version filtering the Timeline To apply the version filtering on the Timeline, follow the steps below: Click on the Context dropdown_._ Select the Version that you would like to see the events."
  },
  "docs/how-tos/import-export-and-clone.html": {
    "href": "docs/how-tos/import-export-and-clone.html",
    "title": "Import, Export, and Clone | XMPro",
    "summary": "Import, Export, and Clone All XMPro Objects allow you to export them as a file and import them to other companies or instances of XMPro Products. This allows you to continue working on the same XMPro Object such as a Data Stream or Application in a different environment without losing all your work or having to start from scratch. Exporting All XMPro Objects (Agents, Applications, Data Streams, and Recommendations) can be exported as a file with a corresponding file extension. XMPro Object Extension Agents .xmp Connectors .xmp Applications .xapp App Pages .xapg Data Streams .xuc Recommendations .xr Forms .xfm This functionality is made available on an XMPro Object's corresponding page, by selecting the \"export\" option. When an XMPro Object is exported, only the latest version will be exported. Thus, if someone imports an XMPro Object, it will be assigned a new ID and contain only one version – the latest version that was exported. Before the exported file is created and downloaded, you will be asked for a password to encrypt the file. The person that imports the Data Stream will have to enter the correct password in order to import it. The App export has an advanced option that allows you to choose which files are included - by default files design time files are included and the uploads folder (runtime files) are excluded. When ticked, you can choose whether to include files that were added at runtime or exclude all files. To export any XMPro Object, follow the steps below: Navigate to the page of the XMPro Object you would like to export. Click on _Export (_For the Data Stream Designer, click on Properties first, then click on Export). Choose a strong password to encrypt the file. The person importing the file must enter the same password. Click on Ok. The exported file will appear in your downloads. Note To export a Widget, follow the steps in Manage Widgets. Importing All XMPro Objects (Agents, Applications, Data Streams, and Recommendations) can also be imported into other companies or instances of XMPro, to avoid needing to replicate a particular Data Stream or Application from scratch. Importing is done from a different location as exporting, which is on the page for the corresponding feature. To import any XMPro Object, follow the steps below: Navigate to the page of the XMPro Object you would like to import. Click on Import. Upload the corresponding import file for the XMPro Object. Enter the password. Click on Upload. 6. Configure any details. 7. When finished, select Save. Note To import an App Page, follow the steps in Import an App Page. Note To import a Widget, follow the steps in Manage Widgets. Cloning To clone any XMPro Object, follow the steps below: Navigate to the page of the XMPro Object you would like to clone. Click on Clone. 3. Give the cloned XMPro Object a new name. 4. Select the version of the app you would like to clone. 5. Click on Ok. The XMPro Object will be cloned and you will be able to see it in the list of Applications, Data Streams, or Recommendations, etc."
  },
  "docs/how-tos/index.html": {
    "href": "docs/how-tos/index.html",
    "title": "How-Tos | XMPro",
    "summary": "How-Tos This section provides step-by-step guides for common tasks in XMPro. Whether you're creating an application, setting up a data stream, or managing recommendations, you'll find detailed instructions here. Sections Agents Learn how to create, configure, and manage agents in XMPro. Applications Discover how to build and manage applications using XMPro's low-code canvas environment. Connectors Find out how to set up and use connectors to integrate with external systems. Data Streams Learn how to create and manage data streams for real-time data processing. Publish Understand the process of publishing your applications and data streams. Recommendations Explore how to create and manage recommendations in XMPro. General Management Import, Export, and Clone Manage Access Manage Categories Manage Landing Pages Manage Site Settings Manage Variables Manage Versions Stream Host"
  },
  "docs/how-tos/manage-access.html": {
    "href": "docs/how-tos/manage-access.html",
    "title": "Manage Access | XMPro",
    "summary": "Manage Access XMPro Objects such as Data Streams, Recommendations, Applications, and more can be shared with different users. The person that originally created the XMPro Object will be listed as the owner. When sharing the XMPro Object with other users, users can either be given Co-owner, read, or write access. Note It is recommended that you read the article listed below to improve your understanding of Managing Access. Manage Access Giving permissions to users To share XMPro Objects with users, follow the steps below: Go to the page of the XMPro Object you would like to manage the access for. Click on Manage Access. Open the Design Access Tab. Click Add. 5. Add the user to give them access. 6. Choose read, write, or co-owner permissions. 7. Click Ok. Editing User permissions To edit the permissions of an existing user, follow the steps below: Go to the page of the XMPro Object you would like to manage the access for. Click on Manage Access. Open the Design Access Tab. Click on a user. 5. Edit their permissions. 6. Click on Save. Deleting permissions from Users To delete the permissions of an existing user, follow the steps below: Go to the page of the XMPro Object you would like to manage the access for. Click on Manage Access. Open the Design Access Tab. Click on a user. 5. Click on Delete. 6. Confirm that you really want to delete all permissions for the selected user. Deleting permissions of multiple Users To delete the permissions of multiple existing users, follow the steps below: Go to the page of the XMPro Object you would like to manage the access for. Click on Manage Access. Open the Design Access Tab. Click on Select. Select from the list of users. 6. Click on Delete. 7. Confirm that you really want to delete all permissions for the selected users. Editing Permissions on the Run Access To edit Run Access permissions, follow the steps below: Go to the page of the XMPro Object you would like to manage the access for. Click on Manage Access. Open the Run Access Tab. Select from the list of users. Click on Save. Further Reading How to Subscribe to Notifications"
  },
  "docs/how-tos/manage-categories.html": {
    "href": "docs/how-tos/manage-categories.html",
    "title": "Manage Categories | XMPro",
    "summary": "Manage Categories A Category is a container that groups related Data Streams and Applications, which are shared between core XMPro Products to provide a homogenous environment. Note It is recommended that you read the article listed below to improve your understanding of Categories. Category Adding a new Category To create a new Category or use case group, follow the steps below: Open the Categories page from the left-hand menu. Click New. Specify a name and description for your new Category. Upload an icon by clicking on the plus-image and browsing to the correct file. Sample icons can be found in the Icon Library. Click Save. Reordering Categories The order of the categories that can be seen on the landing page can be changed using the Categories page. Please note that the order in which the categories appear on this page is the same as the order in which categories are displayed on the landing page. Follow the steps below to reorder the categories on the landing page: Open the Categories page from the left-hand menu. 2. Click Reorder. 3. Drag an item on the list to move it before or after another item. 4. When satisfied with the order of the categories, click on \"Save\". Note If you change your mind about reordering the categories, click Cancel. None of the changes you've made will be saved. Removing Categories Single Category To remove a single category, follow the steps below: Open the Categories page from the left-hand menu. Select the category from the list. Click Delete. Confirm that you would like to delete the category. Multiple Categories To remove multiple categories at the same time, follow the steps below: Open the Categories page from the left-hand menu. Click Select. 3. Select the categories that you would like to remove. 4. Click Delete. 5. Confirm that you would like to delete the categories selected."
  },
  "docs/how-tos/manage-landing-pages.html": {
    "href": "docs/how-tos/manage-landing-pages.html",
    "title": "Manage Landing Pages & Favorites | XMPro",
    "summary": "Manage Landing Pages & Favorites In App Designer, the Landing Page can be set for what Application Landing Page all users in the company will see when they first open App Designer. Apps and Data Streams can be favorited for fast access in future from Subscription Manager. Favorite commonly used Blocks for fast access when building Apps in App Designer. Note It is recommended that you read the article listed below to improve your understanding of Applications. Application Landing Pages & Favorites How to Manage Apps Set a Landing Page To set the Company's Landing Page for either Mobile or Desktop, follow the steps below: Click on Settings Select from the list of available Applications. Click on Save. Note You can only select Applications that are already published. Favorite an App To Favorite Apps, follow the steps below: Click on the star to favorite an App. Go to Subscription manager. Favorited Apps will show on the main page of Subscription Manager. Note Apps can only be favorited using the category tiles list. Favorite a Data Stream To Favorite Data Streams, follow the steps below: Click on the star to favorite a Data Stream. Go to Subscription manager. Favorited Data Streams will show on the main page of Subscription Manager. Note Data Streams can only be favorited using the category tiles list. Favorite a Block Added in v4.4.11 To Favorite a Block, including Widgets, follow the steps below. Click on the star to favorite a Block (hover over the block to reveal the star). Go to the Favorites category. Favorited blocks will appear under Favorites section as well as their original section. Favorite an Agent Added in v4.4.17 To Favorite an Agent, follow the steps below. Click on the star to favorite an Agent (hover over the block to reveal the star). A solid star is the visual indicator for favorited Agents."
  },
  "docs/how-tos/manage-site-settings.html": {
    "href": "docs/how-tos/manage-site-settings.html",
    "title": "Manage Site Settings | XMPro",
    "summary": "Manage Site Settings These settings are used to configure each XMPro Product. To open the settings page, click on the gears icon in the grey bar at the top of the screen. Note Please note that the settings that you will see on this page depend on the role and access rights that have been assigned to you. App Designer Settings Security Enable Audit Trail Enabling this setting would cause logs to be created whenever changes are made to Recommendations, Connectors, and components of Applications. The logs will contain details about who made the change and when it was applied. Encryption Key The encryption key is used to encrypt and decrypt sensitive data configured in the user settings when they are stored or retrieved from the database, for example, passwords. Integration Integration Key This key is used to verify Agents that integrate with the App Designer. The Integration Key will need to be copied into the Agent's configuration settings. User Interface Desktop Landing Page Optionally override the default landing page to use a published Application for the whole company when using a desktop computer. Mobile Landing Page Optionally override the default landing page to use a published Application for the whole company when using a mobile device. Enable Mobile App Optionally override the default landing page to use a published Application for the whole company when using a desktop computer. Mobile Landing Page Optionally override the default landing page to use a published Application for the whole company when using a mobile device. Enable Mobile App Added v4.4.4 This defaults to true on new installations. Toggle it off to hide the mobile app icon on the toolbar. This defaults to true on new installations. Toggle it off to hide the mobile app icon on the toolbar. Metablocks Added v4.4.0 Enable Metablocks Enabling this setting results in Metablocks appearing in the Blocks blade. Reports Added v4.3.7 Standard reports give the administrator a view into where (which Applications) and how (which version) Connectors have been used. The information is presented in a grid that can be sorted, filtered, reorganized, and grouped. It can also be exported as an XLSX file. Connector Usage Report This report shows all loaded Connectors, their versions, and how many times each version is used in an Application, if any. This master list shows the administrator which Connectors are installed and their utilization. This assists in identifying new Connectors or versions not yet added. Connector Usage Details Report This report shows the Applications in which a Connector version has been used. Additional information includes the Application's owner and category. This detailed report assists in gauging the impact of upgrading one or more Connector versions. Scripts Embed Script The supplied html script tag is inserted into every App Designer html page. Use this to load an external JavaScript script, such as to provide localized support or track user usage. For example, the below script embeds a chatbot into App Designer. // example script to embed a fastbot trained on appropriate documentation <script defer src=\"https://app.fastbots.ai/embed.js\" data-bot-id=\"abc\"></script> To improve performance and ensure users always have the latest version of your script: Use version parameters in your script URL to control caching: <script defer src=\"https://example.com/your-script.js?v=1.0.2\"></script> Note Update the version number whenever you modify the script. This ensures: Returning users get the new version immediately Users benefit from browser caching between sessions You control exactly when cache invalidation occurs Consider using content delivery networks (CDNs) for faster loading and improved caching. Use appropriate loading attributes like defer or async to optimize page rendering: defer: Script executes after HTML parsing is complete (recommended for most cases) async: Script executes as soon as it's available, potentially during HTML parsing For guidance on how to use this setting effectively, please contact your XMPro representative. Data Stream Designer Settings Security Encryption Key The encryption key is used to encrypt and decrypt sensitive data configured in the user settings of an Agent when they are stored or retrieved from the database, for example, passwords. Enable Audit Trail Enabling this setting would cause logs to be created whenever changes are made to Agents, Collections, and components of Data Streams. The records will contain details about who made the change and when it was applied. User Interface Enable InputMap Highlights Enables the Canvas arrow highlight which is shown if the arrow's configuration doesn't have mappings. This is useful for demos where the complete configuration has intentionally not been provided. Enable Stream Metrics Enables the logging and display of stream metrics (Stream Load and Stream Errors) in Data Streams. Refresh the page for the setting to be applied to the Data Stream Canvas. Behavior Default Polling Interval (seconds) Added v4.3.7 The default value that is used for the polling interval when a Polling Agent is added to a Streaming type Data Stream. If there is no value provided for this setting, the polling interval defaults to 3600 seconds (1 hour). Note The default is applied when the Agent is added to the canvas. A change to this site setting will only take effect for Agents added afterward. Live View Usage Added v4.3.6 Over time, if users did not close the Live View, these open connections placed an additional load on the Data Stream Designer (DS) as the Stream Hosts continued to send live data back to DS. This reduced overall performance and reliability, and increased infrastructure costs. We recommend that users always close the Live View of a published Data Stream before navigating away. When in doubt, an administrator can force a reset to close all open Live View connections. The Live View Usage includes the following: The number of Stream Objects (Agents) with Live View enabled and the number of Data Streams affected. A button to reset the Live View usage. Resetting the Live View will close any connections that may have been left open if a user closes the Data Stream canvas without first closing the Live View. It will also stop any open Live View blades from receiving data. To start receiving data again, re-open the Live View and reselect the Stream Objects. Refer to the Live View Usage Report for a list of Stream Objects and Data Streams that are preselected for Live View. Note A Stream Object with Live View enabled is an indicator that a user has viewed the data - it is not confirmation whether the user closed the connection. Tip The Live View issue is addressed in the v4.3.7 release: Open connections are closed regardless of how the Live View is closed (e.g. navigating away or closing the tab). All connections are closed when the Data Stream Designer app service is restarted. If you've upgraded to v4.3.7, use the Reset Live View button once to ensure all connections are closed. Reports Added v4.3.7 Standard reports give the administrator a view into where (which Data Stream) and how (which version) Agents have been used. The information is presented in a grid that can be sorted, filtered, reorganized, and grouped. It can also be exported as an XLSX file. Live View Usage Report This report shows all Agents that have Live View enabled. Additional information includes the Data Stream name, Data Stream version, Data Stream owner, the Collection name, the Stream Object name, and whether the Data Stream is published. This report along with the Reset option was useful prior to v4.3.7, to determine where connections may have been left open. Agent Usage Report This report shows all loaded Agents, their versions, their categories, and how many times a version is used in a Data Stream, if any. This master list shows the administrator which Agents are installed and their utilization. This assists in identifying new Agents or versions not yet added. Agent Usage Details Report This report shows Data Streams in which an Agent version has been used. Additional information includes the Data Stream version, its owner, the Collection, and the Stream Object name. This detailed report assists in gauging the impact of upgrading one or more Agent versions. Agent Polling Interval Report This report shows the polling intervals configured on all Agents that have the Polling Interval option. Additional information includes the Agent Name, Agent Category, Data Stream name, Data Stream version, Data Stream owner, the Collection name, the Stream Object name, and whether the Data Stream is published. This master list empowers the administrator to locate those set too short (1s) that may be causing performance issues. 10 seconds may be appropriate during initial testing, but ill-advised in a QA or Production environment. Subscription Manager Settings Security Hide Users Outside Business Role Branch When enabled, users can see the information of users in their business role and any of its parent business roles up to the root. They cannot see any child or sibling business roles and their users. For example, a user cannot tag/search users outside their business role tree path in a comment on a recommendation alert. This defaults to true on new installations. Toggle it off to make all user information visible to all users in the company. Warning The exception for this setting is a user with an Administrator role for the Subscription Manager product. They can assign access to XMPro objects to any user or business role in the company. Global Notification Added v4.4.0 Global Administrators can display a global notification across the top of all products in the XMPro suite for a specific period. This aids in communicating important information to users, such as a notice advising of planned maintenance downtime and a hyperlink to release notes. Choose a type of hint, warning, or error to set the notification icon and banner color. The banner can be dismissed for a session. Discard \"Global Notification\" Settings Clears all Global Notification settings and hides the current message, if any. Type This determines the icon and color of the notification banner. The options are a hint, warning, or error. Message A message of up to 500 characters is displayed in the notification banner. Basic text formatting, lists, and links are supported. Show Now Enabling this setting results in the notification banner being immediately shown across all products, and disables the Show On property. Show On (your local time) The date and time when the notification will be shown to users. It is stored as UTC, but displayed in the administrator's local time zone. This setting is not available when Show Now is enabled. Hide On (your local time) An optional date and time when the notification should no longer be shown to users. It is stored as UTC, but displayed in the administrator's local time zone. Leave blank if you want the message to be shown until it is manually cleared - either by clicking Discard \"Global Notifications\" Settings, clearing the Message or Show option. Support Email The email to which notifications will be sent if a user signs up to XMPro or makes a request, for instance, a request for a Subscription to a Product, or a request for a License. Disable Email Notifications Disables emails sent to the email address above for any reason. If email notifications are disabled then the Global Administrator will need to log in to Subscription Manager to check whether there are any pending requests."
  },
  "docs/how-tos/manage-variables.html": {
    "href": "docs/how-tos/manage-variables.html",
    "title": "Manage Variables | XMPro",
    "summary": "Manage Variables XMPro Variables are placeholders used to hold and maintain certain values. If you may not know some of the values that you might want to use within an XMPro Object, such as credentials or passwords, you can use Variables where the real value can be substituted later. Note It is recommended that you read the article listed below to improve your understanding of Variables. Variable Adding a new Variable Variables can be added via the Variables page before being used in any of the Data Streams or Applications. Click on the Variables page from the left-hand menu. Click on Add. Enter the details of the new Variable. Choose if the value is encrypted. This determines whether or not the value can be seen by the user. Click on Save and Close. Using Variables Variables can be used in Data Streams or Applications to authenticate users or to access certain data sources. Consider having the following agents in a stream: Azure SQL Listener When the Azure SQL Listener is configured the user needs to enter the server details and password in order to access the available tables and columns. In this case, variables that already store the passwords and credentials can be used in the input fields. For example: Add an SQL Listener from the list of Agents. Click on Configure. Select the Server and user details from the list of variables (ensure that the 'Use Connection Variables' option is selected). 4. Select the correct encrypted password variable for the server. 5. If selected correctly, tables and columns can now be accessed. Removing Variables Single Variable To remove a single variable, follow the steps below: Open the Variables page from the left-hand menu. Select the variable from the list. Click Delete. 4. Confirm that you would like to delete the variable. Multiple Variables To remove multiple variables, follow the steps below: Open the Variables page from the left-hand menu. Click Select. Select the variables from the list. Click Delete. 5. Confirm that you would like to delete the selected variables. Overriding Variables The Variables defined can be overridden by the individual Stream Host to provide the unique configuration e.g. per Asset, site, or OPC IP Address. See How to Override Variables for more information."
  },
  "docs/how-tos/manage-versions.html": {
    "href": "docs/how-tos/manage-versions.html",
    "title": "Manage Versions | XMPro",
    "summary": "Manage Versions Versions can be managed for different XMPro Objects, including Data Streams, Agents, Applications, and Recommendations. Copying a Version allows you to continue working and making changes to the XMPro Object while maintaining a Version of it before you made changes, which can also act as a backup mechanism. Note It is recommended that you read the article listed below to improve your understanding of Versions. Version Opening a specific Version Any of the major Versions of an XMPro Object can be viewed at any point in time. To open a specific Version, follow the steps below: Click on \"Versions\". Select the Version you would like to view. Click \"Open\". The images below show how to view the Versions of an Application in the App Designer. The Version you are currently editing is displayed as a subtitle below the name. If you viewed a Version that is not the latest version available and the page is closed, it will not be saved. Thus, if re-opened, the latest version will be displayed, even if an older version is running. You may run any of the Versions available, even if they are not the latest Version. If you are making changes, always make sure you are working on the correct version. Copying Versions To copy a Version, select the Version and click on \"Copy\". The new copy will be created, set as the current version, and will have a version number higher than all the other versions. Thus, a major version increase will be done, e.g. if the version you copied is Version 2.35, but the latest version is Version 3.80, the newly copied version will be Version 4.0. Click on \"Versions\". Select the Version you would like to view. Click on Copy to create a new copy of the selected Version and set the copy to the current Version. Deleting Versions To delete a Version, select the version from the list and click \"Delete\". To delete an XMPro Object completely, the XMPro Object must be deleted itself, rather than the Version. Click on \"Versions\". Select the Version you would like to view. Click on Delete."
  },
  "docs/how-tos/recommendations/README.html": {
    "href": "docs/how-tos/recommendations/README.html",
    "title": "Recommendations | XMPro",
    "summary": "Recommendations Recommendations are created and managed in the App Designer. They allow you to monitor and observe live data and respond to any events through Recommendation Alerts. Note It is recommended that you read the article listed below to improve your understanding of Recommendations. Recommendation Articles Manage Recommendations Create Rules Manage Notifications Manage Notification Templates Subscribe to Notifications Manage Forms Manage Variables Manage Alerts Manage Alerts on Mobile Manage Deleted Recommendation Items"
  },
  "docs/how-tos/recommendations/create-rules.html": {
    "href": "docs/how-tos/recommendations/create-rules.html",
    "title": "Manage Rules | XMPro",
    "summary": "Manage Rules A Rule is a condition that helps the Recommendation determine whether Recommendation Alerts should be created, and what created Alerts should look like. This is needed for you to create the condition(s) that the Recommendation needs to look for. An example of a Rule is \"If the temperature is greater than 50\". This catches any data that does not stay within the safe parameters. {% hint style=\"info\" %} It is recommended that you read the articles listed below to improve your understanding of Recommendations. Rule Manage Recommendations {% endhint %} Create Rules To create a Rule, follow the steps below: Select the Recommendation in the list that you want to add a Rule to. Click the + button at the top-right of the Rules list. Enter the Rule Name. Enter the Alert Headline and Alert Description. If you wish for the values received from the Data Stream to be added to the Headline or Description of the Alerts this Rule will generate, add a tag with the @ symbol and select the Data Stream output. Select the Rule Factor. (Optional) Select an Optional Factor. Select an Alert Ranking. (Optional) Select an Icon. Sample icons can be found in the Icon Library. (Optional) Choose an Impact Metric. Design the Rule Logic that decides when this Rule should generate a Recommendation Alert based on the data received from the Data Stream (Optional) Enable Form and Choose a Form and Form Version Add an Additional Recommendation Management Column. If you wish for the values entered into the Form to be added to the Additional Information column of the Alerts this Rule will generate, add a tag with the @ symbol and select a Field from the Form. Choose a Resolution value. If you want the Alerts generated to automatically resolve themselves if new data is received that doesn't match the Rule Logic, choose Automatic. Choose a Recurrence value. If you want a new Alert to be generated every time data is received that matches the Rule Logic, choose All Occurrences. Choose a Log Data On value. If you want the Event Data of the Alert to be replaced every time data is received that matches the Rule Logic, choose All Occurrences. (Optional) Enable and enter Triage Instructions to be followed in order to resolve the Alert. (Optional) Enable and add Resources to link in the Alert for help in resolving the Alert. ![](/docs/images/Recommendations - Manage Rules - Create Rule 1.png>) ![](/docs/images/Recommendations - Manage Rules - Create Rule 2.png>) ![](/docs/images/Recommendations - Manage Rules - Create Rule 3.png>) View Timeline To view the timeline for a rule, follow the following steps: Select Recommendation Click More Click Timeline Select Rule Delete Rules To delete existing Rule, follow the steps below: Select the Recommendation in the list that you want to delete a Rule from. Select the Rule. Click the Delete button. Confirm the action. ![](/docs/images/image (1310).png>) {% hint style=\"info\" %} Deleting a Rule will not permanently delete it or any Recommendation Alerts generated by it. Deletion can be undone or made permanent from Deleted Items. {% endhint %} Further Reading How to Manage Notifications"
  },
  "docs/how-tos/recommendations/manage-alerts-on-mobile.html": {
    "href": "docs/how-tos/recommendations/manage-alerts-on-mobile.html",
    "title": "Manage Alerts on Mobile | XMPro",
    "summary": "Manage Alerts on Mobile Recommendation Alerts are advanced Alerts that get triggered when real-time data meets the criteria defined in a Recommendation Rule. They notify you when certain conditions occur in your data and provide decision support for how to take action. Recommendations create new Recommendation Alerts based on Business Rules, and the Alerts recommend the best next actions based on expert suggestions. Recommendation Alerts monitor the actions taken and outcomes to close the loop on event response. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Recommendations Alerts. Recommendation Alert Manage Recommendations {% endhint %} How to Use the Alert Details Page The Recommendation Alert page provides details of the Alert and allows you to monitor, discuss, and take action. How to Add Notes Type in designated area for Notes. Tap the three dots to open the menu. ![](/docs/images/image (1055).png>) 3. Tap Save. ![](/docs/images/image (1640).png>) How to Fill Out a Form Tap the Form tab. Fill out the form fields. Tap the Create Work Request button in the form to save the changes. ![](/docs/images/image (601).png>) How to Contribute to a Discussion Tap the Discussion tab. Write your message. Tap the Send button. ![](/docs/images/image (1127).png>) How to Save, Resolve, and Mark as False Positive Tap the three dots to open the menu. Save - Changes will be saved and Alert Details Page will stay open. Mark as False Positive - Will mark the Alert as False Positive and will close the Alert Details Page. Mark as Resolve - Will resolve the Alert and close the Alert Details Page. ![](/docs/images/image (795).png>) ![](/docs/images/image (1558).png>)"
  },
  "docs/how-tos/recommendations/manage-alerts.html": {
    "href": "docs/how-tos/recommendations/manage-alerts.html",
    "title": "Manage Alerts | XMPro",
    "summary": "Manage Alerts Recommendation Alerts are advanced Alerts that get triggered when real-time data meets the criteria defined in a Recommendation Rule. They notify you when certain conditions occur in your data and provide decision support for how to take action. Recommendations create new Recommendation Alerts based on Business Rules, and the Alerts recommend the best next actions based on expert suggestions. Recommendation Alerts monitor the actions taken and outcomes to close the loop on event response. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Recommendations Alerts. Recommendation Alert Manage Recommendations {% endhint %} Finding Recommendation Alerts The search bar can be used to find any specific Recommendation Alerts that you may be looking for. There is a dropdown option where you can specify to search through everything in App Designer, or only for Recommendation Alerts. How to Use the Recommendation Alerts Grid How to Filter the Grid Open the Recommendation Alerts page. Click the filter icon next to the column that you want to filter on. Tick the desired checkbox. Click OK. The Grid with Alerts will be updated. ![](/docs/images/image (592).png>) How to Search the Grid Open the Recommendation Alerts page. Start typing in the search bar. The Grid with Alerts will be automatically updated. ![](/docs/images/image (1546).png>) How to Resolve Alerts From the Grid Open the Recommendation Alerts page. Tick the checkbox of the Alerts that you want to Resolve. Click the Mark As Resolved button. ![](/docs/images/image (1666).png>) How to Show Archived Alerts Open the Recommendation Alerts page. Tick the Show Archived checkbox. The Grid with Alerts will be updated with archived Alerts. ![](/docs/images/image (1240).png>) How to Use the Alert Details Page The Recommendation Alert page provides details of the alert and allows you to monitor, discuss, and take action. How to Add a Notes Type in the designated area for Notes. Click Save. ![](/docs/images/image (1815).png>) How to Fill Out a Form Click the Form tab. Fill on the form fields. Click the Create Work Request button in the form to save the changes. ![](/docs/images/image (518).png>) How to Contribute to a Discussion Click the Discussion tab. Write your message. Click the Send button. ![](/docs/images/image (504).png>) How to Share an Alert Click the Share button. ![](/docs/images/image (1714).png>) 2. Select Users from the dropdown. 3. Add a Note. 4. Click the Share button. ![](/docs/images/image (732).png>) How to Save, Resolve, and Mark as False Positive Clicking the Save button - Changes will be saved and Alert Details Page will stay open. Mark as False Positive - Will mark the Alert as False Positive and will close the Alert Details Page. Mark as Resolve - Will resolve the Alert and close the Alert Details Page. ![](/docs/images/image (91).png>)"
  },
  "docs/how-tos/recommendations/manage-categories.html": {
    "href": "docs/how-tos/recommendations/manage-categories.html",
    "title": "Manage Categories | XMPro",
    "summary": "Manage Categories Recommendations can be grouped into categories. This refers to the category under which the Recommendation is found in the Recommendations list Create a Category To create a new Category, follow the steps below after navigating to the Recommendation management page: Click Manage Categories. Click the plus sign. Specify the name of the new Category. Specify the Score Factor for the Category. Click Add. Please see images below. ![](/docs/images/image (953).png>) ![](/docs/images/Manage Categories - Create Category 1.png>) View Category Timeline The Category Timeline shows the changes that occur when a category is being edited. Follow the steps below to view the timeline: Select the Category Click Timeline Delete a Category To delete an existing Category, follow the steps below: Select the Category. Click Delete. Click Yes. ![](/docs/images/Manage Category - Delete Category.png>) ![](/docs/images/Manage Category - Delete Category 2.png>)"
  },
  "docs/how-tos/recommendations/manage-deleted-recommendation-items.html": {
    "href": "docs/how-tos/recommendations/manage-deleted-recommendation-items.html",
    "title": "Manage Deleted Recommendation Items | XMPro",
    "summary": "Manage Deleted Recommendation Items When a Recommendation is deleted, it is moved to the Deleted Items. This is because there may still be archived Recommendation Alerts created by that Recommendation that you don't want to delete. Once Recommendations are moved to Deleted Items, they can be restored. This ensures nothing is deleted by mistake and allows you to retrieve deleted Recommendations again in the future if you realize they are still needed. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of deleting Recommendations. Deleted Items Manage Recommendations {% endhint %} Restore Deleted Recommendation Items To restore deleted Recommendations, follow the steps below: Hover over More. Click the Deleted Items button. Tick the checkbox for the desired Recommendations, Recommendation Versions, or Rules. Click the Restore Button. ![](/docs/images/image (1248).png>) 5. Confirm the action. ![](/docs/images/image (1006).png>) Permanent Deletion of Recommendations {% hint style=\"danger\" %} Warning! Recommendation Alerts will also be permanently deleted when permanently deleting items. {% endhint %} To permanently delete a Recommendations, follow the steps below: 1. Hover over More. 2. Click the Deleted Items button. 3. Tick the checkbox for the desired Recommendations. 4. Click the Delete Button. ![](/docs/images/image (327).png>) 5. Confirm the action. ![](/docs/images/image (853).png>)"
  },
  "docs/how-tos/recommendations/manage-forms.html": {
    "href": "docs/how-tos/recommendations/manage-forms.html",
    "title": "Manage Forms | XMPro",
    "summary": "Manage Forms A Form is a collection of fields that appear on Recommendation Alerts. Forms can be created and customized to suit the situation, depending on the Recommendation Alert. Forms are useful if you want relevant information, data, comments, or notes to be entered and changed over the course of resolving an Alert and while it is being actioned. {% hint style=\"info\" %} It is recommended that you read the articles listed below to improve your understanding of Recommendations. Form Manage Recommendations {% endhint %} Creating a Form To create a Form, follow the steps below: Open the Recommendations page from the left-hand menu. Click on Forms. Click on New. Enter new Form details. Click on Save. Drag the appropriate blocks onto the canvas to create the Form's labels and input fields. Highlight the block. Click on Field Properties. Change details for the block such as the label. Continue adding blocks until the Form is complete. Click on Save. Highlighting any of the blocks also gives you the option to delete the block. Blocks can also be reordered. Using the Form with a Recommendation To get the Form to appear on Recommendation Alerts, add the Form to the applicable rule that will trigger the alert. Open the Recommendations page from the left-hand menu. Select the Recommendation. Select the Rule. Enable the Form. Select which Form should show with the Recommendation Alert. Select the version of the Form. Click on Save. Deleting the Form To delete the Form, follow the steps below: Open the Recommendations page from the left-hand menu. Click on Forms. Click on the Form you want to delete. Click on Settings. Click on Delete. Actions on the Form Additional actions on the Form include: Managing versions for the Form Manage who has access to the Form Import, Export, or Clone a Form"
  },
  "docs/how-tos/recommendations/manage-notification-templates.html": {
    "href": "docs/how-tos/recommendations/manage-notification-templates.html",
    "title": "Manage Notification Templates | XMPro",
    "summary": "Manage Notification Templates When a Recommendation Alert is triggered by a critical event, the user can receive a notification via text message or email. The notification contains a message that notifies the user of the Recommendation Alert. You can choose to create a custom message template for when a notification is triggered, or use a default template provided. {% hint style=\"info\" %} It is recommended that you read the articles listed below to improve your understanding of Recommendations: Notification Manage Notifications {% endhint %} Add a Recommendation Notification Template To change what message template is used when users are notified to a Recommendation Alert, follow the steps below: Click on Manage Recommendations. 2. Click on a Recommendation. 3. Select a Rule. 4. Scroll down and select a Notification. 5. Select a Notification channel (Email or SMS). 6. Click on Edit Templates. 7. Choose from the list of notification templates. 8. Choose between Default or Custom. 9. Press Save. Add Custom HTML Templates for Email By default, each notification template is set to 'default'. You can add a custom email template instead that includes different styling. The HTML file can also include placeholders for certain data that you would like to show on the notification. Here is an example of an HTML Template file: {% file src=\"../../.gitbook/assets/Recommendation Notification Template.zip\" %} To upload a custom email template, follow the steps below: 1. Click on Edit Templates on any selected notification. 2. Choose from the list of notification templates for Email. 3. Choose Custom. 4. Upload an HTML template file. 5. If you have any custom placeholders, select the values that will be in those fields. 6. Press Save. {% hint style=\"info\" %} Add capitalized placeholders for data within the HTML file between curly bracket symbols. For example, {{ALERTID}}. {% endhint %} {% hint style=\"info\" %} This list of predefined placeholders can be used in the template without mapping: ALERTID HREF TITLE DESCRIPTION NOTE PENDINGTIME RULENAME RECNAME (Recommendation Name) {% endhint %} Click the link to see a preview of the email. Custom Templates for SMS By default, each notification template is set to 'default'. To use a custom SMS template instead, follow the steps below: 1. Click on Edit Templates on any selected notification. 2. Choose from the list of notification templates for SMS. 3. Choose Custom. 4. Enter a custom notification message. 5. Press Save. {% hint style=\"info\" %} Use the '@' symbol to choose tags from your Data Stream. {% endhint %} Examples Default Template Example If ‘Default’ is selected, a default notification message will be sent to your email address or mobile. This is an example of an email notification using a Default Template: Custom Template Example This is an example of an email notification using a Custom Template:"
  },
  "docs/how-tos/recommendations/manage-notifications.html": {
    "href": "docs/how-tos/recommendations/manage-notifications.html",
    "title": "Manage Notifications | XMPro",
    "summary": "Manage Notifications A Notification defines how users will be notified when a Recommendation Alert is triggered by a critical event that meets the conditions set in a Rule. This is useful when you want to send a text, email, or another form of communication to users when something goes wrong and is caught by the Rule's condition. {% hint style=\"info\" %} It is recommended that you read the articles listed below to improve your understanding of Recommendations. Notification Manage Rules {% endhint %} Creating Notifications To create a Notification, follow the steps below: Open existing or Create a new Rule. Click the + button at the top-right of the Notifications list. Enter Notification Name. Select the desired Triggers. Select the Channels for the Notification. Save. ![](/docs/images/image (549).png>) Clone Notifications To clone an existing Notification, follow the steps below: Open existing Rule. Select the desired Notification. Click the Clone button. Enter Notification name. Confirm the action. ![](/docs/images/image (136).png>) Delete Notifications To delete an existing Notification, follow the steps below: Open existing Rule. Select the desired Notification. Click the Delete button. Confirm the action. ![](/docs/images/image (1741).png>)"
  },
  "docs/how-tos/recommendations/manage-recommendations.html": {
    "href": "docs/how-tos/recommendations/manage-recommendations.html",
    "title": "Manage Recommendations | XMPro",
    "summary": "Manage Recommendations If a Run Recommendation Agent is added to a Data Stream, the Recommendation will read the live data in real-time and compare it to the conditions and Rules configured. If any data falls into a Rule's condition then a Recommendation Alert is created. A Recommendation Alert notifies specific users of pending issues or dangers associated with the data and allows them to act upon them. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Recommendations. Recommendation {% endhint %} Manage Recommendations To access the Recommendation management page, follow the steps below: In the App Designer, click the Recommendations button in the left-hand menu. Click the Manage Recommendations button. {% hint style=\"info\" %} If you do not have the right to view Recommendation Alerts, the Recommendation Alerts grid will not be shown and the second step can be skipped. {% endhint %} ![](/docs/images/image (1491).png>) Create a Recommendation To create a new Recommendation, follow the steps below after navigating to the Recommendation management page: Click New. Specify a name for your new Recommendation. Select or create a new Category. ![](/docs/images/Recommendations - Manage Recommendations - Create Recommendation.png>) a. Click the Plus button. ![](/docs/images/Recommendations - Manage Recommendations - Create Recommendation 2.png>) b. Specify the Name. c. Specify the Score Factor for Category c. Click Save. ![](/docs/images/Recommendations - Manage Recommendations - Create Recommendation 3 (1).png>) 4. Specify the Score Factor for the Recommendation 5. Choose a Data Stream to receive data from 6. Click Save ![](/docs/images/Recommendations - Manage Recommendations - Create Recommendation 4.png>) {% hint style=\"info\" %} A Recommendation needs Rules. See the Manage Rules article to find out how to create a Rule. {% endhint %} View Recommendation Timeline The Timeline shows the changes that occur when details in the recommendation is being edited. Follow the steps below to view the timeline: Select Recommendation Click More Click Timeline Delete a Recommendation To delete an existing Recommendation, follow the steps below: Select the Recommendation. Hover over More. Click the Delete button. ![](/docs/images/image (1115).png>) 4. Confirm the action. ![](/docs/images/image (1279).png>) {% hint style=\"info\" %} Deleting a Recommendation will not permanently delete it or any Recommendation Alerts generated by it. Deletion can be undone or made permanent from Deleted Items. {% endhint %} Further Reading How to Manage Rules How to Subscribe to Notifications How to Manage Forms How to Manage Variables How to Manage Alerts How to Manage Alerts on Mobile How to Manage Deleted Recommendation Items"
  },
  "docs/how-tos/recommendations/manage-variables.html": {
    "href": "docs/how-tos/recommendations/manage-variables.html",
    "title": "Manage Variables | XMPro",
    "summary": "Manage Variables Variables are placeholders used to hold and maintain certain values. In some cases, it is possible to not know some of the values that you might want to compare within rules or conditions. In this case, you can use Variables where the real value can be substituted in later. Expressions can also be configured and are useful for doing certain calculations and returning results which can also be used within Recommendation Rules. {% hint style=\"info\" %} It is recommended that you read the articles listed below to improve your understanding of Recommendations. Recommendations Manage Recommendations {% endhint %} Create Variables To create a Variable, follow the steps below: Select the Recommendation in the list that you want to add a Variable to. Click the plus button at the top-right of the Variables list. Enter the Variable Name. Select the Type from the dropdown. The Expression Editor is where an Expression is built. At the top is a text area in which you can type the Expression. Below the text area are three sections - the categories, the Expression terms, and the description areas. Clicking on a category will show different items in the Expression terms area, and clicking on an Expression term will show a description of the term in the description area.‌ Double-clicking an Expression Term will enter that term in the text area at the position of the cursor. ![](/docs/images/image (497).png>) Delete Variables To delete a Variable, follow the steps below: Select the Recommendation in the list where you want to delete a Variable. Select the desired Variable. Click the Delete button. Confirm the action. ![](/docs/images/image (1346).png>)"
  },
  "docs/how-tos/recommendations/subscribe-to-notifications.html": {
    "href": "docs/how-tos/recommendations/subscribe-to-notifications.html",
    "title": "Subscribe to Notifications | XMPro",
    "summary": "Subscribe to Notifications Subscribing to Notifications allows you to receive email or SMS messages for Recommendation Alerts that are relevant to you or your organization. This is useful if you want to keep track of certain Recommendations. {% hint style=\"info\" %} It is recommended that you read the article listed below to improve your understanding of Notifications. Notification Editing Permissions on the Run Access Manage Recommendations {% endhint %} Subscribe to a Notification To subscribe to a Notification, follow the steps below: Hover over your user profile in the top-right of the page in App Designer, and click \"Notification Settings\". Tick the checkbox to subscribe to that Notification. Click the Save button. ![](/docs/images/image (223).png>)"
  },
  "docs/how-tos/stream-host.html": {
    "href": "docs/how-tos/stream-host.html",
    "title": "Stream Host | XMPro",
    "summary": "Stream Host A Stream Host is an application that can either be installed as a Docker container, a Windows Service, or a Console Application. Stream Hosts enable Data Streams to run and execute actions and are also responsible for getting the configurations of Non-Virtual Agents. Note It is recommended that you read the article listed below to improve your understanding of Stream Host. Stream Host How to Install a Stream Host The recommended Stream Host deployment is as a Docker container - see the Docker instructions. Alternate methods can be found at Install Stream Host. Logs How to Check Logs To check the logs for a Steam Host, follow the steps below: Open the Collection page. Select the Collection. Click the Stream Hosts button. Select the desired Stream Host. How to Set the Log Level You can change the Log level to either Info or Trace. To change the Log Level, follow the steps below: Open the Collection page. Select the Collection. Click the Stream Hosts button. Select the desired Stream Host. Note See the Collection and Stream Hosts article for more information on the Log Level. 5. Click on Set Log level. 6. From the dropdown, select either Info or Trace. 7. Click on OK. How to Filter Log Levels You can filter and narrow down the errors and messages that have already been logged in the table. For example, if you filter for Info errors, only info level errors will be displayed. To filter the log level for a Steam Host, follow the steps below: Open the Collection page. Select the Collection. Click the Stream Hosts button. Select the desired Stream Host. 5. Click the icon next to the Level column. 6. Select the Log Level. 7. Click OK. How to Clean the Logs To clean the logs for a Steam Host, follow the steps below: Open the Collection page. Select the Collection. Click the Stream Hosts button. Select the desired Stream Host. 5. Click the Delete Logs button. 6. Confirm your action. How to Export Logs To export the logs for a Steam Host, follow the steps below: Open the Collection page. Select the Collection. Click the Stream Hosts button. Select the desired Stream Host. Click the \"three dots\" button. Click Export all data or Export Selected rows. How to Find Online Hosts To find online Stream Hosts, follow the steps below: Open the Collection page. Select the Collection. Click the Stream Hosts button. How to Override Variables Although each Stream Host in a given Collection downloads the same definition of a Data Stream, the Variables defined in Data Stream Designer can be overridden by the individual Stream Host to provide the unique configuration e.g. per Asset, site, or OPC IP Address. The options to override variables for a Stream Host are: Using Environment Variables - enables the scalable and efficient creation of multiple Docker Stream Hosts through scripting. Using Variables.xv files - requires manual edits for each Stream Host. In practice you'd use one or the other, but it may be helpful that the Stream Host The Stream Host retrieves variable values in the following order: Environment Variable, if no match then... Variables.xv File, if no match then... Variables (i.e. as detailed in Using a Variable) Using Environment Variables v4.4.2 This option can be applied to any Stream Host install scenario, although it is better suited for when running Stream Host on Docker. Create an environment variable that is applied to a running Stream Host instance, prefixing the name of the variable that should be overridden with xmvariable__ xmvariable__NameOfVariable1=foo xmvariable__NameOfVariable2=bar xmvariable__NameOfVariable3=noop Using Variables.xv files Note This option is not supported when running Stream Host on Docker. Open the Collection page. Select the Collection. Hover on More. Click Variables. Click Select file and upload the encrypted variables.xv file, found in the Data folder of the Stream Host's installation directory e.g. \"C:\\Program Files\\XMPro Stream Host\\Data\". You can enter overrides for any Variables. Click Download to get the updated file. Replace the original file in your Stream Host folder with the new one. Restart your Stream Host to load the updated variables."
  },
  "docs/index.html": {
    "href": "docs/index.html",
    "title": "What is XMPro? | XMPro",
    "summary": "What is XMPro? XMPro's Application Development Platform empowers engineers and subject matter experts to build real-time applications without coding. The platform consists of 3 main software components: XMPro App Designer A visual page designer that enables you to create custom page designs by dragging blocks from the toolbox onto your page, configure their properties and connect to your data sources, all without having to code. XMPro Data Stream Designer A drag-and-drop interface to visually design Data Streams (a streaming data pipeline). Use XMPro Connectors in your Data Streams to bring in real-time data from a variety of sources, add contextual data, apply analytics, and initiate actions based on events in your data. XMPro Notebook Harnessing the power of the Jupyter Notebook, XMPro Notebook provides an intuitive and flexible interface for data analysis, scientific computing, machine learning, and more. Users can write and execute code independently, facilitating step-by-step exploration and experimentation with real-time data. XMPro Connectors XMPro's extensible integration library includes 100+ Connectors for industrial automation solutions, IoT platforms, historians, enterprise applications, AI/ML, and collaboration solutions. Watch The Demo Video Watch the demo video to see XMPro's platform in action How The Documentation Is Organized Getting Started - New here? Sign up for a free trial and get started with the End-To-End Use Case sample project. Resources - A goldmine of general articles, such as release news, a sizing guideline, an icon library, and FAQs to elevate your product experience. Concepts - Get detailed explanations of the platform's essential concepts, like Data Streams, Recommendations, Applications, and Connectors. How-To Guides - Follow step-by-step tutorials to help you create Apps and Data Streams. Blocks - Get detailed descriptions of the components you can use to design your App pages and how to configure them. Administration - Find out how to manage users, licenses, and subscriptions in XMPro. This documentation is only relevant to administrators. Installation - Learn how to Install XMPro in a variety of environments. This documentation is only relevant to administrators. Release Notes - Stay up to date on the latest features and bug fixes. Note Can't find what you're looking for? Search the docs for instant results or contact support. Try searching a phrase - such as a release version \"v4.3.2\" to find pages added for that release. Try asking a question - for best results use a full sentence rather than a phrase. DocFX powers Documentation.xmpro.com. The search functionality is driven by an index which includes all XMPro documentation, video tutorials, blogs, publications, and the website. Note The Beta tag indicates incremental functionality, added to prepare for a future feature."
  },
  "docs/installation/complete-installation/configure-auto-scale-optional.html": {
    "href": "docs/installation/complete-installation/configure-auto-scale-optional.html",
    "title": "Configure Auto Scale (Optional) | XMPro",
    "summary": "Configure Auto Scale (Optional) Watch the video tutorial Overview Data caching is a technique used to improve the performance and responsiveness of applications. It works by storing frequently accessed data in a fast and easily accessible location. When an application needs the data it can be retrieved quickly without having to go through time-consuming operations like accessing a database. Distributed caching takes the concept of caching further by caching data in an external service accessible by one or more servers. This approach offers several benefits, including increased capacity to handle larger amounts of data, synchronized data across servers, and better scalability. By distributing the cache, the overall performance of the application can be enhanced, ensuring faster response times and improved efficiency. In-memory caching: Single Server The default behavior for all XMPro installations is that this location is on the host server memory, i.e. a single server. Distributed caching: Multiple Servers Auto Scale, XMPro's implementation of distributed caching, offers a superior caching approach that is highly recommended, particularly for larger production-ready implementations. XMPro utilizes \"Redis\" (Remote Dictionary Server), which is a popular open-source data structure store for XMPro's distributed caching needs. Configure Auto Scale The upgrade path to utilize Auto Scale is as follows: Upgrade XMPro to V4.3.1+. Provision Redis. Refer to Redis documentation for a guide on how to provision Redis. Although Auto Scale has a bigger impact on App Designer performance, it is for all XMPro products that use SignalR, i.e. Data Stream Designer and Subscription Manager too. Application Designer Enable the AutoScale setting in Application Designer. Open the Application Designer appsettings.json file. In the autoscale object, set enabled to true and enter the connectionstring value. Save the file and restart the Application Designer service. { \"xmpro\": { \"autoScale\": { \"enabled\": true, \"connectionString\": \"<redis connection string>\" } } } Upgrade Data Streams Connector to V2.0+. Refer to Manage Connectors for a guide on adding a Connector and view the versions in use. Upgrade Apps to use Data Streams Connector V2.0+. Refer to Data Integration for a guide on how to upgrade an App's connection. Warning Upgrading the Connector will clear the cache from the existing connector, and all data for the consuming App will be lost. You do not have to upgrade all Apps to the new Connector version - their cached data and caching process will continue to work as before. Data Stream Designer Repeat step 1 for Data Stream Designer. Subscription Manager Repeat step 1 for Subscription Manager but in the web.config file. Note It might be encrypted, which will require you to decrypt it first. For instructions, please refer to the How to encrypt and decrypt a web.config file Knowledge Base article."
  },
  "docs/installation/complete-installation/configure-health-checks-optional.html": {
    "href": "docs/installation/complete-installation/configure-health-checks-optional.html",
    "title": "Configure Health Checks (Optional) | XMPro",
    "summary": "Configure Health Checks (Optional) Watch the video tutorial Overview Health checks are the industry standard for the initial diagnostic step of the troubleshooting process, which provides information about the health and readiness of XMPro services. The health checks achieve this by performing periodic evaluations of connected services - each of which returns a status of healthy or unhealthy. The health checks are disabled by default. Once the health checks are turned on, you can opt to view them in the Health UI or add the endpoints to your existing business application health monitoring IT solution. Note It is also possible to include third-party endpoints - enabling you to monitor all of your health checks in a central location. Health Endpoints Each XMPro service has an endpoint that can be accessed to determine its overall health. The following health checks are performed to test different dependencies: Connection to other XMPro services Connection to the databases Connection to Redis Cache The raw JSON of the results can be accessed through the health path of the service's URL. For example, https://xmpro-ad-sample.azurewebsites.net/health Below is the description for each entry in the health check: Property Description Data Additional data provided by the health check. By default, this is empty. Description An optional explanation of the health check. Duration The amount of time that the health check took to run. Status The health check status options are: Healthy (the connection succeeds) Unhealthy (the connection fails or times out) Tags Labels that are used to group or describe the kind of health check being done. Health UI The UI provides a user-friendly display of the responses from the health endpoints. To access the health checks performed, visit the health UI of any of the services using the health-ui path. For example, https://xmpro-ad-sample.azurewebsites.net/health-ui. The health check for XMPro AI was added in v4.4.5. Configure Health Checks The following steps describe how to turn on the health check functionality. Application Designer Navigate to the IIS location where Application Designer has been installed. Open the App Designer appsettings.json file. Set enableHealthCheck in the featureFlags object to true. Note This feature flag turns on both the /health and /health-ui endpoints. Configure the URLs you want to include in the health checks within the healthChecks object. The standard endpoint for each XMPro service is /health/ping. \"healthChecks\": { \"urls\": [ { \"url\": \"<subscription manager url>/health/ping\", \"name\": \"Subscription Manager API\", \"tags\": [ \"api\" ] }, { \"url\": \"<data stream designer url>/health/ping\", \"name\": \"Data Stream Designer API\", \"tags\": [ \"api\" ] }, { \"url\": \"<xmpro ai url>/health/ping\", \"name\": \"XMPro AI API\", \"tags\": [ \"api\" ] } ] } Configure the /health endpoints you want to include in the UI within the HealthChecksUI object. \"HealthChecksUI\": { \"HealthChecks\": [ { \"name\": \"Application Designer\", \"Uri\": \"<application designer url>/health\" }, { \"name\": \"Data Stream Designer\", \"Uri\": \"<data stream designer url>/health\" }, { \"name\": \"XMPro AI\", \"Uri\": \"<xmpro ai url>/health\" } ] } Save the file. Restart the Application Designer service. Data Stream Designer Repeat the above steps for Data Stream Designer, using these values for step 4: \"healthChecks\": { \"urls\": [ { \"url\": \"<subscription manager url>/health/ping\", \"name\": \"Subscription Manager API\", \"tags\": [ \"api\" ] }, { \"url\": \"<application designer url>/health/ping\", \"name\": \"Application Designer API\", \"tags\": [ \"api\" ] }, { \"url\": \"<xmpro ai url>/health/ping\", \"name\": \"XMPro AI API\", \"tags\": [ \"api\" ] } ] } XMPro AI Repeat the above steps for XMPro AI, using these values for step 4: \"healthChecks\": { \"urls\": [ { \"url\": \"<subscription manager url>/health/ping\", \"name\": \"Subscription Manager API\", \"tags\": [ \"api\" ] }, { \"url\": \"<application designer url>/health/ping\", \"name\": \"Application Designer API\", \"tags\": [ \"api\" ] }, { \"url\": \"<data stream designer url>/health/ping\", \"name\": \"Data Stream Designer API\", \"tags\": [ \"api\" ] } ] } Adding Third-Party Endpoints Third-party endpoints are supported and can be included as part of the health check. These are added in the same place as the XMPro service health checks. Note Only endpoints that do not need additional authorization are currently supported for the health checks. Open the appsettings.json file. Add third-party endpoints to the healthChecks object. Save the file and restart the XMPro product service. Below are the properties in this configuration when including other URLs in the health check: Property Description url Determines the URL to check. name Determines the name/identifier of the health check to be performed. tags Determines the tag for the health check. This is useful for tagging/grouping health checks. timeout The amount of time to wait for a response from the site. The health check will return as Unhealthy if the timeout is reached while waiting for the URL. By default, this is empty and will wait for the URL indefinitely."
  },
  "docs/installation/complete-installation/configure-logging-optional.html": {
    "href": "docs/installation/complete-installation/configure-logging-optional.html",
    "title": "Configure Logging (Optional) | XMPro",
    "summary": "Configure Logging (Optional) Watch the video tutorial Overview Application logging refers to recording and storing information about the events and activities that occur within a software application. It involves capturing relevant data, such as error messages, warnings, user interactions, and system events, and storing them in a log file or database. Application logging is essential for troubleshooting and debugging, as it helps identify and analyze issues that may arise during the application's execution. It also provides valuable insights into the application's performance, usage patterns, and security. Serilog is the diagnostic logging library used in the XMPro suite, and the following logging outputs are supported: Logging to file Application Insights Application Insights plus Telemetry (Added v4.3.6) Datadog Note Subscription Manager currently only supports logging to file. Enable Logging Note From v4.4.4., logging for all products is always enabled and the feature flag does not need to be enabled. Lightweight logging to file is active by default and we recommend that it is reviewed. Repeat these steps for any of the XMPro products (Application Designer, Data Stream Designer, and AI XMPro) - except Subscription Manager and Stream Hosts. Navigate to the IIS location where the XMPro product has been installed. Open the appsettings.json file. Set enableLogging in the featureFlags object to true. Save the file. Restart the XMPro product service. \"xmpro\": { \"appDesigner\": { \"featureFlags\": { \"enableLogging\": true } } } Logging to File This utilizes the Serilog file sink. See Serilog sinks file documentation for a complete set of configuration options. Below are examples for each XMPro Product. Subscription Manager Open the web.config file. Add the Serilog keys to the appSettings element. Save the file and restart the Subscription Manager service. <appSettings> <add key=\"serilog:using:File\" value=\"Serilog.Sinks.File\" /> <add key=\"serilog:using:Expressions\" value=\"Serilog.Expressions\" /> <add key=\"serilog:write-to:File.path\" value=\"%BASEDIR%/App_Data/logs/sm-log-.txt\"/> <add key=\"serilog:write-to:File.rollingInterval\" value=\"<rollingInterval> \" /> <add key=\"serilog:write-to:File.rollOnFileSizeLimit\" value=\"true\" /> <add key=\"serilog:write-to:File.outputTemplate\" value=\"{Timestamp} [{Level}] ({Name}) Company: {Company} UserId: {UserId} {NewLine}{Message}{NewLine}{Exception}\"/> </appSettings> Note It might be encrypted, which will require you to decrypt it first. For instructions, please refer to the How to encrypt and decrypt a web.config file Knowledge Base article. Stream Hosts Changed in v4.4.0 Using appsettings.json Close or stop the Stream Host. Navigate to the file location where the Stream Host has been installed. Open the appsettings.json file. Add the \"File Logging\" Serilog configuration. Save the file and restart or reopen the Stream Host. { \"FileLogging\": { \"MinimumLevel\": { \"Default\": \"Information\", \"Override\": { \"Microsoft\": \"Warning\" } }, \"WriteTo\": [ { \"Name\": \"File\", \"Args\": { \"path\": \"C:\\\\some\\\\path\\\\to\\\\logs\\\\log.txt\", \"rollingInterval\": \"Day\", \"outputTemplate\": \"{Timestamp:yyyy-MM-dd HH:mm:ss} [{Level:u3}] {SourceContext} Message: {Message:lj}{NewLine}{Properties}{NewLine}{Exception}{NewLine}\" } } ] } } Using Environment Variables Note This is the preferred approach for our Stream Host Docker Images (Linux). The environment variables will take precedence over the same configuration in the appsettings.json file. Add the environment variables to the platform / service running the Stream Host. Restart the Stream Host. # Example taken from a Docker Compose file. environment: # Other environment variables - xm__filelogging__writeto__1__name=File - xm__filelogging__minimumlevel__default=Information - xm__filelogging__writeto__1__args__path=/app/logs/sh-log-.txt - xm__filelogging__writeto__1__args__rollinginterval=Day - xm__filelogging__writeto__1__args__outputtemplate=[{Timestamp:yyyy-MM-dd HH:mm:ss} {Level:u3} ({TraceId}:{SpanId})]{NewLine} {Message:j}{NewLine} {Properties:j}{NewLine} {Exception}{NewLine} Other XMPro Products Repeat these steps for all the XMPro products except Subscription Manager and Stream Hosts: App Designer, Data Stream Designer, and AI XMPro. Open the appsettings.json file. Add the \"File\" Serilog configuration inside the WriteTo array. Save the file and restart the XMPro product service { \"Serilog\": { \"WriteTo\": [ { \"Name\": \"File\", \"Args\": { \"path\": \"./App_Data/Logs/log_.txt\", \"rollingInterval\": \"Day\", \"fileSizeLimitBytes\": 1048576, \"rollOnFileSizeLimit\": true, \"outputTemplate\": \"{Timestamp:yyyy-MM-dd HH:mm:ss} [{Level:u3}] {Message:j}{NewLine}{Properties:j}{NewLine}{Exception}{NewLine}\" } } ] } } Note The above example provides for the following: Log files will now be added to the \"Logs\" folder located within the \"App_Data\" directory in the site's root folder. Log files are stored in the \"App_Data\" folder because it is a protected directory in IIS, preventing access via direct web links. This ensures the security of the log data. It rolls over to a new file daily, or when the file size exceeds 1MB (this helps to prevent large log files and also gives an easy way to calculate the maximum storage consumption when combined with the retention setting It retains only the last 14 log files File names will have the format log_{Date}.txt with an extra sequence number appended if it rolls over due to the size limit, for example: log_20240321.txt log_20240321_001.txt log_20240321_002.txt log_20240322.txt Application Insights This utilizes the Serilog application insights sink. See Serilog sinks file for Application Insights for a complete set of configuration options. Below are the steps for an example of how it can be used in App Designer, Data Stream Designer, Stream Hosts, and AI XMPro: Open the appsettings.json file. Add the \"ApplicationInsights\" Serilog configuration inside the WriteTo array. Save the file and restart the XMPro product service or Stream Host. { \"Serilog\": { \"WriteTo\": [ { \"Name\": \"ApplicationInsights\", \"Args\": { \"connectionString\": \"<connection string>\", \"telemetryConverter\": \"Serilog.Sinks.ApplicationInsights.TelemetryConverters.TraceTelemetryConverter, Serilog.Sinks.ApplicationInsights\" } } ] } } Application Insights plus Telemetry This utilizes the Serilog application insights sink to write events to Microsoft Azure Application Insights and collect valuable Telemetry data. Below are the steps for an example of how it can be used in App Designer, Data Stream Designer, Stream Hosts, and AI XMPro: Set enableApplicationInsightsTelemetry in the featureFlags object to true - the same as you already did here for enableLogging. Open the appsettings.json file. Add the \"ApplicationInsights\" configuration. Save the file and restart the XMPro product service or Stream Host. { \"ApplicationInsights\": { \"ConnectionString\" : \"InstrumentationKey=<Instrumentation Key here>\"; } } Datadog This utilizes the Serilog datadog sink. See Serilog Sinks File for Datadog for a complete set of configuration options. Below are the steps for an example of how it can be used in App Designer, Data Stream Designer, Stream Hosts, and AI: Open the appsettings.json file. Determine the site parameter by checking here. Get the endpoint/URL and port number by checking here. Add the \"DatadogLogs\" Serilog configuration inside the WriteTo array. Save the file and restart the XMPro product service or Stream Host. { \"Serilog\": { \"WriteTo\": [ { \"Name\": \"DatadogLogs\", \"Args\": { \"apiKey\": \"<api key>\", \"source\": \"<source>\", \"host\": \"<host>\", \"configuration\": { \"url\": \"<url>\", \"port\": port } } } ] } }"
  },
  "docs/installation/complete-installation/configure-sso-optional/index.html": {
    "href": "docs/installation/complete-installation/configure-sso-optional/index.html",
    "title": "Configure SSO (Optional) | XMPro",
    "summary": "Configure SSO (Optional) Single Sign-On (SSO) allows users to access multiple applications with a single set of credentials. XMPro supports SSO integration with various identity providers, making it easier for users to access the platform while maintaining security. Available SSO Options XMPro supports the following SSO options: SSO with Azure AD - Configure Single Sign-On using Microsoft Azure Active Directory SSO with ADFS - Configure Single Sign-On using Active Directory Federation Services Choose the appropriate SSO option based on your organization's identity management infrastructure. Implementing SSO can simplify user access management and enhance security by centralizing authentication."
  },
  "docs/installation/complete-installation/configure-sso-optional/sso-adfs.html": {
    "href": "docs/installation/complete-installation/configure-sso-optional/sso-adfs.html",
    "title": "SSO - ADFS | XMPro",
    "summary": "SSO - ADFS In this article, we will look at how to set up AD FS so that it can be used as an external identity provider for Subscription Manager, allowing single sign-on capability between AD FS and Subscription Manager. Follow the steps below: IIS Navigate to the location in IIS where Subscription Manager was installed. Note You can right-click on the application name in IIS and choose \"Explore\". Note Image not available: open-sub-mgr-app.png Open the web.config file. Note Image not available: open-web-config.png Scroll down to the \"xmpro\" section. Note It might be encrypted, which will require you to decrypt it first. For instructions, please refer to the How to encrypt and decrypt a web.config file Knowledge Base article. Under the \"identityProviders\" element, add a new element called \"adfs\". Specify the metadata address of your AD FS, as per the image below: Note Image not available: SSO_ADFS_web_config_metadata_address.png Note Set the correct URL for the metadataAddress value. An example of how the URL might look is \"_https://adfs.domain.com/federationmetadata/2007-06/federationmetadata.xml_\". Verify your URL by browsing to it in a browser. Copy the \"baseUrl\" value in the web.config - you will need it later in this guide. Note Image not available: SSO_AzureAD_web_config_baseUrl.png Warning You will use this value to create a relying party trust between the Subscription Manager application and AD FS Server Manager Log on to your AD FS server and go to Tools –> AD FS Management Note Image not available: open-ad-fs-management.png Relying Party Trust Click Add Relying Party Trust Note Image not available: click-on-add-relying-trust.png Select Claims aware and click Start Note Image not available: select-claims-aware.png Select Enter data about the relying party manually and click Next Note Image not available: add-data-manually.png Choose a display name and click Next and Next again Note Image not available: choose-a-display-name.png Select Enable support for the WS-Federation Passive protocol, add the URL and click Next Note This is the base URL you copied from the web.config file. Note Image not available: wsf-protocol.png Add the identifier for the application. Use the URL for Subscription Manager Add the URL and click Next Note Image not available: unique-identifier.png Choose an access control policy and click Next. Continue to the last screen Note For this article, we are going to choose Permit everyone Note Image not available: access-control-policy.png Claims Issuance Policy Select Configure claims issuance policy for this application and finish Note Image not available: last-screen.png In the AD FS Management window, click Edit Claim Issuance Policy… and click Add Rule Note Image not available: edit-claim-insurance-policy.png In the Claim rule template drop-down, select Send LDAP Attributes as Claims and click Next Note Image not available: LDAP.png Choose a name for the rule and map the claims Note Images not available: Edit-Rule.png, Choose-a-name-for-the-rule.png Login to Subscription Manager using AD FS Now you should be ready. If you navigate to the Subscription Manager application, you will see the AD FS login option. Log in with your AD FS credentials. Note You will be asked to link your account when you sign in for the first time. If so, fill in your information and click Link Account Note Images not available: add-adfs-credentials.png, adfs-button.png"
  },
  "docs/installation/complete-installation/configure-sso-optional/sso-azure-ad.html": {
    "href": "docs/installation/complete-installation/configure-sso-optional/sso-azure-ad.html",
    "title": "SSO - Azure AD | XMPro",
    "summary": "SSO - Azure AD In this article, we will look at how to set up Azure AD so that it can be used as an external identity provider for Subscription Manager, allowing single sign-on capability between Azure AD and Subscription Manager. Register application Start by registering a new application in Azure AD by following these instructions. Copy application (client) ID Immediately after registering your application, an overview page will be opened for the new application. A unique application (client) ID would have been assigned to the application. Warning Copy this ID. You will add it in Subscription Manager's web.config file shortly. Note Image not available: copy-client-id.png Credentials Next, create a secret for Subscription Manager. Follow the steps below: On the left, click on Certificates & secrets. Click on New client secret. Add a description for your new client secret. Choose a duration. Click Add. Note Images not available: add-secret-1-1.png, add-secret-2-2.png Note Both the application client ID and the secret need to be added to Subscription Manager's web.config file. Navigate to the IIS location where Subscription Manager has been installed. Open the file web.config file. Scroll down to the \"xmpro\" section. Note This section might have to be decrypted, for which you can find instructions here. Add the application (client) ID that you copied earlier to the clientId attribute of the azureAD element Copy the secret and add it to the web.config. Note Image not available: SSO_AzureAD_web_config_clientId_and_key.png Note If you're using the Azure key store to manage app settings and secrets, use the ${} syntax for the azureAD attributes in the web.config, similar to: <azureAD clientId=\"${ADClientID}\" key=\"${ADSecret}\" /> And define the following secrets in the key store: Name Value ADClientID Application Id ADSecret Application Secret Authentication Copy the baseUrl value in the web.config - you will need it later in this guide. Note Image not available: SSO_AzureAD_web_config_baseUrl.png In Azure Portal, click on Authentication and add the following URL in the space provided: The URL where Subscription Manager is hosted (base URL, which you have just copied), ending in \"identity/signin-azuread\" Example: https://mysampleserver/xmprosubscriptionmanager/identity/signin-azuread Note Image not available: authentication-4.png On the Authentication page, scroll down until you see \"Advanced Settings\". Select \"ID tokens\" and click Save. Note Image not available: authentication-advanced-settings.png API permissions Select API permissions on the left-hand menu. Make sure the permissions set on the application correspond to the image below. Note Image not available: permissions-1.png Sync Azure AD Role to SM's Business Role This optional functionality allows a user's Business Role to be synced to a corresponding Azure AD Claim each time they log in. Get the desired user claim name from Azure AD. Navigate to the IIS location where Subscription Manager has been installed. Open the web.config file. Add the claim name to the \"businessRoleClaim\" attribute in the \"identityProviders\" tag. <identityProviders businessRoleClaim=\"PUT THE CLAIM NAME HERE\"> Save the file and restart the Subscription Manager service. See the Sync Business Roles from Azure AD article for more information. Guest User access across Tenants When your Azure AD is in a different Tenant to Subscription Manager and the User has Guest membership in Azure AD, then add the TenantID for Azure AD. Note Image not available: SSO_AzureAD_web_config_guest_tenant.png"
  },
  "docs/installation/complete-installation/index.html": {
    "href": "docs/installation/complete-installation/index.html",
    "title": "Complete Installation | XMPro",
    "summary": "Complete Installation After deploying the XMPro Platform, several additional steps are required to complete the installation and optimize your environment. This section covers the post-deployment configuration and setup tasks. Fig 1: The context of the complete installation steps within the overall process. Configuration Options The following configuration options are available to enhance your XMPro installation: Configure Auto Scale (Optional) - Set up auto-scaling for your deployment Configure Health Checks (Optional) - Implement health monitoring for your XMPro services Configure Logging (Optional) - Set up comprehensive logging for troubleshooting and monitoring Configure SSO (Optional) - Implement Single Sign-On with ADFS or Azure AD Set Up a Tenant Company - Create and configure your first tenant company Install Stream Host - Set up the Stream Host for data stream processing Installing Connectors and Agents A critical part of the installation process is setting up the connectors and agents that enable XMPro to integrate with various data sources and systems. Fig 2: The context of the Agents & Connectors step within the overall process. Install Connectors - Install and configure the necessary connectors for your environment Follow the guides in this section to complete your XMPro installation and prepare your environment for use."
  },
  "docs/installation/complete-installation/install-connectors.html": {
    "href": "docs/installation/complete-installation/install-connectors.html",
    "title": "Install Agents & Connectors | XMPro",
    "summary": "Install Agents & Connectors After you have installed App Designer and Data Stream Designer and set up a new Company, you will want to add Connectors and Agents to the Company. This article will show you step-by-step how to upload the default set of Connectors and Agents. Data Stream Designer - Agents Log into XMPro as a Company Administrator and navigate to the Data Stream Designer Click the Agents button in the menu on the left to open the Agents page Click the Add button Download the files from each of the following links: Tier 5 - Open Source Tier 6 - XMPro Internal Click Select file and upload the Tier 5 - Agents.zip file found in the link above Click Save Click Discard and repeat the above steps for the Tier 6 file App Designer - Connectors Navigate to the App Designer Click the Connectors button in the menu on the left to open the Connectors page Click the Add button Download the file from the following link: Connectors Click Select file and upload the zip file found in the link above Click Save"
  },
  "docs/installation/complete-installation/install-stream-host/azure-web-job.html": {
    "href": "docs/installation/complete-installation/install-stream-host/azure-web-job.html",
    "title": "Run Stream Host as an Azure Web Job | XMPro",
    "summary": "Run Stream Host as an Azure Web Job This guide will walk you through the process of deploying the XMPro Stream Host as an Azure Web Job. Prerequisites An Azure subscription An existing Azure App Service or the ability to create one A valid Connection Profile from Data Stream Designer Overview Azure Web Jobs provide a way to run background processes in the context of an App Service. This is an ideal way to host the XMPro Stream Host in Azure, as it provides: Integration with Azure's infrastructure Automatic scaling capabilities Monitoring and logging through Application Insights Simplified deployment and management Deployment Steps 1. Prepare the Stream Host Package Download the Stream Host installer for Windows (x64) from Data Stream Designer. Extract the contents of the installer to a local folder. Download the Connection Profile from Data Stream Designer. Create a file named settings.job in the same folder with the following content: { \"schedule\": \"0 */5 * * * *\", \"is_singleton\": true } This configures the Web Job to run continuously and restart every 5 minutes if it stops. Create a file named run.cmd in the same folder with the following content: @echo off XMPro.StreamHost.exe Create a file named appsettings.json in the same folder with the following content: { \"ConnectionProfile\": { \"Path\": \"D:\\\\home\\\\site\\\\wwwroot\\\\App_Data\\\\jobs\\\\continuous\\\\StreamHost\\\\connection-profile.json\", \"FileKey\": \"your-file-key\" }, \"Logging\": { \"LogLevel\": { \"Default\": \"Information\", \"Microsoft\": \"Warning\", \"System\": \"Warning\" } } } Replace your-file-key with the key you used when downloading the Connection Profile. Copy your Connection Profile file to the folder and rename it to connection-profile.json. Create a ZIP file containing all the files in the folder. 2. Create an Azure App Service If you don't already have an App Service, you'll need to create one: Log in to the Azure Portal. Click on Create a resource > Web > Web App. Configure the Web App: Subscription: Select your Azure subscription Resource Group: Create a new one or select an existing one Name: Enter a unique name for your App Service Publish: Code Runtime stack: .NET Core 3.1 Operating System: Windows Region: Select a region close to your XMPro server App Service Plan: Create a new one or select an existing one Click Review + create, then Create. Wait for the deployment to complete. 3. Deploy the Stream Host as a Web Job In the Azure Portal, navigate to your App Service. In the left menu, under Settings, click on WebJobs. Click Add. Configure the Web Job: Name: StreamHost File Upload: Select the ZIP file you created earlier Type: Continuous Click OK to create the Web Job. The Web Job will start automatically. You can monitor its status in the WebJobs list. Configuring Application Settings For better security, you can store sensitive information like the File Key in the App Service's Application Settings: In the Azure Portal, navigate to your App Service. In the left menu, under Settings, click on Configuration. Under the Application settings tab, click New application setting. Add a setting: Name: CONNECTION_PROFILE_FILE_KEY Value: Your file key Click OK, then Save. Update your appsettings.json file to use this environment variable: { \"ConnectionProfile\": { \"Path\": \"D:\\\\home\\\\site\\\\wwwroot\\\\App_Data\\\\jobs\\\\continuous\\\\StreamHost\\\\connection-profile.json\", \"FileKey\": \"%CONNECTION_PROFILE_FILE_KEY%\" }, \"Logging\": { \"LogLevel\": { \"Default\": \"Information\", \"Microsoft\": \"Warning\", \"System\": \"Warning\" } } } Re-create the ZIP file and redeploy the Web Job. Monitoring and Logging You can monitor the Stream Host Web Job using the Azure Portal: In the Azure Portal, navigate to your App Service. In the left menu, under Settings, click on WebJobs. Click on the StreamHost Web Job. Click on Logs to view the Web Job logs. For more detailed logging, you can set up Application Insights: In the Azure Portal, navigate to your App Service. In the left menu, under Settings, click on Application Insights. Click Turn on Application Insights. Configure Application Insights and click Apply. Once configured, you can view detailed telemetry in the Application Insights resource. Verifying the Installation To verify that the Stream Host is running correctly: Check the Web Job status in the Azure Portal. It should show as \"Running\". Check the Web Job logs for any error messages. Log into Data Stream Designer, navigate to the Collections page, and select the Collection you used in the Connection Profile. The Stream Host should appear in the Devices list with a status of \"Online\". Troubleshooting If your Stream Host is not appearing in the Collection: Check the Web Job logs for any error messages. Verify that the Connection Profile was correctly configured. Ensure that the App Service has outbound internet access and can reach the XMPro server. Check if the Web Job is running. If it's stopped, start it manually. For more detailed troubleshooting, refer to the Troubleshooting section in the main Stream Host documentation. Scaling One of the advantages of running the Stream Host as an Azure Web Job is the ability to scale: In the Azure Portal, navigate to your App Service. In the left menu, under Settings, click on Scale up (App Service plan) to change the VM size. Alternatively, click on Scale out (App Service plan) to configure auto-scaling based on metrics like CPU usage. Uninstalling To remove the Stream Host Web Job: In the Azure Portal, navigate to your App Service. In the left menu, under Settings, click on WebJobs. Select the StreamHost Web Job. Click Delete and confirm the deletion."
  },
  "docs/installation/complete-installation/install-stream-host/docker.html": {
    "href": "docs/installation/complete-installation/install-stream-host/docker.html",
    "title": "Run Stream Host in Docker | XMPro",
    "summary": "Run Stream Host in Docker This guide will walk you through the process of running the XMPro Stream Host in a Docker container. Prerequisites Docker installed on your host machine A valid Connection Profile from Data Stream Designer Basic knowledge of Docker commands Quick Start The quickest way to get started is to use the following Docker command, replacing the placeholders with your actual values: docker run -d --name xmpro-stream-host \\ -e SERVER_URL=\"https://your-xmpro-server.com\" \\ -e COLLECTION_ID=\"your-collection-id\" \\ -e DEVICE_NAME=\"your-device-name\" \\ -e SECRET=\"your-secret\" \\ -e KEY=\"your-key\" \\ xmpro/stream-host:latest Using a Connection Profile For a more secure approach, you can use the Connection Profile file you downloaded from Data Stream Designer: Download the Connection Profile from Data Stream Designer. Create a directory on your host machine to store the Connection Profile: mkdir -p /path/to/connection-profile Copy the Connection Profile file to this directory. Run the Docker container, mounting the directory containing the Connection Profile: docker run -d --name xmpro-stream-host \\ -v /path/to/connection-profile:/app/connection-profile \\ -e CONNECTION_PROFILE_PATH=\"/app/connection-profile/your-profile-file.json\" \\ -e FILE_KEY=\"your-file-key\" \\ xmpro/stream-host:latest Environment Variables You can configure the Stream Host using the following environment variables: Variable Description Required if not using Connection Profile SERVER_URL The URL of your XMPro server Yes COLLECTION_ID The ID of the Collection Yes DEVICE_NAME The name of the device (Stream Host) Yes SECRET The secret from the Collection Yes KEY The key from the Collection Yes CONNECTION_PROFILE_PATH Path to the Connection Profile file inside the container No FILE_KEY The key used when downloading the Connection Profile Required if using Connection Profile LOG_LEVEL The logging level (Verbose, Debug, Information, Warning, Error, Fatal) No Persistent Storage If you need to persist logs or other data, you can mount volumes to the container: docker run -d --name xmpro-stream-host \\ -v /path/to/logs:/app/logs \\ -e SERVER_URL=\"https://your-xmpro-server.com\" \\ -e COLLECTION_ID=\"your-collection-id\" \\ -e DEVICE_NAME=\"your-device-name\" \\ -e SECRET=\"your-secret\" \\ -e KEY=\"your-key\" \\ xmpro/stream-host:latest Docker Compose Example For a more manageable setup, you can use Docker Compose. Create a docker-compose.yml file with the following content: version: '3' services: stream-host: image: xmpro/stream-host:latest container_name: xmpro-stream-host restart: unless-stopped environment: - SERVER_URL=https://your-xmpro-server.com - COLLECTION_ID=your-collection-id - DEVICE_NAME=your-device-name - SECRET=your-secret - KEY=your-key # Or use Connection Profile: # - CONNECTION_PROFILE_PATH=/app/connection-profile/your-profile-file.json # - FILE_KEY=your-file-key volumes: # Uncomment if using Connection Profile: # - /path/to/connection-profile:/app/connection-profile - /path/to/logs:/app/logs Then start the container with: docker-compose up -d Verifying the Installation To verify that the Stream Host is running correctly: Check the container status: docker ps You should see the xmpro-stream-host container running. Check the container logs: docker logs xmpro-stream-host Log into Data Stream Designer, navigate to the Collections page, and select the Collection you used. The Stream Host should appear in the Devices list with a status of \"Online\". Troubleshooting If your Stream Host is not appearing in the Collection: Check the container logs for any error messages: docker logs xmpro-stream-host Verify that the environment variables or Connection Profile are correctly configured. Ensure that the container has internet access and can reach the XMPro server. For more detailed troubleshooting, refer to the Troubleshooting section in the main Stream Host documentation. Updating the Stream Host To update to the latest version of the Stream Host: Pull the latest image: docker pull xmpro/stream-host:latest Stop and remove the existing container: docker stop xmpro-stream-host docker rm xmpro-stream-host Start a new container using the same configuration as before. Security Considerations Store sensitive information like secrets and keys securely, using Docker secrets or environment variables from a secure source. Consider using a non-root user inside the container for added security. Regularly update the Stream Host image to get the latest security patches."
  },
  "docs/installation/complete-installation/install-stream-host/index.html": {
    "href": "docs/installation/complete-installation/install-stream-host/index.html",
    "title": "Stream Host Installation | XMPro",
    "summary": "Stream Host Installation The XMPro Stream Host is a component that runs on a device or server and connects to the XMPro platform to execute Data Streams. This section provides installation guides for different environments. Overview The Stream Host can be installed in various environments: Windows (x64) - Install on a Windows 64-bit machine Ubuntu (16.04+ x64) - Install on Ubuntu Linux Docker - Run in a Docker container Azure Web Job - Deploy as an Azure Web Job Choose the installation method that best fits your infrastructure and requirements. Prerequisites Before installing the Stream Host, you need: Access to Data Stream Designer in your XMPro environment A Collection created in Data Stream Designer Appropriate permissions to install software on your target environment Download the Installer To download the Stream Host installer: Log into Data Stream Designer. Navigate to the Collections page. Select the Collection where you want to add the Stream Host. Click on the Devices tab. Click Add Device. Enter a name for your Stream Host. Click Download Installer. Select the appropriate installer for your environment: Windows (x64): .exe installer Ubuntu (16.04+ x64): .tar.gz archive Docker: Use the Docker image from Docker Hub Azure Web Job: Use the Windows installer and follow the Azure Web Job guide Download the Connection Profile The Connection Profile contains the configuration needed for the Stream Host to connect to your XMPro environment: After downloading the installer, click Download Connection Profile. Enter a File Key (password) to secure the Connection Profile. Remember this File Key as you'll need it during installation. Click Download to save the Connection Profile file. Installation Process The general installation process follows these steps: Download the appropriate installer and Connection Profile. Install the Stream Host on your target environment. Configure the Stream Host with your Connection Profile. Start the Stream Host service. Verify the installation by checking the status in Data Stream Designer. For detailed instructions, refer to the specific installation guide for your environment. Troubleshooting If you encounter issues during installation or operation of the Stream Host: Common Issues Stream Host not appearing in Collection: Verify that the Stream Host service is running. Check that the Connection Profile was correctly configured. Ensure the machine has internet access and can reach the XMPro server. Connection errors: Check firewall settings to ensure the Stream Host can communicate with the XMPro server. Verify that the Collection ID, Secret, and Key in the Connection Profile are correct. Service not starting: Check the Stream Host logs for error messages. Verify that the required dependencies are installed. Ensure the user running the service has appropriate permissions. Logs The Stream Host logs can provide valuable information for troubleshooting: Windows: Logs are stored in the installation directory under the Logs folder. Ubuntu: Check the systemd journal with journalctl -u xmpro-stream-host. Docker: View container logs with docker logs xmpro-stream-host. Azure Web Job: Check the Web Job logs in the Azure Portal. Support If you continue to experience issues, contact XMPro Support with the following information: Stream Host version Installation environment details Relevant log files Description of the issue Steps to reproduce the issue"
  },
  "docs/installation/complete-installation/install-stream-host/ubuntu-16.04+-x64.html": {
    "href": "docs/installation/complete-installation/install-stream-host/ubuntu-16.04+-x64.html",
    "title": "Install Stream Host on Ubuntu (16.04+ x64) | XMPro",
    "summary": "Install Stream Host on Ubuntu (16.04+ x64) This guide will walk you through the process of installing the XMPro Stream Host on Ubuntu 16.04 or later (64-bit). Prerequisites Ubuntu 16.04 or later (64-bit) .NET Core 3.1 or later installed Sudo privileges on the machine A valid Connection Profile from Data Stream Designer Installation Steps 1. Install .NET Core Runtime If you don't have .NET Core installed, you'll need to install it first: # Add the Microsoft package repository wget https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/packages-microsoft-prod.deb -O packages-microsoft-prod.deb sudo dpkg -i packages-microsoft-prod.deb rm packages-microsoft-prod.deb # Install .NET Core Runtime sudo apt-get update sudo apt-get install -y apt-transport-https sudo apt-get update sudo apt-get install -y dotnet-runtime-3.1 2. Download the Stream Host Download the Stream Host installer for Linux (x64) from Data Stream Designer. Create a directory for the Stream Host: sudo mkdir -p /opt/xmpro/stream-host Extract the downloaded archive to the directory: sudo tar -xzf XMPro.StreamHost.Linux-x64.tar.gz -C /opt/xmpro/stream-host Set the appropriate permissions: sudo chmod +x /opt/xmpro/stream-host/XMPro.StreamHost 3. Configure the Connection Profile Download the Connection Profile from Data Stream Designer. Copy the Connection Profile to the Stream Host directory: sudo cp /path/to/your/connection-profile.json /opt/xmpro/stream-host/ Create a configuration file: sudo nano /opt/xmpro/stream-host/appsettings.json Add the following content, replacing the placeholders with your actual values: { \"ConnectionProfile\": { \"Path\": \"/opt/xmpro/stream-host/connection-profile.json\", \"FileKey\": \"your-file-key\" }, \"Logging\": { \"LogLevel\": { \"Default\": \"Information\", \"Microsoft\": \"Warning\", \"System\": \"Warning\" } } } Save and close the file (Ctrl+X, then Y, then Enter). 4. Create a Systemd Service Create a systemd service file: sudo nano /etc/systemd/system/xmpro-stream-host.service Add the following content: [Unit] Description=XMPro Stream Host After=network.target [Service] WorkingDirectory=/opt/xmpro/stream-host ExecStart=/opt/xmpro/stream-host/XMPro.StreamHost Restart=always RestartSec=10 SyslogIdentifier=xmpro-stream-host User=root Environment=ASPNETCORE_ENVIRONMENT=Production [Install] WantedBy=multi-user.target Save and close the file (Ctrl+X, then Y, then Enter). Reload the systemd daemon: sudo systemctl daemon-reload Enable the service to start on boot: sudo systemctl enable xmpro-stream-host Start the service: sudo systemctl start xmpro-stream-host Running as a Non-Root User (Recommended) For better security, it's recommended to run the Stream Host as a non-root user: Create a dedicated user for the Stream Host: sudo useradd -r -s /bin/false xmpro Change the ownership of the Stream Host directory: sudo chown -R xmpro:xmpro /opt/xmpro/stream-host Update the systemd service file: sudo nano /etc/systemd/system/xmpro-stream-host.service Change the User line from root to xmpro: User=xmpro Save and close the file, then reload the daemon and restart the service: sudo systemctl daemon-reload sudo systemctl restart xmpro-stream-host Verifying the Installation To verify that the Stream Host is running correctly: Check the service status: sudo systemctl status xmpro-stream-host You should see \"active (running)\" in the output. Check the logs: sudo journalctl -u xmpro-stream-host -f Log into Data Stream Designer, navigate to the Collections page, and select the Collection you used in the Connection Profile. The Stream Host should appear in the Devices list with a status of \"Online\". Troubleshooting If your Stream Host is not appearing in the Collection: Check the service status: sudo systemctl status xmpro-stream-host View the logs: sudo journalctl -u xmpro-stream-host -f Verify that the Connection Profile was correctly configured. Ensure that the machine has internet access and can reach the XMPro server. Check if any firewall rules are blocking the connection: sudo ufw status For more detailed troubleshooting, refer to the Troubleshooting section in the main Stream Host documentation. Uninstalling To uninstall the Stream Host: Stop and disable the service: sudo systemctl stop xmpro-stream-host sudo systemctl disable xmpro-stream-host Remove the service file: sudo rm /etc/systemd/system/xmpro-stream-host.service sudo systemctl daemon-reload Remove the Stream Host directory: sudo rm -rf /opt/xmpro/stream-host (Optional) Remove the user if you created one: sudo userdel xmpro"
  },
  "docs/installation/complete-installation/install-stream-host/windows-x64.html": {
    "href": "docs/installation/complete-installation/install-stream-host/windows-x64.html",
    "title": "Install Stream Host on Windows (x64) | XMPro",
    "summary": "Install Stream Host on Windows (x64) This guide will walk you through the process of installing the XMPro Stream Host on a Windows 64-bit system. Prerequisites Windows 10 or later (64-bit) .NET Framework 4.7.2 or later Administrator privileges on the machine A valid Connection Profile from Data Stream Designer Installation Steps Download the Stream Host installer for Windows (x64) from Data Stream Designer. Locate the downloaded installer file (typically named XMPro.StreamHost.Installer.exe) and double-click to run it. If prompted by User Account Control (UAC), click Yes to allow the installer to make changes to your device. The XMPro Stream Host Setup Wizard will appear. Click Next to continue. Read the License Agreement, select I accept the terms in the license agreement, and click Next. Choose the installation folder or accept the default location, then click Next. On the Connection Profile screen, click Browse and select the Connection Profile file you downloaded earlier. Enter the File Key that you used when downloading the Connection Profile, then click Next. Review your installation settings and click Install to begin the installation process. Once the installation is complete, click Finish to exit the setup wizard. Running as a Windows Service The Stream Host is installed as a Windows Service and will start automatically after installation. You can manage the service through the Windows Services console: Press Win + R, type services.msc, and press Enter. Locate the service named XMPro Stream Host. You can start, stop, or restart the service as needed. Verifying the Installation To verify that the Stream Host is running correctly: Log into Data Stream Designer. Navigate to the Collections page. Select the Collection you used in the Connection Profile. The Stream Host should appear in the Devices list with a status of \"Online\". Troubleshooting If your Stream Host is not appearing in the Collection: Check that the Windows Service is running. Verify that the Connection Profile was correctly configured. Ensure that the machine has internet access and can reach the XMPro server. Check the Stream Host logs for any error messages: Logs are stored in the installation directory under the Logs folder. The default location is C:\\Program Files\\XMPro\\Stream Host\\Logs. For more detailed troubleshooting, refer to the Troubleshooting section in the main Stream Host documentation. Uninstalling To uninstall the Stream Host: Open the Windows Control Panel. Go to Programs and Features (or Apps & features in Windows 10/11). Find XMPro Stream Host in the list of installed programs. Select it and click Uninstall. Follow the prompts to complete the uninstallation process."
  },
  "docs/installation/complete-installation/set-up-a-tenant-company.html": {
    "href": "docs/installation/complete-installation/set-up-a-tenant-company.html",
    "title": "Set Up a Tenant Company | XMPro",
    "summary": "Set Up a Tenant Company Create a company subscription Browse to the XMPro Subscription Manager website and click on Sign up for an account. Complete the form, checking the Create new company checkbox, and press Agree Browse to the XMPro Subscription Manager website Log in using the XMPro credentials (admin@xmpro.onxmpro.com) Note The username and password displayed on the final step of the Subscription Manager deployment. Open the Companies page from the left-hand menu Click the Company you have just requested to be added (in this example, \"Company\") Click the Subscription Requests button in the command bar, click the request and click Save Note The requested Company has been approved and subscribed to XMPro Subscription Manager. The user you applied with above can now log in. Press Ok Request a license Next we add subscriptions for Data Stream Designer and App Designer to the new Company: Click the Subscriptions gauge in the Company to open the Company's Subscriptions Click the Add button in the command bar to add a new Subscription Select the Data Stream Designer product Note The exact name will depend on your installation, in this example, it is \"DataStreams\". Click Request a new License Note This sends a request to XMPro for a Data Stream Designer license for this Company. Change the Product to App Designer and request another license Note Licenses are given on an individual basis by the XMPro support team. When you have received a license for each product through an email sent to the email address used to create this account, return to this page. Upload the license Select the Data Stream Designer product, upload its corresponding license and click Save Click Add on the Subscriptions page and repeat the steps above for the App Designer product Add the user Now we will add the user to each subscription: Click the Data Stream Designer product and click the Add button Select the user, role, any permissions the user should have in the product and click Save Repeat the above steps for the App Designer."
  },
  "docs/installation/deployment/aws.html": {
    "href": "docs/installation/deployment/aws.html",
    "title": "AWS | XMPro",
    "summary": "AWS Architecture The following deployment diagram shows an example architecture and the necessary resources for the XMPro platform in AWS. Fig 1: Example XMPro architecture in AWS The solution is deployed as an auto-scaling Elastic Beanstalk Application with 3 environments: SM – Subscription Manager AD – Application Designer DS – Data Stream Designer & API These environments use Redis for a centralized Cache and RDS for database storage. All data transfers are done via HTTPS and the SSL certificates are managed in AWS Certificate Manager. There are two accounts set up: one for production and one for non-production. Both of these environments follow the above architecture and deployment. Prerequisites In order to proceed with the deployment, you are required to complete the steps in the 1. Preparation guide: Meet the hardware requirements Install the software requirements Follow the certificate and communication steps Note Two SSL Certificates are required An SSL Certificate in AWS Certificate Manager, used by IIS (See the Appendix guide). An SSL Certificate, used by the SM instance (added to the S3 Bucket during the installation). Create or ask your administrator for an SSL certificate with the correct DNS name. A self-signed certificate is good enough. There are many ways to generate this certificate, one of which is described in the above 1. Preparation guide. Please note the file names must be called ssl.pfx and ssl.password.txt. Resources We are going to be deploying the following resources, please ensure you have the desired domain names ready. SQL RDS Parameter Store Elastic Beanstalk Application Elastic Beanstalk Environment – Subscription Manager Elastic Beanstalk Environment – App Designer Elastic Beanstalk Environment – Data Stream Designer & API An example of preferred domain names is as follows; each set is for a specific account as per the architecture diagram. For production: https://sm-xmpro․domain․com https://ad-xmpro․domain․com https://ds-xmpro․domain․com For non-production: https://sm-nonprod-xmpro․domain․com https://ad-nonprod-xmpro․domain․com https://ds-nonprod-xmpro․domain․com Log on to the AWS Management Console and switch to the region you want to deploy the solution in, you will need Administrative rights to the subscription to complete the deployment. ElastiCache Search for ElastiCache in the Services dropdown and select it. Click the Get Started Now button from the screen that opens. Make sure Redis is selected, click create. Provide a name for the cache, select the size and leave the rest of the Redis options as defaults. Provide the Subnet information and select the VPC to deploy Redis in. Click Create to complete the Redis configuration and create the cache. Once created, select EC2 from Services, and under Network & Security click Security Groups. Edit the default security group and add Redis Port 6379 to the Inbound rules. Make a note of the Redis endpoint as it will be used later within the Redis Connection string. Note Currently, SignalR doesn't support Redis Clusters https://docs.microsoft.com/en-us/aspnet/signalr/overview/performance/scaleout-with-redis Sticky Sessions must be used for SignalR https://learn.microsoft.com/en-us/aspnet/core/signalr/scale?view=aspnetcore-6.0 Amazon RDS Creation In the AWS Management Console choose RDS under Database in the Services drop-down. Click Databases and then click Create database. Select Easy Create, SQL Server, and the desired Tier for the database instance. Provide the DB instance Identifier, Username, and Password for the RDS database instance. Click create. Once created it will appear as below: Click the DB Identifier just created. Note Make a note of the following: Endpoint - In this example: aero-sql.cug4m2yk6h94.ap-south-1.rds.amazonaws.com User - as specified earlier Password - as specified earlier The security group will need to be modified to allow inbound traffic this is done as follows: 6.1. Click the VPC security groups. 6.2. Select the Default security group, click Inbound then click Edit. 6.3. Add a new rule called MS SQL, with Protocol as TCP and Port Range as 1433; and click Save. Parameter Store Identity and Access Click IAM under Security, Identity & Compliance In IAM click policies click Create policy Select Import managed policy Search and select AmazonSSMManagedInstanceCore then click Import Click Add additional permission Choose service Systems Manager Select Read and click Review Policy Expand resources and resolve all the warnings by clicking All Resources. Enter a Name and Description for the policy and click Create Policy Search for the Newly created policy, select it, and click Policy Actions Select Attach from Policy actions Attach this new policy to the role aws-elasticbeanstalk-service-role and click Attach Policy Create Elastic Beanstalk Application The first step in using AWS Elastic Beanstalk is to create an application, which represents your web application in AWS. In Elastic Beanstalk an application serves as a container for the environments that run your web app and for versions of your web app's source code, saved configurations, logs, and other artifacts that you create while using Elastic Beanstalk. Open the Elastic Beanstalk console, and then, in the regions drop-down list, select your region. In the navigation pane, choose Applications, and then click Create Application. Use the on-screen form to provide an application name. Click Create. Note You have successfully created the application. Next, we'll create the application's environments for each product: Subscription Manager, Data Stream Designer, and App Designer. Subscription Manager Create Environment Select the Application, click on Actions then click Create environment Click Select Provide the Environment name for Subscription Manager. Select the Platform information. Select Sample Application and click Configure more options Click Edit under the Capacity section. Select Load Balanced under Environment Type and set the required Instance Min and Max to 1. (More information can be found here) Change the Instance type to the required instance type. Click Save. Click Edit under the Network section. Under the VPC section select the VPC this environment should run in, set the visibility according to your requirements and select the load balancer availability zones. Scroll down and click Save. Click Edit under the Load balancer section. Select Application Load Balancer and scroll down. Click Add listener. Enter 443 in Port Select Protocol HTTPS. Select the SSL certificate you added in the Certificate Manager earlier on and click Add. Scroll down. Select the default Process and under Actions click Edit. Change the Port to 443 and the Protocol to HTTPS, then scroll down. Tick the Stickiness policy enabled option and click Save. Click Save. Click Create environment to have the defined environment created. Create S3 Bucket In the AWS Management Console, choose S3 under Storage in the Services drop-down. In S3 click Create Bucket to create a new bucket. Enter a name for the bucket name and click Create bucket. Select the Region for your bucket Remove the checkmark for Block Public Access Acknowledge the warning for a public bucket Click Create Bucket Copy the sign.pfx and sign.password.txt files (the signing certificate referenced in the 1. Preparation guide) into the bucket and ensure the files are publicly accessible. Copy the ssl.pfx and ssl.password.txt files (the SSL certificate referenced in the 1. Preparation guide) into the bucket and ensure the files are publicly accessible. Note The signing certificate is between the end user and the load balancer. The instance SSL certificate is used between the instances and the load balancer. Install Subscription Manager Run the installation wizard for Subscription Manager Run Subscription manager as Administrator Follow the instruction in the installation wizard: click Next. Select the Install option (1) and click Next (2). Tick Database (1), Web Application (2), select AWS Package (3), and click Next (4). Enter the secret store prefix (1), the S3 Bucket name from earlier (2), and click Next (3). Select the installation path (1), the DNS name for the site (2), and click Next (3). Enter the SMTP details referenced in the 1. Preparation guide and click Test SMTP settings (1), If successful, click Next (2). Enter the Signing Certificate details: 9.1. Browse to the certificate created earlier 9.2. Enter the certificate password 9.3. Select the subject name 9.4. Select Local Machine 9.5. Click Next Click Next once the installation has completed. Make a note of the Username and password, and click Finish. AWS Systems Manager – Parameter Store Navigate to Parameter Store in AWS Systems Manager. Click Create parameter. Create a SecureString parameter. Browse to the folder where SM was installed Edit the file called App Secrets.xml: create the parameters as per the line items in the file: Locate the S3 folder in the deployment folder. Copy the contents to the S3 Bucket you created. Deploy the Subscription Manager Click Environments in Elastic Beanstalk service Click the SM Environment you created earlier Use the on-screen form to upload the zip file. Select the zip file to deploy from the folder where SM was installed. Click Deploy. Navigate to the URL and log in using the following credentials: admin@xmpro․onxmpro․com Pass@word1 Reset the administrator password and store it securely in a password vault. Click SM Click Products Click Installation Profile Data Stream Designer Create Environment In the AWS Management Console choose Elastic Beanstalk under Compute in the Services drop-down. In the navigation pane, choose Environments On the application overview page, choose Create a new environment. Follow the same instructions on environment creation as done for the Subscription Manager. Run the Data Stream Designer installer as Administrator. Click Next. Select Install (1) and click Next (2). Select the items as shown below and click Next. Provide a Prefix and the S3 Bucket name Provide the Database Details: Provide the SQL endpoint Change the SQL user Select a new DB and provide a name for the DB Provide the DNS name for the Environment Browse to the downloaded installation profile and select it Login using the credentials for SM Click Next Once the installation completes, click Next Click Finish Install & Deploy Data Stream Designer Browse to the installation folder, as outlined in Subscription Manager. Edit the App Secrets.xml file and create the Parameters in System Manager. Upload and deploy the package.zip file to the newly created environment using upload and deploy as per SM deployment. App Designer Create Environment In the AWS Management Console, choose Elastic Beanstalk under Compute in the Services drop-down. In the navigation pane, choose Environments On the application overview page, choose Create a new environment. Follow the same instructions on environment creation as done for the Subscription Manager. After installing Application Designer, run the setup as Administrator and click Next. Select Install and click Next. Select the items as below and click Next. Provide the SQL endpoint and click Next. Provide the DNS name for the environment and click Next. Provide the URL for the Data Stream Designer installed earlier, and click Next. Enter the SMTP details referenced in the 1. Preparation guide and click Next. Enter the Twilio details referenced in the 1. Preparation guide and click Next. If you don't want SMS notifications you can select \"None\" from the \"Select Provider\" dropdown. Browse to the downloaded installation profile and select it. Click Next. Login with SM credentials to authenticate. Click Next. Click Next after the installation is complete. Click Finish. Install & Deploy App Designer Browse to the installation folder, as outlined in Subscription Manager Edit the App Secrets.xml file and create the Parameters in System Manager. Upload and deploy the package.zip file to the newly created environment using upload and deploy as per SM deployment. Appendix SSL certificate in Certificate Manager In the AWS console go to the Certificate Manager Select the region the SSL Certificate is required in The certificate can be either imported or a new certificate can be requested. To request a new certificate Click Get started under Provision Certificate Click Request a certificate Enter the certificate domain name and click Next Select the DNS validation method and click Next Review your settings and click Confirm and request if correct Once the DNS configuration file becomes available, click Continue Contact your IT administrator to complete the DNS verification by adding the CNAME record to your website DNS Once the DNS verification is complete the SSL certificate is added to your certificate manager for the specified region To import a certificate Click Get started under Provision Certificate Click Import a certificate Complete the certificate detail and click Next to import the certificate Create the EB Application URLs Search for ElastiCache in the Services dropdown and select it. In the left-hand panel, click Hosted Zones. Click Create Hosted Zone. In the right-hand panel complete the Domain Name using the domain name you created the SSL certificate for and click Create. Click Create Record Set. Change Alias to Yes, then go to EC2 in AWS services and scroll down to Load Balancing and click Load Balancers. Select a Load Balancer and click Tags to identify what Application is serviced by the selected Load Balancer. When the correct Load Balancer for the Application is identified, click the Description Tab. Copy the DNS Name for the Load Balancer. Go back to the Record Set you created in Route 53. Paste the Load Balancer DNS address in the Alias Target field and click Create. This needs to be completed for each ELB Application. The NS values must be provided to you by the DNS Administrator to create the NS records in the Domain DNS records. This needs to be completed for each ELB Application. Configure the security groups In the AWS Management Console, choose EC2 under Compute in the Services drop-down. Click Security Groups under the NETWORK & SECURITY option. Click Create security group. Create the RDS_security_group and select the VPC. Add the following rules and replace the source with the security groups assigned to the environments you created earlier. Create an additional security group called REDIS_Cache_security_group. Add these rules again using the security groups for the environments created earlier as the source. In Elastic Beanstalk, select the environment you want to change. Click Configuration in the left pane Remove the default security group and click Apply. Do this for all the environments. In Services, selects RDS and click Databases. Select your RDS database and click Modify. Scroll down to Network and Security. Select the RDS security group you created earlier and remove the default security group. Scroll down and click Continue. Select Apply Immediately and click Modify DB Instance. Select ElastCache from Services and click Redis. Select the Redis Cache you created earlier and from Actions click Modify. Edit the Security Groups Remove the default security groups and add the Redis Cache security group created earlier. Click save and modify. Next Step: Complete Installation The installation of the XMPro Platform is now complete, but there are some environment setup steps before you can use the platform. Please click the below link for further instructions: Complete Installation"
  },
  "docs/installation/deployment/azure.html": {
    "href": "docs/installation/deployment/azure.html",
    "title": "Azure | XMPro",
    "summary": "Azure This document will guide you through how to set up the Azure infrastructure and deploy the XMPro Platform. Install Upgrade Uninstall Architecture The following deployment diagram shows an example architecture and the necessary resources for the XMPro platform in Microsoft Azure. Set up of Cloud Stream Host, Master Data, Azure Cache, or Twilio is optional and is dependent on client requirements. As a result, these resources will be excluded from this deployment. Prerequisites In order to proceed with the deployment, you are required to complete the steps in the 1. Preparation guide: Select Azure resources that meet the hardware requirements and software requirements. Follow the certificate and communication steps for an SMTP Account (Recommended). As well as having: Azure portal administrative access Access to Subscription and Resource group Install This section provides information about installing the XMPro platform from scratch in Azure. Log on to the Azure Portal https://portal.azure.com/ (with Company Administrator access) In the search bar type \"Deploy\" and select \"Deploy a custom template\" Click \"Build your own template in the editor\" Download the latest mainTemplate_[Version].json file from your XMPro account manager or support team. Select \"Load File\" and then Open the downloaded \"mainTemplate_[Version].json\" Once loaded, click Save - do not change the template. Complete the form. Warning Enter the username in the correct format i.e. firstname.lastname@companyname.onxmpro.com The following special characters are not supported in the passwords: ` ' \\ $ @ Take note of the passwords used as this will not be displayed again and they are required later. Verify the information is correct and click Create. After successful deployment, the following items have been installed: Subscription Manager Data Stream Designer App Designer Stream Host Proceed to Restart the App Services after they are all deployed. Restarting App Services This step explains how to restart your app services in the Azure Portal. Type \"Resource groups\" in the search bar and select \"Resource groups\". Search for the Resource Group created during installation and select it. Select Subscription Manager, Data Stream Designer, and App Designer and restart the applications. The below GIF shows how to restart your app service. Logins User Type Password admin@xmpro.onxmpro.com Super Admin as entered during setup firstname.lastname@companyname.onxmpro.com Admin as entered during setup Request a License A new company is created as part of the installation process but needs a valid license to work. Note Login using the Super Admin account **admin@xmpro.onxmpro.com**. Click Company in the left menu to open the Companies page. Click on the Company. Click on the Subscriptions gauge to open the Subscriptions page. Click on a Subscription. Click the Update License button in the command bar. Click Generate a license request, enter the number of days, and submit. When you have received the license from XMPro support, upload it. Click Save. This sends a request to XMPro for an App Designer license for this Company. Warning The Generate a license request link will only work if SMTP was set up during installation. Change the Product to Data Stream Designer from the list and request another license Note Licenses are given on an individual basis by the XMPro support team. When you have received a license for each product through an email sent to the email address given during installation, follow the steps below to upload the license for both App Designer and Data Stream Designer. Next Step: Complete Installation The installation of the XMPro Platform is now complete but before you can use the platform, some steps are needed to set up the environment. Further instructions about the configuration can be found below: Install Connectors Upgrade Warning Before beginning the upgrade, back up databases using this guide. Follow the same steps as a new Install to upgrade your XMPro platform. Make sure to: Download and use the latest \"mainTemplate_[Version].json\". Use the same Resource Group and credentials used during the original Install when completing the Custom deployment form. Uninstall This section provides information about uninstalling the XMPro platform from Azure. Open the resource group where XMPro is installed Click Delete resource group Enter the name of the resource group and click Delete"
  },
  "docs/installation/deployment/index.html": {
    "href": "docs/installation/deployment/index.html",
    "title": "Install XMPro | XMPro",
    "summary": "Install XMPro XMPro Platform can be installed using a range of Infrastructure, please choose the option below that best suits your requirements: {% content-ref url=\"azure.md\" %} azure.md {% endcontent-ref %} {% content-ref url=\"aws.md\" %} aws.md {% endcontent-ref %} {% content-ref url=\"on-premise.md\" %} on-premise.md {% endcontent-ref %}"
  },
  "docs/installation/deployment/on-premise.html": {
    "href": "docs/installation/deployment/on-premise.html",
    "title": "On-Premise | XMPro",
    "summary": "On-Premise This document will guide you through how to deploy the XMPro Platform in an on-premise environment. Install Upgrade Architecture The following deployment diagram shows an example on-premise architecture and the necessary resources for the XMPro platform in Microsoft Windows. Prerequisites In order to proceed with the deployment, you are required to: Complete the steps in the 1. Preparation guide: Meet the hardware requirements Install the software requirements Follow the certificate and communication steps Enable Active Scripting and Allow Scriptlets (so App Designer and Data Stream installers can authenticate with Subscription Manager) Open Internet Explorer. Open Tools. Open Internet Options. Switch to the Security tab. Click the \"Custom level...\" button. Find and enable Active Scriptlets under ActiveX controls and plug-ins. Find and enable Active Scripting under Scripting. For Windows Server, disable these Internet Explorer settings (so App Designer and Data Stream installers can authenticate with Subscription Manager) Open Server Manager. Click on Local Server from the left menu. Find the IE Enhanced Security Configuration on the right side and click on the \"On\" value. On the pop-up, select \"Off\" for Administrators and apply the changes. Install IIS Service on a fresh Virtual Machine (VM) Open Server Manager. Click on Manage from the top right menu. Select Add Roles and Features, and click Next until you see Server Roles highlighted on the left. With Server Roles highlighted on the left, tick the Web Server (IIS) role... Expand Web Server and Common HTTP Features, untick WebDAV Publishing, and click Next. With Features highlighted on the left, tick everything under the .NET Framework features. Click Next twice. With Role Services highlighted on the left, tick everything except CGI under the Application Development section. Click Next and Install. Go to the Start button and Administrative Tools to confirm IIS Manager was installed. Note Please contact your XMPro account manager or XMPro support to request the installers. Install Subscription Manager Start the installation process by running the Subscription Manager.exe file, received from your Global Administrator. Click the \"I Agree\" button and press \"Next\" Follow the instructions and when the installation is finished click \"Close\" Note This \"Setup\" will install the installer you will use to install the database and website When this initial installation is complete, open the start menu Search for \"XMPro Subscription Manager\" and click on Run as Administrator Component Choice When the installer launches, choose \"Install\" and click \"Next\" Select the components that you would like to install and click \"Next\" Note If this is the first time you are installing Subscription Manager, it is highly recommended that you select both \"Database\" and \"Web Application\" Database Server Select the server instance to which you would like to connect Note If you already know the server instance name, it can be entered manually. Otherwise, use the refresh button on the right to load all available servers. Selecting the \"Local Servers\" check box will limit the search to the local network. Authentication Method Specify the authentication method that should be used: Windows or SQL 9.1. Windows Authentication: you may leave the options as is Warning Configure a service account that can be used for Windows authentication. 9.2. SQL authentication: Click the \"Change\" button Select the \"Use SQL Authentication\" option Enter the username and password of the SQL Server instance you're connecting to Warning The SQL user must have permission to create databases on the server. Database Note The Database section allows you to configure if you would like to use an existing database or create a new one. Leaving the options as default will result in a new database being created. To change the pre-populated name of the new database or to select to use an existing database: Click the \"Change\" button Make the changes needed by selecting the correct option Specify the name of the new database or select an existing database from the drop-down Web Application DNS Name Verify if your DNS name is correct, if not, edit the value to contain the correct DNS name Note This is your fully qualified domain name (FQDN). Please find some examples below explaining the DNS name. https://localhost/xmprosubscriptionmanager https://desktop-f64k32e/xmprosubscriptionmanager https://demo.azurewebsites.com Complete Address DNS Virtual Directory https://localhost/xmprosubscriptionmanager localhost xmprosubscriptionmanager https://desktop-f64k32e/xmprosubscriptionmanager desktop-f64k32e xmprosubscriptionmanager https://demo.azurewebsites.com demo.azurewebsites.com Virtual Directory Select the parent site from the Web Site drop-down Note By default, the Virtual Directory name will be \"xmprosubscriptionmanager\" which will be created within IIS for the Subscription Manager site. If you wish to change the name you can specify it in the \"Virtual Directory Name\" text box. Verify if the value in the content directory field is correct. If not, apply any changes needed Note By default, the option to create a sub-directory within the content directory is checked and you can specify a name in the \"Sub-Directory\" text box. Application Pool If you wish to change this name or use an existing application pool, click the Change button Note By default, a new application pool will be created when installing the site. The new application pool will have the same name as the name specified in the \"Application Pool Name\" field. Either select the \"Create a new Application Pool\" or \"Use an existing Application Pool\" option Note If you choose \"Create a new Application Pool\", give it an appropriate name. If you choose \"Use an existing Application Pool\", select an existing application pool from the drop-down. Security Account Select a security account that can be used Note The default option is \"Local System\", which is a built-in security account. You can either change it by selecting a different built-in security account from the drop-down or by specifying your own security account. Warning If you selected Windows authentication to connect to the database, you must choose \"Specify your own Security Account\" and provide the correct credentials. The service account must have batch logon rights enabled. More Information on how to set up a custom application pool in IIS as well as steps on how to enable batch logon rights can be found in this link. SMTP Enter the SMTP details referenced in the 1. Preparation guide. By default, the \"Enable Email Notification\" is checked. Note SMTP can be disabled by unchecking the \"Enable Email Notification\" checkbox if you don't want to receive email notifications. If at a later stage email notifications are needed, the installer can be run again to add SMTP functionality. Warning You are required to set up an SMTP account. Failing to do so will make registering new users very cumbersome. Check your connection to the email server using the \"Test SMTP settings\" button. Certificates During the installation process, you will be asked to upload two certificates: a signing certificate and an encryption certificate. You may use the same certificate for both options. The instructions on how to create a certificate can be found in the 1. Preparation guide. Signing Certificate Start by browsing to a suitable .pfx certificate file. Specify the password for the certificate Use the dropdown to select \"Subject Name\" Note It is recommended that you choose \"LocalMachine\" as the Location for the signing certificate. Encryption Certificate Start by browsing to a suitable .pfx certificate file. Specify the password for the certificate Use the dropdown to select \"Subject Name\" Note It is recommended that you choose \"LocalMachine\" as the Location for the encryption certificate. Warning Both certificates must contain a private key. Final Steps Continue through the wizard, confirm the installation and the components will be installed Warning Note the username and password on the last screen of the installer. This user has been created during installation as Subscription Manager itself needs at least one user in the system. Without it, you cannot add other users. Change the password of the default user to a new, secure password after logging in for the first time. Accessing the Website Using Web Browser Access the website by putting the URL into your browser Note The format of the URL will be as follows: \"https://yourdnsname/virtualdirectoryname/\" Obtaining an Installation Profile To install the Data Stream Designer and App Designer, you will need an Installation Profile. Navigate to the XMPro Subscription Manager site as above Go to the Subscription Manager page Click Products in the menu and click the Installation Profile button Enter a File Key and press OK to download the file Warning Remember the file key as it is needed when installing Data Stream Designer and App Designer. Optional: IIS User Permissions If you've chosen to use a custom service account during installation, you may have to perform an extra step. An error may be shown after logging into Subscription Manager, even after giving the IIS_USRS group permission on the signing certificate private keys. The error would be as follow: \"We could not grant you access to the requested subscription. There was an unexpected error\". The logs would also contain the following error: \"System.Security.Cryptography.CryptographicException: Keyset does not exist\". To solve this issue, use this article as a guideline to grant access for the Application Pool Identity (in some cases a domain account) on the signing certificate private keys. Data Stream Designer Start the installation process by running the Data Stream Designer.exe that you've received from your Global Administrator. Click the \"I Agree\" button and press \"Next\"​ Follow the instructions and when the installation is finished click \"Close\" When this initial installation is complete, open the start menu Search for \"Data Stream Designer\" and click on Run as Administrator Component Choice When the installer launches, choose \"Install\" Select the components that you would like to install Note If this is the first time you are installing the Data Stream Designer, it is highly recommended that you select both \"Database\" and \"Web Application\". Database Server Select the server instance you would like to connect to. Note If you already know the server instance name, it can be entered manually. Otherwise, use the refresh button on the right to load all available servers. Selecting the \"Local Servers\" check box will limit the search to the local network. Authentication Method Specify the authentication method that should be used: Windows or SQL 9.1. Windows Authentication: you may leave the options as is Warning Configure a service account that can be used for Windows authentication. 9.2. SQL Authentication: To connect to the database using SQL Server authentication, click the \"Change\" button Select the \"Use SQL Authentication\" option Enter the username and password of the SQL Server instance you're connecting to Warning The SQL user must have permission to create databases on the server. Database Note The Database section allows you to configure if you would like to use an existing database or create a new one. Leaving the options as default will result in a new database being created. To change the pre-populated name of the new database or to select to use an existing database: Click the \"Change\" button and select the appropriate option Specify the name of the new database or select an existing database from the drop-down Encryption Upgrade If you are upgrading from 4.0 to 4.1 or greater, you will be shown the Encryption Upgrade Settings page. This will assist you in migrating existing Server Variables to the new method of encryption. Warning To upgrade existing Server Variables, the details of the Subscription Manager database are required, not the Data Stream Designer database (provided on the previous page). Upgrade Server Variables? Tick to automatically upgrade the Server Variables. It is recommended, but not required. None of the other settings on this page are required if you choose not to upgrade. Server Select the server instance you want to connect to Authentication Method Specify the authentication method that should be used: Windows or SQL Database Select the Subscription Manager database and click Next Web Application DNS Name Verify if your DNS name is correct. If not, edit the value to contain the correct DNS name Note This is your fully qualified domain name (FQDN). Please find some examples below explaining the DNS name. https://localhost/xmprosubscriptionmanager https://desktop-f64k32e/xmprosubscriptionmanager https://demo.azurewebsites.com Complete Address DNS Virtual Directory https://localhost/xmprosubscriptionmanager localhost xmprosubscriptionmanager https://desktop-f64k32e/xmprosubscriptionmanager desktop-f64k32e xmprosubscriptionmanager https://demo.azurewebsites.com demo.azurewebsites.com Virtual Directory Select the parent site from the Web Site drop-down Note By default, the Virtual Directory name will be \"DataStreams\" which will be created within IIS for the Data Stream site. If you wish to change the name you can specify it in the \"Virtual Directory Name\" text box. Verify the value in the content directory field. If incorrect, apply any changes needed Note By default, the option to create a sub-directory within the content directory is checked and you can specify a name in the \"Sub-Directory\" text box. Application Pool If you wish to change the name or use an existing application pool, click the Change button Note By default, a new application pool will be created when installing the site. The new application pool will have the same name as the name specified in the \"Application Pool Name\" field. Either select the \"Create a new Application Pool\" or \"Use an existing Application Pool\" option Note If you choose \"Create a new Application Pool\", give it an appropriate name. If you choose \"Use an existing Application Pool\", select an existing application pool from the drop-down. Security Account Select \"Local System\" as the security account. Note The two options available to choose from are using a built-in security account or specifying your own security account. Warning If you selected Windows authentication to connect to the database, you must choose \"Specify your own Security Account\" and provide the correct credentials. The service account must have batch logon rights enabled. More Information on how to set up a custom application pool in IIS as well as steps on how to enable batch logon rights can be found in this link. Installation Profile Click the Browse button to upload an installation profile for Subscription Manager Select a file and click \"Next\" Note This file ensures the Data Stream Designer contains the correct details for the Subscription Manager instance you would like to use. The file can be obtained through the steps outlined previously in this tutorial. After you press \"Next\", authenticate yourself using Subscription Manager credentials Warning If you are unable to sign in at this step, please follow this link to disable Internet Explorer Enhanced Security Configuration. Final Steps Continue through the wizard, confirm the installation and the components will be installed App Designer Start the installation process by running the App Designer.exe file that you've received from your Global Administrator. Click the \"I Agree\" button and press \"Next\" Follow the instructions and click \"Close\" when the installation is finished Note This \"Setup\" will install the installer you will use to install the database and website When this initial installation is complete, open the start menu Search for \"App Designer\" and click on Run as Administrator Component Choice When the installer launches, choose \"Install\" and click \"Next\" Select the components that you would like to install and click \"Next\" Note If this is the first time you are installing Subscription Manager, it is highly recommended that you select both \"Database\" and \"Web Application\". Database Server Select the server instance you would like to connect to Note If you already know the server instance name, it can be entered manually. Otherwise, use the refresh button on the right to load all available servers. Selecting the \"Local Servers\" check box will limit the search to the local network. Authentication Method Specify the authentication method that should be used: Windows or SQL 9.1. Windows Authentication: you may leave the options as is Warning Configure a service account that can be used for Windows authentication 9.2. SQL Authentication: Click the \"Change\" button Select the \"Use SQL Authentication\" option Enter the username and password of the SQL Server instance you're connecting to Warning The SQL user must have permission to create databases on the server. Database Note The Database section allows you to configure if you would like to use an existing database or create a new one. Leaving the options as default will result in a new database being created. To change the pre-populated name of the new database or to select to use an existing database: Click the \"Change\" button and select the appropriate option Specify the name of the new database or select an existing database from the drop-down Encryption Upgrade If you are upgrading from 4.0 to 4.1 or greater, you will be shown the Encryption Upgrade Settings page. This will assist you in migrating existing Server Variables and Connector settings to the new method of encryption. Warning To upgrade existing Server Variables, the details of the Subscription Manager database is required, not the Data Stream Designer database (provided on the previous page). App Designer Encryption Key Enter the App Designer Encryption Key Note To find the App Designer Encryption Key, inspect the appsettings.json file in the web server files. It will be found under the JSON path \"xmpro.appDesigner.encryptionKey\". If that path does not exist, it is stored in a cloud-service key vault. Search for the \"xmpro.keyVault\" JSON object for the details required to find the encryption key. Documentation for the Azure and Amazon key vaults have been linked for convenience. Upgrade Server Variables? Tick to automatically upgrade the Server Variables. It is recommended, but not required. None of the other settings on this page are required if you choose not to upgrade. Server Select the server instance you want to connect to Authentication Method Specify the authentication method that should be used: Windows or SQL Database Select the Subscription Manager database and click Next Web Application DNS Name Verify if your DNS name is correct, if not, edit the value to contain the correct DNS name Note This is your fully qualified domain name (FQDN). Please find some examples below explaining the DNS name. https://localhost/xmprosubscriptionmanager https://desktop-f64k32e/xmprosubscriptionmanager https://demo.azurewebsites.com https://localhost/xmprosubscriptionmanager localhost xmprosubscriptionmanager https://desktop-f64k32e/xmprosubscriptionmanager desktop-f64k32e xmprosubscriptionmanager https://demo.azurewebsites.com demo.azurewebsites.com Virtual Directory Select the parent site from the Web Site drop-down Note By default, the Virtual Directory name will be \"AppDesigner\" which will be created within IIS for the Data Stream site. If you wish to change the name you can specify it in the \"Virtual Directory Name\" text box. Verify if the value in the content directory field is correct. If not, apply any changes needed Note By default, the option to create a sub-directory within the content directory is checked and you can specify a name in the \"Sub-Directory\" text box. Application Pool If you wish to change this name or use an existing application pool, click the Change button Note By default, a new application pool will be created when installing the site. The new application pool will have the same name as the name specified in the \"Application Pool Name\" field. Either select the \"Create a new Application Pool\" or \"Use an existing Application Pool\" option Note If you choose \"Create a new Application Pool\", give it an appropriate name. If you choose \"Use an existing Application Pool\", select an existing application pool from the drop-down. Security Account Select \"Local System\" as the security account Note You can either change it by selecting a different built-in security account from the drop-down or by specifying your own security account. Warning If you selected Windows authentication to connect to the database, you must choose \"Specify your own Security Account\" and provide the correct credentials. The service account must have batch logon rights enabled. More Information on how to set up a custom application pool in IIS as well as steps on how to enable batch logon rights can be found in this link. Integration Details Type in the URL of Data Stream designer in the text box SMTP Enter the SMTP settings referenced in the 1. Preparation guide. By default, the \"Enable Email Notification\" is checked. Note SMTP can be disabled by unchecking the \"Enable Email Notification\" checkbox if you don't want to receive email notifications. If at a later stage email notifications are needed, the installer can be run again to add SMTP functionality. Warning You are required to set up an SMTP account. Failing to do so will make registering new users very cumbersome. It is highly recommended to check your connection to the email server using the \"Test SMTP settings\" button. Twilio (Optional) Enter the Twilio details referenced in the 1. Preparation guide. If you don't want SMS notifications you can select \"None\" from the \"Select Provider\" dropdown. Installation Profile Click the Browse button to upload an installation profile for Subscription Manager Select a file and click \"Next\" Note This file ensures the App Designer contains the correct details for the Subscription Manager instance you would like to use. The file used can be obtained through the steps outlined previously in this tutorial. The Installation Profile generated for Data Stream Installer can be used in this step. After you press \"Next\", authenticate yourself using Subscription Manager credentials Warning If you are unable to sign in at this step, please follow this link to disable Internet Explorer Enhanced Security Configuration. Final Steps Continue through the wizard, confirm the installation and the components will be installed Next Step: Complete Installation The installation of the XMPro Platform is now complete, but there are some environment setup steps before you can use the platform. Please click the below link for further instructions: Complete Installation Upgrade XMPro It is necessary to first uninstall the installers on the host server before proceeding with the upgrade installers. Once you have removed the installers, the procedure for the upgrade is the same as the original install. This section aims to provide step-by-step instructions on how to safely uninstall only the XMPro installers and not the on-premises deployed XMPro solution. The process of uninstalling the XMPro installer must be repeated for each application individually. The steps are the same for each application installer and the sequence is not significant. Uninstall Subscription Manager Installer Log on to the instance where the XMPro installers are installed. From the start menu select XMPro Subscription Manager and right-click it. Click Uninstall. Select XMPro Subscription Manager from the list of programs displayed. Click Uninstall. When prompted to confirm uninstall click Yes. Click Next. When prompted to select the install to modify click Cancel. We do not want to remove the installed XMPro site, only the installer."
  },
  "docs/installation/index.html": {
    "href": "docs/installation/index.html",
    "title": "Installation | XMPro",
    "summary": "Installation XMPro Platform consists of three main components: App Designer Data Stream Designer Subscription Manager XMPro supports a wide range of deployment options including Cloud, Docker, On-Premise, and more. The complete process - encompassing preparation, installation, setup, and loading templates - is depicted in the flowchart below. Fig 1: The installation process overview. Installation Process The installation process consists of several key steps: Preparation: Set up prerequisites and gather necessary components Deployment: Install XMPro on your chosen platform (AWS, Azure, On-Premise) Complete Installation: Configure additional components and settings Prerequisites Before attempting any of the supported XMPro deployment options, ensure you have the following prerequisites: Hardware Requirements (see Sizing Guideline) Software Requirements Signing Certificate HTTPS/SSL Certificate SMTP Account Twilio Account (Optional) Fig 2: The sequence of the 'Prepare to Install XMPro' step within the overall process. Note For browser requirements, see Browser Requirements Supported Browsers Supported Operating Systems Third-Party Cookies Artifacts Request Installers Request Tiers 1 - 4 Download and install Tier 5 & 6 Files Links for the larger AI & ML Agents are on their individual documentation pages, as indicated here. Download GitHub Templates"
  },
  "docs/introduction.html": {
    "href": "docs/introduction.html",
    "title": "Introduction | XMPro",
    "summary": "Introduction"
  },
  "docs/release-notes/archived/release-notes.html": {
    "href": "docs/release-notes/archived/release-notes.html",
    "title": "Archived Release Notes | XMPro",
    "summary": "Archived Release Notes This section contains archived release notes for older versions of XMPro. v4.3.x Releases v4.3.12 v4.3.11 v4.3.10 v4.3.9 v4.3.8 v4.3.7 v4.3.6 v4.3.5 v4.3.4 v4.3.3 v4.3.2 v4.3.1 v4.3.0 v4.2.x Releases v4.2.3 v4.2.2 v4.2.1 v4.2.0 v4.1.x Releases v4.1.13 v4.1.0"
  },
  "docs/release-notes/archived/v4.1.0.html": {
    "href": "docs/release-notes/archived/v4.1.0.html",
    "title": "v4.1.0 | XMPro",
    "summary": "v4.1.0 App Designer Change Type Description Feature App Designer Canvas validation - Blocks now highlight required properties and show a validation status if not configured. Feature New Blocks: File Library File Uploader Time Series Chart Time Series Insight (Edit: subsequently deprecated) Data Operations Feature Data source Expressions can now be used in blocks like Data Grid, Chart, List, etc. Feature Block properties now allow using Server Variables. Feature Import and Export Widgets Feature Set an App as a landing page. Enhancement Navigation Pivots - View Apps per Connector. Enhancement Support for Notification template in Recommendation Rule notification. Enhancement Add the ability to use a mailto link in Action properties of the blocks. Enhancement Improved styling for Fieldset and Field block. Enhancement Navigation enhancements - Support for navigating to the previous page as per browser history. Enhancement Data Grid enhancements Ability to set default filters Support column reordering, resizing, and state persistence Support for column editors like Indicator, Hyperlink, and Select box Enhancement Tree Grid - Ability to stop a tree grid at a certain level Enhancement Batch with External Save - Data Grids and Tree Grids now allow controlling the Insert, Update, and Delete using the Integration options. Enhancement Chart enhancements Panes are now listed and managed along with Series configuration Title Color for plot axes is now configurable Enhancement Data source validation Enhancement Connector Framework - API extended to expose Count which allows Paging of large datasets at the server. Enhancement New Session Variables such as isMobile or isTablet. Enhancement Text control - Improve wrapping of text. Enhancement Support to update Page Data on an action. Enhancement Tab - Two way binding for Tab selected index Enhancement Allow Action on value change of non-action blocks. Enhancement Ability to specify/override Primary Key for Data source. Enhancement Recommendation - Ability to add a new category Enhancement New access rights to hide the App bar (left menu) for lite users. Enhancement Esri map Ability to add a text label on a marker Ability to change marker and label color Fix GetMonth function in expression is getting the range from 0-11, instead of 1-12. Fix Recommendation Widget keeps showing Loading Icon after load. Fix Ticket 7782 Double-clicking in a Tabs block causes it to corrupt the block and its contents. Fix The default value for variables of type Int, Long, and Double is an empty string instead of null. Fix Date Time not converting to local time in the Data Grid Block. Fix Page parameters with dots would cause errors when refreshing the page. Fix Mobile - datetime picker is out of view. Data Stream Designer Change Type Description Feature Admins can Publish or Unpublish all Data Streams. New Navigation pivots: Data Streams per Collection Data Streams per Agent Feature Published Data Stream Metrics Feature Stream Error Flow - Support to design stream flow which is to be executed when an error occurs. Feature Data stream logs Enhancement Agent framework extended to request decryption of Server variables in one call. Enhancement Canvas enhancements: Context Menu added Disable stream object - A disabled stream object is ignored by the canvas Ability to Copy, Paste and Delete a Stream Object Enhancement Environment Variables - Stream Host can override the Server Variables if a matching OS Environment Variable is found. Fix Data Stream gets stuck and does not start, keeps saying \"Starting\" or \"Stopping\" especially if unpublished immediately or in the first poll. Fix INFO logs have stopped appearing. Fix Data Streams are not sorted alphabetically in the lists. Subscription Manager Change Type Description Feature Ability to send a password recovery link for forgotten passwords to recover/create a new password. Enhancement Company Logo - Changed the recommended size to a max height of 28px. Enhancement Redesigned landing page. The logon navigation behavior updated as per below: When the user logs in, they will be redirected to App Designer if they have a subscription to it If not, they will be redirected to Data Stream designer if they have a subscription to it If the user does not have a subscription for either, they will be redirected to the new Subscription Manager landing page Fix Unable to reset password if forgotten when the username has _ character in it. Fix Company Admin is unable to update a license for his subscription. Common Change Type Description Feature Added ability to add Data Streams and Applications as favorites. Enhancement License Expiring message will only be shown to Administrators. Enhancement Search bar added to App Designer and Data Stream Designer to search for Apps, Data Streams, Alerts, Recommendations, and more. Fix Postback issue with configuring agents and connectors with RichText Editor in both App Designer and Data Stream Designer. v4.1.0.1 Change Type Description Fix Time Series Chart block substitutes null values as zero. Fix Tree Grid block doesn't support column editors like Hyperlink, Dropdown, or Indicator. Fix Application Export/Import loses custom primary keys if specified in Page Data. Fix Time Series Chart breaks the lines if there are empty buckets. v4.1.0.2 Change Type Description Fix Unable to import Data Streams due to the missing Error Log Agent (required for Data stream logs)."
  },
  "docs/release-notes/archived/v4.1.13.html": {
    "href": "docs/release-notes/archived/v4.1.13.html",
    "title": "v4.1.13 | XMPro",
    "summary": "v4.1.13 App Designer Change Type Description Feature New Blocks added: Azure Digital Twin Hierarchy - An interactive UI component that visualizes a large amount of time-oriented data. It allows the user to compare data at run-time based on asset ID by dropping markers on the chart, as well as panning and zooming, and can show or hide specific assets or parameters via the hierarchy panel. Image Map - allows you to render dynamic and static content onto an Image. You can drag on content such as indicators or text and these render on top of the image at runtime, at the specified coordinates. A new Unity Block was added to support Unity 2020 and above. The previous Unity Block was renamed Unity (Legacy) and continues to support Unity 2019. Feature Connector Logs allow the composer of an App, the developer writing a new Connector, or an Administrator to view messages generated by a Connector when it is in use as a Connection in an App. Feature Alerts for selected Recommendations can be added as markers to time series data: Azure Digital Twin Hierarchy Time Series Chart Time Series Insights (Edit: subsequently deprecated) Enhancement A new property, Store User Selection, has been added to the Dropdown Grid and Select Box Blocks. Enable it to remember the user's selection when the page loads on these blocks that take more than one value. Enhancement Extend the Tree Grid with the following: Allow Drag & Drop is now independent of Allow Updating, so you can choose whether users can amend the hierarchy, the data, or both. Added Hyperlink, Lookup, and Indicator column types Added Store User Selection, to remember the user's selection when the page loads Enhancement Format numeric output in the Data Grid and Tree Grid as currency or percentage. Enhancement Extend the Tab Block to allow for App page reuse with the new Hide Tab Navigation property Enhancement Time Series Chart hierarchy selection pop-up widened to avoid truncating parameter names. Enhancement Consolidated Data Stream Connection mapping added the App Import to simplify moving Apps between environments. Enhancement SVG file types are supported for all image properties. See here for a full list of supported image file types. Enhancement The current app page URL parameters (appId, appVersion, pageId, categoryId and categoryName) were added to the expression editor so that a back URL can be passed to a drill-down when using the Navigate To property. Enhancement Renamed 'Open in New Window' to 'Open in New Tab/Window' under Navigate To property and expanded the documentation. Enhancement Add a tooltip to the Text Block and the Fieldset Block's Field Label. This is useful to keep the text short and use a tooltip for longer descriptions. The Fieldset's padding and margin now default to 0 (previously 20 and 30px). The styling is now consistent with other Blocks with the padding coming from the container e.g. Stacked Layout. Enhancement Extend the Map Block to overlay an image or Geo Json pattern on Google Maps. Enhancement The height of the Quick Expression Editor was increased from 1 to 4 lines and scrollbars were added too. Enhancement Added font-size support for relative css units vh and vw to the Typography option. Fix Recommendation Block not refreshing results when using dynamic Entity Id. Implement auto refresh when using a dynamic Entity Id. Fix Unable to Navigate To URL from Chart Block. Exposed Chart's Show Drilldown property. Enable this when using Actions such as Navigate To. Fix Keyboard input is disabled at runtime after rendering Unity. Follow this guide to prevent the keyboard input from being locked to the Unity model. Fix Time Series Chart fixes: The timestamp dropdown doesn't scroll when there are more than 30 columns and can no longer be filtered with text. The cause was 2 columns with the same name, but different capitalization. A Connector configuration now warns about case sensitivity and prevents duplicated column names. Clicking the Internal Size unit dropdown caused a logic dialog to open too Only the unit dialog opens When specifying many selections, the header does not wrap so not all of the series names are visible and long names are truncated. A scrollbar has been added to the individual names as well as the chart header. The Pan & Zoom mode affects this panel only. The displayed data now expands to fill space created by a compacted Pan & Zoom panel. Parameters are not passed when reading the max timestamp. Parameters defined on the data source are now passed for both the min and max timestamps. A hidden chart (Visible is dynamically bound) makes an unexpected data call. This affects all Blocks: the data source is now refreshed when the control becomes visible rather than on initialization. Fix Tree Grid is stuck loading once expanded and several levels are clicked. There may be a delay while the controls are refreshed, but the App will not freeze. Data Stream Designer Change Type Description Enhancement A visual indicator that a Data Stream has started successfully has been added to the Stream Metrics. In the past, you would've had to publish your Data Stream and wait for events in the Live View - or utilize a console Stream Host and view its log - as confirmation there were no errors when publishing. Enhancement The Agent single and bulk upload now prepopulates the Agent category - when specified in the .xmp file. The Agent upload process is now a single file per tier because the category is clear for Agent families like CSV and SQL. Fix Cannot upload Tier 5 Agents The Agent bulk upload file size was increased. Fix The Refresh button on the Logs blade clears the table but doesn't reload the logs. The Refresh button reloads the logs. Subscription Manager Change Type Description Enhancement Optional functionality to sync the user's Business Role to a corresponding Azure AD Claim each time they log in. SSO using Azure AD is a pre-requisite. Package Manager Change Type Description Enhancement You can run multiple instances of Package Manager at the same time. This side-by-side comparison is helpful when developing a new Agent or Connector that is similar to another or different versions of the same Agent. Enhancement When packaging an Agent or Connector, you can opt to export the configuration as a JSON file too. This is useful either to compare packages or for source control and version management. Enhancement You can package the Agent's Category so that it is prepopulated when uploading to Data Stream Designer. The Agent name defaults to the preferred format [Category]_[Name]_v[Version].xmp Fix Package Manager crashes when adding a Group with no contents. An empty Group type caused the issue and is now defaulted. Fix Package Manager crashes when adding an option to a Drop Down for a Connector. Drop Down options can now be added to Connectors too. Common Change Type Description Enhancement The App Designer and Data Stream Designer landing page styling has been refreshed, including labeling the category card statistics. When selecting a category in App Designer, the Applications can be filtered by status and tag. Yes, you can now tag Applications too. When selecting a category in Data Stream Designer, the Data Streams can be filtered by status, tag, and/or collection; and Stream Metrics can be switched on for those which are published. Enhancement The description text height on Application and Data Stream imports has been increased so that it can be read without scrolling. Fix Special characters broke the Azure deployment script. The Azure deployment script was updated to specify the few unsupported special characters for passwords. Fix Single Sign Out Issue when a user signs out of one company to switch to another company, the previous company name and categories were still visible in tabs open for other products. Signing out of one product completely signs a user out of all products. v4.1.13.1 Change Type Description Fix Optional properties were treated as required when adding or upgrading a Connector. Optional properties behave as expected and don't require values. Fix The tab selection was not caching correctly on the first tab of a Tab Block. So another tab stayed selected, even after refreshing - or navigating away and returning to the App page. The first tab would fail to cache, this has now been resolved. Fix Widening the Time Series Chart hierarchy selection pop-up in v4.1.13 - to avoid truncating parameter names - caused the list to shift out of view if a resource icon is clicked. Reverted the hierarchy prompt to its original size, but made the size adjustable. You can now click on either the resource icon or name. v4.1.13.2 Change Type Description Enhancement The Beta tag indicates incremental functionality, added to prepare for a future feature. For example, the factors added to Recommendations in preparation for a future Scoring enhancement. Enhancement The Time Series Chart now supports dynamic range and initial selection, which determines the starting date for the Pan & Zoom and the time interval that is initially selected. Enhancement Azure AD SSO now allows for Guest User access across tenants. Fix Data Source expressions were not available for selection in the Time Series Chart configuration. Data stream expressions now appear correctly in the Time Series Chart. Fix Alerts were navigable without run access. Restricted access to alerts for which the user does not have run access. v4.1.13.3 Change Type Description Enhancement Released an early access Database Migration tool for automating new database installs and upgrades. Fix Time Series Chart fix: the Initial Selection's Unit of Time was using hours when dynamic, despite being set as another option in the static value. The Initial Selection's Unit of Time is correctly applied. v4.1.13.4 Note XMPro has changed its product URLs. A redirect is in place for the sign-in page, but we recommend that you bookmark the new URLs for future reference: https://xmpro-sm.azurewebsites.net/ https://xmpro-ds.azurewebsites.net/ https://xmpro-ad.azurewebsites.net/ Change Type Description Enhancement Hyperlink columns in a Data or Tree Grid now have the option to open in the same or a new tab/window. We recommend opening XMPro URLs in the same tab/browser if the app design means it is likely a user will drill down into multiple tabs in a single sitting. A large number of XMPro tabs open at the same time is likely to cause performance-related issues. Enhancement The Time Series Chart's initial selection's unit of time is now a standalone property (here) for intuitive configuration. Update We've reduced our Linux-based Stream Host offering to support Ubuntu only, to increase our test coverage and ability to release often. Linux RHEL and Debian Stream Hosts are no longer supported. Fix The detailed view of a Recommendation Alert - including its event data - is visible to users with run access, whether it was accessed through manipulation of URL string parameters or not. An error message advising they do not have access is displayed. Fix The parent blade name (or menu item on primary blades) was used as the default tooltip in blades across all products. Tooltips are only showing where intended. v4.1.13.5 Change Type Description Fix The Help blade across all products showed the major version i.e. 4.1.0.0. The Help blade now shows the public release version e.g. 4.1.13.5. Fix An issue was introduced in 4.1.13.4, where Recommendation Category names were not editable. Recommendation Category names can be amended."
  },
  "docs/release-notes/archived/v4.2.0.html": {
    "href": "docs/release-notes/archived/v4.2.0.html",
    "title": "v4.2.0 | XMPro",
    "summary": "v4.2.0 App Designer Change Type Description Feature The new feature, Recommendation Scoring, allows for further fine-tuning of alert rankings to aid in prioritization. This feature utilizes a scoring system that takes into account multiple Score Factors that can be configured within: - Recommendation - Recommendation Category - Recommendation Rule. A Score Matrix was added to allow you to see the score and the corresponding recommendation, category and rule that comprise of the score. This provides greater visibility and understanding of the score composition. Feature A timeline of events is now available for Recommendations, Rules and Recommendation Categories. This feature enables you to view the changes made to a recommendation or category and the user who performed them. The timeline includes search and filtering options to facilitate the viewing of changes/events. Enhancement The Recommendation Landing Page has been updated to include sorting by scoring, allowing users to view the calculated score and alert ID. This enhancement facilitates the prioritization of alerts that require more attention. Enhancement Creating Recommendation Categories has been moved to a new blade to accommodate the addition of the score factor. Enhancement Recommendation Alerts that are Auto Escalated will now be automatically assigned to the previous owner of the alert. Enhancement Hyperlinks in Tree Grid and Data Grid now default to opening within the same tab. This is done to make viewing hyperlinks more responsive. Enhancement General system performance improvements were made to make a more responsive user experience. Enhancement General security updates were added. Fix The Time Series Chart's Data Source Parameters were not accessible by its parent Hierarchy. This issue has been resolved. Data Stream Designer Change Type Description Enhancement Introduced a visual indicator to the Data Stream Designer's canvas and toolbox to better distinguish the Agent's Category using color and text. Enhancement The Stream Host runtime has been upgraded, taking advantage of the enhanced performance and features of .NET Core 6. For more information on installing Stream Hosts, please visit this link. Subscription Manager Change Type Description Enhancement The new site setting, Hide Users Outside Business Role Branch, allows you to control whether users can see the details of other users outside of their business role. This is useful when the structure makeup is confidential. Enhancement General security updates were added."
  },
  "docs/release-notes/archived/v4.3.0/index.html": {
    "href": "docs/release-notes/archived/v4.3.0/index.html",
    "title": "v4.3.0 | XMPro",
    "summary": "v4.3.0 XMPro AI Change Type Description Feature This feature infuses the Digital Twin Platform with XMPro AI. This introduces the XMPro Notebook, which is an embedded version of Jupyterhub and will be available for evaluation on new XMPro Freemium accounts. Existing customers and Freemium users can contact us for access and licensing options. Please visit XMPro AI for more information about XMPro AI and XMPro's Intelligent Digital Twin Suite. App Designer Change Type Description Enhancement The number format displayed in the Data Grid, Tree Grid, and Pivot Grid now supports the comma format, providing improved readability and ease of data interpretation. Fix An issue has been addressed where the Score Matrix did not open. Fix An issue where an \"Encryption Key\" error was displayed when updating the \"Enable Audit trail\" and \"Landing Page for Desktop/Mobile\" Application Designer settings has been resolved. Fix The Recommendation Chart block included all recommendations when not configured. If it is not configured, it will now state \"This control is not yet configured. Please use the block properties to configure it\". Fix The XMPro logo was displayed in the XMPro mobile app. The mobile app correctly displays the company logo configured in Subscription Manager. Data Stream Designer Change Type Description Fix When navigating to the connectors page and selecting a connector's version, all previous versions of the selected connector were highlighted. The selected version is the only row highlighted and the Applications blade opens correctly. Subscription Manager Change Type Description Enhancement The Subscription page now shows the license expiry date and the number of days remaining. Previously, the global or company admin would have to open each subscription to view the expiration date of a license. Common Change Type Description Feature Logging provider support via Serilog, a diagnostic logging library, enhances system logging functionality to provide administrators with valuable insights into the behavior and performance of XMPro. Three logging outputs are supported: Logging to file support has been added for all XMPro products, whereas Application Insights and Datadog support has been added for all products aside from Subscription Manager. These are cloud-based application monitoring and analytics services. Enhancement Standard logging has been improved across all products with the addition of more contextual information. Users can now access important details such as CompanyId, CompanyName, UserId, Username, and more, enabling efficient issue investigation. Enhancement We've changed the way our database installs and upgrades are applied. For new installs, our products will automatically install the required database changes. For upgrades, our products will detect what database changes are needed and make these. We are moving away from doing database installs and upgrades from the desktop installer, with all database installs and upgrades happening automatically from within the products. Accelerate time to value by choosing to automatically deploy the regular, smaller releases to your pre-prod environment, rather than less frequent, larger upgrades. Fix SignalR displayed errors in the browser console. These errors have been rectified. Fix An issue has been resolved where the on-premise installer failed to create an AWS package. Fix Access token vulnerabilities have been addressed. XMPro has masked logging of sensitive data like access_tokens used in SignalR connections to align with best practices. Fix When assigning Run Access to XMPro Objects, selection changes were lost if you scrolled out of sight (further down) in the user list. Selections are retained through scrolling and saved as expected. Fix The link in the Help Blades' \"Release Notes\" section did not direct users to the version number that matched the release version of XMPro when they tried to navigate to it. The link now directs the user to the documentation version number corresponding with the release version of XMPro."
  },
  "docs/release-notes/archived/v4.3.0/v4.2.1.html": {
    "href": "docs/release-notes/archived/v4.3.0/v4.2.1.html",
    "title": "v4.2.1 | XMPro",
    "summary": "v4.2.1 App Designer Change Type Description Enhancement Various application upgrades, including updates to the latest versions of the .NET and JavaScript web framework (Angular). Enhancement Improvements to Recommendation Alerts performance. Enhancement Improvements to Widgets performance. Additionally, the Widgets accordion is collapsed by default. Enhancement On auto-escalated alerts, a hyperlink has been added to the original alert. Fix Unable to add a variable name that had been used before. The error has been resolved. Fix The upgrade introduced an error on newly added checkboxes - the internal value changed when clicked but not the UI. When adding a Check Box Block to a page, the default state is now unchecked and the default label is \"Checkbox\". Fix If two users try to concurrently manage the run access of an application, an error is triggered for the user who saves their change last. The run access is updated in real-time for both sessions. Data Stream Designer Change Type Description Enhancement The new site setting, Enable Stream Metrics, allows you to toggle on and off the Stream Load and Stream Errors cards on the Stream Metrics panel. It applies to both the published Data Stream canvas and the list of Data Stream cards accessed from the landing page. While these two metrics are valuable during the development lifecycle, the toggle is useful as they do impact performance. Fix Unable to open the error flow canvas or import streams that use it. The Canvas now switches from the data flow to the error flow canvas, which contains the error agent. Fix The error displayed when Categories' stats on the Data Stream home page has been resolved."
  },
  "docs/release-notes/archived/v4.3.0/v4.2.2.html": {
    "href": "docs/release-notes/archived/v4.3.0/v4.2.2.html",
    "title": "v4.2.2 | XMPro",
    "summary": "v4.2.2 App Designer Change Type Description Fix The Data Grid's UI behaves correctly, but the internal value retains the last unselected row value when Allow Multiple Selection is enabled. Unselecting a row consistently removes it from the Block's value. Fix The Select Box search function ignores the configured filter when the user starts typing in the select box. The Data Source and Search filters are both applied. Common Change Type Description Fix Users are unable to upload an avatar to their profile. The issue has been fixed. Fix During upgrades and new installations: there was a warning that the default user did not have a business role. the silent renewal process fails for the default user and an expiry message is shown. The default user is added to the 'All Employees' business role during the installation. The warning has been removed because a user's business role is not mandatory. The default user's silent renewal occurs seamlessly in the background."
  },
  "docs/release-notes/archived/v4.3.0/v4.2.3.html": {
    "href": "docs/release-notes/archived/v4.3.0/v4.2.3.html",
    "title": "v4.2.3 | XMPro",
    "summary": "v4.2.3 App Designer Change Type Description Enhancement Performance of the Time Series Charts, when using the new TSC SQL Connector, has been significantly enhanced due to optimized client-side querying. Enhancement We've added a view of all the Data Stream Connections between an App and its related Data Streams. You can use this list to verify the current version of the Data Stream in use and make any necessary updates if required. Common Change Type Description Feature The new health check feature provides initial troubleshooting of the health of XMPro services. It displays the status of the XMPro services along with their ability to establish timely connections. It is also possible to add third-party endpoints to monitor all of your health checks in a central location. Enhancement The installation process of the XMPro application on Azure infrastructure has been improved: the application version number is now appended to the \"mainTemplate.json\" filename. For instance, the filename would be \"mainTemplate_4.2.3.json\" for version 4.2.3 of the application. Fix SSO logins post initial login fails for guest accounts. This issue has been resolved, including a change in the way the access token is passed for live connection subscribers."
  },
  "docs/release-notes/archived/v4.3.1.html": {
    "href": "docs/release-notes/archived/v4.3.1.html",
    "title": "v4.3.1 | XMPro",
    "summary": "v4.3.1 Common Change Type Description Feature Auto Scale, XMPro's implementation of caching has been overhauled with a distributed storage feature that makes use of multiple smaller cache entries. It offers a superior caching approach that is highly recommended, particularly for larger production-ready implementations. Fix Shared Sites showed the XMPro logo and later the company logo with slow internet. The company logo loads in every scenario unless one has not been configured, in which case the XMPro logo is used. App Designer Change Type Description Fix An issue has been resolved where you could only click on resources in the Time Series Chart hierarchy. Icons are included for the hierarchy as well as resources, allowing you to select all fields. Fix After opening Pass Page Parameter for a Box Hyperlink Block, all other blocks' properties and the side navigation blades were blank. The error has been resolved. Data Stream Designer Change Type Description Enhancement Various application upgrades, including updates to the latest versions of the .NET and JavaScript web framework (Angular)."
  },
  "docs/release-notes/archived/v4.3.10.html": {
    "href": "docs/release-notes/archived/v4.3.10.html",
    "title": "v4.3.10 | XMPro",
    "summary": "v4.3.10 App Designer Change Type Description Enhancement Search functionality was added to the Tree Grid Block. Fix When using a variable as a data source parameter, updates to that variable are not applied when refreshing the data source (introduced in v4.3.5). Variables as data source parameters are updated as expected. Fix Bold and Italic styling is not applied to Recommendation Alert Triage Instructions. The Bold and Italic styling is correctly applied. Data Stream Designer Change Type Description Enhancement Users able to delete agent versions see the total usage count. Previously, admins saw the total agent version usage, and everyone else saw the usage in data streams to which they had access. Now users with DeleteAgent rights can also see the total usage - empowering them to see which agent versions can be deleted. Fix The Collection view's 'Started On' Stream Metrics are correct when the page loads, but do not refresh if a Stream Host later goes online or offline. The 'Started On' Stream Metrics are updated in real-time."
  },
  "docs/release-notes/archived/v4.3.11.html": {
    "href": "docs/release-notes/archived/v4.3.11.html",
    "title": "v4.3.11 | XMPro",
    "summary": "v4.3.11 Common Change Type Description Enhancement Provide a visual representation of the XMPro product suite (each app service excluding Subscription Manager) to facilitate quicker analysis and the diagnosis of issues. The ARM deployment template includes a Log Analytics workspace, Application Insights, and an Availability Test (using the AD and DS health check endpoints). App Designer Change Type Description Enhancement As highlighted in v4.3.8, we can do better for data privacy by adding more stringent authorization controls to authenticated endpoints. In this release, access and product rights were added to low-priority recommendation, variable, and application endpoints. Fix The Unity Block does not show a data stream's real-time values updated on the model when using a connector with live updates. Live data is supported on the Unity block. Fix Non-admin user is unable to save, resolve, or send discussion messages on alerts to which they have run access (Error message: Sorry, something went wrong. Unable to send message.) Users with the View Recommendation Alert product access right and Run Access for an alert can interact with it. Fix Although Log Data On was set to 'All Occurrences', the Event Data was not updated if the Rule's Recurrence was 'First Occurrence'. With the same configuration, one alert is generated but the event data is updated with every occurrence that the rule logic is met. Fix Users able to delete connector versions see the total usage count. Previously, admins saw the total connector version usage, and everyone else saw the usage in apps to which they had access. Now users with DeleteConnector rights can also see the total usage - empowering them to see which connector versions can be deleted. Fix The Connector Usage Details report does not match the Connector Usage report when an Application has multiple versions. Application Version was added to the Connector Usage Details report and the total matches the summary report. XMPro Notebook Change Type Description Enhancement Added OpenCV Python to the default XMPro Notebook libraries."
  },
  "docs/release-notes/archived/v4.3.12.html": {
    "href": "docs/release-notes/archived/v4.3.12.html",
    "title": "v4.3.12 | XMPro",
    "summary": "v4.3.12 Common Change Type Description Update The supported operating system for Windows was updated to 11. App Designer Change Type Description Fix Attempting to edit or delete a row in the Tree Grid Block results in an error (Error message: Error reading JObject from JsonReader. Current JsonReader item is not an object: Integer.) This error has been resolved."
  },
  "docs/release-notes/archived/v4.3.2.html": {
    "href": "docs/release-notes/archived/v4.3.2.html",
    "title": "v4.3.2 | XMPro",
    "summary": "v4.3.2 App Designer Change Type Description Feature New Blocks added to make your interactions with Recommendation Alerts composable: Recommendation Analytics - reuse this handy analytics section for a quick view of the percentage change, and alerts generated per rule for your asset for a period of time. Recommendation Alert Discussion - reuse this powerful discussion section to unlock team collaboration on individual Recommendation Alerts. We've added an optional summary of all alert discussions for an Entity Identifier. Enhancement Finer-grained product rights around uploading and deleting App Files have been added to App Designer: UploadAppFile and DeleteAppFile. Fix Design User couldn't Add New Connection. A new permission has been created for managing connections, and associated with the appropriate user types. Fix After updating the recommendation factor, the Recommendation blade becomes stuck loading. The error has been resolved. Fix An error is triggered when version 1.07 of the DS Connector is absent from the company. The error has been resolved. Fix Updating an optional factor does not prompt a reason for the change. The error has been resolved. Fix The recommendation factor is not included in the export and import of a recommendation. The error has been resolved. Fix There is an error occurring when attempting to delete variables. The error has been resolved. Fix Selecting an application version, highlight all version rows. The error has been resolved. Fix The alert tagging feature is rendering incorrectly, leading to significant white spaces and layout issues when utilized. The error has been resolved. Data Stream Designer Change Type Description Fix Intermittent collection-based datastream initiation issues during publication. The error has been resolved."
  },
  "docs/release-notes/archived/v4.3.3.html": {
    "href": "docs/release-notes/archived/v4.3.3.html",
    "title": "v4.3.3 | XMPro",
    "summary": "v4.3.3 App Designer Change Type Description Fix Editing or deleting a widget results in the page becoming unresponsive. The error has been resolved. Fix After generating a new Widget with a personalized icon, all subsequent Widgets that were created would adopt the personalized icon of the initial Widget. The error has been resolved. Fix Unpublishing an application did not prevent access to the application when navigating to the URL directly. The error has been resolved. Fix Intermittently, when clicking on the landing page, the last accessed application is reloaded instead. Reproduce this error on slow internet connections or access the landing page while another application page is busy loading. The landing page consistently opens as expected. Data Stream Designer Change Type Description Fix The error filter on a category's Data Streams was giving incorrect results - skipping those where at least one Stream Host was not online. All filters on the category's Data Streams view are accurate. Fix The categories in the Data Stream's canvas toolbox were in the incorrect order. The toolbox categories are ordered correctly. Fix The settings icon did not match that used in other XMPro products. The settings icon is standard across the XMPro product suite. Subscription Manager Change Type Description Fix Some users encountered an inability to progress on the 'Link Account' page, while Single Sign-On (SSO) validation appeared to be functioning correctly. The error has been resolved."
  },
  "docs/release-notes/archived/v4.3.4.html": {
    "href": "docs/release-notes/archived/v4.3.4.html",
    "title": "v4.3.4 | XMPro",
    "summary": "v4.3.4 Common Change Type Description Enhancement Updated Serilog to the latest stable version. Stream Hosts Change Type Description Enhancement Logging provider support via Serilog, a diagnostic logging library, has been added to Stream Hosts to provide administrators with valuable insights into the behavior and performance of XMPro. App Designer Change Type Description Fix Block styling to achieve rounded corners (using Border Radius properties) is ignored at runtime to ensure the child elements are visible. The Overflow property was added to all Visual and Recommendation Blocks. Designers can determine whether block styling covers the corners of child elements (hidden) or not (visible). Fix The Indicator Block color can be set using a static value, but using a quick expression has no effect at runtime. As a workaround, set a variable using the expression and bind the variable to the Color property. The Indicator Block color can be set using a static value or a quick expression. Fix Although users can view all the alerts displayed by a Recommendation Block, they can only drill down into the details of alerts to which they have run access. The Recommendation Block displays the alerts to which the user has run access. Data Stream Designer Change Type Description Fix The Notes text area of an Agent on the canvas is not editable until selecting a font option, such as bold or underline. You can start typing in the Notes text area when you select it. Subscription Manager Change Type Description Fix Product Right's blade is stuck loading for guest tenant users. The error has been resolved for guest tenant users and anyone with an extra-long username. Fix ARM template's subscription blade is stuck loading for Global Admin (not Company Admin). Subscriptions load for all admin users so the existing licenses can be applied during the installation."
  },
  "docs/release-notes/archived/v4.3.5.html": {
    "href": "docs/release-notes/archived/v4.3.5.html",
    "title": "v4.3.5 | XMPro",
    "summary": "v4.3.5 App Designer Change Type Description Enhancement Additional parameters have been added to the Connector's Subscribe method, used to implement live updates on data sources. Connector creators can implement the same pre-defined filtering and sorting applied when an App Page is refreshed. Enhancement Pagination was added to the Data Streams Connector to optimize retrieving large datasets. Subscription Manager Change Type Description Fix Error when attempting to apply notebook license for an existing company. The default product role for XMPro Notebook was renamed from DesignUser to Admin as Subscription Manager requires at least one Admin role per product. Stream Hosts Change Type Description Enhancement Agent lifecycle events have been added to the standard logging: when they started processing input, how long it took to complete the process, more context as well as exceptions. Fix The Windows x64 Stream Host application was missing an icon under Programs. Added the XMPro icon to the Stream Host Windows Installer."
  },
  "docs/release-notes/archived/v4.3.6.html": {
    "href": "docs/release-notes/archived/v4.3.6.html",
    "title": "v4.3.6 | XMPro",
    "summary": "v4.3.6 Data Stream Designer Change Type Description Enhancement Over time, if users do not close the Live View, these open connections can place additional load on Data Stream Designer (DS) as the Stream Hosts continue to send live data back to DS. This may reduce overall performance and reliability, and increase infrastructure costs. With the new Live View Usage setting, admins can view usage and force a reset that closes all open Live View connections. App Designer Change Type Description Fix App Pages that include Blocks configured using variables, such as Esri Maps, fail to load for users without the Manage Variables product access right. App Pages now load for users with run access to the App. Fix The 'Replace cache' option on the XMPro App Action Agent works correctly for single records but is inconsistent for batched data. Each batch of data is now displayed in App Designer and reloading always shows the last batch of data. Stream Hosts Change Type Description Enhancement Introduced a consistent context data structure for logged events that can be searched and filtered to enable successful and reliable monitoring and reporting on Data Streams and Agents in tools such as Application Insights. Enhancement We've added a new feature flag to enable Application Insights with Telemetry as a logging option for Stream Hosts."
  },
  "docs/release-notes/archived/v4.3.7.html": {
    "href": "docs/release-notes/archived/v4.3.7.html",
    "title": "v4.3.7 | XMPro",
    "summary": "v4.3.7 Common Change Type Description Feature A new reports section under site settings empowers administrators to examine aspects of Connector and Agent usage. Enhancement As promised, the Application Insights with Telemetry logging option introduced last release for Stream Hosts is now available for all products except Subscription Manager. App Designer Change Type Description Fix No line is rendered on the Time Series Chart when an attribute returns a zero value. Every value appears on a Time Series Chart, including zero. Data Stream Designer Change Type Description Enhancement The Polling Agent's default interval increased from 10 seconds to a sensor-appropriate value of one hour (3600 seconds). Administrators can set this value using the new site setting Default Polling Interval (seconds). Fix The Live View issue is addressed in the v4.3.7 release: 1. Open connections are closed regardless of how the Live View is closed (e.g. navigating away or closing the tab). 2. All connections are closed when the Data Stream Designer app service is restarted. If you've upgraded to v4.3.7, use the Reset Live View button once to ensure all connections are closed (added in v4.3.6). Stream Hosts Change Type Description Fix Azure SQL and SQL Server Agents were unable to run on an Ubuntu Stream Host. This error has been resolved. Refer to the Agent documentation for information on the Stream Host version pre-requisite."
  },
  "docs/release-notes/archived/v4.3.8.html": {
    "href": "docs/release-notes/archived/v4.3.8.html",
    "title": "v4.3.8 | XMPro",
    "summary": "v4.3.8 App Designer Change Type Description Enhancement Although our endpoints are authenticated, we can do better for data privacy by adding more stringent authorization controls. These will be rolled out over the subsequent few releases. In this release, access and product rights were added to high-priority recommendation endpoints. Data Stream Designer Change Type Description Enhancement The list of Stream Hosts for a Collection is sorted alphabetically. Fix Republishing a recurring Data Stream where the Start Repeat On date is now in the past does not work as expected. When the Start Repeat On date is in the past, treat the recurrence as if Start Repeat was set to Immediately. Fix The Calculated Field Agent's 'Expression Text Area' is not maximized when the blade is maximized. For all Agents that utilize a Script Box in their configuration, the editable area is expanded when the configuration blade is maximized. Fix The Data Stream's Help toolbar button resulted in a 'page not found' error. The Help toolbar button points here."
  },
  "docs/release-notes/archived/v4.3.9.html": {
    "href": "docs/release-notes/archived/v4.3.9.html",
    "title": "v4.3.9 | XMPro",
    "summary": "v4.3.9 App Designer Change Type Description Enhancement As highlighted in v4.3.8, we can do better for data privacy by adding more stringent authorization controls to authenticated endpoints. In this release, access and product rights were added to medium-priority recommendation endpoints. Data Stream Designer Change Type Description Enhancement The bulk Agent upload has a size limit of 100mb. We've added the individual uncompressed agent file size as well as the total file size for added clarity on why an upload may fail. Fix The Stream Load and Stream Errors cards on the Stream Metrics panel are always zero (introduced when the feature flag was renamed in 4.3.0). The correct Stream Load and Stream Errors values are displayed. Fix Only the first 39 events defined for Calculated Field Agent are displayed in the Event Definitions list. For all Agents that utilize an Edit List in their configuration, such as the Calculated Field Agent and the Event Simulator, there is no limit to the number of items displayed. Stream Hosts Change Type Description Feature We've added a new feature flag, Enable Luigi Preview, to allow beta testing of the new Stream Host core. Fix Fix app settings loading error. Common Change Type Description Fix Updated the installation software requirements to clarify that the hosting bundle should be used for on prem. Fix Fixed a typo on the AWS Installer package creation for App Secrets."
  },
  "docs/release-notes/index.html": {
    "href": "docs/release-notes/index.html",
    "title": "Release Notes | XMPro",
    "summary": "Release Notes This section contains the release notes for XMPro, organized by version. The release notes provide information about new features, improvements, and bug fixes in each release. Latest Release v4.4.18 - Latest release with new features and improvements Previous Releases v4.4.17 v4.4.16 v4.4.15 v4.4.14 v4.4.13 v4.4.12 v4.4.11 v4.4.10 v4.4.9 v4.4.8 v4.4.7 v4.4.6 v4.4.5 v4.4.4 v4.4.3 v4.4.2 v4.4.1 v4.4.0 Archived Releases For older releases, please see the archived release notes."
  },
  "docs/release-notes/v4.4.0.html": {
    "href": "docs/release-notes/v4.4.0.html",
    "title": "v4.4.0 | XMPro",
    "summary": "v4.4.0 Warning We strongly recommend that all Stream Host installations are upgraded to this version and that the feature flag 'Enable Luigi Preview', is enabled to leverage the enhanced capabilities fully. If you upgrade to the XMPro Product Suite v4.4.0 but continue using an older Stream Host (v4.3.12 or earlier), you will have a degraded experience because the Stream Host logs will not be visible in Data Stream Designer. This applies to both Stream-specific logs and Collection Logs in general. Integrations Change Type Name Description New Azure OpenAI v1.02 Prompt your model with real-time sensor values. New Fixed Width File Reader v1.0 Process data from fixed-width text files in your data stream. New OPC DA Listener and Action Agent Read and publish tag values to an OPC DA Server in your data stream. Enhancement Azure Digital Twin Listener v1.08 Query support added. Useful for splitting a model into submodels and joining separate queries if you've reached the DTDL limit of 300 properties. Enhancement Azure IoT Hub Listener v3.06 Support Array Values visualization in the Live View. Enhancement Calculated Field Transformation v3.44 Added error handling to prevent circular dependencies in expressions. Do not use - compatible with Stream Host v4.4.0 and v4.4.1 only. Enhancement OPC DA Listener v1.45 Support including the tag value's quality in your output. Enhancement OSIsoft PI Connector v1.13 Added rounding support and error handling when no results are returned. Enhancement Salesforce Context Provider v3.00 Added support for filtering on dates, escape characters, result count, and selecting which columns to return. Enhancement Row Count Transformation v1.12 Added group and sort of data and assign distinct row numbers to each group. Enhancement XML File Reader Action Agent v1.18 Added support to batch process multiple files. Common Change Type Description Enhancement We've listened to your feedback and made several usability enhancements to file keys and dashboard tags: Introduced an eye on all import and export functions: if you click on the file key eye, what you have typed will show up to enable you to check the typing of your file key. Improved how multiple tags are shown on a card: the ellipsis was replaced with a horizontal scrollbar and the white space was reduced. Global search: tags were added as an option and can be searched too. Category Filter: your tag selection is persisted as you open and close Apps and Data Streams, so that you don't need to reapply your filter Enhancement Various application upgrades: Upgrade XMPro Notebook, Application Designer, Data Stream Designer, and Stream Hosts to .NET 8. Enable http2 on Azure ARM deployments Upgrade SignalR packages to latest compatible versions Fix WebSocket connection errors in some Apps in some environments. Intermittent SignalR connection failures due to long query strings or incompatible client/server versions were resolved. Fix Selecting the Network Service account during an On-Premise installation causes app files to fail and the inability to write logs to the Application Directory. The On-Premise installation documentation now advises to select \"Local System\" when opting for a built-in security account. XMPro AI and Notebook Change Type Description Feature XMPro AI and Notebook are now available to deploy via an XMPro-assisted ARM template. Enhancement Upgrade the XMPro Notebook's default Python library version from 3.8 to 3.10. App Designer Change Type Description Feature The new feature, Metablocks, is the first step towards plug-and-play Blocks. The first two Metablocks, Unity and Unity (Legacy), demonstrate how the modular approach improves performance. It opens up the ability to support different web technologies. Feature New Blocks added: Tree Map - visualize hierarchical data as well as the order of magnitude, for example, to create a quick view of high-volume situations. Live Feed - incorporate your IP Live Feed camera into an application alongside key statistics related to the area under surveillance. Enhancement As highlighted in v4.3.8, we can do better for data privacy by adding more stringent authorization controls to authenticated endpoints. In this release, access and product rights were added to the final batch of endpoints: low-priority recommendation, application, connector, and integration endpoints. Enhancement We've listened to your feedback and made several usability enhancements to the App Designer Canvas: Block Toolbar: tooltips were added to highlight the button functions. Block Styling: advanced options - rarely used except by advanced users - were moved to a new Advanced Styling accordion to simplify your styling choices. Block Styling: Font Style was added to Typography, adding support for italic and oblique text. Page Layers: added a horizontal scrollbar for when there are multiple nested layers of components on a page. Blocks tab: show 3 tiles per row so that it is quicker to locate a Block. Dynamic and expression block properties: added a Full Name property to the User Details - and renamed User to Username for increased clarity. Data Streams Connector: prefix the entity with the data source version so that you don't have to open and scroll to verify the correct version is in use. Enhancement These Tree Grid Block navigation and selection features were added: 'Expand All Rows By Default' property, is useful when you have a small dataset. 'Allow Multiple Select' property, as per the Data Grid. Node run-time context menu to expand and collapse its children. Enhancement ViewAppBar right, added in v4.1.0, is now included in all default App Designer roles. Global admins have the option to customize the product roles for specific implementations. Enhancement Append the underlying connector integration error to the \"Unable to get integration details\" error message, to allow for effective troubleshooting. For example, a 500 error may indicate an unsupported Azure Digital Twin configuration. Enhancement The option to export to Excel was added to the Recommendation Alerts grid. Fix Non-Admin users encountered an error ('Unable to save notification subscription changes') when subscribing to recommendation notifications. Non-Admin Users can now amend their Notification Settings. Fix The category filter's checkboxes no longer overlap the text. Fix Clicking the Site Setting Report carats opens an empty blade. Users can click on the carat or the report name to open it. Fix The Azure Digital Twin Connector's 'Query Text Area' is not maximized when the blade is maximized. For all Connectors that utilize a Script Box in their configuration, the editable area is expanded when the configuration blade is maximized. Fix A Connector was accidentally written that included credentials in an error message, which was displayed to a standard User when they encountered the error while running an App that used the Connector. Connector error messages are available in the logs to those with appropriate access, such as administrators, and a generic error message is reported to end users. Fix Tooltips on Button Blocks are now shown at runtime. Fix Font selection is ignored on the Data and Tree Grid Blocks at runtime. Font Block Styling is applied to the Data Grid, Tree Grid, and Tree List at runtime. Data Stream Designer Change Type Description Enhancement We've listened to your feedback and made several usability enhancements to the Data Stream Designer's Toolbox: Collapse the Toolbox when you're not using it to increase the canvas size. The Category header remains sticky as you scroll down so that you don't lose context and access the accordion to collapse the category. Enhancement The IPollingAgent interface was changed from required to recommended with the addition of a RequiresPolling property. This gives greater flexibility as typically non-polling agents can also poll. Enhancement The Adding an Agent hint was reworded for clarity to 'Max file size is 100 Mb. For a bulk upload, the limit applies to the unzipped size'. Enhancement The Agent's TokenBox behavior changed from a postback per selection of an item to when the selection is complete, i.e. focus is lost on the property. This is helpful when a designer needs to select multiple items and previously would have had to wait for the blade to reload each time. Fix Clicking the Site Setting Report carats opens an empty blade. Users can click on the carat or the report name to open it. Fix Closing a Data Stream from My Sandbox navigates back to the landing page. Closing a Data Stream navigates back to the previously selected category. Fix My Sandbox category tile appears on the home page despite containing no data streams. The 'My Sandbox' category tile is shown either if it contains data streams or if there are no data streams in other categories. Subscription Manager Change Type Description Feature Global Administrators can display a global notification across the XMPro suite for a specific period. This aids in communicating important information to users, such as planned maintenance downtime along with a hyperlink to release notes. Choose a type of hint, warning, or error to set the notification icon and banner color. The banner can be dismissed for a session. Fix An investigation into the progressive slow performance of SM revealed a correlation with an increasing number of memory handles. Notably, the number of handles consistently rises over time and only resets with a restart of the application process. The Subscription Manager memory leak issue has been addressed through optimized usage of the custom Identity Server ViewService. If you have noticed these symptoms, monitoring of memory, handles, and page response times is essential until Subscription Manager can be upgraded to v4.4.0 to resolve the issue. Fix The welcome email used the subject 'Welcome to XMPro'. All SM email notifications implement the template's title tag and only use 'Welcome to XMPro' if none was supplied in the template. Fix Users were advised their password reset had expired after they had entered a new password and clicked 'Update Password'. Users are advised their reset password link has expired when the reset password page loads. Fix The (email) SM Access Request Link redirects to App Designer if the user is already registered to App Designer. The SM access request link was suffixed with '/Home/Admin' to prevent unintended redirects. Fix The welcome email includes incorrect login details for those who sign up using SSO. The username provided is a generated username like user_domain_com#ext#mail.company, whereas the user should use their SSO email. The welcome email for users who sign up using SSO contains the correct login details. Stream Host Change Type Description Feature Stream Host optimizations are complete, ensuring more reliable orchestration of data streams and interactions such as publish, unpublish, sync with Data Stream Designer, and handling network disconnections. Although these improvements apply to both the rewritten core (Luigi) and the legacy core, to maximize the benefits of these optimizations, we highly recommend transitioning to the Luigi core. The Luigi core has transitioned out of its beta phase (v4.3.9) and is now the recommended Stream Host core as of this release (v4.4.0). Additionally, the Stream Host Logging to File section has been updated to accommodate changes in specifying the destination for log files. Enhancement The console output is colorized and formatted for readability, as well as more detail."
  },
  "docs/release-notes/v4.4.1.html": {
    "href": "docs/release-notes/v4.4.1.html",
    "title": "v4.4.1 | XMPro",
    "summary": "v4.4.1 Integrations Change Type Name Description New REST API Connector v1.0 Bring data from REST API into your App. New XMQ Listener v1.0 XMQ Action Agent v1.0 Lightweight, self-healing, frictionless messaging Agents. New Coupa Context Provider v1.0 Coupa Action Agent v2.0 Read and update Coupa invoices from within your Data Stream. Enhancement OPC DA Listener v1.46 OPC DA Action Agent v1.33 Support variables and standardize property names. Enhancement Window Transformation v3.08 Support combining the count and delay buffer options into Count with Delay. App Designer Change Type Description Feature New Blocks added: Visual Media Capture - enrich your events with an image or video captured using your device input. Location Capture - capture the precise geographical coordinates of an asset or incident using your device. ChatGPT Copilot - bring ChatGPT into your App. Azure Copilot - bring Azure's OpenAI into your App. Enhancement Recommendation names are auto-populated during import (when exported using v4.4.1+). Enhancement When using the TSC SQL Connector, the Time Series Chart's performance is enhanced with optimized client-side querying (v4.3.2). The new ITSCConnector interface instructs the Time Series Chart to use optimized client-side querying. Use it when building new TSC Connectors that pre-process large volumes of data and return it in buckets. Fix When using the Image Map, clicking on a hyperlink in either Dynamic or Static content does not work - the navigation on action is not working. I can navigate to the configured action for both Dynamic and Static content in an Image Map. Note that the Dynamic Template's Data Source and Data must be configured for dynamic content to appear at runtime. Data Stream Designer Change Type Description Enhancement Data Stream names are auto-populated during import (when exported using v4.4.1+). Stream Host Change Type Description Enhancement The \"Enable Luigi Preview\" feature flag, introduced in v4.3.9 for beta testing of the new Stream Host core, has been replaced with the \"Enable Legacy Core\" feature flag. The new, improved Stream Host core is now the default in all new installations, in line with the recommendation made in v4.4.0. Please reach out should you want to revert to the old core as we'd like to understand why and guide you through the process."
  },
  "docs/release-notes/v4.4.10.html": {
    "href": "docs/release-notes/v4.4.10.html",
    "title": "v4.4.10 | XMPro",
    "summary": "v4.4.10 Integrations Note Security update: Mitigate a high-severity vulnerability by upgrading Azure SQL and SQL Server Agents. Note: This update is incompatible with Stream Host versions prior to 4.3.7. Change Type Name Description Security Azure SQL Listener v5.50 Azure SQL Context Provider v5.50 Azure SQL Action Agent v5.50 Applied the CVE-2024-0056 security vulnerability update Security SQL Server Listener v5.50 SQL Server Context Provider v5.50 SQL Server Action Agent v5.50 Applied the CVE-2024-0056 security vulnerability update Enhancement MQTT Listener v3.02 MQTT Action Agent v3.02 Support status sending to the broker so you can monitor whether the Agent is online. Enhancement OSIsoft PI Connector v2.24 Improve performance. Support clearing metadata cache. Enhancement OSIsoft PI Listener v4.04 Improve performance. Support configurable element cache duration. Fix exclusion condition issue. Enhancement OSIsoft PI Action Agent v3.10 Improve performance. Fix variables validation error. Fix Azure Data Explorer Action Agent v1.05 Added DLL to resolve an error when publishing on a Stream Host v4.4+ Fix Azure Data Factory Action Agent v1.10 Added DLL to resolve an error when publishing on a Stream Host v4.4+ Fix Azure Digital Twin Action Agent v1.21 Added DLLs to resolve an error when publishing on a Stream Host v4.4+ Retired Azure Time Series Listener Microsoft retired this service in July 2024 App Designer Change Type Description Enhancement We've added the Application name as a hover tooltip to the Application listing so that you don't have to expand the blade to see the longer names. Fix I selected 'None' in the new Alert Discussion Block, but it reverts to 'Entity'. You can select 'None' as the type in the Alert Discussion Block, so that there is no header grid. Fix I've ticked to Group By Asset in a Time Series Analysis Block, but I can't choose an Asset Id Expression because the dropdown is empty. The Asset Id Expression dropdown is populated, but there will be a follow up fix in a future release because when a value is later selected for the secondary data source, the dropdown value will appear empty. This is a UX issue only - your value is still present and the block will work as expected. Fix My Free Trial's demo App, Renewable Condition Monitoring, is not displaying any alerts, but if I navigate to the recommendations menu I can see alerts have been generated. New Free Trials are correct. Prior to v4.4.10, please correct the configuration: Edit the demo app, Renewable Condition Monitoring Select the Alert List Block (previously known as Recommendations) Click Block Properties and expand the Behavior accordion Tick the Oil Recommendation Save your change Fig 1: App Designer's Free Trial configuration fix Data Stream Designer Change Type Description Fix In v4.4.9, I opened a data stream with multiple versions and noticed the first version opened and not the latest. Opening a data stream shows the latest (highest number) version. This and related functionality have also been optimized for a faster experience."
  },
  "docs/release-notes/v4.4.11.html": {
    "href": "docs/release-notes/v4.4.11.html",
    "title": "v4.4.11 | XMPro",
    "summary": "v4.4.11 Integrations Note Security update: Mitigate a high-severity vulnerability by upgrading Azure SQL and SQL Connectors. Change Type Name Description Security Azure SQL Connector v2.00 Applied the CVE-2024-0056 security vulnerability update Security SQL Server Connector v2.00 Applied the CVE-2024-0056 security vulnerability update Enhancement Neo4J Connector v1.02 Support CRUD operations on tables Support Live Updates Enhancement OSIsoft PI Connector v2.26 Added error logging Fixed ReturnAll error Enhancement REST API Context Provider v3.13 Support nested JArray response. Support auth token via another REST call. Enhancement REST API Action Agent v3.38 Support nested Jarray response. Support auth token via another REST call. Support entering parameters manually. Enhancement Twilio Action Agent v2.15 Support a message prefix - useful to differentiate between environments Fix MongoDB Listener v1.15 MongoDB Context Provider v1.15 MongoDB Action Agent v1.15 Updated a reference to resolve an error on Stream Host v4.4+ Common Change Type Description Fix I ran the on-prem installers for App Designer and Data Stream Designer using the Installation Profile, but an \"invalid_grant\" error is thrown. This error, introduced with v4.4.7's package upgrades, has been resolved. App Designer Change Type Description Enhancement Favorite your commonly used Blocks to access them more easily from a new Block category, Favorites. Enhancement The Metablock supports Value Mappings so that static, data source of expression values can be passed into the Metablock on page load. Fix I tried splitting my script into 3 and using the new Metablock, but my App is throwing 500 errors for the styling and script files. The single script file works as expected. The Metablock handles CSS and JavaScript injection at runtime, so explicit import code isn’t needed and will result in the 500 error. See here for more. Fix I created a new expression on my new App. I did something else and when I returned to Page Data, the expression was missing. I tried to add it again, but observed an error that the name was in use. The display has been corrected so that newly created expressions remain on the list of data source columns until deleted. Fix I opened Manage Access on my App to grant Design Access to one user, but clicking one user selects them all. Clicking an individual user only selects that user and no others. Fix I clicked Edit Templates after adding a new notification to a Recommendation Rule, but clicking one template selects them all. Clicking an individual template only selects that one and no others. Data Stream Designer Change Type Description Fix I cannot upload a bulk agent file even though it meets the requirements - the compressed size is 29MB and the pre-compression contents are 90MB. The internal API limit was increased to the bulk Agent file limit of 100MB."
  },
  "docs/release-notes/v4.4.12.html": {
    "href": "docs/release-notes/v4.4.12.html",
    "title": "v4.4.12 | XMPro",
    "summary": "v4.4.12 Integrations Change Type Name Description Enhancement Calculated Field v3.46 Support processing records separately so that an error doesn't stop the execution of a batch. Enhancement MQTT Listener v3.04 MQTT Action Agent v3.04 Support message correlation added for parallel processing and synchronization. Enhancement Ethereum Smart Contract Listener v1.05 Ethereum Smart Contract Action Agent v1.04 Added variable support. Support uint256 data type. Update packages and frameworks. Enhancement TSA SQL Server Connector v2.0 Implement the new ITSAConnector interface and naming aligned to the Time Series Analysis block. Security TSC SQL Server Connector v1.60(deprecated) Applied the CVE-2024-0056 security vulnerability update. Common Change Type Description Enhancement The user profile view shows the language name rather than the two character Isocode. App Designer Change Type Description Enhancement Introduced the ITSAConnector interface (to reflect the Time Series Analysis Block renamed in v4.4.9) and deprecated the ITSCConnector interface. New Connectors have been released for use from v4.4.12+, but the older ones using the deprecated interface are still compatible. Fix I archived an Alert that had an associated Form with control values entered, but I noticed the control values weren't cleared. Control values are removed when an alert is archived. Fix I archived an Alert with an associated Form from the Recommendations grid, but I noticed the 'ArchivedForm' value is NULL. The Form is added to the Archived Form' as a JSON when an Alert with an associated Form is archived. Fix The Metablock's border - introduced in v4.4.8 - has been removed. Stream Host Change Type Description Feature Support a zipped file to be packaged in an Agent."
  },
  "docs/release-notes/v4.4.13.html": {
    "href": "docs/release-notes/v4.4.13.html",
    "title": "v4.4.13 | XMPro",
    "summary": "v4.4.13 Common Change Type Description Patch The v4.4.13 release includes a fix for an unreleased feature introduced in v4.4.12, which does not impact any current customer functionality."
  },
  "docs/release-notes/v4.4.14.html": {
    "href": "docs/release-notes/v4.4.14.html",
    "title": "v4.4.14 | XMPro",
    "summary": "v4.4.14 Integrations Note Although Run Recommendation v3.00 is backward compatible with App Designer, it was not compatible with Stream Host pre v4.4.2. This is rectified by Run Recommendation v3.03. Change Type Name Description Feature Meta Action Agent v1.00 Our new Meta Agent allows you to run scripts on input values and use the results in your data stream. Similar to the Metablock, the scripts can be written in languages other than C# and leverage 3rd party libraries. Enhancement MySQL Listener v2.00 MySQL Context Provider v2.00 MySQL Action Agent v2.00 Updated to latest NuGet package. We recommend upgrading your MySQL server to the latest version in response to medium CVE-2023-3817 security update. Enhancement Run Recommendation v3.03 Added Rule Id to the output payload. Enhancement TSA Azure Data Explorer Connector v1.10 Implement the new ITSAConnector interface and naming aligned to the Time Series Analysis block. Common Note Security update: Mitigate high-severity vulnerabilities by upgrading AD, AI, and DS. Change Type Description Security Various NuGet and NPM packages upgraded for AD, AI and DS: Upgraded database access packages (System.Data.SqlClient, Microsoft.Data.SqlClient) to address security vulnerabilities Updated Azure.Identity package to latest secure version Updated Microsoft.VisualStudio.Web.CodeGeneration.Design to resolve NuGet package vulnerabilities Enhancement We're improving our multilingual capabilities: Your preferred language can be entered at signup, rather than once you've logged in for the first time. The signed out pages, where your preferred language is not known, are now available in Brazilian Portuguese, with other languages to follow. Subscription Manager is now available in Brazilian Portuguese, with other languages to follow. App Designer Change Type Description Fix Several issues were corrected on the Metablock: The default functions (onDataLoaded, onDataChanged, and onValueMappingLoaded) did not execute when added to a script tag on the HTML page. The Value Mapping was being called twice. The Value Mapping was not optional Fix I created a new Recommendation, but when I opened the Version blade I noticed the title was blank. The title of a new Recommendation default to \"Initial Version\". Fix I am the owner of a Recommendation, but I cannot view it's timeline - yet my colleague with Run Access can. Users with design access are able to view a Recommendation's timeline. Stream Host Change Type Description Fix I accidentally used the incorrect password on an MQTT agent and published the data stream. The stream did not start due to the connection error on the agent, but I noticed the agent continues attempting to connect to the MQTT broker. The Stream Host ensures that if a data stream fails to start or is unpublished, all of that stream's agents (whose Create() methods have already run) are destroyed."
  },
  "docs/release-notes/v4.4.15.html": {
    "href": "docs/release-notes/v4.4.15.html",
    "title": "v4.4.15 | XMPro",
    "summary": "v4.4.15 Integrations Change Type Name Description Fix Erbessd Listener v3.40 Fixed an output payload error when transpose data to columns is ticked without specifying an axis. Fix Event Simulator v1.41 Added error handling to prevent crashing the Stream Host if any other Agent in the same Data Stream fails to start while publishing the Data Stream. App Designer Change Type Description Fix The Metablock usually works as expected, but in one of my App Pages it loads without the CSS styling. The CSS styling is now applied after the iframe is loaded to ensure the page loads correctly - even under slow loading conditions."
  },
  "docs/release-notes/v4.4.16.html": {
    "href": "docs/release-notes/v4.4.16.html",
    "title": "v4.4.16 | XMPro",
    "summary": "v4.4.16 Integrations Change Type Name Description Enhancement Meta Action Agent v1.10 Breaking change to support Git Repo, self-packaging, log outputs, batch processing/output. Enhancement REST API Connector v1.01 Support content type application/x-www-form-urlencoded, commonly used in Azure for OAuth2/token authentication requests. Common Note Security update: Mitigate a high-severity vulnerability by upgrading AD. Change Type Description Security As part of our ongoing commitment to product security and stability, we've implemented several improvements in this release: Enhanced data validation to reinforce our security framework to mitigate the CWE-89 security vulnerability update Updated third-party dependencies to maintain optimal security standards These proactive updates are part of our regular security maintenance process, ensuring our products continue to meet high security standards. App Designer Change Type Description Documented Added an authenticated Autodesk Platform Services (APS, formerly Forge) Viewer example for the Metablock. Data Stream Designer Change Type Description Enhancement The Data Stream timeline has been refreshed so that you can easily identify which version was published, group canvas changes, and filter updates for a specific version. Fix Although no logs are displayed, a user without ViewHostLogs permission can see and click the (Stream Host) Logs button. Users without ViewHostLogs permission do not see the Logs button. Subscription Manager Change Type Description Fix In v4.4.15, I cannot create an account without a company invite because the preferred language dropdown options are cleared once I enter a company name. New accounts can be created from the signup link or a company invite."
  },
  "docs/release-notes/v4.4.17.html": {
    "href": "docs/release-notes/v4.4.17.html",
    "title": "v4.4.17 | XMPro",
    "summary": "v4.4.17 Common Note Security update: Mitigate a high-severity vulnerability by upgrading all products. Change Type Description Security As part of our ongoing commitment to product security and stability, we've implemented several improvements in this release: Mitigated high-severity CVE-2023-36414 in Subscription Manager by upgrading Azure.Identity. Mitigated high-severity CVE-2024-30105 in all products by upgrading System.Text.Json to v8.04. Translation The signed-out pages and Subscription Manager are now available in Spanish. Integrations Change Type Name Description Enhancement Meta Action Agent v1.12 Standardized the output payload property name and added guardrail to avoid incorrect setup. Replaced the logging endpoint with logging to the Stream Host for centralized monitoring. The following Agents were repackaged to translate the configuration properties: Change Type Name Translation Azure Data Explorer Listener v1.16 Azure Data Explorer Context Provider v1.14 Azure Data Explorer Reader v1.10 Azure Data Explorer Action Agent v1.06 Translation Azure Data Factory Action Agent v1.11 Translation Azure Data Lake Action Agent v1.21 Translation Azure Digital Twin Listener v1.10 Azure Digital Twin Context Provider v1.06 Azure Digital Twin Action Agent v1.22 Translation Azure Event Hub Listener v3.09 Azure Event Hub Action Agent v3.09 Translation Azure IoT Hub Listener v3.07 Translation Cognite Listener v1.22 Cognite Context Provider v1.32 Cognite Action Agent v1.02 Translation Email Listener v3.26 Email Action Agent v4.30 Email Download Action Agent v1.07 Translation Ethereum Smart Contract Listener v1.06 Ethereum Smart Contract Action Agent v1.05 Translation Excel File Reader Action Agent v1.13 Translation Litmus Edge OPC UA Listener v1.02 Litmus Edge OPC UA Action Agent v1.02 Translation MongoDB Listener v1.16 MongoDB Context Provider v1.16 MongoDB Action Agent v1.16 Translation MOVUS Alarms Listener v1.62 MOVUS Device Samples Listener v1.92 MOVUS Device Samples Context Provider v1.92 MOVUS Devices Context Provider v1.42 MOVUS Events Action Agent v1.02 Translation MySQL Listener v2.01 MySQL Context Provider v2.01 MySQL Action Agent v2.01 Translation Neo4j Listener v1.02 Neo4j Context Provider v1.02 Neo4j Action Agent v1.02 Translation OPC DA Listener v1.47 OPC DA Action Agent v1.34 Translation OPC UA Listener v4.03 OPC UA Action Agent v1.03 Translation Oracle Server Action Agent v1.02 Translation Snowflake Listener v1.01 Snowflake Context Provider v1.01 Snowflake Action Agent v1.04 Translation Twilio SMS Action Agent v2.16 Package Manager v1.3.18 Change Type Description Feature The ability to translate Agent and Connector properties into all languages supported by XMPro. You can use Azure OpenAI, or edit the JSON file. Feature The ability to import a JSON file, allowing changes to be made outside of Package Manager, and then imported to package an XMP file. Enhancement The ability to add a zip file as a reference, which is used by the Meta Agent when self-packaging. Enhancement A new category: \"Generative AI\", which will be available in Data Stream Designer shortly. Enhancement The references layout caters for integrations with many reference files: The reference type is shown in brackets after the file name. The Agent or Plugin file is shown first, with the rest of the files in alphabetical order. Files can be dragged and dropped into the files input. Enhancement The reference and settings lists scroll separately from their edit panels, thus the properties will not scroll when navigating through a long list. App Designer Change Type Description Deprecation The Autodesk Forge Block is deprecated. Please update your Apps by implementing the Metablock as shown in the example provided. Security The Azure Copilot and ChatGPT Copilot Block keys are now encrypted. Existing Apps will continue to work until they are edited. Keys entered manually entered will automatically encrypt themselves if the Block is selected, but you will need to save the App. Keys stored in variables will need to be changed to encrypted variables. Feature Embed Script is a new feature that allows company admins to integrate a JavaScript snippet into the pages of App Designer. This can be used to for a variety of purposes, ranging from collecting user analytics to a chatbot trained on company documentation. Enhancement The Metablock value mapping now supports variables, ensuring credentials for 3rd party libraries are not exposed. Enhancement Preferred Language is a new User Details expression that returns the regional language tag of the logged in user's preferred language. Use this to adjust text when an App will be used in multiple languages Enhancement Enhanced Date & Time Display in Grids. Date and DateTime columns in Data Grid and Tree Grid Blocks now automatically display in your browser's locale format, replacing the previous American date format (M/d/yyyy, h:mm a). You can override this by specifying your preferred format using the new \"Date Time Format\" property. Enhancement App export has a new advanced option and by default files in the uploads folder are excluded. When ticked, you can choose whether to include files added at runtime or exclude all files. This gives you greater control over which files to include, which affects the size of the file export. Enhancement New Image Storage Options for Blocks. Image and Image Map Blocks now support storing images in App Files through the new \"Image Source\" property. This enhancement allows images to be shared across multiple blocks and improves storage and retrieval efficiency. Previously, images could only be embedded within individual blocks. Fix I added a value mapping to my Metablock with a static key and an expression value. When I changed the key to dynamic, expression, and back to static, at runtime the mapping now shows the expression as text rather than the result of the expression, i.e. Reverse(\"foo\") and not \"oof\". Metablock expressions work as expected. Data Stream Designer Change Type Description Enhancement The \"Event Buffer\" property has been renamed to \"Agent Event Queue Capacity\" to more accurately represent its function on the Stream Host. Previously, this setting defined the queue limit in kilobytes, potentially leading to inconsistencies in event handling. Now, it specifies the maximum number of events, providing a more consistent approach to managing event queues across agents within a data stream. Fix I want to see who stopped my data stream, but the timeline view is blank. Sometimes the values in the context drop down are duplicated. A new \"Switch to archived Timeline\" button is available to access entries generated prior to upgrading to Data Stream v4.4.16+, and the context values are not duplicated. Fix My data stream's card is green to indicate it is published with no errors. When I open the data stream, I noticed the \"Started On\" stream metric is zero - so it had actually failed to start on all the Stream Hosts. The data stream's card status and \"Starting On\" stream metric are accurate. On the landing page, we've refined how the Active, Error, and Draft counts are tabulated. We've fine-tuned how device failure counts in active stream hosts are tracked and updated when stream hosts disconnect. Subscription Manager Change Type Description Fix I added a license to my new company, but the expiry date didn't appear until I added a user to the subscription. The license expiry date is now shown immediately after applying the license."
  },
  "docs/release-notes/v4.4.18.html": {
    "href": "docs/release-notes/v4.4.18.html",
    "title": "v4.4.18 | XMPro",
    "summary": "v4.4.18 Common Note Security update: Mitigate a high-severity vulnerability by upgrading certain Agents. Change Type Description Security We've replaced the deprecated System.Data.SqlClient library with Microsoft.Data.SqlClient, which is the recommended SQL Server data provider for all .NET applications. This update mitigates the risk of security vulnerabilities in the deprecated library while providing improved security, performance, and ongoing support. Refer to Integrations below for affected Agents. Security This release focuses on enhancing platform security and automated test coverage. Key improvements include endpoint authorization updates and significant platform engineering work to improve system stability and performance. Fix When upgrading using an ARM Template, there is a hard failure on the Data Stream Designer (DS) database upgrade. Later, I cannot retrieve the DS values needed to configure a Stream Host. Fixed the Azure ARM Template not populating Data Stream/Stream Host Environment Variables. Fix When installing in AWS, there is an IIS error when I log into XMPro. AWS .ebextensions script assigns iis_user read access to the certificate used by Subscription Manager. Integrations Change Type Name Description New HiveMQ Listener v1.00 HiveMQ Action Agent v1.00 HiveMQ ensures efficient real-time data exchange in environments with limited bandwidth or high network traffic. Security Data Streams Connector v2.01 Read Action Request v2.07 Read Recommendation v3.01 Resolve Recommendation v2.48 Run Recommendation v3.04 Update Recommendation v3.01 Close Action Request v1.02 Replaced deprecated System.Data.SqlClient library to avoid security vulnerabilities. The following Agents were repackaged to translate the configuration properties: Change Type Name Translation Azure SQL Listener v5.51 Azure SQL Context Provider v5.51 Azure SQL Action Agent v5.51 Translation Boon Amber Action Agent v1.01 Translation Convert Flow Units Function v1.07 Translation Coupa Context Provider v1.01 Coupa Action Agent v2.01 Translation CRC16 Function v1.02 Translation CSV Listener v3.52 CSV Context Provider v1.13 CSV Writer (Action Agent) v1.02 Translation Erbessd Context Provider v2.06 Erbessd Listener v3.41 Translation FFT Function v1.32 Translation FinOps Action Agent v6.83 FinOps Context Provider v5.63 Translation Fixed Width File Reader Action Agent v1.01 Translation Goal Seek Function v1.41 Translation ifm Listener v1.03 Translation InfluxDB Listener v1.26 InfluxDB Context Provider v1.03 InfluxDB Action Agent v.1.44 Translation iPOS Action Agent v1.81 Translation JSON File Reader Context Provider v1.22 JSON Serializer Transformation v1.12 JSON Deserializer Transformation v1.32 Translation Linear Interpolation Function v1.13 Translation MLflow AI & ML Agent v1.08 Translation MQTT Listener v3.05 MQTT Action Agent v3.05 Translation Nanoprecise Listener v2.04 Nanoprecise Context Provider v2.05 Nanoprecise Data Reader v1.02 Nanoprecise Action Agent v2.04 Translation OData Context Provider v1.33 OData Action Agent v1.63 Translation ODBC Listener v1.12 ODBC Context Provider v1.03 Translation OSIsoft PI Listener v4.05 OSIsoft PI Context Provider v4.31 OSIsoft PI Action Agent v3.11 Translation PDF Converter Action Agent v5.11 Translation Python AI & ML Agent v1.66 Translation Read Recommendation v3.01 Resolve Recommendation v2.48 Run Recommendation v3.04 Update Recommendation v3.01 Close Action Request v1.02 Read Action Request v2.07 Translation Rest API Context Provider v3.14 Rest API Action Agent v3.39 Translation Rounding Function v1.03 Translation RScript AI & ML Agent v3.08 Translation Salesforce Listener Agent v1.21 Salesforce Context Provider v3.01 Salesforce Action Agent v1.18 Translation SAP Context Provider v5.21 SAP Action Agent v6.09 Translation SAP HANA Context Provider v2.23 SAP HANA Action Agent v2.23 Translation Signal Filter v4.03 Translation Sparkplug B Listener v2.01 Sparkplug B Action Agent v2.01 Translation SQL Server Listener v5.51 SQL Server Context Provider v5.51 SQL Server Action Agent v5.51 Translation Streaming Data Platform Listener v1.01 Streaming Data Platform Context Provider v1.01 Translation Tango Listener v1.25 Tango Context Provider v1.16 Translation Telit deviceWise Listener v1.22 Telit deviceWise Context Provider v1.01 Telit deviceWise Action Agent v1.11 Translation Telit MQTT Listener v2.01 Telit MQTT Action Agent v2.01 Translation Telit OPC UA Listener v1.01 Telit OPC UA Action Agent v1.01 Translation WebScraper Context Provider v1.02 Translation XML File Reader Action Agent v1.19 Translation XMQ Listener v1.05 XMQ Action Agent v1.05 The following Connectors were repackaged to translate the configuration properties: Change Type Name Translation Azure Data Explorer Connector v1.98 Translation Azure Digital Twin Connector v1.11 Translation Azure SQL Connector v2.01 Translation Data Streams Connector v2.01 Translation Erbessd Connector v3.01 Translation HTTP Connector v1.01 Translation JSON Connector v1.01 Translation Nanoprecise Connector v1.03 Translation Neo4j Connector v1.03 Translation OSIsoft PI Connector v2.27 Translation OSIsoft PI Histogram Connector v1.11 Translation REST API Connector v1.02 Translation Snowflake Connector v1.01 Translation SQL Server Connector v2.01 Translation TSA Azure Data Explorer Connector v1.11 Translation TSA SQL Connector v2.01 App Designer Change Type Description Enhancement We've made security and usability enhancements to the Metablock: Intercept encrypted server variables when used inside a WebSocket. Requests are proxied so that credentials inside the Metablock are not exposed when sending messages via WebSockets. You can now authenticate securely and simply using encrypted server variable value mappings in a WebSocket/MQTT message. Allowed Downloads security feature. We've enabled this security feature so that you can build a Metablock that includes file download functionality. Utilize multiple script files using App Files. Although the Metablock supports a single HTML, CSS, and JavaScript file, you can now reference App Files as a source for additional resource files. This means you can have multiple files rather than consolidating into a single file. Fix I was able to mark an Alert as resolved, even though there was a required Form field. Resolve Alert Action Block respect required Form fields as expected (i.e. a False Positive can be used without filling in required Form fields. Fix I configured a Time Series Analytics Block with a range of 12 hours, initial selection of 3 hours. When I launch the App, the range is 3 hours and the initial selection is 3 minutes. An error introduced in v4.4.17 has been corrected and the Time Series Analysis Block filter works as expected. Data Stream Designer Change Type Description Enhancement Favorite your commonly used Agents to visually identify them with a star. In future, we'll add a new Favorites category so you can access them more easily. NuGet Packages Common Change Type Description Enhancement Modernize and enable publishing of NuGet packages. This enriches NuGet package metadata and introduces SourceLink capabilities to enhance the Developer experience. Maintenance Implement EditorConfig and apply Whitespace and Usings rules, ensuring uniformity in internal coding style. Agent Development XMPro.XMIoT.Framework Change Type Description Security Updated third-party dependencies to maintain optimal security standards. Enhancement Support Agent property translations with a new property, TranslationMap. Enhancement Support a zipped file to be packaged in an Agent. Enhancement When an Agent and the Stream Host supply the same assembly, the correct one is selected. In addition, detailed debug information is now logged when loading agent assemblies to better troubleshoot Agent and Stream Host issues. There is a check for certain agent assembly loading conditions, which will be visible in the Data Stream log viewer. Enhancement The IPollingAgent interface was changed from required to recommended with the addition of a RequiresPolling property. This gives greater flexibility as typically non-polling agents can also poll. Maintenance Remove redundant Hybrid and Cloud Agent types - only Edge is supported. Maintenance Remove redundant Java references - only Agents written in C# or using the Meta Agent are supported. Maintenance Remove SCC references from csproj files - redundant in current source control. Connector Development XMPro.Integration.Framework Change Type Description Enhancement Added parameters to the Connector's Subscribe method, used to implement live updates on data sources. Connector creators can implement the same pre-defined filtering and sorting applied when an App Page is refreshed. Enhancement The new ITSCConnector interface instructs the Time Series Chart to use optimized client-side querying. Use it when building new TSC Connectors that pre-process large volumes of data and return it in buckets. Later, the ITSAConnector interface was added (to reflect the Time Series Analysis Block renamed in v4.4.9) and the ITSCConnector interface deprecated. XMPro.Integration.Helpers No additional changes. XMPro.Integration.Settings Change Type Description Enhancement Support Agent property translations with a new property, TranslationMap."
  },
  "docs/release-notes/v4.4.2.html": {
    "href": "docs/release-notes/v4.4.2.html",
    "title": "v4.4.2 | XMPro",
    "summary": "v4.4.2 Integrations An issue was discovered with Python Agent v1.61 in where certain combinations of agents resulted in all data streams failing to startup on a Stream Host. This version of the agent was incorrectly packaged and has since been rectified. Upgrading the Python Agent to v1.65 and using the latest Stream Host (v4.4.2) is recommended. An issue was discovered with Calculated Field Agent v3.44 where it was not compatible with pre v4.4.0 of the Stream Host. This version of the agent was incorrectly packaged and has since been rectified. Upgrading the Calculated Field agent to v4.45 and using the latest Stream Host (v4.4.2) is recommended. Change Type Name Description New HTTP Connector v1.00 Include data from an HTTP endpoint in your Application. New Neo4J Connector v1.00 Include graph data in your Application. New Neo4j Listener v1.00 Neo4j Context Provider v1.00 Neo4j Action Agent v1.00 Fetch data from, insert, update, create or merge data into, a Neo4j graph database. New Sparkplug B Listener v1.01 Sparkplug B Action Agent v1.01 Receive and publish Sparkplug B-compliant messages to and from an external MQTT broker. New TSC Azure Data Explorer Connector v1.00 Utilize an optimized query that enhances the Time Series Chart performance. Enhancement Email Action Agent v4.29 Support CC and BCC. Support variables for the From address. Enhancement Ethereum Smart Contract Listener v1.04 Updated Nethereum Nuget to 4.19.0. Enhancement MLflow AI ML Agent v1.06 Support a Staging model or a specific version. Validate the model signature. Fix Calculated Field Transformation v3.45 Replacement for v3.44 released in v4.4.0, which was not backward compatible with older Stream Hosts (pre v4.40). Fix Run Recommendation v2.64 Collection was modified error. Fixed a rare concurrent access exception when fetching updated rules. Fix Python Agent v1.65 Fixed a rare issue where the python agent when used in certain combinations of agents in a data stream would cause the SH to stop, subsequentially stopping all Data Streams that SH runs. The issue was introduced in v1.61 of the agent where it was packaged incorrectly. App Designer Change Type Description Enhancement The Visual Media Capture Block's App File storage has been replaced with external storage providers, i.e. Azure Blob and Amazon S3. Note: this is not backward compatible and you will need to update the block configuration if you used it in v4.4.1. Fix My lightweight App or App Page fails to export. We've optimized how we store grid filter properties, reducing the exported App size. Subscription Manager Change Type Description Enhancement Microsoft .NET Framework upgrade from 4.7.2 to 4.8.1 Fix The default Subscription Manager log to file path is accessible through a web browser. The default log to file location is App_Data, which is hidden by default globally on IIS and therefore not public. Stream Host Change Type Description Feature A Docker Stream Host image is now supported. Enhancement New free trial Stream Hosts will be deployed as docker containers, rather than Azure Webjobs. In the future, this will become the recommended Stream Host deployment. Enhancement Utilize environment variables to override variables for a Stream Host, best suited when running the Stream Host in Docker. Note: the prefix changed from xm: to xmvariable__. Enhancement We've improved the encryption of agent parameters and variables. A new enableLegacyCrypto feature flag, which defaults to false, allows you to revert to the historic encryption method. We recommend sticking with the new method as the legacy method will eventually be deprecated. If you're running Data Stream Designer v4.2.2+, no action is necessary. If xmpro:XMCryptography:TripleDES:Key is specified in the configuration, a warning log will inform this is obsolete and can be removed. If you're running Data Stream Designer older than v4.4.2, you must: add xmpro:Gateway:FeatureFlags:EnableLegacyCrypto=true to the config specify the xmpro:XMCryptography:TripleDES:Key configuration (go to Data Stream Designer's site settings and copy the Encryption Key) If you don't, the Stream Host will fail at start-up and the resulting log will remind you to take the above actions. An older Stream Host with an older Data Stream Designer will continue functioning as usual. Fix The required assembly is missing when using the Calculated Agent v1.65 with an older Stream Host. When an Agent and the Stream Host supply the same assembly, the correct one is selected. In addition, detailed debug information is now logged when loading agent assemblies to better troubleshoot Agent and Stream Host issues. There is a check for certain agent assembly loading conditions, which will be visible in the Data Stream log viewer. Fix The configuration blade for any SQL Server Agent v5.0 is blank when using the latest Stream Host (v4.4.2). Added error handling for Agents packaged with a 'null' configuration key. Fix Encountered a 'Collection Modified' error on the Run Recommendation Agent. This was fixed in v4.4.1."
  },
  "docs/release-notes/v4.4.3.html": {
    "href": "docs/release-notes/v4.4.3.html",
    "title": "v4.4.3 | XMPro",
    "summary": "v4.4.3 Integrations Change Type Name Description Enhancement Azure SQL Connector v1.70 Support Insert and Update statements in a Stored Procedure. Enhancement SQL Server Connector v1.70 Support Insert and Update statements in a Stored Procedure. Enhancement OSIsoft PI Connector v2.21 Updated to use the new PI Client library. Enhancement OSIsoft PI Listener v4.02 OSIsoft PI Context Provider v4.3 OSIsoft PI Action Agent v3.02 Updated to use the new PI Client library. Enhancement SAP Context Provider v5.20 SAP Action Agent v6.08 Support variables. Fix XMQ Listener v1.01 XMQ Action Agent v1.01 Fixed a concurrent access exception. Fix Azure Digital Twin Listener v1.09 Added the Azure Client DLL to resolve an error when opening the Agent's configuration. App Designer Change Type Description Fix The value I updated on the Recommendation Form (e.g. work request number) is no longer reflected in the Recommendation Alert grid's Additional Information column. The Recommendation Alert grid's Additional Information column displays data as expected. Fix I now get an error 'Cannot read properties of undefined (reading 'length') when inserting new records using the Data Grid's Batch With External Save edit mode. The Data Grid's Batch With External Save functions as expected. Fix I am no longer able to save a resource on a Recommendation Rule. Resources can be added to or removed from a Recommendation Rule as expected. Data Stream Designer Change Type Description Fix My older Agent, Tungsten, is missing from the Agent Polling Interval Report. A few Data Streams in the Agent Usage Details Report have blank names and an owner \"Unknown User\". The standard reports work as expected for Agents that predate the IPolling Interface. Fix I want to see the Event Simulator Agent on the Agent Polling Interval Report. Even though it is not a Listener or Context Provider, the frequency of the events generated has an impact on performance. The Event Simulator Agent is included in the Agent Polling Interval Report. Fix I am assigned the default 'User 'Role, but I can edit an Agent's category. EditAgent right was removed from the default User role. Subscription Manager Change Type Description Fix I've signed out of SM with my Single Sign-On (SSO), but if I'm quick I can open AD using a bookmark and continue using it - and vice versa. Signing out from one product takes effect instantly on all the products. Stream Host Change Type Description Enhancement We've added a second Docker image flavor, Alpine, which is a lightweight option, capable of running most Agents. The image flavor released in v4.4.2 is suitable when using the Python Agent. Enhancement We've also added documentation on how to create a custom image using pip, useful if you need a Stream Host with different capabilities."
  },
  "docs/release-notes/v4.4.4.html": {
    "href": "docs/release-notes/v4.4.4.html",
    "title": "v4.4.4 | XMPro",
    "summary": "v4.4.4 Integrations Change Type Name Description New Azure AI Document Intelligence v1.0 Extract data from documents via the Azure AI Document Intelligence service. New Ollama AI & ML v1.0 Interact with a local instance of Ollama. Enhancement Azure SQL Action Agent v5.20 Support dynamic SELECT SQL queries. Enhancement SQL Server Action Agent v5.20 Support dynamic SELECT SQL queries. Enhancement OPC UA Listener v4.02 Support tag selection using Tag IDs. Increased client certificate expiry. Fix Read Recommendation v2.84 Support form fields in a live view for resolved recommendation alerts. Common Change Type Description Enhancement Lightweight logging to file is enabled by default for XMPro AI, App Designer, and Data Stream Designer - to avoid an early situation where troubleshooting is required but logging has not yet been configured. We recommend that administrators adjust the logging configuration to best suit their requirements. Enhancement Application Insights logging is turned on by default with the Azure ARM template. Enhancement We've extended our User Access documentation to also include the rights included in each of the default product roles. App Designer Change Type Description Enhancement Should you not wish to make use of the XMPro mobile app, use the new Hide Mobile App site setting to remove the mobile icon from the top toolbar. Enhancement Recommendation Alerts can be assigned - and reassigned - to anyone with appropriate access. We've also added the recommendation and rule name to the recommendation detail page for ease of reference. Enhancement The edit pencil, visible to designers on an App at runtime, has been moved so that it is not accidentally clicked while using the application. Likewise, the user profile menu is activated by mouse click rather than on hover. Data Stream Designer Change Type Description Enhancement Users can choose the version when upgrading an Agent in a Data Stream. This de-risks the upgrade process where previously the latest version was the only option with no rollback. Subscription Manager Change Type Description Enhancement It can be challenging to manage access by dragging a user to a different business role if you have many users and business roles. You can now change a user's business role from the user blade."
  },
  "docs/release-notes/v4.4.5.html": {
    "href": "docs/release-notes/v4.4.5.html",
    "title": "v4.4.5 | XMPro",
    "summary": "v4.4.5 Integrations Change Type Name Description Enhancement Azure OpenAI AI & ML v1.04 Support dynamic messages and token counting Enhancement Ollama AI & ML v1.04 Support single quotes in the input text. Common Change Type Description Enhancement All logged-out state pages, such as the sign-in, will show in dark mode if a device's dark mode is activated. Enhancement The user profile menu is activated by mouse click rather than on hover - to avoid accidentally triggering it when interacting with items at the top right of the page. This was introduced to App Designer in v4.4.4 and now applies to all products. Enhancement The health checks were extended to include XMPro AI. App Designer Change Type Description Enhancement A new advanced styling option, Cursor, has been added to Block Styling. This allows you to choose the pointer icon for a block, such as to show that a Hyperlink Box is clickable. Enhancement The Navigate To Action's option to Open in New Tab/Window is now available for a Page too. Enhancement The default App Designer product role was amended from 'DesignUser' to 'Design User' for new installations for greater legibility and accessibility. For existing installations, the global administrator is encouraged to amend the spelling. Fix I cannot update a page parameter when a user clicks a Hyperlink. The Indicator, Templated List, and Hyperlink Box can pass a parameter to another page using an expression Fix When I select Library for a Button's icon mode, there is no spacing between the Icon's image and description in the dropdown. Padding was added between the Icon's image and description for accessibility. Fix My Indicator label only shows if the X-Axis and Y-Axis position is configured - even though the visibility is set to \"On Hover\" or \"Always\". The Indicator label visibility of Never, On Hover, and Always work as expected. This fix applies to newly added blocks. For indicator blocks added before v4.4.5, change the position in advanced styling from static to relative. Fix The v4.4.4 change to move the edit pencil, visible to designers on an App at runtime, was reverted to its original position. Data Stream Designer Change Type Description Fix I've filled out my data stream's business case, but I get an error 'Unable to save business case changes' when trying to save it. A red outline has been added to indicate that the business use case text is a required input. Subscription Manager Change Type Description Fix Users who have registered, but have not yet been approved by an administrator, can be added to a product subscription. Only approved users are available for selection to add to a product subscription. Stream Host Change Type Description Enhancement Python-capable Docker Stream Host images were added for Ubuntu and Alpine - and Python was removed from the default Stream Host images to keep them lightweight."
  },
  "docs/release-notes/v4.4.6.html": {
    "href": "docs/release-notes/v4.4.6.html",
    "title": "v4.4.6 | XMPro",
    "summary": "v4.4.6 Integrations Change Type Name Description Enhancement Erbessd Listener v3.10 Support getting only new values from the second poll onwards. Support an asset not having the point sensor installed. Enhancement Neo4J Connector v1.01 Repackaged to align property names to the Neo4J Agents. Enhancement MLflow AI & ML v1.07 Repackaged as non-virtual as the Stream Host is required to access the MLflow server. Enhancement XMQ Listener and Action Agent v1.04 Support an external router for when the Stream Hosts run on different machines. App Designer Change Type Description Feature New Blocks added: Metablock - add your tech to XMPro. Recommendation Alert Form - reuse this form section to add a collection of fields on individual Recommendation Alerts. Enhancement The site settings were tidied up: The UI category was renamed to User Interface The Hide Mobile App property (added v4.4.5) was renamed Enable Mobile App The Company Landing Page category was incorporated into the User Interface category. Fix When I add a stored procedure as a data source, its parameters appear in the data source column list. Stored procedure parameters are excluded from appearing in the data source column list. Fix When I have the data source of two different apps open, each in their own browser tab, and I update one data source, I note that the primary key of the other data source is cleared too. Updating the data source of one App does not affect another App's data source. Fix When I remove multiple connections, any connections below them in the list are hidden until I refresh the page. Only selected connections are removed and unselected connections remain on the list. Fix No Alerts are triggered for my Recommendation, but I can't see any error in the Stream Host log. Null values are handled by the Recommendation engine and errors are logged in the Stream Host logs. Fix The bottom border and shading was added to the profile menu. Fix I've noticed a performance issue when using a Select Box in my App as it results in multiple read calls on the page load. All Blocks were reviewed to ensure no more read calls were made than necessary. Most only require one read. The Select Box has been optimized, but is still requires at least two: one to read the selected row and another for the rest of the data. We have opened a support ticket with the third party regarding the Lookup Block, as it still sends multiple calls for an unknown reason. Data Stream Designer Change Type Description Enhancement We've added the Data Stream name as a hover tooltip to the Data Stream listing so that you don't have to expand the blade to see the longer names. Subscription Manager Change Type Description Fix As a global admin, when I switch between product roles, the product rights of the previous roles are displayed. The correct product rights are displayed when switching between roles."
  },
  "docs/release-notes/v4.4.7.html": {
    "href": "docs/release-notes/v4.4.7.html",
    "title": "v4.4.7 | XMPro",
    "summary": "v4.4.7 Integrations Change Type Name Description Enhancement Neo4J Agents v1.01 Repackaged to update the Neo4J icon. Fix Azure Digital Twin Context Provider v1.05 and Action Agent v1.20 Added the Azure Client DLL to resolve an error when opening the Agent's configuration. Fix Azure SQL Action Agent v5.30 Fixed the update to update not create Fix SQL Server Action Agent v5.30 Fixed the update to update not create App Designer Change Type Description Feature New Blocks added to make your interactions with Recommendation Alerts composable: Alert Timeline - reuse this timeline section to list activities on the alert. Alert Triage - reuse this triage section to include useful information on suggested actions to resolve the alert. Alert Event Data - reuse this event data section to view conditions that gave rise to the alert. The Recommendation Alert Form Block, introduced in v4.4.6, has been renamed Alert Form. Enhancement The Metablock Block, introduced in v4.4.6, has added presentation and styling file options, which empowers designers to increase reuse while reducing complexity. Enhancement The Lookup Block no longer makes multiple read calls on page load. Fix I changed the data type of one of the data stream tags from a double to a string, refreshed the recommendation rule, and now the rule logic box is no longer editable. The error is appended to the data stream tags in the rule logic box to communicate the problem to the designer, e.g. \"(INVALID DATA TYPE STRING)\". Fix I'm using a Unity v2022 model, but it doesn't load cached data when launched and there is also a zooming in/out issue. The Unity Block's cached data and zoom works as expected. Data Stream Designer Change Type Description Enhancement Notes are now distinct per version rather than per Data Stream, thus allowing the notes to track the differences between versions. Enhancement We've further improved the Agent upgrade experience: the button name changes to 'Downgrade' if the Agent is on the latest version and the list of versions includes the same information as the Agent's version blade, i.e. the date it was added and the number of data streams in which it is used. Enhancement We've reduced the size of the Data Stream Designer application package by decoupling the Stream Host installer download. From v4.4.7 onwards, ensure your network policy allows access to the download.app.xmpro.com domain when downloading the Windows or Ubuntu Stream Host installer. Better yet, use our Docker Stream Host option. Fix I cannot upload the 'Tier 5 - Agents.zip' file even though it meets the requirements - the compressed size is 31.7 MB and the pre-compression contents are 88.2 MB. Bulk adding agents validates the 100 MB limit using the XMP file size, not the converted JSON object size. Subscription Manager Change Type Description Enhancement Microsoft .NET Framework downgrade from 4.8.1 to 4.8 in preparation for Docker image. Stream Host Change Type Description Fix I've upgraded my Stream Host to v4.4 and my Data Stream that contains the Azure IoT Hub Listener Agent has errors in the log. Upgrade the Azure IoT Hub Listener Agent to v3.06, released earlier this year, with the newer version of Microsoft.Azure.Amqp.dll."
  },
  "docs/release-notes/v4.4.8.html": {
    "href": "docs/release-notes/v4.4.8.html",
    "title": "v4.4.8 | XMPro",
    "summary": "v4.4.8 Integrations Change Type Name Description Enhancement MQTT Listener v3.0 MQTT Action Agent v3.0 Support MQTT Protocol v5 Enhancement Read Recommendation v3.0 Support optional classification, assign, and score Enhancement Run Recommendation v3.0 Support optional classification by asset, process, and KPI Enhancement Update Recommendation v3.0 Update optional classification by asset, process, and KPI App Designer Change Type Description Feature Classify your Recommendation Alerts to better group for action or analyze for trends. The classification options are Asset, Process, KPI, Work Order, or Work Request. Set the classification using new versions of the existing Run and Update Recommendation Agents. Use the extended classification identifiers as filters in these enhanced Blocks: Alert Analytics, Alert Discussion, and Alert List. Feature The last of the new Blocks to make your interactions with Recommendation Alerts composable: Alert Action - add buttons to take actions on an alert, such as Share Alert Survey - add buttons for feedback from users on a Recommendation Enhancement The Metablock Block has added security attributes to prevent malicious code from being introduced. Fix I have a drilldown configured on an Indicator Block. The drilldown page doesn't load if I navigate before the page is fully loaded. I can navigate using any navigation link before all the indicators load. Data Stream Designer Change Type Description Enhancement Along with the distinct notes per version introduced in v4.4.7, you can also add a version title to add context of why the version was created."
  },
  "docs/release-notes/v4.4.9.html": {
    "href": "docs/release-notes/v4.4.9.html",
    "title": "v4.4.9 | XMPro",
    "summary": "v4.4.9 Integrations Change Type Name Description Enhancement Sparkplug B Listener v2.0 Sparkplug B Action Agent v2.0 Support MQTT Protocol v5 Enhancement Telit MQTT Listener v2.0 Telit MQTT Action Agent v2.0 Support MQTT Protocol v5 App Designer Change Type Description Enhancement The Time Series Chart Block, introduced in v4.1, has been renamed Time Series Analysis to better reflect it's capability Fix The Alert Discussion Block's type reverts from 'None' to 'Entity' on save. The Alert Discussion Block works as expected. Fix When using a SQL Stored Procedure as a data source in v4.4.6, I cannot see the gear icon that used to open a blade on the right to configure the parameters. The data source parameters gear icon appears as expected."
  },
  "docs/resources/faqs/agent-faqs.html": {
    "href": "docs/resources/faqs/agent-faqs.html",
    "title": "Agent FAQs | XMPro",
    "summary": "Agent FAQs Frequently asked questions regarding Agents. Are Agents thread-safe when running in a Data Stream? Yes, Agents are thread-safe. Each Stream Object (an Agent added to a Data Stream), including multiple instances of the same Agent, is an independent entity. How do Agents process events? Each Stream Object (an Agent added to a Data Stream) has its own thread-safe incoming event queue and processing thread. They receive events via input their endpoints according to the arrows added in the Data Stream. Further Reading Agent Endpoints Can Agents influence each other outside of the regular publish/subscribe mechanism? While Agents are designed to communicate only through the arrows, there are potential ways they might interact: Shared global variables (as the current Stream Host runs streams in a single process) Network communications Indirect effects, such as: One agent consuming excessive CPU processing power Multiple Agents making conflicting use of a shared resource (e.g., a physical device) Multiple Agents using the same shared library in a non-thread-safe manner What is a Stream Host? A Stream Host is the environment in which Data Streams run. It manages the execution of multiple Stream Objects within a single process. Further Reading Collection and Stream Host What are Arrows in the context of XMPro? Arrows are the links between Stream Object endpoints in a Data Stream. They represent the pathways through which Stream Objects pass data to each other. Further Reading Adding an Agent to the Canvas Input Mapping and Arrow Configuration What is a Data Stream? A Data Stream is a visual display of data flow, where the flow is represented by Stream Objects connected by arrows that perform actions on the data in real-time. Further Reading Data Stream Do Agents have memory buffers, and how do they work? Yes, each Agent has a memory buffer for events to be processed. This buffer is part of the Agent's thread-safe incoming event queue. Important characteristics of these buffers include: Fixed size: The buffer has a predetermined capacity to prevent excessive memory consumption on Stream Host devices, especially in cases of misconfigured data streams. Overflow behavior: When an agent's buffer is full, new incoming events will not be processed. This can lead to data loss if the incoming data rate consistently exceeds the agent's processing rate. Configuration considerations: It's crucial to properly configure data streams and monitor agent performance to ensure that buffer overflow situations are minimized, maintaining efficient data processing. Further Reading Buffer Size"
  },
  "docs/resources/faqs/configuration.html": {
    "href": "docs/resources/faqs/configuration.html",
    "title": "Configuration FAQs | XMPro",
    "summary": "Configuration FAQs Find answers to some of the most frequently asked configuration questions. App Designer How do I rotate text in App Designer? In the video below, we demonstrate how to rotate text -90 degrees around the z-axis using block styling's transform option. Further Reading Block Styling - Extra Note Video Reference: How to rotate text in App Designer This video demonstrates how to rotate text -90 degrees around the z-axis using block styling's transform option. Watch on YouTube How do I export Grid Data from my App to Microsoft Excel? In the video below, we demonstrate how to toggle on your Data Grid's export button so that at runtime the grid contents can be exported to Excel. Further Reading Data Grid Properties - Allow Export To Excel Note Video Reference: How to export Grid Data from App to Microsoft Excel This video demonstrates how to toggle on your Data Grid's export button so that at runtime the grid contents can be exported to Excel. Watch on YouTube Why doesn't the Date selector in my App match my computer's local date format? \"My computer is configured to use the date format DD/MM/YYYY, but my date selector is formatted as MM/DD/YYYY. How do I change it?\" The Date Selector date display format is based on the browser display language, not your computer's local settings. For example: When the browser display language is set to English (Australia), the date format will be DD/MM/YYYY. When the browser display language is set to English (United States), the format will be MM/DD/YYYY. You should adjust your browser's display language settings - rather than your computer's local settings - to change the date format in XMPro. Data Stream Designer Can I use an older version of an Agent in a Data Stream? \"I'm copying the same pattern used in another Data Stream and I want to use the same version of a specific Agent (v2.7) when a newer version is available (v2.8).\" No, using an older version of an Agent when a newer version is available is not possible. The latest Agent should incorporate all of the functionalities of the previous version as well as any further modifications made. However, you could clone the original Data Stream and choose not to upgrade the Agent to the latest version. Further Reading Cloning Upgrade a Stream Object Version How do I view errors on my Stream Host? In the video below, we demonstrate how to view the log for a particular Stream Host in a Collection. The log contains errors encountered when publishing or running a Data Stream. Further Reading Stream Host Logs How to check Data Stream Logs Note Video Reference: How to view errors on Stream Host This video demonstrates how to view the log for a particular Stream Host in a Collection. The log contains errors encountered when publishing or running a Data Stream. Watch on YouTube How are the Stream Load Metrics calculated? \"I'm trying to reconcile the Stream Load metric in Data Stream designer with what I'm expecting based on the listeners. Is the metric calculated on the stream outputs or the inputs? I have 3 listeners, each running at a 1 second interval, so I expect a 180/min metric. Is the ~360/min metric because there are double the number of outputs (action agents) per listener?\" Note Stream load is calculated based on the total number of events published by all Stream Objects on the canvas. Stream load is different from ingestion rate. The Stream load represents the total number of events published by all Stream Objects on the canvas. In this case, 60 x the total number of agents on the screen (assuming your Listeners bring back 1 record on every poll). You would notice the actual number is much less because although you want it to run every second, the calls to dependencies like SQL etc do not return its data as quickly and hence the actual rate is less. Generally, the number of events published per Agent decreases as you work through your data stream, because the intention is to work towards a smaller payload focused on the event in which you're interested. Can I use more than one Run Recommendation Agent in a Data Stream? We advise you to only have one recommendation agent on a data stream. A recommendation rule is configured against a single Data Stream, not a given Stream Object in a Stream. It will find the first Run Recommendation in the selected Data Stream and let you define your Recommendation Rule against the output payload of that Agent. If the payload differs at runtime, you may get weird results when triggering a recommendation alert. If the data cannot be merged (using a join or union transformation) and used in the different recommendations, then consider creating 3 different data streams. Remember you can have one data stream feed data to as many recommendations as you want to, BUT you should only have one run recommendation agent on a data stream canvas. My Data Stream Connector can do everything. Why use the other Connectors? Although a wide variety of data can be surfaced from a Data Stream into your Application using the Data Stream Connector, there is a downside. The Data Stream is constantly pushing data, which can lead to out-of-resource errors - which appear as XMPro product errors. If your data requirement is ad-hoc, consider the other Connectors. Further Reading List of App Designer Connectors Connector I've added a new recommendation - why can't I see the triggered alerts in the recommendation grid view? You, as the owner of the recommendation, will not see the recommendation alerts unless you give yourself Run Access to your own recommendations. Further Reading How to manage run access How do I drill down with data from a chart? You can achieve this by combining navigating between pages and passing data to the Page by configuring the Pass Page Parameters. The data passed can be static, an expression, or dynamic. Please refer to the how-to article below for step-by-step instructions. Further Reading Navigation and parameters How to pass dynamic data to the page How do I set up Stream Host Variables/provide unique Asset configuration? Although each Stream Host in a given Collection downloads the same definition of a Data Stream, the Variables defined in Data Stream Designer can be overridden by the individual Stream Host to provide the unique configuration per Asset e.g. OPC IP Address. Please refer to the how-to articles below for step-by-step instructions. Further Reading How to override Stream Host variables How to manage variables I am the co-owner of the data stream and an administrator - why is the \"Delete\" button disabled? To delete a Data Stream, your account must meet the following conditions: You need Co-Owner or Write access to the specific Data Stream. You must have the DeleteUseCase product right assigned to your user account, which is configured in Subscription Manager. Once you have access to the Data Stream and the right to delete, click Properties to access the Delete button. The Delete button on the canvas is used to delete Stream Objects. Further Reading Sharing Access to a Data Stream Editing Rights and Access for a User Deleting a Data Stream Deleting a Stream Object"
  },
  "docs/resources/faqs/external-content/blogs/2010/copy-me.html": {
    "href": "docs/resources/faqs/external-content/blogs/2010/copy-me.html",
    "title": "| XMPro",
    "summary": "Blogs: 2010 The Business Drivers Preserving Capability and Agility Mobile BPM"
  },
  "docs/resources/faqs/external-content/blogs/2010/index.html": {
    "href": "docs/resources/faqs/external-content/blogs/2010/index.html",
    "title": "2010 Blogs | XMPro",
    "summary": "2010 Blogs Articles from the XMPro blog published in 2010. Articles copy-me Mobile BPM Preserving Capability and Agility The Business Drivers"
  },
  "docs/resources/faqs/external-content/blogs/2010/mobile-bpm.html": {
    "href": "docs/resources/faqs/external-content/blogs/2010/mobile-bpm.html",
    "title": "Mobile BPM | XMPro",
    "summary": "Mobile BPM Blog, CEO'S Blog Mobile BPM Posted on May 20, 2010 by Pieter van Schalkwyk Mobile BPM introduces a new set of design, use and security considerations and XMPro will publish some design guidelines in the course of the next few weeks to address how deploy Mobile BPM while taking the following into consideration: > Smaller screen sizes of mobile devices; > Users that work discontinuously; > Security of data in transit and at rest; > The use of different platforms and “personally owned devices”; > Limited bandwidth and complex transactional forms; and > Synchronising users that switch between desktop and mobile devices. XMPro’s XMMobile workspace provides business users access to processes that were previously only accessible from your desktop or notebook. See our News sections for announcements on our commitment to delivering mobile BPM to people that what to get better at getting work done."
  },
  "docs/resources/faqs/external-content/blogs/2010/preserving-capability-and-agility.html": {
    "href": "docs/resources/faqs/external-content/blogs/2010/preserving-capability-and-agility.html",
    "title": "Preserving Capability and Agility | XMPro",
    "summary": "Preserving Capability and Agility Blog, CEO'S Blog Preserving Capability and Agility Posted on May 2, 2010 by Pieter van Schalkwyk he responsibility of IT managers to provide solutions that will preserve capability with fewer employees serving the same number of customers as well as remain flexible in changing market conditions requires those managers to consider the processes that drive their businesses. Business Process and Performance Management (BPPM) links the operational enterprise workflow capabilities of products like XMPro to strategic performance management requirements to steer businesses through these though times. BPPM projects should typically be high priority initiatives in times like this as it supports all the major objectives set by Fiona as major strategic projects. BPPM projects define the required process performance measures upfront and then set out to achieve those through enterprise workflow automation and constant process monitoring. It optimises the use of scarce people resources and reduces process lag time in email inboxes waiting for actions. It notifies and escalates tasks and informs managers when service levels to customers are not met. It actively manages business processes on autopilot with the necessary feedback systems to those in charge. It is really the only way I can see how organisations can preserve capability, remain agile and work within organisational and regulatory requirements. Interestingly enough, the same newspaper has an interview with Mr. Barry Simpson, CIO for Coca Cola Amatil in Australia, and he suggested that they are looking to accelerate some of their IT projects that give them strategic advantage in an attempt to turn up the heat on competitors. “When business is though, and the economy is though, that’s when strong companies get stronger. This is when you want to be investing in business to drive that growth and to take advantage of weakened competitors”. Definitely a strategy that could also benefit from BPPM."
  },
  "docs/resources/faqs/external-content/blogs/2010/the-business-drivers.html": {
    "href": "docs/resources/faqs/external-content/blogs/2010/the-business-drivers.html",
    "title": "The Business Drivers | XMPro",
    "summary": "The Business Drivers Blog, CEO'S Blog The Business Drivers Posted on April 28, 2010 by Pieter van Schalkwyk There are 6 business value drivers identified by Jack Alexander in his book Performance Dashboards and Analysis for Value Creation that are under management’s control to influence: 1. Sales Growth 2. Relative Pricing Strength 3. Operating Effectiveness 4. Capital Management 5. Cost of Capital 6. Intangibles, Credibility, Future Expectations These business value drivers can be influenced through a pro-active performance management approach. Improving the business performance can be achieved through a number of mechanisms both hard and soft in the organisation. Business processes are one of the tangible mechanisms that can easily be addressed to impact business performance. Pro-active process management impact most of the value drivers in a direct fashion. It can influence sales growth, drive and manage operating effectiveness, assist with capital management and manage intangibles. Business Process Management is the one business methodology that can have a significant impact on business performance and value. I will address the concept of effective performance management through business process management in future posts."
  },
  "docs/resources/faqs/external-content/blogs/2011/benefits-of-bpm-v-10.html": {
    "href": "docs/resources/faqs/external-content/blogs/2011/benefits-of-bpm-v-10.html",
    "title": "Benefits of BPM v 1.0 | XMPro",
    "summary": "Benefits of BPM v 1.0 Blog, CEO'S Blog Benefits of BPM v 1.0 Posted on January 28, 2011 by Pieter van Schalkwyk BPM Benefits v 2.0 will be addressed in a follow up post and revolves around the additional benefits gained from a dynamic BPM approach and the value of getting better at getting work done. For now we will list the benefits from a basic BPM implementation or project. The traditional benefits of BPM (v 1.0) are primarily focused around: Reducing Cost Improving Revenue or Customer Service Managing Risks and Ensuring Compliance Creating Competitive Advantage Enabling Innovation and Process Improvement The additional (lower level) benefits of electronic forms processesing includes: Improved throughput (productivity) Removing unnecessary manual controls and steps (efficiency) Improve accuracy and repeatability (quality) Improve visibility, decision trails and process logging (customer service and governance) Improve monitoring and load planning (operational efficiency and improvements) More options for re-location, hosted services, outsourcing (organisational flexibility) BPM can be done without systems but the November 2008 Aberdeen Research Report – BPM and Beyond : The Human Factor of Process Management – reports that Best in Class organisations that have implemented BPM systems achieved a 53% improvement in process consistency, compared to a 13% improvement for the Industry Average and a 13% decline for Laggards. Roger Tregear at Leonardo Consulting provides an expanded version of these v 1.0 benefits: Reduce costs, remove waste. Why waste time and resources doing unnecessary things or doing necessary things the hard way? Avoid opportunity losses. Deeper understanding of processes and their relationships reduces the chance of missed opportunities. Improve customer service (value delivery). Business processes are the only way any organization can deliver value to its customers. Increase organisational agility. Change demands understanding. Big change and fast change demand intimate understanding. Improve risk management. The more you understand a process the better you can predict and protect its weaknesses. Improve compliance management. Effective ongoing compliance management is an automatic consequence of a managed business process architecture. Document processes. Simply documenting a process provides new understanding and reference material for training and review. Process consistency. Documenting how a process work provides a clear guideline for how that same process should be executed everywhere in the organisation. Protect intellectual capital. The fragile and portable heads of key staff members is not a good place to store an organisation’s intellectual capital. Support contingency planning. Process-based management focuses the development of contingency processes on the things that will matter in a crisis. Improve strategy execution. Business processes are the way in which every organisation executes it’s strategy. Reduce complexity. Unnecessary complexity in any aspect of an organisation is a handicap to optimum performance. Improve IT outcomes. The purpose of IT systems is to support the execution of business processes. How can that happen without shared process understanding? Improve effective performance measurement. A process-based management approach allows us to measure the full set of things that really matter. Support staff to achieve success. If “people are our most important asset”, why do we so often frustrate them with broken processes. Roger also has an interesting way to describe what BPM is (while we are on the subject): Organisations exist to deliver value to customers and stakeholders. That’s strategy. They do this via a series of coordinated activities across a number of functional elements of the organisation. That’s a process. It makes sense to optimise these processes so that they satisfy the requirements of customers and other stakeholders. That’s process improvement. Taking a coordinated view of the performance of the processes by which an organisation delivers value, optimises performance. That’s process management. Process management allows organisations to focus on processes that create the market differentiation described by the strategy. That’s execution. This is not an exhaustive list or explanation of the benefits but a good conversation starter on how BPM can have significant impact in your organisation."
  },
  "docs/resources/faqs/external-content/blogs/2011/copy-me.html": {
    "href": "docs/resources/faqs/external-content/blogs/2011/copy-me.html",
    "title": "| XMPro",
    "summary": "Blogs: 2011 Is mobile BPM now essential to the business? Stretch Socially Dynamic Processes To Fit Your Business Social Listening – Get Control Of The Conversation Operations Management – The Keys To KPIs Benefits of BPM v 1.0 How to Prioritise Processes"
  },
  "docs/resources/faqs/external-content/blogs/2011/how-to-prioritise-processes.html": {
    "href": "docs/resources/faqs/external-content/blogs/2011/how-to-prioritise-processes.html",
    "title": "How to Prioritise Processes | XMPro",
    "summary": "How to Prioritise Processes Blog, CEO'S Blog How to Prioritise Processes Posted on April 28, 2011 by Pieter van Schalkwyk There are a number of approaches based on the culture and maturity of the organisation and we are currently working on a “Diamond Approach To BPM Projects”. It classifies BPM projects according to four dimensions: novelty, technology, complexity and pace. I’ll post more on this in the next few weeks. But we still want something simple to guide us in the early phases of the BPM journey. We found that it helps in many cases just to have a list of possible processes. It may sound strange but until it is on an ordered list, we don’t seem to comprehend the scope of what we are trying to achieve with our processes. I’ve used a simple weighting scale based on simple priorities to rank processes for clients as a starting point. The Process Priority Analyser Spreadsheet is a quick way to identify processes with high business impact and low implementation risk to use as first processes. The spreadsheet is based on ranking processes in four different quadrants: Do Now : Those with high impact and low complexity Plan For : High impact with high complexity Nice to Have : Low impact and low complexity Discard : Low Impact and high complexity"
  },
  "docs/resources/faqs/external-content/blogs/2011/index.html": {
    "href": "docs/resources/faqs/external-content/blogs/2011/index.html",
    "title": "2011 Blogs | XMPro",
    "summary": "2011 Blogs Articles from the XMPro blog published in 2011. Articles Benefits of BPM v 1.0 copy-me How to Prioritise Processes Is mobile BPM now essential to the business? Operations Management – The Keys To KPIs Social Listening – Get Control Of The Conversation Stretch Socially Dynamic Processes To Fit Your Business"
  },
  "docs/resources/faqs/external-content/blogs/2011/is-mobile-bpm-now-essential-to-the-business.html": {
    "href": "docs/resources/faqs/external-content/blogs/2011/is-mobile-bpm-now-essential-to-the-business.html",
    "title": "Is mobile BPM now essential to the business? | XMPro",
    "summary": "Is mobile BPM now essential to the business? CEO'S Blog Is mobile BPM now essential to the business? Posted on November 30, 2011 by Pieter van Schalkwyk This is the topic of a question posted on ebizQ forum. I posted a comment there, but thought I will also re-post it here as mobility is one of the major disruptive trends that are influencing business and ultimately business processes. Here is my reply to the question on the forum I don’t think it is a question anymore if Mobile BPM is essential to business but rather what is it best suited to. Not all aspects of BPM are useful on all form factors of the various mobile options out there. Mobile can mean a “mobile worker” that is out in the field using a conventional notebook device (doesn’t that sound weird, that notebooks are now “old”) to connect to a cloud based BPM solution (private or public cloud) and access their conventional browser based user interface to do work. They may even do some process modeling (or agile BPM changes) if their solution allows remote access to the modeling environment. They are likely to access process performance dashboards to look at key metrics or process improvement opportunities. That “mobile worker” can be someone who accesses their BPM solution from a tablet/slate device like an iPad and even though they can access all of the features of the notebook, they will probably use it for quick response “always on” type customer relationship processes. They may even do the odd leave approval here and there. These devices are great for doctors on their rounds updating patient records or, as in the case of one of our customers, the funeral arranger that sits with a family to plan the logistics of a funeral. By the way, this is a pretty unstructured process. It is also great for social BPM where discussion threads in transactions, for example, need quick response. They may have some key process metrics as graphs on the process form but are not likely to do complex process analytics on their iPads for now. It will mostly be operational metrics. The chance of them using the touch, pinch and slide to model or change processes are highly unlikely at the moment. That doesn’t say it won’t change in the future. The “mobile worker” on his or her smartphone is likely to use it almost exclusively for approvals of customer facing or high priority processes. The user interface is optimized for the small form factor and only shows the relevant information to make a quick decision. It is designed for the “always on” employee to will quickly check their email on their phone at a Saturday morning kids soccer game or the manager who needs to approve customer credit notes while waiting to board a plane. Requisitions and purchase order approvals are often candidates for “smartphone” BPM. They are unlikely to have any process analytics capability, but the order approvals process may, for example, have some predictive intelligence in the form. They are also highly unlikely candidates for mobile modeling. The form factor makes it just too hard. Mobile BPM is here and used in many processes in many industries and verticals. Mobile BPM is, however, not just a case of taking and existing process and looking at it on your iPhone. The form factor should suit the requirement of the “mobile worker”."
  },
  "docs/resources/faqs/external-content/blogs/2011/operations-management--the-keys-to-kpis.html": {
    "href": "docs/resources/faqs/external-content/blogs/2011/operations-management--the-keys-to-kpis.html",
    "title": "Operations Management – The Keys To KPIs | XMPro",
    "summary": "Operations Management – The Keys To KPIs Blog, CEO'S Blog Operations Management – The Keys To KPIs Posted on August 15, 2011 by Pieter van Schalkwyk The APQC PCF (Process Classification Framework) gives a bird’s eye view on the anatomy of work activities that keep the organisational gears churning. Even the industry-specific PCF models like banking, education and pharmaceutical are all based on the same primary operational processes and secondary support processes as the main PCF model. The Operational processes span across the value chain and stretch from supplier interaction right through to customer satisfaction. Activities that make up the operational process group are also the activities that contribute most to an organisation achieving their business KPIs. It is also the role of the Operations Manager/VP operations/General Manager or MD to manage the KPIs for this area of the business. They have multi-disciplinary teams reporting to them with a broad range of skills, both technical and soft skills. Operations are supported by IT/Finance/HR functions but the primary success of the business is dependent on them achieving their KPIs. Having highly effective Business Operations is CRITICAL for achieving business KPIs, while having highly effective HR Management is DESIRABLE. Operational KPIs are the overall measures to, firstly, indicate if business is EFFECTIVE in achieving planned objectives and, secondly, if these objectives are achieved EFFICIENTLY. Both of these are critical success factors in process and performance improvement initiatives. The complexity of controlling sophisticated operational systems, a broad base of people and skills as well as maintaining a customer focus through business processes require a technology enabled operations management control centre. XMPro is the leading, most effective Operations Management tool out there that gives Operations Managers control of people, processes and business systems while assisting them to achieve KPIs. It recognizes that Operations Managers work in multidisciplinary environments that require both technical specialists as well as soft skills. It recognizes that only around 20% of work processes can modelled in pre-defined workflows while 80% of work is dynamic and unstructured. It recognizes the impact of social rules, that business rules can’t be hard and fast and that these rules need to be adaptive for knowledge workers."
  },
  "docs/resources/faqs/external-content/blogs/2011/social-listening--get-control-of-the-conversation.html": {
    "href": "docs/resources/faqs/external-content/blogs/2011/social-listening--get-control-of-the-conversation.html",
    "title": "Social Listening – Get Control Of The Conversation | XMPro",
    "summary": "Social Listening – Get Control Of The Conversation Blog, CEO'S Blog Social Listening – Get Control Of The Conversation Posted on November 8, 2011 by Pieter van Schalkwyk Gartner’s Jim Sinur posted an interesting blog post titled “The Big Oops: Forget About Social Networking” where he describes 3 scenarios where the feedback from the collective was ignored and caused severe customer backlash, loss of brand credibility and a negative financial impact. We recently saw the world-wide grounding of the whole Qantas fleet as a result of a breakdown in negotiations between the company and unions. This stranded around 68,000 passengers world-wide. The socialsphere was in meltdown and Twitter was in a tweet tsunami. Qantas responded well under the circumstance and even though there was definite anger and frustration they seemed to be on tweet alert. The challenge is not just monitoring the socialsphere in crisis times like the Qantas example, but to do it consistently and have the ability to manage the dynamic and unstructured work real time with decision trails and tracking. It is about creating social intelligence and adaptive processes that can learn from previous experience to improve customer experience, brand and ultimately retaining customers. Not everyone has the luxury of designated social media monitoring and response teams like Qantas, and more than often internal marketing teams monitor social media stream in an ad-hoc and inconsistent manner. Responses are not coordinated, tracked or reviewed and often nothing is done at all. As Jim points out in ‘The Big Oops’, businesses can’t ignore the impact of social anymore. Mark Zuckerberg, CEO of Facebook, pointed out at their recent 2011 f8 Developer conference that, according to Facebook data, the information shared per person on Facebook, doubles every 2 years. It is the Moore’s law of social media. Social conversations about organisations will increase exponentially in the foreseeable future. How will you manage with the increasing number of social media (channels), the increased conversations (content) and corresponding response (contact)? XMPro for Social is a new breed of social management processes that harness XMPro’s ability to ‘listen’ to social networks such as Twitter, Facebook, Google Alerts, Digg, LinkedIn to name a few, for specific words or phrases. If these conversations are found, XMPro will create a process activity and the business rules and logic can benefit from XMPro’s dynamic, unstructured work approach. Conversations become activities, each with their own tracking and decision trails, business rules and escalation paths. Each can be dealt with differently to suit the specific goals of the business at that point in time. Response teams can be scaled at the click of a button at peak times, such as the Qantas situation, and switched back to normal processes if the conditions change. Combined with XMPro’s agile approach to BPM, i.e. intelligent experimentation and the use of historical process data, organisations can now improve their social management processes and customer service. Figure 1 – Social PR Task List and Action Forms (see below for expanded view) The XMPro screens above show how Qantas, for example, could “listen” for Twitter messages tagged #qantas and create them as new process activities. Each activity can then be managed and tracked as a XMPro dynamic process or case. These social channel messages (or aggregated into message groups) create fully managed social processes. With XMPro v6 (currently in beta) social discussions form part of the process transaction. These social discussions provide a new level of interaction that was previously not part of process management solutions. Figure 2 – The Social Activity stream as a XMPro task list XMPro for Social distributes work actions in a dynamic, unstructured way to effectively deal with responses and customer/employee service improvement; controls large volumes of social media feeds, data and decision trails, all from within a single environment; enables social data aggregation for analytics and customer service improvement; provides an effective multi-channel social media management solution that does not only “listen” but also “act” on social conversations; and creates a social governance platform that allows quick and appropriate response to conversation on social networks. Enable your marketing, customer service or social response team to deal with “the collective” quickly and effectively and learn from the experience to continue to improve your social management processes. Don’t let it be a Big Oops."
  },
  "docs/resources/faqs/external-content/blogs/2011/stretch-socially-dynamic-processes-to-fit-your-business.html": {
    "href": "docs/resources/faqs/external-content/blogs/2011/stretch-socially-dynamic-processes-to-fit-your-business.html",
    "title": "Stretch Socially Dynamic Processes To Fit Your Business | XMPro",
    "summary": "Stretch Socially Dynamic Processes To Fit Your Business CEO'S Blog Stretch Socially Dynamic Processes To Fit Your Business Posted on July 28, 2011 by Pieter van Schalkwyk I just read Gartner’s Jim Sinur’s blog post and he has started writing more and more on the social aspect of business process management. Jim has always been one of the analysts that gets the heartbeat of these things right. We’ve been talking about Social BPM for a while (not sure if it is Facebusiness or Processbook) but when the likes of Jim join the conversation we know that it is more than just a tweet (pun intended). His blog post addresses the fact that business process management is not about the features anymore. It is not about dynamic processes, rules, content/data, BI or complex event management but how all of these are needed in BPM technologies that tie human and machine interaction together. Processes of the future need to cater for answers where the questions are not defined yet. It requires new social interaction. This new paradigm that business finds itself in is not unique. I received a newsletter from our local school today where the principal quoted a book, The ABC of XYZ, by social commentator Mark McCrindle that had this to say in a chapter on ‘Educating and Engaging’: Age is no longer a factor in learning. We are all students in this information age. While younger generations are now staying in formal education for longer, older generations are continuing their learning experiences well past middle age. Today’s younger generation have been born into a time that has seen the printed word morph into an electronic form where communication is not restricted to the spoken and written word but is multi-modal. The age of reason has given way to the age of participation. It is not the era of experts but the era of user-generated opinion. In these post-modern times statistics don’t influence with the same power as story. It’s not content but process that dominates. While schools structure learning by subject, Generations Y and Z live life in a hyperlinked world. Teachers deliver formal lessons, yet students are experiential and participative. We test academic knowledge and memory in examinations yet they, with the always-on internet, are living in an open-book world, only ever 20 seconds from any piece of information. Generation Z is highly intuitive and confident unaided users of digital technology who are too young to remember its arrival. And while the majority of today’s learners are not yet employed, in a decade they will comprise approximately ten per cent of the workforce. Therefore the future of education depends on understanding and engaging with these 21 century learners. Whilst our schools and structures are very traditional in form, we need to move with steady creative certainty toward many new modes of delivering curricula and co-curricula activities I don’t think that it is just the education system that is seeing these changes to how people react and interact. I could not help but think of how accurately the quote describes “Gen Me” in the current business context. We may not be there yet and we are all thinking about and working on next generation BPM technologies but our requirements for work management systems are changing. Process management tools will have to stretch itself in many ways to handle the social and dynamic requirements that “Gen Me” users will have in the months and years to come."
  },
  "docs/resources/faqs/external-content/blogs/2012/copy-me.html": {
    "href": "docs/resources/faqs/external-content/blogs/2012/copy-me.html",
    "title": "| XMPro",
    "summary": "Blogs: 2012 Why Intelligent Business Operations is Mobile, Social and Smart Why Do You Want Intelligent Business Operations? How big of a problem are ‘dark processes’? Operational Risk: When You Stick Your Head In The Sand The Difference Between Event-based And Workflow-based Processes"
  },
  "docs/resources/faqs/external-content/blogs/2012/how-big-of-a-problem-are-dark-processes.html": {
    "href": "docs/resources/faqs/external-content/blogs/2012/how-big-of-a-problem-are-dark-processes.html",
    "title": "How big of a problem are ‘dark processes’? | XMPro",
    "summary": "How big of a problem are ‘dark processes’? CEO'S Blog How big of a problem are ‘dark processes’? Posted on August 8, 2012 by Pieter van Schalkwyk Peter Schoof posted this question on ebizQ and here was my reply: Let’s look at Jim’s definition “A dark process is an unofficial process used to deliver results and not visible to management”. So “unofficial process” means that these are not identified, recognized, sanctioned or supported. We know they exist but they are not “visible to management”. It doesn’t make them right or wrong. Yet. The key is the visibility to management. That would bring them into the light. Email is probably the best evidence of the existence of these dark processes. How many “processes” in your organization are done in emails? How many conversations are trapped in your inbox with information that would help improve business productivity if it was “visible to management”? That will tell you how much of a problem the dark processes are in your business. The reason why people revert to email is because of the rigid nature of flow-based process solution where you have to define the exact path of process upfront. It works for a number of rigid processes in an organization but not for those unpredictable processes where people (and most often customers) decide on the next step in the process. I suggest that you read Keith’s “Mastering the Unpredictable” if you want to understand the increased interest in event-based process solutions and adaptive case management solutions. The shear volume of dark process information necessitates the use of technology tools. It is not tool vendors looking for a new market. The problem (and market) exist, we are looking for a way to shine some light on these dark processes. It can be through an event based BPMS or through case tools. It is also through process mining tools linked to an event based BPMS, for example, that you can look at real historical process paths that these unpredictable processes take. It gives management visibility at both an individual transactional level as well as at a aggregated level to identify process flows that may impact operational risk, for example. I use an example of a Warranty Claim process for a high value item in a blog post that I recently did. It highlights the fact that the multiple outcomes that exist are not necessarily wrong. Each process path and outcome suits the unique circumstances and context of each case. More and more businesses recognize this and some are starting to see the light in the dark tunnel where dark processes live. Anyway, enough said, let me get back to my email inbox…"
  },
  "docs/resources/faqs/external-content/blogs/2012/index.html": {
    "href": "docs/resources/faqs/external-content/blogs/2012/index.html",
    "title": "2012 Blogs | XMPro",
    "summary": "2012 Blogs Articles from the XMPro blog published in 2012. Articles copy-me How big of a problem are ‘dark processes’? Operational Risk: When You Stick Your Head In The Sand The Difference Between Event-based And Workflow-based Processes Why Do You Want Intelligent Business Operations? Why Intelligent Business Operations is Mobile, Social and Smart"
  },
  "docs/resources/faqs/external-content/blogs/2012/operational-risk-when-you-stick-your-head-in-the-sand.html": {
    "href": "docs/resources/faqs/external-content/blogs/2012/operational-risk-when-you-stick-your-head-in-the-sand.html",
    "title": "Operational Risk: When You Stick Your Head In The Sand | XMPro",
    "summary": "Operational Risk: When You Stick Your Head In The Sand Blog, CEO'S Blog Operational Risk: When You Stick Your Head In The Sand Posted on July 18, 2012 by Pieter van Schalkwyk Your problem is that you are not addressing 80% of your problem. What do I mean by that? Your problem is not really the 20% of the work, where we know the process flow, which handles 80% of the transactions in your organization. It is the 80% remaining work, where we rely on people and we don’t know the flow upfront, which accounts for 20% of your business transactions. This is the work that happens on the right hand of the graph. The challenge is that transactional risk is the inverse of the transactional volume. This means that the RISK PER TRANSACTION is much higher on the right hand side. Let’s take a simple example. Withdrawing cash at an ATM machine is a highly predictable process with a small number or process variations but it accounts for millions of transactions per day in most medium to large banks. These are the processes on the left of the curve. They are generally well supported by Line of Business (LOB) applications and all the potential options are well defined and mapped into the business process flow. If the ATM system goes down for a few hours and the bank loses $10m as a result, the transactional risk is still only a few cents per transaction. And it doesn’t happen often. These failures are systemic and generally addressed with high urgency. These processes are also very well defined and all the potential variations are built as pre-defined flows in the line of business applications. We have control of the structured processes in the business. It is the unpredictable processes on the right that have the high transactional risk. In our banking example it is the private client on-boarding that may have a very low transaction volume but a very high transactional risk. The process path is unpredictable, even though there are a finite number of events that can occur. The customer will partly dictate how the process evolves and much of the routing and decisions are made by senior relationship (case) managers based on experience, knowledge and intuition. Peter Drucker called it “knowledge work”. Just imagine the impact of process failure in this example. The risks for this process can be fiscal but also brand damage and legal or compliance risk. Mortgage applications and derivate trades are other banking examples. There are many more in ALL ORGANIZATIONS and it can account for 80% of work done. These processes rely on people to make decisions that cannot be built into pre-defined flows. But this does not mean that we don’t have control. The process flows are unpredictable but not unidentifiable. We may not know in advance how a specific transaction may evolve, but we do know how to identify most, if not all, of the potential events that can happen. Managing unpredictable processes require moving away from trying to define the process flow for these “knowledge-style” processes. It requires an event-based approach rather than a workflow-based approach where we recognize that the “machine” cannot make decisions for these processes. It is best left to people to make the right decisions and determine the process flow based on their knowledge, experience and intuition. The best you can do is to provide them with the best decision support possible during the course of the process. Also allow them to have multiple process options that support unstructured but not uncontrolled processes. Let them work inside a governance framework, connect them to the existing Line of Business applications and let them service those unpredictable customers to get predictable results. XMPro’s iBOS (Intelligent Business Operations Server) is an event-based iBPMS that reduces uncertainty and risk. Our iBOS gives (Operations) Managers control by: Allowing processes to evolve on a case-by-case basis within a governance framework; Providing decision support for people that make decisions that determine the eventual outcome of the process; and Capturing critical social conversations and collaboration in these unstructured processes as part the audit trail of each process transaction. We typically integrate these processes with the other systems in your business and become the glue that keeps it all together. It is even “backward-compatible” to those structured processes if you want to put it all in one Intelligent Business Operations Server. Research by the leading analyst firms like Gartner and Forrester shows that you can get more than 30% efficiency gain by managing the 20% structured processes in your business. Just imagine what you can achieve by managing the 80% unpredictable, unstructured ones."
  },
  "docs/resources/faqs/external-content/blogs/2012/the-difference-between-eventbased-and-workflowbased-processes.html": {
    "href": "docs/resources/faqs/external-content/blogs/2012/the-difference-between-eventbased-and-workflowbased-processes.html",
    "title": "The Difference Between Event-based And Workflow-based Processes | XMPro",
    "summary": "The Difference Between Event-based And Workflow-based Processes Blog, CEO'S Blog The Difference Between Event-based And Workflow-based Processes Posted on May 14, 2012 by Pieter van Schalkwyk There is a long running misconception that business processes are workflow-based. It stems from Business Process Analysis (BPA) efforts to try and map the flow of a process, document it and then look at possible ways to improve on it. The BPA and Business Process Reengineering (BPR) movements needed exact flows to firstly understand processes and secondly try and improve efficiency gains from these processes. Many of it stemmed from a “factory-styled” view where processes flow sequentially and sometimes even in parallel but always in a pre-determined route. In these instances processes start with the flow in mind. The activities that happen along the flow path are secondary to the flow. Many workflow and BPM tools also work on this premise. It starts of by mapping the process flow in an exact flow model and even in a flow executable that can be plugged into a flow engine. This workflow-based approach requires that all potential exceptions are included in the model (which can make for an incomprehensible diagram in complex real-world processes) and routing is often decided by hard-coded business rules and yes/no decisions. This approach works for a limited number of processes in modern organizations with the rise of what Peter Drucker called “Knowledge Workers”. A workflow-based approach is restrictive for most knowledge workers. A key part of knowledge work is to make decisions when the business rules fail to understand the context of a process. Paul Harmon of BPTrends.com describes this decision-making process very well in a recent article. This diagram from the article describes the problem with decision-making in processes. (I suggest that you click on the image and read the article by Paul) When processes are viewed at a high level (as often happens in BPA) then all tasks seem to be based on some predetermined decision logic. But as you get to specific tasks and activities lower in process it appears that many require user decision-making that is based on contextual information and past experience. A simple example is when a sales person decides to refer an account for a credit check prior to continuing an on-boarding process. This may be as a result of reading a newspaper report on the current prospect. There are many combinations and permutations of possible customer on-boarding activities that may/need to occur to take a customer from the CRM system to the Accounts Receivable system. A workflow-based approach in this instance is too restrictive. An Event-based approach on the other hand looks at all the possible events that can occur for a specific process. These events or activities are placed in a single process “container” but the flow relationships are not pre-defined. Event-based processes are sequenced when the user need to access that specific event. A sales person may have a number of options like “Credit Check”, “Technical Review”, “Legal Review” and many more. The context of a specific transaction will determine the route that will be chosen for that specific transaction. It can be completely different for the next transaction. This provides the flexibility to cater for the work that knowledge workers do. It also allows for new events to be “discovered” and added to the process container without jeopardizing the “workflow”. An event-based approach starts with the activity or event in mind, its characteristics, human interface (if required), integration, documents and artifacts, and business rules (if applicable). The workflow characteristics are secondary to the activities or events that occur. There are many benefits of an event-based approach above a workflow-based approach. Here are a few: It allows business users to respond to business processes in the context of each specific transaction and respond appropriately to each; With the use of Business Intelligence or reporting it is possible to analyze the “real” flows in processes retrospectively. This is highly valuable information for process improvement initiatives; Additional events can be added without jeopardizing complex workflow routings; It is possible to create “hybrid” processes that may have workflow-like structure in some areas while it is case-style unstructured in other parts of the process. Most real-world complex processes resemble this behavior; and Event-based is not prescriptive and if it is combined with process goals it can be used to drive process outcomes similar to a business GPS that allows users to take different options and routes to suit the circumstances. The best analogy on the difference between workflow-based and event-based is that you are not running on train tracks, you are driving with a GPS."
  },
  "docs/resources/faqs/external-content/blogs/2012/why-do-you-want-intelligent-business-operations.html": {
    "href": "docs/resources/faqs/external-content/blogs/2012/why-do-you-want-intelligent-business-operations.html",
    "title": "Why Do You Want Intelligent Business Operations? | XMPro",
    "summary": "Why Do You Want Intelligent Business Operations? Blog, CEO'S Blog Why Do You Want Intelligent Business Operations? Posted on May 3, 2012 by Pieter van Schalkwyk There are a number of views on what Intelligent Business Operations (IBO) is, whether it is new and whether it brings any new benefits over BPM and workflow. Here is a brief synopsis of what I see IBO as and how it could be of benefit. Gartner described Intelligent Business Operations in June 2010 as: Companies in many different industries are improving their effectiveness and efficiency by making certain aspects of their operations more intelligent. Intelligent business operations are a style of work in which real-time analytic and decision management technologies are integrated into the transaction-executing and bookkeeping operational activities that run the business. Intelligent business operations are becoming increasingly practical because of the growing amount of data generated by sources inside and outside the company, and because of the wide availability of software tools to process that data immediately. The March 2012 version of their definition provides even more specific of what Intelligent Business Operations look like: The next generation of business processes will have to move beyond cost savings and efficiency, and become more adjustable to changing market and customer dynamics. “Tomorrow’s business operations will integrate real-time intelligence,” said Janelle Hill, vice president and distinguished analyst at Gartner. “This will require a new approach using IBO — a style of work in which real-time analytic and decision management technologies are integrated into the transaction-executing and book-keeping operational activities that run a business.” To me, Intelligent Business Operations is “state of business” that is achieved as result of leveraging the power of: Structured and unstructured processes to support knowledge style work 80% of work is unstructured according to Gartner. Workflow proves restrictive in these scenarios where knowledge workers have contextual information about processes and want to act on that in the way that they see fit. (I’ll explain scenarios of these in future posts) Many processes are combinations or hybrids of workflow and case style work. Define process flow at “run time” rather than at “design time”. Let knowledge workers make routing decisions based on BI and experience. Dynamic, rules –based, event driven actions that are goal oriented using predictive analytics, process mining and business intelligence Agile, flexible processes supported by a rules framework that dynamically adjust process events based on the context of the process Use data mining and predictive analytics to advise the “Next Best Action” Visualize “Process Goals” at each work step to guide business decisions Social conversations in and around the business and it’s transactions In the business through discussions around process improvements and about process transactions Around the business by monitoring social channels and turn “tweets into tasks” The benefits of IBO that I immediately see are: Better business decisions by better informed and guided employees, customers and suppliers Empower knowledge workers to act in a non-prescriptive way on contextual knowledge within a governance and control framework Make business intelligence insights actionable It turns BI dashboards into predictive guides (GPS’s) rather than rear view mirrors through visible process goals Improved collaboration by harnessing social interaction I’ll explain all the elements of IBO in follow on posts and also expand on the benefits of all of this in practical examples. In the mean time, it will be great to get your views and comments on IBO."
  },
  "docs/resources/faqs/external-content/blogs/2012/why-intelligent-business-operations-is-mobile-social-and-smart.html": {
    "href": "docs/resources/faqs/external-content/blogs/2012/why-intelligent-business-operations-is-mobile-social-and-smart.html",
    "title": "Why Intelligent Business Operations is Mobile, Social and Smart | XMPro",
    "summary": "Why Intelligent Business Operations is Mobile, Social and Smart Blog, CEO'S Blog Why Intelligent Business Operations is Mobile, Social and Smart Posted on August 3, 2012 by Pieter van Schalkwyk I introduced the Unpredictable Process graph in a previous blog post. It is actually more of an “Unpredictable Operational Risk” curve as organizations come to realize that they understand more of their high volume transactional processes than of the low volume human-centric processes. They have process controls in their “Systems of Transaction” to manage transactional risk. They are finding that it is the unpredictable processes that rely on people’s knowledge, experience and intuition that pose high operational risk and most of it has limited process controls. Major decisions are communicated in emails with no “decision trails”, limited supporting documentation or business intelligence and many of these decisions rely on someone’s experience to take the right action. There are no pre-defined workflows for this. Most of these tasks are done by “knowledge workers” that are on the road, in meetings and different “hot seats” every day. We can’t map out their work in clear linear flow diagrams with all the decision options to handle exceptions. They often work within a governance framework, but make decisions that impact the outcome of a process. They are at the heart of the business operations. Improving operations means improving the way these people work. One of the worst mistakes that I have seen is to try and map what these knowledge workers do, wrap it in some business rules, and try to turn it into a pre-defined workflow. I appreciate the value of business rules, but it should support and guide them, not try and automate their decisions. We need to make business operations smart or “intelligent” by providing the best decision support and social collaboration that we possibly can and then deliver it in a way that works the way these knowledge workers work. We still need to do this within the legal and organizational governance frameworks that these decisions are made in. So let’s look at a simple example. Let’s say you have a production facility where you make and service large complex machines. There is a job management system in your ERP solution that creates job cards, issue spares from inventory and does billing etc. for the repair work that your company does. It is very structured and easy to define and control the process flow. The exceptions are handled with simple workflow and some of that is in the ERP solution that you bought. It doesn’t require mobility or social discussions every time a job card is printed. In that same repair workshop you now get a warranty claim on a $250,000 unit that came from your production facility. Decision time. You rely on the experience and knowledge of your people to make the right decision. So what is the RIGHT decision? Your business rules may prescribe a certain set of actions but the real answer is “IT DEPENDS”. It depends on who the customer is and what their LTV (Life Time Value) is. We get that from the CRM and embed it as Process Intelligence in the “Next Best Action” advice that we are constructing for the assessor (case manager) to make a decision on the outcome of the warranty claim. We also include information on similar failures from the same customer or product range. It helps making smarter decisions. The assessor may want to have a quick discussion with the sales representative for that account to find out what big deals are in the pipeline and include the COO that will have to justify or support the decision later. Someone will have to authorize this, either way. The COO is overseas on a mobile device and his feedback is now part of the decision trail. The process is social and mobile. The process is supported by business intelligence and data from inside and outside the organization. The process is smart. The operational process is now intelligent, irrespective of the warranty outcome. There is no RIGHT answer. There is no RIGHT PATH. It differs on a case-by-case basis. That is the nature of knowledge worker processes. The example also illustrates the difference in the tools and support that knowledge workers need. When we create the job cards for the production line from the ERP application we need a server and a structured process flow. High volume, low transactional risk. When we do the warranty claim we need to be able to work anywhere (mobility), we need collaboration (social) and we need support information (smart) to make informed decisions. Intelligent Business Operations is about increasing the overall Productivity of an organization. But productivity is not just about efficiency (doing things faster), it is also about effectiveness (doing the right things). The best productivity gains are achieved when you first do the right things before you do them fast. For knowledge work the challenge is all about doing the right things. It is about being connected, being social and being smart. In my next blog post I’ll explain why it is ok for a 6 step process to have 46,656 potential paths and why workflows don’t work for these unpredictable processes."
  },
  "docs/resources/faqs/external-content/blogs/2013/best-next-action-is-the-next-big-thing-for-intelligent-operations.html": {
    "href": "docs/resources/faqs/external-content/blogs/2013/best-next-action-is-the-next-big-thing-for-intelligent-operations.html",
    "title": "Best Next Action Is The Next Big Thing For Intelligent Operations | XMPro",
    "summary": "Best Next Action Is The Next Big Thing For Intelligent Operations Blog, CEO'S Blog Best Next Action Is The Next Big Thing For Intelligent Operations Posted on February 2, 2013 by Pieter van Schalkwyk What if every person in your organization knew what the Best Next Action is every time they need to make decisions on customer on-boarding, support, sales, procurement, hiring, firing, and in actual fact, every operational process that you have in your organization? What if they weren’t forced down a prescriptive archaic flow path or workflow, but rather guided by a Best Next Action that is based on some organizational goal or KPI to optimize revenue, reduce cost, ensure compliance or improve customer service? What if the Best Next Actions in your organization are predictive and based on an analysis of the results of previous occurrences in your business? What if you could take the way people really worked across a thousand or so transactions and look at the most effective ways or work patterns that brought in most sales, reduced new product development cycles, reduced costs and avoided massive fines and penalties for unknowingly (or sometimes knowingly) putting the organization in high risk, non-compliant situations? “Next Best Action” is a familiar term in sales and marketing and regularly used for “upselling” or trying to move a customer along a desired sales route. The better term to describe this approach is actually “Best Next Action” as it clearly explains what the best action is to follow on from an existing action or process step. “Next Best” actually describes the 2nd best option. Getting people in your organization to follow a Best Next Action approach in their day-to-day work requires a different view on processes as we know them. Most people think of a flow diagram with a clear path, some “yes/no” decision points and rigid business rules that determine how the process will “flow”. A Best Next Action approach requires you to view process as “emergent”. It means that the best next action will “emerge” when you are busy with a current step and you have data and information about your environment, context, and where you are in respect of achieving a process goal or KPI. It is how we intuitively work. We assess the current progress or activity and then decide on the next step. The problem with rigid workflow is that it will force the next step based on a very simple formula or model-based (flow diagram) view of the world. The end result is that smart people will circumvent the rigid flow, use the data at their disposal, discuss it with peers and colleagues (usually in emails) and then make their own next action decision based on what they know. This is just how knowledge workers work. That’s why workflow systems for knowledge workers fail. The best way to make sure your processes support your business goals or KPIs is to make sure your processes allow the best next actions to “emerge” as smart people do the work. Let the work be guided by goals and advice on the best action that is likely to get you to the goal or KPI. It does, however, require that your work management system support both emergent work as well as best next action guides. It needs to handle work actions as a collection of possible events and allow users to sequence these events as they see fit. It needs to use business rules to set up guardrails for compliance, but not force the flow of work. It needs to look at the success of previous transactions and advise the Best Next Action based on previous successes, analysis of current work and other “big data” sets and use predictive analytics to suggest Best Next Actions. Advising Best Next Actions requires a work management tool that is built on an event-based architecture that allows options rather than rigid flow paths. It requires a solution that embeds BI and predictive analytics in the work and adjusts as you go. It also requires a solution that allows collaboration and discussion as part of work, to get feedback, ask questions and help you make better process decisions. XMPro combines loosely-coupled (flow-less) event based tasks, operational intelligence and social collaboration to achieve competitive advantage with predictable revenue, cost, compliance and customer service goals. XMPro is the only native event-based iBPMS on the market today. It is the only solution designed to support the Best Next Action in your business today. XMPro’s Best Next Action is the Next Big Thing to manage smart work better."
  },
  "docs/resources/faqs/external-content/blogs/2013/copy-me.html": {
    "href": "docs/resources/faqs/external-content/blogs/2013/copy-me.html",
    "title": "| XMPro",
    "summary": "Blogs: 2013 Best Next Action Is The Next Big Thing For Intelligent Operations The learns from two ‘Best in class’ organisations acquiring BPM technology"
  },
  "docs/resources/faqs/external-content/blogs/2013/index.html": {
    "href": "docs/resources/faqs/external-content/blogs/2013/index.html",
    "title": "2013 Blogs | XMPro",
    "summary": "2013 Blogs Articles from the XMPro blog published in 2013. Articles Best Next Action Is The Next Big Thing For Intelligent Operations copy-me The learns from two ‘Best in class’ organisations acquiring BPM technology"
  },
  "docs/resources/faqs/external-content/blogs/2013/the-learns-from-two-best-in-class-organisations-acquiring-bpm-technology.html": {
    "href": "docs/resources/faqs/external-content/blogs/2013/the-learns-from-two-best-in-class-organisations-acquiring-bpm-technology.html",
    "title": "The learns from two ‘Best in class’ organisations acquiring BPM technology | XMPro",
    "summary": "The learns from two ‘Best in class’ organisations acquiring BPM technology Blog The learns from two ‘Best in class’ organisations acquiring BPM technology Posted on July 25, 2013 by xmpro I have recently been privileged to participate in the process of product selection by two ‘best in class’ organisations. It is my experience each time you deal with a Best in Class organisation you always have learns and are reminded there is no substitute for doing a process properly. As my dear mother always said, “ If it is worth doing, it is worth doing well.” Firstly, some quick background on the organisations. This blog is not about the companies per se but rather how ‘best in class’ organisations go about their work. Both organisations are global, one organisation operates in business services, and the other in the resources sector. Both businesses approached their problems with laser like focus on their own business pain point but also ensuring whatever solution was deployed could be used as a whole of company business process solution. Another notable aspect was that both organisations sought to heavily involve their business users in the selection process. In itself not noteworthy except the goal of the selection was that, going forward, the business users would actively participate and even build processes – to succeed in the eyes of the business these were not to be IT dependant projects. The one organisation went so far as to ensure that the choice of product would not create new workloads for IT. This was unique and recognised the maturity of the organisation in addressing change and new technologies. Staff outside the IT department needed to be able to build production quality processes. The view, at this organisation, was also that all processes did not need to be fully researched and defined before they could be deployed. It was acceptable that the definition of the process, to start the build, did not have to be complete from cradle to grave with all permutations thought through and documented. The organisation is mature enough to adapt the processes as part of continuous improvement. That is to say that if the initial processes were undercooked, the fact that the journey of digitisation had started was success in itself, rather than imperfection being considered a failure. It does remind me of a favourite saying from a business colleague of mine, Michael Sheehan, who always says “An imperfect idea acted on, beats a perfect idea not acted on.” Both organisations started out looking at only 2 vendor companies and after discussions went forward with a POC (Proof of Concept). In the case of both companies I think this approach was not arrogance or misplaced belief but rather a sign of their confidence they have in executing projects. They are “Best in class” after all. They know that when they approach a project they will achieve world class outcomes, they have the people, processes and attitude to ensure that what they do is the best. Over the years I have heard often the arguments for and against using a POC. In both these product selection processes the POC was used to good effect and maximised the benefit to the buying organisation. The interesting point about the ‘business pains’ that both companies wanted to resolve was that the outcome could be measured with empirical data. The one organisation wanted to remove lag time from activities, as this reduction would directly translate into lower project costs and quicker conversion to a revenue stream. The other organisation wanted to guarantee that all staff head count increases were processed through the company’s procedures and all reviewers participated, with the aim that all head counts translated directly into achieving company goals – thus keeping their biggest cost under tight control. These ‘pain points’ drove the product selection process but the remarkable fact is that both organisations kept the goal of having an overall enterprise solution. The Aberdeen Group recently released a survey report where they found that “best in class” organisations are 65% more likely than “all others” to have implemented BPM software. To start out on a BPM project the survey found the key BPM strategies Best in Class organisations adopted include 56% – Remap and re-engineer business process to be more efficient, 35% – had the strategy to promote collaboration between disparate business processes and units and 25% to improve visibility into workflows. In both the reviewing organisations these strategies were key to their decision making process. Ultimately the real take away was both organisations avoided soft options and soft justifications. It was a pleasure for my organisation to be involved. If you are wondering … we were successful in one bid and unfortunately we lost out to a great competitor for the other deal. Our team is better for the experience. The author represents XMPro Intelligent Business Operations software, a leading provider of BPM software. http://www.aberdeen.com/Aberdeen-Library/8533/RA-business-process-management.aspx"
  },
  "docs/resources/faqs/external-content/blogs/2014/copy-me.html": {
    "href": "docs/resources/faqs/external-content/blogs/2014/copy-me.html",
    "title": "| XMPro",
    "summary": "Blogs: 2014 Making Business Operations More Intelligent"
  },
  "docs/resources/faqs/external-content/blogs/2014/index.html": {
    "href": "docs/resources/faqs/external-content/blogs/2014/index.html",
    "title": "2014 Blogs | XMPro",
    "summary": "2014 Blogs Articles from the XMPro blog published in 2014. Articles copy-me Making Business Operations More Intelligent"
  },
  "docs/resources/faqs/external-content/blogs/2014/making-business-operations-more-intelligent.html": {
    "href": "docs/resources/faqs/external-content/blogs/2014/making-business-operations-more-intelligent.html",
    "title": "Making Business Operations More Intelligent | XMPro",
    "summary": "Making Business Operations More Intelligent CEO'S Blog Making Business Operations More Intelligent Posted on December 2, 2014 by Pieter van Schalkwyk Gartner’s Roy Schulte and Michele Cantara published a great research article on “Practical Ways to Make Business Operations More Intelligent” that highlights how Operational Intelligence provides a mechanism to manage work in an event-driven world. (It does require a Gartner Research subscription). They explain that an intelligent business operation is different from a traditional operation because people and systems are able to make faster, more-precise and more-consistent fact-based decisions. This definition of an “intelligent business operation” is the best and simplest I’ve come across. Intelligent operations require a more responsive approach than traditional operations as typical business operations becomes more event-driven. What does it mean to be event-driven? Firstly, an event is “anything that happens” and event-driven is anything that is triggered when an event occurs. So it is really the response to things that happen. Every day your business is exposed to more and more internal and external events that need to be responded to. These business events can come from: the actions of people in your business; the actions of your competitors, customers, or suppliers; the Operational and Business Intelligence that you gather from your business applications, data sources and web services; and more recently, the influx of information from the Internet of Things (IoT) with sensor-based or smart device machine-born data. Event-driven businesses are differentiated in how they “sense” these events that occur, how they “decide” on which events to respond to and then how they “act” in a timely manner to the opportunities or threats that these events bring. Gartner calls these opportunities and threats “business moments” and they are time sensitive. Some business moments only provides milliseconds of opportunity (think of an automated trading system that responds to fluctuations in foreign exchange currencies) while others may have windows of seconds, minutes, or hours where we can still exploit these business moments. Operational Intelligence (OI) differs from Business Intelligence (BI) in the sense that OI provides insight into real-time or near real-time business moments where BI provides historical insight based on events that have passed. With BI we seek to analyze, understand and report on historical events. We want, for example, to see the sales by product by region by quarter, sliced and diced in a number of ways to explain what happened in the past. Operational Intelligence tells us that there is a premium customer in the store right now and that customers spent 3 minutes in front of a 60” TV before spending 4 minutes looking at a 55” 3D version. Using machine learning and predictive analytics we can predict the likelihood (based on previous purchase history and behavior of other similar prospects) of the prospect buying a new TV. With this new Operational Intelligence we don’t only send the customer a discount offer to their mobile phone but we proactively create a task for a nearby sales person to approach the customer, greet them by their name, explain/compare the products they looked at, and clarify the conditions of the discount offer. It captures a business moment that present a great opportunity to increase revenue without adding any costs to the sales process. It just requires some Operational Intelligence in near real-time. The second part of the example is extremely important from an ROI on Operational Intelligence point of view. The traditional approach to OI is to detect the event and create an alert or automated action such as sending a discount code to the prospect’s mobile phone. Intelligent Business Operations not only use Operational Intelligence to create alert, but leverage process management to proactively engage knowledge workers and specialists to maximize opportunities or manage potential threats. XMPro’s Intelligent Business Operations Suite (iBOS) is a single, unified platform that allows organizations to Sense > Decide > Act on these business moments. It is a core enabling technology for the Digital Enterprise by: Combining real time data streams and device (IoT or SCADA) data with existing business information to create new actionable insights Leveraging Operational Intelligence and Predictive Analytics to proactively create work tasks Providing collaboration and decision support to operations to make better decisions, faster Ensuring operational tasks, structured or unstructured, gets done effectively and efficiently XMPro’s Operational Intelligence capability is based of 8 main functions. I’ll expand on each of these with some examples in following blog posts. Intelligent Operations is about working smarter, being event-driven and responding in time to exploit the business moments as they arrive."
  },
  "docs/resources/faqs/external-content/blogs/2015/10-predictive-analytics-use-cases-by-industry.html": {
    "href": "docs/resources/faqs/external-content/blogs/2015/10-predictive-analytics-use-cases-by-industry.html",
    "title": "10 Predictive Analytics Use Cases By Industry | XMPro",
    "summary": "10 Predictive Analytics Use Cases By Industry Blog 10 Predictive Analytics Use Cases By Industry Posted on August 12, 2015 by xmpro One of the truly great uses for any business operations tool set is the ability to generate predictive analytics, which enables businesses to identify potential events and opportunities, and either avoid or capitalize on them, as the case may be. Through the use of analytical tools, large amounts of data can be mined to identify the indicators of events and opportunities, and use that data to make predictions that benefit the business. All of that sounds great, but the real value of these analytical tools can best be illustrated by describing the major use cases that exist in business today, and further describe those use cases in the context of their most applicable industries. 1. Churn Prevention When a business loses customers, it needs to bring new customers in to replace the loss in revenue. And that can get very expensive, because the costs of new customer acquisition is usually much more expensive than existing customer retention. Predictive analytics help to prevent churn in your customer base, by identifying signs of dissatisfaction among your customers, and identify those customers or customer segments that are at the most risk for leaving. Using that information, companies can then make the necessary changes to keep those customers happy and protect their revenue. Key Industries: Automotive, Banking, Insurance, Retail, Telecommunications 2. Customer Lifetime Value One of the more difficult things to do in marketing is to identify those customers that are going to spend the most money, in the most consistent way and over the longest period of time. This kind of insight allows companies to optimize their marketing to increase their share of that segment of the business, and gain those customers that will have the greatest lifetime value to your company. Key Industries: Banking, Insurance, Retail, Telecommunications, Utilities 3. Customer Segmentation Different companies define their markets differently, and segment their markets according to those aspects that offer the most value to their particular industry, products and services. A good use of predictive analytics is to identify target markets based on real data and indicators, and further identify the segments of those markets that are most receptive to what your company offers. This same data can also help to identify segments and potentially even entire markets that you didn’t even realize existed. Key Industries: Automotive, Banking, Life Sciences/Pharmaceutical, Insurance, Retail, Telecommunications, Utilities 4. Next Best Action Defining your primary market segments and customers is a critical use case for predictive analytics. But that only provides an incomplete picture of what your marketing approach should be. Analytics can also provide insight on the best way to approach individual customers within those segments, by analyzing everything from buying patterns to consumer behavior to social media interactions, giving you insight into the best times and channels to connect to those customers. Key Industries: Banking, Education, Insurance, Telecommunications 5. Predictive Maintenance In many industries, containing costs is as valuable a strategy and increasing revenue. And for companies with a major investment in infrastructure and equipment, the ability to manage that capital outlay is critical. By analyzing metrics and data related to the lifecycle maintenance of technical equipment, companies can predict both timelines for probable maintenance events and upcoming capital expenditure requirements, allowing them to streamline their maintenance costs and avoid critical downtime. Key Industries: Automotive, Manufacturing, Logistics & Transportation, Oil & Gas, Utilities 6. Product Propensity Product propensity analytics combine data on purchasing activities and behavior with online behavior metrics from things like social media and e-commerce, and performs correlations of that data to provide insight into the effectiveness of different campaigns and social media channels when it comes to your company’s products and services. This allows your company to predict not only what customers are more likely to buy your products and services, but what channels are most likely to reach those customers, allowing you to maximize those channels that have the best chance of producing significant revenue. Key Industries: Banking, Insurance, Retail 7. Quality Assurance Quality control is key to not just the customer experience, but also to your bottom line and operational expenses as well. Over time, inefficient quality control will affect your customer satisfaction, buying behaviors, and ultimately impact revenues and market share. And the costs don’t stop there. Poorer quality control leads to more customer support costs, warranty issues and repairs, and less efficient manufacturing. Good predictive analytics, however, can provide insight into potential quality issues and trends before they become truly critical issues. Key Industries: Automotive, Life Sciences/Pharmaceutical, Manufacturing, Logistics & Transportation, Oil & Gas, Utilities 8. Risk Modeling Risk comes in a number of forms, and can originate from a variety of sources. Predictive analytics can glean potential areas of risk from the massive number of data points collected by most organizations, and sorting through them to identify potential areas of risk, and trends in the data that suggest the development of situations that can affect the business and bottom line. By combining these analytics with a cogent risk management approach, companies can capture and quantify risk issues, evaluate them, and decide on a course of action to mitigate those risk factors deemed most critical. Key Industries: Automotive, Banking, Manufacturing, Logistics & Transportation, Oil & Gas, Utilities 9. Sentiment Analysis It’s very difficult to be everywhere at all times, especially in the online world. Likewise, capturing and reviewing everything that’s said about your company or organization is virtually impossible. However, by combining web search and crawling tools with customer feedback and posts, you can create analytics that give you a picture of your organization’s reputation within your key markets and demographics, and provide you with proactive recommendations as to the best ways to enhance that reputation. Key Industries: Life Sciences/Pharmaceutical, Education, Insurance, Retail, Telecommunications 10. Up- and Cross-Selling Your customer base is the source of both existing revenue and revenue growth for your company. Because of this, it’s critical to maximize the revenue opportunities that are possible within your market segment and product set. Predictive analytics can provide suggestions on which products might be combined to appeal to which market segments, to increase both your value to your customers, and the revenue derived from your customers. Key Industries: Banking, Insurance, Retail, Telecommunications The Value of Predictive Analytics Without intelligent business operations software, your data is only so valuable. But with the right operations management platform, you’re capable of managing all of the inputs, events, and data that provide real time insight into your enterprise. And with predictive analytics, you have the ability to move beyond simple reactive operations and into proactive and predictive activities that help you to plan for the future, and identify new areas of business."
  },
  "docs/resources/faqs/external-content/blogs/2015/6-myths-about-machine-learning.html": {
    "href": "docs/resources/faqs/external-content/blogs/2015/6-myths-about-machine-learning.html",
    "title": "6 Myths About Machine Learning | XMPro",
    "summary": "6 Myths About Machine Learning Blog 6 Myths About Machine Learning Posted on August 17, 2015 by xmpro As the concepts of big data and operational automation have seen their profiles rise over the last years, so to has the concept of machine learning – the ability of automated systems to intelligently evaluate and manipulate data, and modify its own approach, based on the data and information that it encounters. And as machine learning systems continue to evolve, they begin to show promise across a range of industries, from financial through utilities to healthcare, among many others. Machine learning systems give us the ability to rapidly gain insights and make adjustments to our approach in a much more efficient and timely manner than we’ve been able to achieve before. Myths Behind Machine Learning That said, sometimes the hype behind big data exceeds the reality of its use. While machine learning is a powerful tool, it’s not necessarily meant to be used in all scenarios, and like all technologies it works better in some use cases than others. With that in mind, let’s explore some of the more pervasive machine learning myths in the market today: 1. Machine learning removes human bias completely It’s true, machine learning does remove a certain level of bias. However, to suggest that bias is completely removed is simply not true. That’s because the initial algorithms, the data sets analyzed, and even the platforms chosen are all done by humans. And so, while it’s true that machine learning does remove much human bias, it doesn’t completely remove it. 2. Machine learning is real-time This is actually a more common myth, which doesn’t make a lot of sense once you have even a rudimentary understanding of the topic. At a very high level, machine-learning technologies are meant to run a series of algorithms against data, and build useful models for organizations and systems to use based on the results of that analysis. It’s true, those models are meant to be used in real time, for the most part. But the learning and analysis that built those models is not. 3. Machine learning will produce results from any data, in any situation This is also a common myth, the idea that any data can be introduced into the system, and the machine will automatically produce useful data, no matter what kind of data it is. Also, this simply isn’t true – an old saying in computer science is ‘garbage in, garbage out’, and that is just as true in machine-learning systems as anywhere. It is true that machine-based systems can often find patterns and insights that were previously hidden. But that doesn’t mean that one can simply feed garbage data into a system without any shaping or preparation, press ‘Go’, and have useful insight instantly present itself. It doesn’t work that way. 4. Machine learning is really only used for predictive analytics This myth is based on some truth, and it’s certainly no secret that predictive analytics does present a category of use cases that are well suited to machine learning. But that’s a limited view of the uses for machine-learning systems. Machine-learning algorithms have a large range of uses in the classification, regression analysis, and clustering of data, all of which can provide useful information to the enterprise. Plus, these use cases are typically less complex and expensive to implement than predictive analytics use cases. 5. Machine learning only applies to Big Data Again, another myth that has some basis in reality. It’s true that the rise of big data has given rise to machine-learning systems, and those systems have a unique applicability to analyzing large data sets and providing useful insights that really can’t be produced any other way, at least not in a timely fashion. But machine learning can be applied to traditional relational and structured data as well. 6. Machine learning is expensive This myth certainly had some basis in reality at one point in time. As the use of machine-learning systems was developing, the practice relied on expertise and tools that were prohibitively expensive for many enterprises. But, as with any technology, over time the costs have dropped as it gains acceptance, and new tools enter the market. Likewise, machine-learning platforms at providers offered by providers like Microsoft now incorporate the ability to utilize machine-learning systems on a use basis, no longer requiring the same capital outlay as previous years. As machine learning progresses into the market place, other myths will undoubtedly emerge; the usual vendor marketing and competitiveness will more or less ensure that. But machine-based learning systems are like any other technology – they have use cases that make sense, and some that don’t. And like any technology, as long as a balanced and realistic approach is taken, it can be an enormously powerful tool. As long as that kind of approach is taken, machine-learning approaches can produce value for the enterprise for years to come."
  },
  "docs/resources/faqs/external-content/blogs/2015/copy-me.html": {
    "href": "docs/resources/faqs/external-content/blogs/2015/copy-me.html",
    "title": "| XMPro",
    "summary": "Blogs: 2015 How Important Are Processes To The Internet Of Things? Understanding the Value of Real Time KPI Management as Your Next Strategic Project 6 Myths About Machine Learning 10 Predictive Analytics Use Cases By Industry What is a “Business Moment” in your business? Does Operational Intelligence Make Business Intelligence Obsolete? How To Reduce Operational Costs by 36% with Predictive Analytics From Many, One – The Nature of Complex Event Processing Herding Cats: What Enterprise Architects need to know about Business Process Management"
  },
  "docs/resources/faqs/external-content/blogs/2015/does-operational-intelligence-make-business-intelligence-obsolete.html": {
    "href": "docs/resources/faqs/external-content/blogs/2015/does-operational-intelligence-make-business-intelligence-obsolete.html",
    "title": "Does Operational Intelligence Make Business Intelligence Obsolete? | XMPro",
    "summary": "Does Operational Intelligence Make Business Intelligence Obsolete? Blog Does Operational Intelligence Make Business Intelligence Obsolete? Posted on June 2, 2015 by Kirsten Schwarzer It doesn’t require a massive leap of faith to believe that business intelligence has become a mainstream tool for many, if not most, enterprises today. And it’s likewise not difficult to see why. Business intelligence (BI) is able to comb through massive amounts of data, interrogating that data to derive trends and key performance indicators (KPIs) that give management insights into their business that hadn’t been available before, and allows them to make decisions based on that data. The usefulness of business intelligence as a practice has already been shown through numerous examples. Coca-Cola, for example, has implemented BI to analyze disparate data from a large number of databases and subsidiaries to gather useful intelligence about its sales and inventories trends. And Target uses analytics to learn about its customer’s shopping habits, including every product that every customer has ever purchased. This allows them to make informed decisions related to everything from merchandising to inventory, and promotions to supply-chain integration. An Uncomfortable Question For BI In fact, BI is an excellent tool for many enterprises, across many scenarios. But BI, almost by definition, involves the analysis and correlation of data that’s already been captured and warehoused, with BI KPIs derived from that historical data. But what about scenarios where businesses need to make decisions about what’s happening now, based on data that’s coming in from the field, and which requires a decision now, not at some later date? After all, business is about moving forward, and there are many instances where decisions must be made looking forward, not to the past. As a somewhat wise man once said – you can drive a race car using only the rear-view mirror, but it’s a lot easier to drive it by looking out the front windshield. All of this leads to a single, somewhat uncomfortable question – if business is all about moving forward, why do enterprises rely on yesterday’s information to make today’s decisions? Enter Operational Intelligence Make no mistake – we’re not discounting business intelligence as a valuable tool for most enterprises. As shown by the examples above, its value has already been shown, time and again. But BI has its limitations, and it’s not a good fit for all enterprises. It seems that some businesses have also come to this conclusion as well. Research from Gartner suggests that growth in the business intelligence market is dropping to the single digits, and one of the key reasons for this decline in growth is that, after the initial wave of implementations and successes, companies are having more difficulty figuring out how to leverage BI – indeed, it may be that many are realizing those previously-discussed limitations, and seeking different approaches. Business Intelligence (store and analyze) Operational Intelligence (OI), in contrast, is a methodology whereby data is analyzed for insight and correlation, much like BI – but operational intelligence relies on incoming, near real-time data streams and data points, and analyzes data as it happens, not after it’s been warehoused. Operational Intelligence (analyze and store) Likewise, Gartner has begun to cover operational intelligence platforms as they enter the market, and the benefits being realized by different organizations. Among the use cases noted are companies where real-time decisions based on incoming data streams are required for successful operations. And that’s a pretty wide range, which includes anything from oil and gas pipelines and refinery operations to financial futures and commodities firms. In a recent Gartner Market Guide for Operational Intelligence Platforms (that also features XMPro), they observe that too many business decisionmakers use stale information from reports or spreadsheet that are hours or days out of date because they don’t have access to current information. OI provides real-time visibility from business events, applications and devices and enables businesses to have sense and respond capability through a continuous monitoring platform. It improves situational awareness that is increasingly driving competetive advantage in a world where information about events that impact us is readily available to us (and our competitors). Does OI make BI Obsolete? The question that many people are now asking is if OI will make BI obsolete? Why look in the rear view mirror if you have a large, highly intelligent front window to navigate your way? The reality is that there is so much data that can be gathered by an OI system that it is impossible, in the first place, to analyze this manually, and secondly, to know what to look for as the information is streaming in. This is where BI offers some breadcrumbs of what to look for with an OI system. BI tools enable us to find historical events that presented either an opportunity or threat but we didn’t or couldn’t act on it as we didn’t know about it in time. BI tools help us to identify those events that happened in the past that we would like to sense in time in the future. This will allow us to respond in time to exploit a “business moment” that is either an opportunity or a threat. OI doesn’t make BI obsolete but it can use historical data analysis capability of BI to help us decide what to look for in the future. OI will not replace BI but its real-time sense and respond capability will enable organizations to drive their business with eyes wide open and forward looking rather than trying to navigate through a small rear-view mirror. OI doesn’t make BI obsolete but organizations that don’t leverage the situational awareness from OI will be left in the dust (and rear-view mirrors) of those that do."
  },
  "docs/resources/faqs/external-content/blogs/2015/from-many-one--the-nature-of-complex-event-processing.html": {
    "href": "docs/resources/faqs/external-content/blogs/2015/from-many-one--the-nature-of-complex-event-processing.html",
    "title": "From Many, One – The Nature of Complex Event Processing | XMPro",
    "summary": "From Many, One – The Nature of Complex Event Processing Blog From Many, One – The Nature of Complex Event Processing Posted on July 22, 2015 by Kirsten Schwarzer Initially, systems management software was used to detect singular events within a given system, by detecting log entries or event triggers located within a device or system or application or database. The management system would then detect an event within that subsystem, create an alert, and leave it to humans to diagnose the ultimate problem. This worked fine for simple systems, and simplistic management paradigms. But as systems rapidly evolved into complex, heterogeneous environments comprised of multiple subsystems, the ability to spot events in a near real-time capacity became nearly impossible. Complex systems can create complex events, where multiple subsystems each give a partial indication of an event, but the reality can’t be understood from any one indicator. In other words, in a complex system, some events can’t be ‘seen’ unless all of the smaller indicators are gathered and correlated to provide a complete, holistic picture of the situation. Principal uses of Complex Event Processing Complex Event Processing (CEP) tools, then are critical for understanding and managing events across complex, distributed systems, to uncover things that are happening in the environment, which might not be detected by any one singe system. As such, complex event processing is used in a number of ways, including: Security intrusion and attack detection – Intrusion detection, almost by definition, relies on complex event processing to detect patterns of attack and intrusion across different systems. For example, a router may log traffic coming from a given IP range, a server may detect a login by an account with admin level rights, and a database might log a a download of data. All of these alone are not necessarily of note. But, taken altogether, they signal a potential compromise in process. Fraud prevention – Certain fraud schemes, especially within financial systems, can be extremely difficult to detect. And just like the security event described above, any one event in a fraud detection event might not be noteworthy in and of itself, but becomes very interesting when correlated with other events to paint a complete picture. Scheduling and control automation – Complex events are not just related to security, however, and detection and management of complex events can also lead to increases in productivity and bottom-line revenues. By detecting events as they are in progress, a CEP tool allows for the automated response to those events, to minimize response to certain events much more quickly than can be achieved via human interaction. CEP tools allow individual systems to modify their settings and controls in response to complex events, in order to either minimize damage caused by that event, maximize gains by capitalizing on the event, or both. Business process automation – Once complex events are understood and modeled, business processes can be developed to manage and capitalize on those events to benefit the organization. CEP tools allow process owners to not only model and capture events, but design and implement those business processes. Common Criteria of Complex Event Processing Tools Complex event processing systems can be used across a variety of complex systems, in a variety of ways. And as noted above, CEP tools can be used not only to detect and mitigate problems, but also to Modeling of complex events and their criteria – One of the key aspects of capturing and managing complex events lies in the ability to model those events, and understand how they manifest themselves within the underlying sub-systems of the environment. This allows the management systems to understand what it’s ‘looking’ at, and identify complex events on a more timely basis. Hierarchical classification of events – Closely tied to the point above is the ability to arrange singular events into a hierarchical classification, which allows for both the prioritization of those events and the remediation and response. This hierarchical classification ties into things like event mapping, to aid in the modeling and classification of the complex events. Event-driven Architecture – CEP tools commonly utilize an event-driven architecture, which captures and correlates individual messages and event indicators, classifies and prioritizes them, and proceeds with automated responses, if those responses have been defined. XMPro and Complex Event Processing Complex event processing is one of the primary functions of the XMPro toolset, and both the Business Activity Monitor and Intelligent Operations Monitor modules are built on CEP principles. XMPro is designed to model and automate both events and the operational and business processes to respond to and capitalize on those events as they occur. XMPro utilizes an event-driven architecture comprised of activity triggers, which are processed into an activity queue and ultimately lead to the invocation of activity agents that allow businesses to respond to events quickly and effectively."
  },
  "docs/resources/faqs/external-content/blogs/2015/herding-cats-what-enterprise-architects-need-to-know-about-business\u00A0process-management.html": {
    "href": "docs/resources/faqs/external-content/blogs/2015/herding-cats-what-enterprise-architects-need-to-know-about-business\u00A0process-management.html",
    "title": "Herding Cats: What Enterprise Architects need to know about Business Process Management | XMPro",
    "summary": "Herding Cats: What Enterprise Architects need to know about Business Process Management Blog Herding Cats: What Enterprise Architects need to know about Business Process Management Posted on May 18, 2015 by Kirsten Schwarzer A Business Process, put simply, is a collection of activities toward a common business goal, usually with the end result being the completion of an objective that leads to increased revenue, reduced costs, improved compliance and better customer service. A business process can be as simple as the process for collecting payments from customers and depositing them in the bank, or as complex as moving natural resources like oil through production and distribution in a complex multinational environment. Business processes are, or should be, one of the key areas of focus for any Enterprise Architect, because the work you do directly affects those processes, and may even create new ones for the business. Understanding the business processes that your organization uses to achieve its goals is critical, because you’re in a unique position to affect, support, and create those process. Creating Information From Data There are a number of considerations that any Enterprise Architect needs to take into account when they’re engaged in Business Process Management (BPM). Obviously, an understanding or catalog of at least the principal business processes used by the enterprise is probably step number one. But a key part of your Enterprise Architect BPM job is to find ways to improve those processes, or develop new processes that benefit the business. To perform those Enterprise Architect BPM activities usually requires a number of tools and methods, and a coherent approach to BPM projects. BPM projects can get very complex very quickly, so care should be taken to understand the actual schedule requirements before the project is begun. Process discovery typically takes the greatest amount of time, but other areas like functional and technical specifications and documentation, tool evaluation and selection, and implementation of the new process all take significant amounts of time as well. The Ideal Process vs. Competing Factors As mentioned above, navigating the business processes of almost any enterprise or organization can be a complex activity. The bias, of course, is to believe that all processes within a given enterprise are rational, and only performed because they bring an easily identifiable value to the organization, and that the way they’re performed is the most logical way to achieve the end result. But while most processes usually start out that way, over time they become subject to obsolescence, politics, budgetary pressures, and other factors that alter the process (or everything around it), so that it’s no longer as efficient as it once was. Part of any Enterprise Architect BPM project is to sort through that mess, to figure out the ‘whys’ in the process, keep any parts that still make sense, and replace or refine those that no longer do. Another aspect of processes that enterprise architects need to take into account is the presence of any regulatory requirements that might affect the processes under evaluation. In some cases, regulatory requirements might help explain why a process functions the way that it does, and those same requirements have to be addressed by any new or refined process that is developed. To that end, the enterprise architect needs to not only understand those regulatory requirements, but they also need to be able to map their BPM project to those requirements, and track that progress. Putting it all into Perspective Once the relevant processes have been captured, generally the next step is to model them, so that a visual representation can be created that lets the team understand how the overall process works, and identify areas for improvement. This is critical for either existing processes under review, or during the development of new business processes, because it helps the team to reduce much of the surrounding complexity and see the overall process in a more simple light. Everything mentioned above just scratches the surface of what’s involved in any enterprise architect’s BPM project. When you consider the financial considerations, business case generation, requirements documentation, tool selection, implementation, and everything else that goes in to such a project, you begin to understand just how complex these projects can get. XMPro iBOS Part of our rationale for developing XMPro iBOS is because we understand that complexity, and the level of support that a BPM tool needs to provide. And one of the things that iBOS takes into account is the amount of unplanned events and unstructured work that should be taken into account by any BPM platform, but often isn’t. iBOS captures those things that shouldn’t impact your business processes, but do. iBOS was built with Business Process Management in mind, and integrates its ability to receive inputs and detect events from a large range of sources like applications, web services, devices, time triggers and people, and integrate them into a single operational view, which not only allows you to track overall operations, but gives you a real time view of the effect of your BPM projects, so that you can track the actual effects of your projects. As an enterprise architect, you need to work within a larger organization, and different groups have their own priorities and considerations when it comes to the processes that affect them and their team. That’s why you need to be able to capture those inputs and incorporate them into your business process review and overall BPM project. To that end, the iBOS tool allows you to gather input from all over your organization, and include them into your overall BPM project, using crowd questions and surveys, social-style messaging, and comments. iBOS also allows you to make those interactions part of the audit trail of a transaction. Using a customizable workspace, iBOS allows you to track your BPM projects by defining the business rules, resources, and scheduling, and provides reporting around all of it. Users can interface with iBOS via Microsoft Outlook, Microsoft Sharepoint, Salesforce, web browser or mobile device. Implementation and Conclusion Once the processes have been captured and modelled, new requirements captured, and the tools for that process have been selected, the next general phase is to implement the new process. Team and project management is critical to this phase, especially when many of the team members are part of different groups and organizations. The tool you use to track their process and the progress of your project should be able to track relevant jobs and tasks, their status, time to complete, and so forth. iBOS was designed to do just this, and provide reporting on these tasks from a variety of standpoints. When you consider all of the factors that go into Business Process Management, it’s easy to get lost in the weeds. There are, after all, a huge number of considerations, inputs, personnel and data points to consider, and it can get overwhelming quickly. But if you take the time to choose a tool that’s purpose-built for enterprise architecture and business process management, it can make your project flow much more smoothly and efficiently, and help you to positively impact the larger organization."
  },
  "docs/resources/faqs/external-content/blogs/2015/how-important-are-processes-to-the-internet-of-things.html": {
    "href": "docs/resources/faqs/external-content/blogs/2015/how-important-are-processes-to-the-internet-of-things.html",
    "title": "How Important Are Processes To The Internet Of Things? | XMPro",
    "summary": "How Important Are Processes To The Internet Of Things? Blog, CEO'S Blog How Important Are Processes To The Internet Of Things? Posted on February 18, 2015 by Pieter van Schalkwyk In my view the question should be focused on the business outcome: How important is it to sense key business events from machine-born data (devices, sensors and IoT), decide if action is needed and then respond in an appropriate manner. Deconstructing the question like this leads to 3 key elements for consideration. Firstly, organizations need to gather Operational Intelligence using event stream processing and complex event processing. To do this the OI platform needs some form of adapters or connectors to easily “Ingest” information into the OI platform. The streaming event data (from IoT or other sources) may need “Conditioning” before it is “Integrated” with contextual business data before “Rules” are applied to identify key business moments or trends. The second element is to provide decision support. It can be as simple as a business rule or it can be more involved with social collaboration (asking others for their opinion) or provide BI style analytical support. Machine Learning can applied at this point for predictive analysis if the use case supports it. The objective of the decision support element is to decide if an action is required. Not all IoT data needs to be acted on. The third element requires an “appropriate” response based on application. Sometimes it can as simple as a well-defined workflow for a planned response to an anticipated exception. BPM adds value in this scenario. More often than not it requires an unplanned response to an unanticipated event. In this case it may be as simple as sending an SMS alert or, as we often suggest to customers, an Adaptive Case Management (ACM) solution to orchestrate the response on a case by case basis. The appropriate response often requires automation of actions in another business system such as the Supply Chain Management module of an ERP solution. In that case BPM does not play a role as an OI solution with agent behavior can accomplish this. There is no simple answer for dealing the sea of data from IoT other than creating some Operational Intelligence from it, deciding if any action should be taken, and then using the appropriate technology to manage (machine or human) the response. BTW my simple definition on the difference between BI and OI is that BI requires storing data and then creating some retrospective intelligence over it (what happened) where OI first interrogates event data (what is happening) before deciding if it should be stored."
  },
  "docs/resources/faqs/external-content/blogs/2015/how-to-reduce-operational-costs-by-36-with-predictive-analytics.html": {
    "href": "docs/resources/faqs/external-content/blogs/2015/how-to-reduce-operational-costs-by-36-with-predictive-analytics.html",
    "title": "How To Reduce Operational Costs by 36% with Predictive Analytics | XMPro",
    "summary": "How To Reduce Operational Costs by 36% with Predictive Analytics Blog How To Reduce Operational Costs by 36% with Predictive Analytics Posted on October 23, 2015 by xmpro Tony Cosentino of Ventana Research, a leading research and benchmarking firm, recently published these comments on the results of study into the benefits of using predictive analytics in their business operations. “One of the key findings in our latest benchmark research into predictive analytics is that companies are incorporating predictive analytics into their operational systems more often than was the case three years ago. This trend is not surprising since operationalizing predictive analytics – that is, building predictive analytics directly into business process workflows – improves companies’ ability to gain competitive advantage: those that deploy predictive analytics within business processes are more likely to say they gain competitive advantage and improve revenue through predictive analytics than those that don’t.” The challenge comes in knowing where and how to use predictive analytics in your business. We compiled this matrix to highlight specific use cases by industry where customers are seeing value in using predictive analytics baked into their Operational Intelligence and Business Process Management platforms."
  },
  "docs/resources/faqs/external-content/blogs/2015/index.html": {
    "href": "docs/resources/faqs/external-content/blogs/2015/index.html",
    "title": "2015 Blogs | XMPro",
    "summary": "2015 Blogs Articles from the XMPro blog published in 2015. Articles 10 Predictive Analytics Use Cases By Industry 6 Myths About Machine Learning copy-me Does Operational Intelligence Make Business Intelligence Obsolete? From Many, One – The Nature of Complex Event Processing Herding Cats: What Enterprise Architects need to know about Business Process Management How Important Are Processes To The Internet Of Things? How To Reduce Operational Costs by 36% with Predictive Analytics Understanding the Value of Real Time KPI Management as Your Next Strategic Project What is a “Business Moment” in your business?"
  },
  "docs/resources/faqs/external-content/blogs/2015/understanding-the-value-of-real-time-kpi-management-as-your-next-strategic-project.html": {
    "href": "docs/resources/faqs/external-content/blogs/2015/understanding-the-value-of-real-time-kpi-management-as-your-next-strategic-project.html",
    "title": "Understanding the Value of Real Time KPI Management as Your Next Strategic Project | XMPro",
    "summary": "Understanding the Value of Real Time KPI Management as Your Next Strategic Project Blog Understanding the Value of Real Time KPI Management as Your Next Strategic Project Posted on April 28, 2015 by Kirsten Schwarzer It’s easy to get caught up in a given project. An executive sponsor, in response to a given business or competitive situation, tells the engineering team that a particular functionality or platform is a requirement, and needs to be in place as soon as possible. Almost overnight, a project plan is created, an architecture is defined, resources are engaged, and DevOps formulates a methodology for supporting the new platform. But, as with many other projects in the past, the question is asked – is this project really providing value to the business? Is it really fulfilling the objective(s) it was intended to address? And if the answer is yes, then comes the next question – how do you know? Designing the Right Processes to Meet the Right Goals It’s your job as an Enterprise Architect or team lead or CIO to make sure that the projects that your team builds and implements are built and managed in a responsible manner. And that means defining data points, or indicators that will help you track whether the system you’ve built is truly functioning as it should, and whether it’s providing value to the larger enterprise. For most of us involved with enterprise architecture and systems design, the concept of Key Performance Indicator (KPI) metrics is pretty basic, and we’ve been designing them into systems for a long time now. But the reality is, no matter how sophisticated we get, the business still requires new functionality, development timeframes get compressed, and systems become very complex, to the point that defining and managing the right KPIs becomes a lot more challenging than we’d like. But KPI management, especially in real-time, is critical to the success of any enterprise, as it provides insight into the operation of the business, as those operations are happening. Data is Great – Decisions are Better The collection and analysis of KPIs valuable, no doubt – this post has done nothing if it hasn’t made that point. But data isn’t of much value unless it leads to conclusions which ultimately produce action. And that’s the real value of implementing KPI management: good KPI management systems not only capture data, but evaluate that data against a set of criteria composed of business and performance objectives, and provide recommendations as to possible courses of action. Every process should have a goal. That’s a given. And once you automate that process, you should be able to map your KPIs to those process goals, and track your progress in meeting those goals. And based on those metrics, you should have the capability to generate predictive next-action recommendations. This is where the value of that data truly makes itself felt. Data Is Great, Decisions are Better – and Intelligence May Be Best Of All Gathering data from existing processes is valuable, and generating KPIs from that historical data has value as well. But no business exists in a vacuum, and every market is a constantly changing landscape that can drive your business in different and unexpected directions. Because of this, any operational management tool should be able to collect and provide intelligence from internal and external data streams, and incorporate that intelligence into not only the decision-making process for the company, but also into the KPI metrics and ongoing operations reporting. If you can’t adjust your KPIs as new data is gathered, you lack the ability to adjust your operations to new business realities. And After the Decision Has Been Made – Action Success in business is about nothing if not intelligent decisions leading to real action. And it’s because of this that any business operations tool should not only provide data and intelligence, but provide an integrated capability to track those actions. Things like process goals become more valuable when you associated them with activity, and those activities in turn become more valuable when their progress and completion are tracked. Things like resource scheduling, collaboration, case management and analytics should all be part of your business operations approach. The Real Value of Real Time KPI Management Obviously, I’m biased – after all XMPro provides software that focus on the management of real time KPI metrics – but the truth is that while it’s necessary to define the requirements and the design of a system, without the right data generated from that system, which in turn supports the right decisions and actions, the system is of limited value. If you can’t see into the processes you’re following in your enterprise, map those processes to real time KPI metrics, and track your operational activities to meet your operational goals, you have a problem. But if you can see how a business or operational goal leads to a process, how that process is measured through the use if KPIs, and track the successful completion of those goals, it can lay the foundation for not only successful operations, but strategic growth initiatives as well. And that alone is an excellent reason for creating real time KPI management as your next strategic project."
  },
  "docs/resources/faqs/external-content/blogs/2015/what-is-a-business-moment-in-your-business.html": {
    "href": "docs/resources/faqs/external-content/blogs/2015/what-is-a-business-moment-in-your-business.html",
    "title": "What is a “Business Moment” in your business? | XMPro",
    "summary": "What is a “Business Moment” in your business? CEO'S Blog What is a “Business Moment” in your business? Posted on March 23, 2015 by Pieter van Schalkwyk (This article was first published on LinkedIn Posts) Leading research firm Gartner is talking about “Business Moments” as a key focus point for Digital Business and Operations. Gartner defines a “Business Moment” as a “transient opportunity, exploited dynamically”. So what does this mean? A transient opportunity means that it is only an opportunity for a limited period of time. A customer in your store (virtual or bricks and mortar) is only an opportunity when the customer is there. Knowing that the customer is there is key to exploiting a “business moment” in this example. We call this Situational Awareness. It can also be seen as real-time Operational Intelligence, rather than Business Intelligence that is retrospective (looking at insights over historical data). The value of information about a Business Event diminishes over time as mentioned above. If we know about eminent equipment failure within a few minutes, we can respond proactively, whereas knowing about it in a few hours reduces our ability to prevent failure and quite often it is too late to do anything about it. The same way that we can do something when we know there is a “loyal” customer in our store or e-commerce site. The sooner we know, the sooner we can tell someone who cares. Exploiting these Business Moments dynamically requires that you, firstly, know more about the context of the opportunity and, secondly, know how to respond. This requires Operational Intelligence where real-time event data is combined with other contextual data in your business to create new “intelligence” that improves your situational awareness. In the example of the customer on an e-commerce store, we can combine event data from web click streams with information in our CRM system to match the customer to our records. Once we know the context (platinum customer) and relevance (in the store now), we can provide a personalized discount offer or have an informed sales agent engage with the customer. Using technology that is commercially available today, we can even predict the customer’s likelihood to buy using a customer propensity machine learning model. Creating new Operational Intelligence that improves situational awareness and supporting it with intelligent decision support for the appropriate actions to follow, is how smart businesses exploit these Business Moments. Finding these Business Moments is key to acting on them. Event Stream Processing (ESP) and Complex Event Processing (CEP) provides the mechanisms to actively listen for these events that impact business operations. (read more on Sense > Decide > Act here) How will you find those Business Moments in your business operations that will have a significant impact on revenue, cost, governance and customer relations if you can find them early enough to exploit the opportunities that they offer?"
  },
  "docs/resources/faqs/external-content/blogs/2016/3-ways-the-internet-of-things-is-transforming-field-service.html": {
    "href": "docs/resources/faqs/external-content/blogs/2016/3-ways-the-internet-of-things-is-transforming-field-service.html",
    "title": "3 Ways The Internet of Things is Transforming Field Service | XMPro",
    "summary": "3 Ways The Internet of Things is Transforming Field Service Blog 3 Ways The Internet of Things is Transforming Field Service Posted on January 11, 2016 by xmpro The mobile app economy made us become accustomed to being ‘always on’, having the right information when we need it and getting real-time notifications of events as they happen. We can’t imagine a world without our connected smartphone. The Internet of Things is changing Field Service in a similar way. Situational awareness through sensors and real-time data is changing the competitive landscape of Field Services. – Pieter van Schalkwyk, CEO XMPro We see 3 ways in which the Internet of Things is changing field service: Predictive maintenance is delighting customers In February 2015, the Aberdeen Group surveyed 219 service and manufacturing organizations. They found that customer satisfaction was the #1 indicator for measuring the success of a service organization.1 Best in class organizations are already connecting their equipment to the Internet of Things. And as a result they’re seeing customer satisfaction improve. How did they get started? By investing in an Intelligent Business Operations Platform that makes it possible to use real-time data from sensors and the IoT with predictive analytics to predict and prevent asset failure. This allows them to be proactive instead of reactive. An Intelligent Business Operations Platform like XMPro embeds actionable predictive analytics into your business processes. This makes it easy for your technicians to make data-driven decisions and achieve what wasn’t possible before – fixing things before they break. Higher asset uptime leads to more satisfied customers who continue spending their money with you. Real-time data is leading to more first-time fixes More than 4000 people from all demographics were recently polled about their experience with field service technicians. They were asked about the #1 reason for their problem not being fixed the first time. The most frequent answer? The technician didn’t have the correct parts with him to finish the job.2 But, for most organizations we speak to, field inventory is at an all-time high and technicians are still complaining that they don’t have the right parts. The Internet of Things now makes it possible to bridge this gap between what your technicians have and what they need. The premise is simple. When you know what the problem is you’ll have a better understanding of the parts you’ll need to fix it. Giving technicians access to real-time data about an asset’s condition makes it possible to diagnose issues remotely. This allows them to find solutions before they leave for the customer’s site. And it means they’ll need less ‘just-in-case’ inventory in their truck. Let’s say you have 10 000 field service trucks with $3000 of rolling inventory in each. That’s $30 million driving around. By making it easy for technicians to know which parts they’ll need, you can reduce the rolling inventory in each truck to $2000. This brings your total rolling inventory down to $20 million. And gives your CFO $10 million less to worry about. Combining remote diagnostics and real-time parts management, will reduce the number of truck rolls you need to do. Having access to live data about rolling inventory makes assigning work orders a much easier task. By knowing which parts are required for a job and being able to see who has those parts with them, scheduling becomes more efficient and effective. Outcome-based revenue models are taking you from provider to partner Paul Daugherty, CTO of Accenture, defined the outcome economy in his WSJ article as: “Where companies create value not just by selling products and services, but by delivering solutions that directly produce quantifiable results.”3 Adopting outcome-based revenue models can change the relationship you have with your customers. The Internet of Things is making it possible for you to provide customers with outcomes and not just services. How could you change your business model to take advantage of the outcomes your customer wants? Could you deliver asset uptime as an outcome? Could you provide recommendations on how to improve processes based on the asset data you’re collecting? Conclusion Your field service organization is uniquely positioned to make the most of the opportunities presented by the Internet of Things. By combining predictive maintenance, remote diagnostics and outcome-based revenue models you can decrease service costs, improve first-time fix rates and build lasting relationships with your customers."
  },
  "docs/resources/faqs/external-content/blogs/2016/7-types-of-industrial-iot-data-sources-and-how-to-use-them.html": {
    "href": "docs/resources/faqs/external-content/blogs/2016/7-types-of-industrial-iot-data-sources-and-how-to-use-them.html",
    "title": "7 Types of Industrial IoT Data Sources (And How To Use Them) | XMPro",
    "summary": "7 Types of Industrial IoT Data Sources (And How To Use Them) Blog 7 Types of Industrial IoT Data Sources (And How To Use Them) Posted on August 30, 2016 by xmpro Where does industrial IoT data come from? Most people would say it comes from assets like pumps, turbine engines and drilling rigs. But there’s more to industrial IoT than machine data. Machine data doesn’t tell a complete story in every case. By combining data from disparate sources you can create new insights. And give engineers a complete view of the problem they need to solve. In this post, I’ll show you the 7 different types of data sources you can use to create IoT applications. 1. Industrial Control Systems IoT makes it possible to leverage the data you already have in your SCADA system or historian. A lot of companies we talk to have been gathering data in these systems for almost 30 years. But this data could only be used retrospectively until now. Machine learning services like Cortana Analytics, SAP HANA and IBM Watson have opened the doors for IoT-based predictive maintenance. By applying a machine learning algorithm to your SCADA data, you can predict a pump failure. Or determine the remaining useful life of a turbine engine. If you already have a large volume of machine log data, machine learning will help you put that data to good use. But knowing about an imminent failure isn’t enough. That’s why our IoT Application Suite has a strong focus on driving real-time actions. 2. Business Applications Data silos are still very common in industrial organizations. And this leads to missed opportunities because the data is already there. The right people just don’t have access to it when they need it. Data from applications like your CRM, ERP or EAM can provide context that goes beyond what’s wrong with a machine. IoT-enabled field service can dramatically improve customer experience. Giving technicians access to CRM data from their tablet shows them a detailed customer history. And they won’t have to call the office to answer the customer’s questions. You can also build upon predictive maintenance with business data. When the machine learning algorithm predicts an asset failure you connect to your EAM system and check the warranty. If the EAM data shows that the asset is still under warranty, you don’t send a maintenance crew. Instead, you can have it kick off a task for someone to call out the manufacturer to fix the problem. By automatically checking the warranty, you can prevent compromising warranties and reduce maintenance costs. 3. Wearables Data from smart watches and fitness trackers aren’t as useful as machine data for IIoT. But there is a new breed of industrial wearables making a name for itself. These new wearables promise to make difficult and often dangerous jobs safer and easier. Data from wearable gas detection sensors can track employee exposure levels. That data can then be displayed alongside their work schedule. This helps dispatchers adjust the schedule based on the worker’s exposure. And ultimately it leads to fewer health issues. Another wearable that’s gaining popularity with large mines and constructions companies is the SmartCap. The SmartCap was created to prevent accidents. It measures truck driver fatigue levels by monitoring their brain activity. When it picks up driver fatigue, an alarm will trigger to stop the driver and also let their manager know of the event. The possibilities to use this data go even further than just sounding alarms. If you have a lot of drivers, you can use machine learning to predict where and when they are likely to get tired. This also helps you improve schedules, routes and safety practices. You’ll know which times and areas are high risk for fatigue. 4. Sensors & Devices Advances in sensor technology have made streaming real-time data easier than ever. Temperature, flow, pressure and humidity sensors have become big sources of industrial IoT data. Sensors like this one from Libelium simplify remote water quality monitoring. Process industries produce waste water that could contaminate drinking water if procedures aren’t followed. Contamination does damage to more than the environment. It often results in a PR disaster for the company responsible. By monitoring water quality, you can respond to contamination faster than ever before. But what if you could predict the contamination before it happened? That’s what the next type of data source is for. 5. Open & Web Data What’s the most common example of using open and web data? It’s usually how to improve customer service by using social media posts. You employ a sentiment analysis algorithm and respond to negative posts quicker. But in this post, we’re going to cover an industrial story that builds on the water contamination example. Here’s how you can use web data to prevent waste water in effluent dams from overflowing and killing cows on the farm next door. Using online weather services, you can predict when effluent dams are likely to overflow. Which means they are likely to contaminate water in the surrounding area. Combine that with map data and you can also predict which specific reservoirs are in danger. This means you can take preemptive action and prevent the contamination from happening. Open data sources aren’t limited to weather, traffic and maps. You can also use open data from places like the NYC Open Data project. 6. Media Smartphones have made it possible to get real-time access to photos, videos and audio from the field. But in industrial cases, we can go beyond using smartphones to upload a picture of a broken machine. One way to use media as a data source in oil and gas is to stream real-time infrared images when inspecting flare stacks. Flare systems need to be inspected regularly for fouling and corrosion. By using a UAV to do the inspection, you can get information without interrupting operations. You also won’t be putting workers in danger. Keep an eye out for a more in-depth use case we’ll be publishing about this soon. 7. Location Location data could come from mobile devices, location beacons, GIS systems or even drones (UAV’s). You could combine GPS data from a vehicle with traffic reports to optimize your delivery routes in real-time. Or you could place track-and-trace sensors on expensive mobile assets that often get stolen or misplaced. But to prove how powerful the use of real-time location data can be, let’s take the example of avoiding accidents with mining vehicles. Mining trucks accidents are often fatal. Like this accident in 2013, where a contractor’s Toyota Land Cruiser collided with a loaded dump truck weighing 380 tons. Because the truck driver is seated in such an elevated position, it is often hard to see what’s happening directly in front of him. Streaming real-time data from location beacons can help prevent fatal accidents like these. When a vehicle passes a beacon, the IoT application can automatically check whether the vehicle has the correct clearance certificate. You can also add GPS data displays (similar to radars in aircraft) to show truck drivers where light vehicles are around them. Conclusion There’s more to industrial IoT than just using machine data for predictive maintenance. By using and combining these 7 types of industrial IoT data sources, you can enable smarter decision making and faster responses across your organization. You’ll see the results in your bottom line, customer happiness and your safety record."
  },
  "docs/resources/faqs/external-content/blogs/2016/copy-me.html": {
    "href": "docs/resources/faqs/external-content/blogs/2016/copy-me.html",
    "title": "| XMPro",
    "summary": "Blogs: 2016 How To Get Started With Industrial IoT How To Overcome The Top 5 Challenges To Industrial IoT Adoption What is an IoT Platform vs. an IoT Business Application Suite? Industrial IoT: How To Get Started with Predictive Maintenance 3 Ways The Internet of Things is Transforming Field Service 7 Types of Industrial IoT Data Sources (And How To Use Them)"
  },
  "docs/resources/faqs/external-content/blogs/2016/how-to-get-started-with-industrial-iot.html": {
    "href": "docs/resources/faqs/external-content/blogs/2016/how-to-get-started-with-industrial-iot.html",
    "title": "How To Get Started With Industrial IoT | XMPro",
    "summary": "How To Get Started With Industrial IoT Blog How To Get Started With Industrial IoT Posted on January 22, 2016 by Pieter van Schalkwyk Is the Industrial Internet of Things (IIoT) a solution looking for a problem? There is currently a lot written about the 25-50 billion devices (depending who you talk to, but either way, it’s a lot of smart sensors) that will make up IoT by 2020. Analysts predict that around 40% of that will be industrial (it depends on which analyst you talk to, but again, it’s a huge number). What is often overlooked is the prediction that it will take 5 million applications to manage these 50 billion devices. Alas, a lot of apps and software applications will be needed to “make it work”. Most of the focus at the moment is on the device, sensor and cloud aspects of both IoT and IIoT. There seems to be a fascination with the blinking LEDs on the bare boards and the cloud stores where we push the sensor readings to. We are building great “solutions” that can push more data through the firehose of big data that is already streaming into our businesses. It is a deluge of data. When we speak to clients we hear “we don’t know what to do with all the data we are collecting” and others say “we know that there are great benefits in using IIoT, we just don’t know where to start or how to articulate the benefits or ROI”. Therein lies the problem. Too often we find that IIoT solutions start with “we have sensors, what can we measure for you?” rather than “what business problem do you need to solve and how will you build a business case for it?”. That is also the way to get started with IIoT. Start with the problem and work your way to the sensors Our interest as engineers and developers is with the gadgets, sensors, and blinking lights and that is most often our starting point. The right approach is to start with the business problem and understand what data you need for operational intelligence and situational awareness. Then see what sensors will be able to give you that. We generally need a lot less sensors and data than what we had when we started the other way around. Typical business problems in industrial use case include: We need to optimize well production across an oil field by identifying those wells that have declining production or those that are predicted to decline based on real-time lead indicators that we can monitor. We need to improve the first-time fix rate of our field service crews and reduce the number of truck rolls by providing the right insight on the performance and diagnostics before they do a customer or plant call. A big differential on inlet and outlet pressures may indicate a filter issue and the field service technician can go with the right spares and improve first-time fix rate. Not only does this improve customer satisfaction, but it also improves safety (less time on the road), it reduces the inventory on trucks, and increase the profitability of field services. Both these examples illustrate the benefit starting with the problem and working your way to the sensors. It is a solution for a real problem, not a solution looking for a problem. It makes it easy to define the metrics to use for the business case and pilot. It also provides insight to the requirements of the app or application that will be used as the “front-end” to your solution. Build a pilot IoT application to prove the business case, then scale it out Once you know what problem you are solving and how you will measure the benefits, the next step is to create a pilot to validate your business case on ROI. IIoT requires investment in sensors, connectivity, applications and people. Right now there is very little empirical evidence of “how much will it cost”. There are very few large-scale, enterprise deployments with publicly available cost and ROI metrics. This means you will have to prove the value by creating a pilot which can later be scaled. There are key differences between pilots and enterprise deployments (which I will address in a separate blog post) but a pilot makes it easy to prove the ROI, reduce business risk and overall costs. Use a BYOA IoT Application Suite to get started with IIoT Both “start with the problem” and “build a pilot to prove the ROI” require a customized approach to get started. There is no end-to-end or COTS (commercial, off-the-shelf) application that will do that for you. Just collecting data and publishing it to a dashboard doesn’t represent how this will work in a full enterprise deployment. BOYA (Build Your Own Application) using an IoT Business Application platform like XMPro IoT makes it easier to get started with real IIoT applications and brings these benefits: A quick time-to-market for innovative IoT applications – create IoT applications in days rather than months with XMPro’s comprehensive model driven design tools. Extend the use of IoT data beyond OT by integrating it with other internal and external data sources – create new information mashups for decision support that didn’t exist before. Extend interventions beyond notifications and alarms; create rich forms, workflows, and processes that drive actions and not only insights. Works in complex, distributed environments as is typically the case with industrial applications – work on-premise, in the cloud, or a hybrid of both. Integrate and leverage other IoT device platforms and machine learning as part of your applications – focus on the business problem, we look after the tools. Easy to change or extend – create as many applications as you need to address all your IoT needs, from pilot to the plant. Getting started is not hard, just start at the business end and work your way to the technology end. Pilot and prove before you scale. Don’t let IIoT become a solution looking for a problem in your business."
  },
  "docs/resources/faqs/external-content/blogs/2016/how-to-overcome-the-top-5-challenges-to-industrial-iot-adoption.html": {
    "href": "docs/resources/faqs/external-content/blogs/2016/how-to-overcome-the-top-5-challenges-to-industrial-iot-adoption.html",
    "title": "How To Overcome The Top 5 Challenges To Industrial IoT Adoption | XMPro",
    "summary": "How To Overcome The Top 5 Challenges To Industrial IoT Adoption Blog How To Overcome The Top 5 Challenges To Industrial IoT Adoption Posted on August 15, 2016 by xmpro The key to successful industrial IoT adoption is to ensure you start with an IOC. According to the 2016 Current State of IIoT research study, 4 out of 5 manufacturing leaders say adopting IIoT technology is critical to their future success. They believe industrial IoT has the potential to grow their organization, make it more agile and improve compliance. But there are challenges that need to be addressed to implement a successful IoT initiative. The Key To Successful Industrial IoT Adoption Initial Operational Capability (IOC) is a military term used to describe when one or more subsets of the capability can be deployed on operations. For industrial IoT applications, that means creating a fully-functional application that addresses each of these problems for a small subset of assets. How is an IOC different from a POC? A POC is a once-off experiment that proves you can solve the technology problem. But it’s not something you build upon over time. In contrast, an IOC proves that you can solve the business problem. It considers more than just technology. An IOC also considers data security, a valid business case, data quality, staff skills and how long it takes to create new applications. By focusing on setting up full operational capabilities for a specific business problem, you can take a systematic approach to overcoming the challenges of successful IoT adoption. So how does an IOC address critical challenges that industrial organizations face when adopting IoT technology? 1. Data Security Concerns Using an IOC approach allows you to implement Security By Design from the start of your industrial IoT journey. By starting out small you can secure your devices, connections, cloud data storage and applications. Without too many variables to control you can set things up the right way from the beginning. 2. No Clear Business Case for IIoT At XMPro we believe in starting with the business problem you want to solve and not the technology. With an IOC, you can choose a specific business problem that you want to solve. And then prove the ROI without investing millions in new infrastructure and technology. If you’re looking for ideas go to our Use Case Library. Once you have a few ideas for business problems you can solve with IoT, it’s time to rank them by readiness and business impact. Interested in doing IoT-based predictive maintenance on assets? Learn more about getting started in this blog post. When you’ve determined the business problem you want to solve the next question to answer is, ‘What are the key ROI metrics?’. This is going to help you stay focused on the business problem and prove how industrial IoT can impact your organization. 3. Data Quality Issues One of the main opportunities industrial IoT presents is the ability to make data-driven decisions. But in order for that to work, the data needs to be as accurate as possible. Starting with an IOC helps you find problems in data quality with a manageable dataset. You can set up automatic data cleaning and transformation in XMPro. The software can fill in missing values and normalize data to make it more usable. The cleaned data can then be put into a machine learning model to test the accuracy with minimal setup. Once you’ve proven that this results in data you can use to make better decisions, you can add these data cleaning and transformation actions to all of your future IoT use cases inside XMPro. 4. Insufficient Skills of IT Staff Most industrial companies don’t have the IT resources or skills to build an IoT solution internally in a way that’s cost-effective and fast. Internal projects of this nature often try to reinvent the wheel and eventually run over deadlines and budgets. But using an IOC approach along with an Agile Application Suite like XMPro can help you build IoT solutions without being limited by your IT team’s resources. First, you need to choose an IoT Application Suite that lets you create applications without having to code. We built our code-free application suite to empower engineers to build Industrial IoT applications. Why engineers and not IT? Because engineers understand the business problem they’re trying to solve. And this approach makes it possible for your IT team to focus on implementing security and governance best practices. Using an IoT Application Suite means you won’t need to build custom integrations. And you can plug machine learning engines like Azure Machine Learning, SAP HANA or IBM Watson into your technology stack using a drag and drop interface. But what about the algorithms you need for machine learning? The field of data science has grown exponentially in the past few years. There are a number of alternatives to hiring in-house data scientists to create algorithms for your machine learning models. XMPro works with teams of data science experts to help you get the most predictive power out of your data. You could use open source galleries for common models like predictive maintenance or sentiment analysis. Or you could use an algorithm marketplace like Algorithmia, which charges per API call. Another option is to host a competition on Kaggle, where data scientists from more than 100 countries compete to create a winning model to solve your problem. 5. Inability to do fast experiments After implementing an IOC you can build on and expand the initial capabilities in an iterative way. We recommend using an Agile approach to continue developing your IoT applications. Because engineers can build their own applications with a suite like XMPro, you can set up your first IoT application in a matter of weeks. And use what you’ve built to scale across different asset types and use cases in record time. Conclusion By using the IOC approach you can overcome the top 5 challenges organizations face when it comes to industrial IoT adoption. The IOC approach can help you grow your revenue, reduce costs and improve compliance. The key is to build an application with full operational capability on a small scale to address each of these challenges. Then you’ll start to see what the industrial IoT hype is all about."
  },
  "docs/resources/faqs/external-content/blogs/2016/index.html": {
    "href": "docs/resources/faqs/external-content/blogs/2016/index.html",
    "title": "2016 Blogs | XMPro",
    "summary": "2016 Blogs Articles from the XMPro blog published in 2016. Articles 3 Ways The Internet of Things is Transforming Field Service 7 Types of Industrial IoT Data Sources (And How To Use Them) copy-me How To Get Started With Industrial IoT How To Overcome The Top 5 Challenges To Industrial IoT Adoption Industrial IoT: How To Get Started with Predictive Maintenance What is an IoT Platform vs. an IoT Business Application Suite?"
  },
  "docs/resources/faqs/external-content/blogs/2016/industrial-iot-how-to-get-started-with-predictive-maintenance.html": {
    "href": "docs/resources/faqs/external-content/blogs/2016/industrial-iot-how-to-get-started-with-predictive-maintenance.html",
    "title": "Industrial IoT: How To Get Started with Predictive Maintenance | XMPro",
    "summary": "Industrial IoT: How To Get Started with Predictive Maintenance Blog Industrial IoT: How To Get Started with Predictive Maintenance Posted on June 24, 2016 by Pieter van Schalkwyk Predictive maintenance is one of the areas that benefit most from machine learning algorithms with predictive capability. Predictive maintenance has always focused on how to predict when certain conditions are going to occur and when machines will fail. With the advent of machine learning and the ability to do it at scale, you now have a unique use case. Predictive maintenance is not just reserved for a few large organizations anymore. It’s now available to use for a broad range of asset-intensive industry applications. How To Get Started We often get questions around, “How do we get started with predictive maintenance?”. And especially, “How do we use the Internet of Things to do predictive maintenance, not just on an ad hoc basis but on a real-time continuous basis?” Before I jump into what IoT predictive maintenance solutions look like, here’s a quick history of how maintenance has evolved over the last couple of decades. The Evolution of Maintenance You started off with reactive maintenance. When things broke down, you would go out and fix that. Then some preventative maintenance started to occur where you would do it based on fixed schedules. It’s like when you take your car in for scheduled maintenance every 10 000 miles or every 12 months. There’s a certain threshold and that dictates when maintenance will occur. You would often maintain equipment that doesn’t need maintaining. Or you would find things break down before that scheduled interval. In the case of warranties, manufacturers are trying to limit their risk. So they create those service intervals to minimize any impact it would have for them. Using the manufacturer’s warranty that they set up initially as your continued interval for scheduled maintenance may not be the best schedule to maintain assets on a long-term basis. Quite often assets are maintained on a higher frequency than what is required. And quite often those maintenance interventions also create what we would call maintenance induced failures. If it ain’t broke don’t fix it is another way of looking at that. You got smarter around understanding the condition of equipment by taking measurements with vibration monitors or ultrasonic devices. You would listen to cavitation on pumps and measure the vibration on certain equipment. You created a predictive maintenance capability but it was still manual. This has been around for quite a while but it often required an operator going out with a measuring device. They would have to physically go out on a schedule, take certain readings, capture that data and plot it out. They would spend hours creating spreadsheets. Then based on their analysis, they would come up with a predictive maintenance schedule based on the condition of the equipment. It was a great idea but the execution was done on an ad hoc basis. IoT-based Predictive Maintenance The advent of Internet of Things is changing the way we do predictive maintenance. You can now have real-time monitoring devices at a low cost that send data to an algorithm on a continuous basis. It can detect whether there’s something going wrong with a machine or use machine learning to make a prediction. That is where predictive maintenance is going now with what we call IoT-based predictive maintenance. Here are the four main blocks of how maintenance has evolved: Reactive maintenance Preventative maintenance Manual predictive maintenance IoT-based predictive maintenance (where we are right now) You can now take advantage of having real-time sensors installed on equipment. They will provide real-time data that you can put into predictive models to help you determine when something is about to fail or what the remaining useful life is for that equipment. You can then schedule maintenance based on that data. What is the Internet of Things? Another question that we often get, “What is the Internet of Things?” Here’s a great definition by Naveen Balani in his book Enterprise IoT: A Definitive Handbook: Internet of Things is a vision where every object in the world has the potential to connect to the internet and provide their data so as to derive actionable insights on its own or through other connected objects. The Internet of Things is not about connecting a whole bunch of sensors and just using that to run your factory on a day-to-day basis or get information, which is typically on the automation control side of the business. The value comes from taking that information available to the business side of your organization. This will allow them to create actionable insights like predictive maintenance. Combine Real-Time & Contextual Data We’re getting insight into the condition of the equipment and then based on that we can plan interventions. We can either take one sensor or it can be a combination of many sensors and data sources. We may combine weather information with equipment data and then determine when to send a crew for predictive maintenance. The traditional challenges with predictive maintenance were that the data was limited to sensor data. And it was mostly used for operational control, where we wanted to control the machines, make them faster, slower, or switch them on and off. From an automation perspective just around the control side of things. Machine Learning at Scale Predictive maintenance was often based on predictive analytics that required expert data scientists and complex machine learning models. This meant that machine learning was limited to a few large organizations. It wasn’t something you could do at scale. Even at those large organizations, you had to do it on a batch basis. You would go out and take readings and put that data into the machine learning tools to come up with schedules. A lot has changed since them. You now have the ability to continuously parse data through machine learning models and continuously get predictions back. The new low costs mean that it’s not just reserved for large organizations anymore. It’s also available to small and medium-sized businesses. It’s available to OEM’s and equipment manufacturers that want to provide predictive maintenance services with the equipment they are selling. The #1 Question You Need To Answer The biggest question that we get is, “How do I know where to start with this predictive maintenance thing?”. The best way to approach this is to say, “Where will I find the most ROI for this?”. What you don’t want to do is start it with all the equipment and all the potential things that happen in your business. You want to narrow it down to specific pieces of equipment that will give you the best return on investment in the short term, and then expand that out. We often start with, “Where’s the ROI in the business?”. That’s more of a problem driven approach than a discovery-driven approach. A discovery-driven approach, which we see quite often, is where you say, “I’ve got all of this data, what can I do with it?” A problem-driven approach focuses on something specific that you want to do. “I want to reduce the downtime of my cooling tower pumps. If I can reduce that by ten percent I know what the impact will be. What maintenance tasks am I looking to improve?” Start With The Business Problem We like to restate the problem. Doing this ensures that we start with the business question that we’re trying to answer. We don’t start with, “I have sensors, what can I predict?” We’d rather start with, “What would the impact of zero downtime be for certain of my critical assets?” For example, “Zero downtime on my cooling tower pumps will increase production yield by fifteen percent.” That helps you to identify the assets that are suitable for predictive maintenance. Not all equipment behaves in a way that you can predict it’s likelihood to fail. Because of that, you also need to do a further analysis by taking a reliability centered maintenance approach. Define what the failure modes are and whether those failure modes are completely random like you would find with a light bulb, or if you can understand the deterioration and predict that it is likely to fail. Analyze & Rank Your Assets Above we have a ranking matrix where you take the potential failures or the historical failures that occurred on certain equipment, rank the number of occurrences and add the business impact that it has. For example: If a cooling tower pump goes down, does it bring down the whole production line? If it does, the impact is massive. Taking a matrix based approach like this, we analyze the failure modes for specific critical equipment. It then allows you to create a ranking matrix with four quadrants. You look at the business impact of a certain failure and assess your ability to do predictive maintenance on it, by asking questions like: Do we currently have sensors on it? Is it something that we can predict? That will give us a ranking matrix. We do this on a scale of one to ten on each axis. Now we look at the cooling tower pump example and the failure mode where there is complete pump failure. We have existing flow meters on it and the impact is significant. This is an area where we would like to start with predictive maintenance right now. Just below that are two other areas where the pump fan is not starting, or the fan won’t turn off. This means the water temperature is wrong, which is affecting the supply to the production plant. Those two scenarios don’t have detection capabilities yet, but the impact is high. It’s easy to add the IoT sensor-based detection capabilities. In that instance, we can plan for that and get it ready. The other two axes are where the business impact is low. Here you need to consider whether it’s worth your while. Conclusion This outlines our approach to deciding which assets provide the most leverage for IoT-based predictive maintenance. It gives you a business-oriented system to follow, which focuses on the ROI of using machine learning algorithms to predict equipment failure."
  },
  "docs/resources/faqs/external-content/blogs/2016/what-is-an-iot-platform-vs-an-iot-business-application-suite.html": {
    "href": "docs/resources/faqs/external-content/blogs/2016/what-is-an-iot-platform-vs-an-iot-business-application-suite.html",
    "title": "What is an IoT Platform vs. an IoT Business Application Suite? | XMPro",
    "summary": "What is an IoT Platform vs. an IoT Business Application Suite? Blog, CEO'S Blog What is an IoT Platform vs. an IoT Business Application Suite? Posted on August 26, 2016 by xmpro With the number of connected IoT devices growing exponentially (common estimates are 20-50 billion by 2020, and 500 billion by 2030), one is reminded of the famous quote by Robert Metcalfe: In network theory, the value of a system grows as approximately the square of the number of users of the system. Clearly, connecting to these devices and leveraging the information they produce has tremendous potential. Furthermore, McKinsey estimates that “40 percent of the total value that can be unlocked with the Internet of Things requires different IoT systems to work together.” There are currently more than 360 “IoT Platforms” in the market1 and the number is continuing to grow (also at a seemingly exponential pace!). Not every platform is the same, however, and this is a topic we spend considerable time addressing at XMPro. What is an IoT Platform? In its most simple form, an IoT platform is just about enabling connectivity between objects. In a more sophisticated form, the platform consists of a variety of important building blocks: Connectivity & normalization; device management; database; processing & action management; analytics; visualization; additional tools; and external interfaces. Roughly 75% of today‘s IoT platforms focus solely on providing connectivity1. Looking at this another way, four major technological building blocks of IoT are emerging: Hardware – where data is produced, includes the physical devices with their in-built microprocessors, sensors, actuators and communication capabilities Communication – how data gets transported, ensures the hardware is connected to the network, using proprietary or open-source communication protocols Software backend – where data is managed, including all connected devices, networks, integration, and interfaces to other systems Applications – where data is turned into value, the orchestration of actions by and between users, systems and devices to produce the desired business results Security is an additional element that is so important it needs to be mentioned as a foundation for each of the above (See Figure 1). Figure 1 - Central building blocks of IoT | Source: IoT Analytics Considering this model, the vast majority of IoT platforms exist independently between the hardware and application layers of the IoT technology stack. What is an IoT Business Application Suite? Improvements in productivity and overall operating performance are often the key outcomes sought by industrial organizations exploring or beginning to implement their IIoT (Industrial IoT) strategies and programs. In these complex, distributed and highly heterogeneous environments, IoT Platforms and internet-based architectures are key to enabling these objectives. It is the applications that are executed through this architecture, however, which ultimately deliver the business value. Figure 2 - The Gartner IoT Solution Scope Reference Model | Source: Gartner As highlighted in Figure 2 above, this includes the orchestration of actions performed not only by employees, customers and partners, but also incorporates the information and actions available from the various IT / Line of Business Applications, Manufacturing Systems and Operational Technology, streaming web services, sensors and devices which are already in use today. So an IoT Business Application Suite is where you build, deploy and maintain the end-to-end applications which actually produce the desired business outcomes. What Makes XMPro Different than Typical IoT Platforms? As an IIoT Business Application Suite, XMPro focuses on the end business application, not the typical IoT Platform plumbing, and brings these benefits: Enables industrial business users (engineers) to compose their own use case applications using the data they are most familiar with Quickly build, deploy and iterate applications to test hypotheses and solve real business problems Orchestrate specific actions in response to unique events as found in your IoT data Extend interventions beyond notifications and alarms to drive actions, not just insights Create complete applications with complex decision support, live dashboards, intelligent forms and best-in-class workflow Seamlessly integrate to other enterprise applications and operational systems Built to do the heavy lifting in complex, distributed and heterogeneous environments Different sensors from different providers Different IoT platforms Different vendor & device cloud platforms Create as many applications as required to address all of your IIoT needs, easily scaling from Pilot to Plant, while focusing on the business problems at hand, not the underlying technologies Conclusion Both IoT Platforms and IoT Business Application Suites are key pieces of the overall IIoT solution stack. With the proliferation of companies using similar terms to describe their products and IoT offerings, it is easy to get lost in the technology jargon. By starting with the business problem you are trying to solve, versus simply comparing technologies, you will quickly discover the solution you really need. Sources 1. IoT Analytics, The central backbone for the Internet of Things, November 2015 Figure 1 – IoT Analytics, The central backbone for the Internet of Things, November 2015 Figure 2 – Gartner, Best Practices in Exploring and Understanding the Full Scope of IoT Solutions, Benoit J. et al., 26 March 2015 This graphic was published by Gartner, Inc. as part of a larger research document and should be evaluated in the context of the entire document. The Gartner document is available upon request from http://www.gartner.com/document/3015518. Gartner does not endorse any vendor, product or service depicted in its research publications, and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of Gartner’s research organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose."
  },
  "docs/resources/faqs/external-content/blogs/2017/copy-me.html": {
    "href": "docs/resources/faqs/external-content/blogs/2017/copy-me.html",
    "title": "| XMPro",
    "summary": "Blogs: 2017 The Top 5 Reasons to Invest in an IIoT Development Platform IoT Business Solutions Start with Big Data & Create Business Outcomes How AI Bots Bring Digital Twins to Life"
  },
  "docs/resources/faqs/external-content/blogs/2017/how-ai-bots-bring-digital-twins-to-life.html": {
    "href": "docs/resources/faqs/external-content/blogs/2017/how-ai-bots-bring-digital-twins-to-life.html",
    "title": "How AI Bots Bring Digital Twins to Life | XMPro",
    "summary": "How AI Bots Bring Digital Twins to Life Blog, CEO'S Blog How AI Bots Bring Digital Twins to Life Posted on March 1, 2017 by Pieter van Schalkwyk Industry 3.0 was characterized by factory automation where manufacturing robots replaced humans to do repetitive, manual tasks at a much higher productivity rate, doing it safer, for longer shifts and more importantly with highly predictable production and quality outcomes. Industry 4.0 adoption is racing ahead faster than expected and brings with it the promise of smart factories where machines are interconnected and publish and subscribe to information from other machines. The Industrial Internet of Things (IIoT) brings classic factory automation through Operational Technology (OT) into the World Wide Web world of IT. It exposes industrial sensing and control equipment to internet protocols such as https and MQTT. Machines can now talk a universal language instead of their historical and proprietary past. Information about these smart machines can now be captured and stored in a new master data management system or what IIoT calls Digital Twins. We now have a digital representation of the physical world. Just like Master Data Management, Digital Twins provide context about the machine, its operational information and potentially its health and likelihood to fail. Most IIoT solutions focus on turning machine-born big data into smart data through analytics. Engineers can then create dashboards and reporting solutions to gain some business intelligence around these machines and how they operate. How about letting a machine talk to you, explain its operational conditions, challenges and tell you when it is not feeling well and what the symptoms are? This is not a futuristic vision, but something that is possible and done by some of the best in class organizations in the world right now. Imagine a machine sending you a Skype or Slack message and then engaging in a conversation with you. Software robots are deployed across multiple industries to run help desks, provide customer support and do repetitive configuration and scripting tasks. Your bank or telco’s online support chat function is likely a “bot” that has been trained to assist you with 95% of common support questions. It continuously trains and new bots have cognitive capability that allows it to learn on its own. Subject matter experts (SMEs) are used to train these bots and they use machine learning in the background to make sense of large data sets that we as humans cannot process. Some bots can read incoming invoices attached to an email and enter the details into the AP module of the ERP, while sending the supplier an acknowledgment of receipt email. The same bots can be used to give your machine, or their Digital Twins at least, a voice. Rockwell Automation recently announced Shelby, the Siri of the M2M world which is built on the Microsoft Cortana Analytics suite. Microsoft released its bot framework that enables developers to create and deploy these bots. I created my own bot on Skype that can chat to me. QED (quite easily done). The real challenge comes in on how to make these bots context aware (which machine’s voice are they?) and bring them into the day-to-day operational processes in an organization. The Digital Twin repository brings a vast amount of data about an asset, machine or “thing” and IoT analytics makes it possible. The challenge lies in the practical implementation of these bots in IoT enabled operational processes. Creating smart bots is one thing (and becoming easier by the day) but baking them into the operational processes of a manufacturing plant, a mine or an oil well is more challenging. XMPro’s visual, model-driven way of creating new IoT process apps provides a mechanism to have these intelligent bots as “action agents” instead of just sending an email, SMS, or create a simple task. The XMPro “context providers” in our visual event streams provide rich data to feed the bot with the information it needs to run diagnostics, predict failure and communicate possible next best actions to take. XMPro’s Action Agents provides an ideal platform to introduce bots that take action. Here is an example of a bot calling me up on Skype: This is an example of how a bot could interact with a user and it is initiated in XMPro by the following event flow: Real-time temperature and vibration data is read from IoT devices or a historian application like OSIsoft and is combined with information from SAP AIN (Asset Intelligence Network), for example, to provide context for a failure prediction model. In this example, we first predict if it is going to fail and if it returns a positive result we run another model that will predict the remaining useful life. It also checks the EAM (Enterprise Asset Management) system for any open maintenance orders. If no order exists it creates one in the ERP system and at the same time it starts the interactive discussion in the Skype example. (Contact us if you want a demonstration of our IoT Application Suite to see how to easily create these event streams) Right now, robotic processes or “bots” are not associated with IoT, but at XMPro we see this as one of the most influential aspects of IoT in the future. It will become as important on the process end as data aggregation from sensors are on the front end. These bots have the capacity to process vast amounts of data and bring machines to life through the voice of their digital twin. Integrating this with augmented reality will not only bring a voice, but potentially a face, to digital twins. Assistive technologies like Siri and Cortana are becoming part of our everyday life and changing the way we interact with our mobile smart devices. Future generations of IoT users will expect the same level of interaction and built in intelligence in the way they interact with machines."
  },
  "docs/resources/faqs/external-content/blogs/2017/index.html": {
    "href": "docs/resources/faqs/external-content/blogs/2017/index.html",
    "title": "2017 Blogs | XMPro",
    "summary": "2017 Blogs Articles from the XMPro blog published in 2017. Articles copy-me How AI Bots Bring Digital Twins to Life IoT Business Solutions Start with Big Data & Create Business Outcomes The Top 5 Reasons to Invest in an IIoT Development Platform"
  },
  "docs/resources/faqs/external-content/blogs/2017/iot-business-solutions-start-with-big-data--create-business-outcomes.html": {
    "href": "docs/resources/faqs/external-content/blogs/2017/iot-business-solutions-start-with-big-data--create-business-outcomes.html",
    "title": "IoT Business Solutions Start with Big Data & Create Business Outcomes | XMPro",
    "summary": "IoT Business Solutions Start with Big Data & Create Business Outcomes Blog IoT Business Solutions Start with Big Data & Create Business Outcomes Posted on April 24, 2017 by peterk While many IoT solutions focus on collecting, analyzing and visualizing machine data, the organizations using IoT technologies are mainly interested in business outcomes. Business outcomes are achieved by the capabilities that organizations use to create, sell and service products. Capabilities are usually described as a combination of people, processes, and technologies, with “The Things” certainly being part of the technology dimension. To support a capability business solutions are created and operated. If those solution use IoT data they can be labeled IoT Business Solution. Those can consist of multiple systems that are required to provide the necessary functionality to enable the effective and efficient execution of processes with and without human involvement. From an IoT perspective the 3 system categories are: Systems of Record: System that collect all the machine data and support core business processes, where the organisation wants to apply industry best practices. Systems of Differentiation: Systems that support differentiated business processes, processes that are different from the competition and create a unique value proposition for the organization, enabled by IoT technologies. Systems of Innovation: Here new IoT innovations are tested for their (technical) feasibility, (business) viability and (customer) desirability. It is the world prototypes and proof of concepts, potentially envisioned and implemented using a Design Thinking[1] approach. While many people talk about things, data and application, business processes are often ignored in the discussions, but in the end, business processes deliver the product and services of an organisation to its customers and hence create business outcomes. An IoT Business Solution should therefore orchestrate all the solution components, including IoT technologies, into a process-centric system of systems. Most organizations have a heterogeneous technology environment and IoT products add to the complexity for the overall technology landscape. This means that an end-to-end IoT Business Solution that realises business processes is composed of different technology products from different vendors that need to be orchestrated to achieve the desired business outcomes. XMPro sees itself as being the software glue in those process-driven, heterogeneous, end-to-end IoT Business Solutions. The starting point of an IoT Business Solution is a specific Business Use Case that create a business outcome. The “Things” produce Big Data that can be stored on IoT Data Aggregator platforms. IoT Data Analytics help to convert this Big Data into Smart Data, leveraging capabilities of advanced data analytics platforms, including machine learning. The smart machine data is then enhanced by context data from the enterprise business systems that support the core business processes of an organization. As those system implement best practice processes, systems of differentiation are needed to implement business processes that should differentiate the organization from its competitors. Here industry best practices are not sufficient, but uniqueness is a critical success factor. Creating such an end-to-end solution in an heterogenous environment may seem to be a huge effort, but with the visual and process-centric application building capabilties of XMPro, a rapid development of unique IoT applications if possible and a fast time to value can be achieved. Sources [1] Brown, T. (2008, June_). Design Thinking_. Retrieved from https://hbr.org/2008/06/design-thinking"
  },
  "docs/resources/faqs/external-content/blogs/2017/the-top-5-reasons-to-invest-in-an-iiot-development-platform.html": {
    "href": "docs/resources/faqs/external-content/blogs/2017/the-top-5-reasons-to-invest-in-an-iiot-development-platform.html",
    "title": "The Top 5 Reasons to Invest in an IIoT Development Platform | XMPro",
    "summary": "The Top 5 Reasons to Invest in an IIoT Development Platform Blog The Top 5 Reasons to Invest in an IIoT Development Platform Posted on March 31, 2017 by xmpro Working in Business Development for a provider of an Industrial Internet of Things (IIoT) development platform is the most interesting job I’ve had since working as a green young soldier in the 82nd Airborne Division nearly 30 years ago. In many ways I see companies jumping into IoT projects the same way I hurled myself out of the US Air Force’s C-130 Hercules aircraft. They stick their “knees in the breeze” and hope for the best. Over the course of my tenure here it’s obvious this is a new and growing category of improvement programs that most F1000 business are dedicating time and resources to understanding. I’ve turned back the question I’m often asked “why should I consider looking at an IIoT development tool or platform” onto a number of companies that have made the investment and here’s what they said: 1. IoT is here to stay and the projects have real ROI/payback The ability to gather new information and insights into assets and business critical processes AND act upon issues in the present not past tense is proving extremely valuable to the industrial marketplace. The most critical element is not necessarily the insight gathered from mining the “big data” created but the delivering the “call to action” at the right time that drives the ROI/payback. 2. Complex environments that present unique challenges IIoT projects can add complexity to the traditional project lifecycle. The constantly changing hardware and software (often device firmware) along with balancing the need to leverage cloud applications, edge computing and integrate both traditional and homegrown operational systems is enough to overwhelm a project team in the early project stages. Combine that with the questions of how to document the system and provide the proper security and it’s easy to see why a foundational tool set is a consideration for so many companies. 3. An agile development platform accelerates project completion The right platform provides the right balance of structure and flexibility to accelerate the first IIoT project, but it’s the ability to use those building blocks to drive faster adoption of subsequent projects that makes the biggest difference. From predictive maintenance to asset management and track and trace applications, there is no shortage of low hanging fruit for those exploiting IIoT project opportunities. 4. Put the power of project into the hands of your subject matter experts (SMEs) The latest generation of IoT development platforms focus on visual and graphic user interfaces that allow companies’ field engineers, plant maintenance teams and service technicians to design and test their processes (See the example of XMPro’s Stream Designer pictured). They remove the burden of specifications translation and supporting the highly iterative nature of IoT process design from the Information Technology (IT) team by allowing the SME closest to the project to control the design-test-refine cycle. Doing this prior to deployment is saving as much as 50% of the time typically needed for implementation. 5. Protect your intellectual property (IP) Due to the complexity of many IIoT environments the vast majority of initial implementations have been done by third party firms specializing in creating custom solutions. It’s only natural that these same firms look to similar companies to repeat many of these high impact projects. By keeping the project in the hands of their SME’s companies inherently protect and maintain their IP creation. In this way they protect their unique processes and system designs that set them apart, enhancing their competitive advantage."
  },
  "docs/resources/faqs/external-content/blogs/2018/3-patterns-of-industrial-iot-use-cases.html": {
    "href": "docs/resources/faqs/external-content/blogs/2018/3-patterns-of-industrial-iot-use-cases.html",
    "title": "3 Patterns of Industrial IoT Use Cases | XMPro",
    "summary": "3 Patterns of Industrial IoT Use Cases CEO'S Blog 3 Patterns of Industrial IoT Use Cases Posted on November 15, 2018 by Pieter van Schalkwyk One of the biggest challenges that industrial companies have when evaluating industrial IoT is finding “use cases” or scenarios where it can be used to improve operations. IoT is expected to bring value to an organization in one of the following ways: Reduce cost Increase revenue Increase customer satisfaction Reduce risk (improve compliance) Use cases or applications for IoT in asset intensive industries such as Mining, Oil & Gas, Energy & Utilities and Manufacturing differ from applications in finance or healthcare amongst others. Asset intensive industries often have large, complex equipment units such as SAG mills, offshore platforms, massive haul trucks or processing plants with pumps, heat exchangers, mixers etc. Even though it may be the same type of equipment used in different applications, it will operate differently and have its own asset performance characteristics based on how and where it is used. For this reason, use cases or scenarios for IoT in these asset intensive industries will differ almost on a case by case basis. Industrial IoT is not like buying a marketing automation platform or CRM where the application and use cases are similar across most industries. There is no single “use case” or scenario that solves all asset performance problems. We often hear, “this is GREAT, do you have a solution for: Long, high volume conveyors Hydrocarbon compressors Cyclone pumps (insert your ‘biggest headache’ equipment)” The good news is that in working with a number of the largest asset intensive companies in the world, we discovered the 3 Industrial IoT patterns that describe most of their scenarios or use cases. It revolves around the problem that they want to solve, where they are in their journey and what the level of their IoT readiness is. The 3 common patterns that describe more than 90% of all the applications we’ve come across are: Condition Monitoring Predictive Operations Asset Portfolio Management Condition Monitoring Even though the end goal of most organizations is to have predictive and prescriptive maintenance and operations, almost all start the IoT journey with condition monitoring. This requires integration to front-end IoT and OT data from a range of data sources. Once the data is combined and accessible in a data stream, basic analytics and business rules can be applied to find exceptions or thresholds that are exceeded. A typical example would be to monitor the temperature and vibration on a bearing and create an exception when either the vibration or temperature goes above a set limit. This would then trigger an action to respond to this exception in a timely manner. The action could be as simple as an SMS or email or as advanced as the automatic creation of a work order in an EAM system. It could also be a custom workflow application that is specific to the equipment or problem type. Most companies can start with Condition Monitoring use cases with low investment of resources and cost. Solutions can be built in a matter of days and deliver advanced decision support quickly. The ROI is quick and with XMPro’s Data Stream Designer end-users such as engineers can create new scenarios (use cases) quickly and in an iterative and agile way to respond to continuous intelligence requirements as they emerge. It is not uncommon for a reliability engineer to create 10-15 data streams to monitor key assets. Previously, these solutions would have to be custom coded. The solutions would not only be costly to create but they would also be outdated when completed, as plant conditions may have changed. Condition Monitoring may not be as sophisticated as predictive use cases, but it often delivers significant savings or opportunities that prove the value of Industrial IoT applications to the executives of organizations that are traditionally conservative and don’t want to be “leading edge”. Examples include monitoring mobile assets (haul trucks etc.) processing plants, supply chains, long, high volume conveyor systems, rotating equipment etc. Predictive Operations The second “pattern” that we found in industrial IoT scenarios is where companies add predictive capability to the condition monitoring pattern. It still brings data from a multitude of sources, but it extends the analytics with predictive and prescriptive capabilities. Organizations “wire-in” AI and machine learning models into the data streams to predict failure and operational status based on real-time data. XMPro’s data streams provide a vehicle to “weaponize” AI and integrate it with the business workflows to change the way operators respond to potential failures. It acts as a real-time recommendation engine. By adding XMPro’s workflow-based Action Hub with business rules and human-to-human workflow it turns the predictive solution into a prescriptive solution with Best Next Actions and skills-based routing. XMPro supports most predictive analytics platforms and all the model types, ranging from multi-variate anomaly detection through to regression, neural networks and other deep learning approaches. Examples includes predictive maintenance, predictive quality and predictive operations. Asset Portfolio Management Asset Portfolio Management is a pattern that combines condition monitoring and predictive operations across a portfolio of assets. It provides an OEE (Overall Equipment Effectiveness) measure for a collection of assets. It leverages the capability of “monitoring” and in some cases “predicting” for applications where the business requirement has moved beyond managing single assets to the performance of a collection of assets. Examples includes a fleet of mobile assets to prioritize maintenance, predicting which wells are declining on an oil field or where in a continuous product process a bottleneck is likely to occur that will impact the throughput. Other Applications Most of the other scenarios and use cases that we see are based on the 3 patterns described above. An Intelligent Supply Chain “Track & Trace” solution with a blockchain Smart Contract is based on the Condition Monitoring pattern where the actions now include IoT data appended to a blockchain and instead of creating a work order, we now pass the (oraclized) IoT data to a distributed ledger-based smart contract. If we build some predictive capabilities into the supply chain solution to predict demand for example, it now follows a Predictive Operations pattern. The underlying technology (traditional or emerging) does not change the business objective of a solution: What is happening to a critical asset and how do I respond? (Condition Monitoring) What is likely to happen to that asset, when and how do I respond? (Predictive Operations) What is going on in my operations and how do I plan what to do? (Asset Portfolio Management) Which pattern does your “use case” or business problem follow? See our Blueprints for each of these patterns to get you started for quick time to value."
  },
  "docs/resources/faqs/external-content/blogs/2018/copy-me.html": {
    "href": "docs/resources/faqs/external-content/blogs/2018/copy-me.html",
    "title": "| XMPro",
    "summary": "Blogs: 2018 XMPro IoT Operational Capability Survey Results 2018 What is a Digital Business Platform and Why Should I Care? [Robotic] Process Automation for IoT 3 Patterns of Industrial IoT Use Cases The CXO’s Guide to Digital Transformation – May The Five Forces Be With You Is Security More Important Than Trustworthiness for Industrial IoT? XMPro at bpmNEXT 2018: Watch The Presentation"
  },
  "docs/resources/faqs/external-content/blogs/2018/index.html": {
    "href": "docs/resources/faqs/external-content/blogs/2018/index.html",
    "title": "2018 Blogs | XMPro",
    "summary": "2018 Blogs Articles from the XMPro blog published in 2018. Articles 3 Patterns of Industrial IoT Use Cases copy-me Is Security More Important Than Trustworthiness for Industrial IoT? [[Robotic] Process Automation for IoT](robotic-process-automation-for-iot.md) The CXO’s Guide to Digital Transformation – May The Five Forces Be With You What is a Digital Business Platform and Why Should I Care? XMPro at bpmNEXT 2018: Watch The Presentation XMPro IoT Operational Capability Survey Results 2018"
  },
  "docs/resources/faqs/external-content/blogs/2018/is-security-more-important-than-trustworthiness-for-industrial-iot.html": {
    "href": "docs/resources/faqs/external-content/blogs/2018/is-security-more-important-than-trustworthiness-for-industrial-iot.html",
    "title": "Is Security More Important Than Trustworthiness for Industrial IoT? | XMPro",
    "summary": "Is Security More Important Than Trustworthiness for Industrial IoT? Blog, CEO'S Blog Is Security More Important Than Trustworthiness for Industrial IoT? Posted on March 12, 2018 by Pieter van Schalkwyk Security is mentioned in almost every meeting that I have with CIOs, Enterprise Architects, Operational Technology (OT) specialists and the Operations Engineers. It is a high priority topic, and everyone is looking for the holy grail answer to “How do you secure this XYZ IoT project?”. It is an important consideration in an Industrial IoT project where a compromise in security can lead to catastrophic failure with the potential impact of a Chernobyl or Bhopal. By putting security in the spotlight, we often overlook the bigger issue of the TRUSTWORTHINESS of an industrial IoT solution. The Industrial Internet Consortium’s (IIC) Vocabulary Technical Report describes trustworthiness as the degree of confidence one has that the system performs as expected with characteristics including safety, security, privacy, reliability and resilience in the face of environmental disruptions, human errors, system faults and attacks. The IIC’s Industrial Internet of Things, Volume G4: Security Framework (IISF) describes this trustworthiness with this simple model. Trustworthiness of an IIoT System Trustworthiness addresses all the potential risk areas rather than just attacks or intrusions. It questions how we deal with environmental disruptions, human errors and system faults or “bugs” over and above security. Each aspect is described extensively in the IISF, but here are the key definitions: Security is the condition of the system being protected from unintended or unauthorized access, change or destruction. Safety is the condition of the system operating without causing unacceptable risk of physical injury or damage to the health of people, either directly or indirectly, as a result of damage to property or to the environment. Reliability is the ability of a system or component to perform its required functions under stated conditions for a specified period of time. Resilience is the emergent property of a system that behaves in a manner to avoid, absorb and manage dynamic adversarial conditions while completing the assigned missions, and reconstitute the operational capabilities after causalities. Privacy is the right of an individual or group to control or influence what information related to them may be collected, processed, and stored and by whom, and to whom that information may be disclosed. Planning and architecting an industrial IoT solution requires that each of these key system characteristics be assured to deliver a trustworthy business solution. This assurance requires the collection and analysis of evidence that supports the design, construction, deployment and test of the system, and its activities in operation. The evidence must support the claim that the right mixture of innate system capabilities and compensating security controls to mitigate risks has been put in place and provide the overall trustworthiness of the IoT system. The IIC’s Industrial Internet Security Framework (IISF) provides practical guidance to address the assurance requirements of each of the system characteristics from both a business as well as functional and implementation viewpoints. It covers trustworthiness from connectivity at the device level through to the business processes that support the IoT solution. It is interesting to ask the question “Is security more important that trustworthiness?” in response to “how do you secure your IoT solution?” question and address key system characteristics that were not considered in the hype around IoT security. My answer to the “How do you secure this XYZ IoT project?” question is always the same . . . “let’s design it to be trustworthy, then it will be secure”. You can get a summary of the IIC frameworks and technical documents here and you can contact us to discuss how to create a trustworthy industrial IoT solution."
  },
  "docs/resources/faqs/external-content/blogs/2018/robotic-process-automation-for-iot.html": {
    "href": "docs/resources/faqs/external-content/blogs/2018/robotic-process-automation-for-iot.html",
    "title": "[Robotic] Process Automation for IoT | XMPro",
    "summary": "[Robotic] Process Automation for IoT Blog, CEO'S Blog [Robotic] Process Automation for IoT Posted on July 12, 2018 by Pieter van Schalkwyk While Process Automation for asset-intensive industries is not new, Robotic Process Automation (RPA) is. Traditional Process Automation is used to automatically control manufacturing processes in industries such as chemical plants, oil refineries, steel mills, and paper and pulp factories. Process Automation is typically the domain of control engineers who use a network to interconnect sensors, controllers, operator terminals and actuators, and it is generally known as Operational Technology or OT. It is all managed by automation software that is either centralized or distributed in DCS systems. Robotic process automation (RPA), on the other hand, is an emerging form of business process automation technology based on the notion of software robots or artificial intelligence (AI) workers. “Use cases for RPA (Robotic Process Automation) according to Gartner, include instances when an organization wants to work with structured data to : Automate an existing manual task or process with minimal process re-engineering. Reduce or remove headcount from batch data input and output tasks or data rekeying. Link to external systems that cannot be connected to using other IT options. Avoid major system integration projects or specific new major application deployments. Replace individual ’shadow or citizen IT‘ desktop automation with enterprise-wide automation.”1 OT departments are starting to face the same challenges as IT departments when it comes to the verocity of data that they are dealing with. New “smart” IoT devices are proliferating through businesses and data-hungry AI and ML algorithms are demanding more from OT systems than their initial design constraints around industrial control intended. Engineers are becoming data scientists and “copy and paste to Excel” doesn’t cut it anymore. They want to connect to heterogeneous data sources from all over the organization, and this includes the operational databases such as Historians, MES and SCADA systems. Robotic Process Automation (RPA) for Industrial IoT holds the key to connect OT and IT systems and address key challenges: Automate Manual Tasks “Copy and Paste” data from multiple systems is fraught with possibilities for “finger trouble” errors and it is also not scalable. Highly repetitive data manipulation tasks are often relegated to business process outsourcing centers in low-cost geographies, but OT departments don’t have that luxury. Data latency, security and strict safety considerations most often eliminate process outsourcing options. IoT specific RPA can automate these manual tasks on a 24x7x365 basis, with high accuracy, repeatability and security. Reduce or Remove Headcount Reducing or removing headcount is not a primary driver for OT departments as it is for IT but reducing growth in headcount is. All the new IoT-related data sources are putting pressure on OT departments that are already facing challenges with an aging (knowledgeable) workforce that is retiring. Capturing and codifying their tribal-lore is a key benefit of using industrial RPA. Link External Systems Gartner states “We predict that through 2018, half the cost of implementing IoT solutions will be spent on integration”.2 Heterogenous data in multiple formats from multiple vendors is one of the main challenges in digitization projects in industrial environments. Custom coding integration is one of the biggest inhibitors of business agility. A robust RPA solution with extensive OT and IT integration libraries significantly reduces the complexity and cost of linking external systems with repeatable scripts that can be tested, audited and adapted as requirements change. Replace Individual Desktop Automation How reliant are you on that single individual that has the macros for the ETL (extract, transform, and load) process in Excel on her local notebook PC? This is IP that goes home with her every evening. Move her subject matter expertise to a scalable, secure platform that removes tedious ETL tasks and let her focus on value-adding activities that cannot be achieved by robots (or agents as we call them at XMPro). Use Domain-specific RPA for Industrial IoT Not all RPA tools are created the same. Some are better suited to application “screen scraping” in legacy applications in banking or insurance. Others are better to assist customer contact center operators and a CRM-centric. Only XMPro is suited to industrial [Robotic] Process Automation. XMPro’s unique visual approach simplifies the process to create RPA streams that cater for industrial IoT scenarios: Create “scripts” in drag and drop UI with an “agent” for each function that abstracts the code from the business process view. Listener Agents (OT and IT data) Context Agents Transformation Agents Action Agents Drive actions and notifications that include updating other business systems such as ERP and CRM solutions while interacting with end users through simple SMS’ through to sophisticated XMPro chatbots Embed AI and machine learning with standard AI Action Agents as the organization’s maturity increases with clean, trustworthy data from XMPro’s robust, industrial RPA platform. The only RPA solution with quick-start blueprints for industrial applications Asset Condition Monitoring Predictive Maintenance Asset Performance Monitoring Predictive Quality Management Anomaly Detection for Process Technology Anomaly Detection for Manufacturing Processes Process Safety Monitoring and Reporting Sources: 1. Gartner, Inc., Use Cases for Robotic Process Automation: Providing a Team of ‘Virtual Workers’, Cathy Tornbohm, 5 March 2018. 2. Gartner, Inc., Market Guide for IoT Integration, Benoit J. Lheureux et al., 24 October 2017. Disclaimer: Gartner does not endorse any vendor, product or service depicted in its research publications, and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of Gartner’s research organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose."
  },
  "docs/resources/faqs/external-content/blogs/2018/the-cxos-guide-to-digital-transformation--may-the-five-forces-be-with-you.html": {
    "href": "docs/resources/faqs/external-content/blogs/2018/the-cxos-guide-to-digital-transformation--may-the-five-forces-be-with-you.html",
    "title": "The CXO’s Guide to Digital Transformation – May The Five Forces Be With You | XMPro",
    "summary": "The CXO’s Guide to Digital Transformation – May The Five Forces Be With You Blog, CEO'S Blog The CXO’s Guide to Digital Transformation – May The Five Forces Be With You Posted on August 29, 2018 by Pieter van Schalkwyk C-Level executives are under pressure to have their organizations remain competitive, profitable, retain customers and grow. Never before have we seen all the forces at work at the same time and at the scale as is currently the case. Digitalization and disruption are occurring in almost every industry. The forces I am referring to are Michael Porter’s Five Forces Framework that was first published in the Harvard Business Review in 1979. It is as if all five forces are converging and accelerating the impact on existing businesses. It is an attack of inter-galactic proportion. Not only is digitalization empowering customers and suppliers to negotiate terms and prices, but the barriers to entry for new competitors are lowered significantly through technology and digital infrastructure such as cloud and mobile computing. Substitute products, through digitalization, are creating new revenue opportunities for competitors who are adapting their business and operating models. Competitors, suppliers and customers are all digitally transforming how value is created, delivered and captured. Many of the new digitalization capability is delivered through disruptive technology such as IoT, AI, 3D printing and many others that can be easily consumed “as a Service”. There are a large number of incumbent technology providers competing with innovative start-ups. This ends up creating a smorgasbord of disparate systems that all provide a piece of the puzzle in an increasingly complex digitalization effort. CXO’s are trying to avoid the hype of the next shiny object, but some of these disruptive technologies are impacting their ability to remain competitive. Irrespective of their disposition towards these new technologies (whether they like it or hate it), the reality of their situation is that it impacts the business outcomes in either a positive of negative way. The challenge is that there is a significant gap in going from disruptive technology to positive business outcomes. Navigating this gap is a critical success factor in effective Digital Transformation. There are fundamental differences between Digitalization and Digital Transformation that I describe in more detail here, but the four quadrants of this Digital Transformation Roadmap explain it at a high level. Organizations can digitize business processes without changing business models and this most often brings efficiency gains. It doesn’t improve the organization’s competitive position, but it does lead to cost reduction, improved asset performance and other efficiency gains. Real Digital Transformation happens when new operating or business models are enabled through disruptive, digital technology. Businesses are then transformed with new products and services that bring new revenue lines, new customers and new competitive advantages to disrupt markets. These are the real objectives of CXOs that are looking to close the gap between the opportunity of disruptive technology and the promise of new/more revenue, lower cost, lower risk and increased valuations due to successfully transforming their organizations. The challenge for CXOs is to find a balance between the hype and their unique situation. So how do you navigate your enterprise through the hype of emerging and disruptive technology as you transform it to remain competitive? Do you avoid or discard these new technologies and keep your troglodyte blinkers on and hope competitors don’t realize? Or do you throw caution to the wind and buy one of each new technology that appears on the Hype Cycle every year? The answer lies in looking at each Digital Transformation opportunity or initiative (the orange dots in the quadrants) and identifying which recombination of the different technologies will provide the desired business outcome. It is a process of Digital Business Process Reengineering (Digital BPR) that evaluates each use case and recombines the right disruptive technologies to provide an outcome where the impact of the new Digital Business Process is greater than the sum of the parts. Recombination, instead of simple integration, is a powerful mechanism to leapfrog innovation cycles and create new digital assets that will increase the valuation of an organization the same way Netflix’s Recommendation Engine adds more than $1 billion in value per year. I explain more on the power of recombination in this article. Recombinant innovation refers to the way that existing ideas can be reconfigured in new ways to make new ideas. It is similar to the way an agricultural research station develops improved plant varieties by cross-pollinating existing varieties. It means you don’t have to invent new technologies and processes, but recombine what you have and what is available to produce new ways to create, deliver and capture value in a digital business model. As a CXO, you should be thinking about how you can harness the power of these disruptive technologies at the right time and place. How can you funnel the right technologies such as AI and IoT into a “Recombinator” with the right business logic to create digital data streams that deliver the right real-time, Continuous Intelligence for each Digital Transformation initiative? How do you intelligently integrate this into existing transactional and operational business systems and how do you create intelligent applications to support new business models and digital transformation? This recombination of different, disparate, “… as a Service” technologies is a key capability of the XMPro Digital Business Platform. It separates the business logic for business outcomes in new Digital Business Processes from the underlying technologies that supports it. It blends disruptive new technologies in the right formulation with existing business systems to create new continuous intelligence for real-time business processes and operations. It enables new operating and business models and a visual way of wiring the Digital Enterprise. XMPro’s Digital Business Platform (DBP) is a recombination, through intelligent integration and orchestration**, of different digitalization technologies in a single solution**(or solution stack) that enables you to create new business capabilities. It turns technology into key business capabilities for Digital Transformation. It provides continuous intelligence from digital data streams for Intelligent Digital Apps. These apps recombine existing or emerging business technologies with new digital business processes to support the desired business outcomes. XMPro’s digital business processes provide the necessary orchestration to place the right intelligence at the right point in the value chain. Intelligence, such as AI and Advanced Analytics, can be at the edge, the cloud or any point in between where it best addresses the requirement of the digital business process. XMPro’s DBP supports the full lifecycle of discovery, testing, fine-tuning and the continuous improvement of business and operating models as CXOs embark on the journey from the current “As Is” business state to an enterprise that is digitally transformed, competitive and disruptive in its chosen markets. Let us show you how to create the new digital nervous system to power new products and services, improve customer service, reduce risk and cost and be the disruptor that your competitors fear. May the five forces be with you!"
  },
  "docs/resources/faqs/external-content/blogs/2018/what-is-a-digital-business-platform-and-why-should-i-care.html": {
    "href": "docs/resources/faqs/external-content/blogs/2018/what-is-a-digital-business-platform-and-why-should-i-care.html",
    "title": "What is a Digital Business Platform and Why Should I Care? | XMPro",
    "summary": "What is a Digital Business Platform and Why Should I Care? Blog, CEO'S Blog What is a Digital Business Platform and Why Should I Care? Posted on August 15, 2018 by Pieter van Schalkwyk XMPro was recently named a Hot Vendor for Digital Business Platforms by Aragon Research and also included as an Innovator in their inaugural Globe for Digital Business Platforms. So, what is a Digital Business Platform or DBP and why should I care? Is it just a new three-letter acronym to add to the IT alphabet soup that my organization already has, or is it something new that will add value to my business? Where does it fit my ERP, CRM, EAM, HRM, SCM, IoT, or YAA (Yet Another Acronym)? What does it mean for our Digital Transformation and digitalization projects? The best way to explain a Digital Business Platform is to look at what it is, what it does, and what it means for you. What It Is It is a recombination, through intelligent integration and orchestration**, of different digitalization technologies in a single solution** (or solution stack) that enables you to create new digital business assets. These new digital assets are key capabilities that support real digital transformation of business models and processes. (More on the difference in my Forbes article) Andrew McAfee and Erik Brynjolfsson describe the power and impact of recombination in the digital era in their brilliant book The Second Machine Age – work, progress and prosperity in a time of brilliant technologies. It is where the new whole is greater than the sum of the old parts. Recombinant innovation refers to the way that old ideas can be reconfigured in new ways to make new ideas. It is similar to the way an agricultural research station develops improved plant varieties by cross-pollinating existing plant varieties. McAfee and Brynjolfsson use the example of Waze as a recombination of a location sensor, data transmission device (phone), GPS system, a map, and social network. The team at Waze added machine learning to these existing technologies and now deliver a new digital solution to “…100 million drivers who share real-time traffic and road info, saving everyone time and gas money on their daily commute”. A Digital Business Platform provides the same situational awareness, event response and collaboration from digitization technologies and IoT (sensors) to business users as Waze does for commuters. In the 1998 Quarterly Journal of Economics, Martin Weitzman outlined the impact of recombinant growth and that “…the ultimate limits to growth lie not so much in our ability to generate new ideas as in our ability to process an abundance of potentially new ideas into usable form.” A Digital Business Platform provides such a usable form of a recombination of digital technologies that will help organizations grow and compete (or survive) in the Machine Age 2.0 What It Does Digital Business Platforms create new digital business capabilities by recombining existing technologies as I’ve previously pointed out. But what is the difference between technology and capability? .NET or Java are technologies, while ERP or CRM solutions built on these technologies provide business capabilities. The technology landscape grew exponentially over the past few decades and where many technologies developed in isolation, recombination and convergence are forcing businesses to look at the bigger digital transformation picture rather than just the traditional technology silos. 3D printing, drones, IoT, AI, blockchain and other disruptive technologies are all converging on organizations at the same time and this needs an adaptive and agile approach to integrate these technologies to create new and disruptive capabilities. It, however, needs to be done in an architecturally sound way to maximize the opportunities that this recombination offers to digital transformation. Digital Business Platforms provide new capabilities that enable: real-time, situational awareness and continuous intelligence. Little did Bill Gates now in 1999 what Business @ The Speed of Thought would really mean when he wrote his book on digital infrastructure and how it would change the competitive landscape. support for streaming data for IoT and other business events to find key business moments with significant impact or opportunity; decision support using advanced analytics as part of normal business processes – smart processes are the new normal; automating not only the business processes but automating decision-making as part of the business process; intelligent integration and orchestration that embed AI and other disruptive technologies in new or existing business workflows; and new or changed business models for Digital Transformation. Gartner analyst, Bill Swanton, rightly points out that you cannot buy a Digital Business Technology Platform as it is based on technology (most of which you probably already own). The key technology platforms that Gartner list include: IT Systems – everyone already has their back-office IT systems, such as ERP. These may need to be wrapped to expose APIs to participate in the platform. Customers – most organizations have a variety of customer facing systems, such as CRM, and may have evolved this into a more complete Customer Engagement Hub. Intelligence – Existing investments in data warehouses, data marts, data lakes must be drawn on not to just gain insights into digital business activity, but to build algorithms and artificial intelligence to automate decision making and responses to digital customer’s needs. Ecosystems – most companies have existing B2B communications to transact with ecosystem partners, but these will become more complex as partners work closer together to respond to a business moment. Internet of Things – manufacturing and supply chain companies probably have a variety of controls and automation (Operational Technology) but may need to extend this much further. A Digital Business Technology Platform, however, differs from a Digital Business Platform. One focuses on technology and the other on capabilities. A Digital Business Platform provides a mechanism to recombine the technologies in ways to deliver new business capabilities in innovative and transformative ways. It provides a logical layer that separates the business logic for digital transformation from the technology applications that power it. O’Reilly describes what these recombined capabilities can do for digital transformation projects (How enterprises can build a digital business platform with pervasive integration): “Digital transformation relies on connecting data and systems, people and processes. Integration technologies have traditionally formed the nervous system of a large enterprise, connecting systems and moving data. But the human nervous system doesn’t just connect and sense; it also acts on data in real time. A digital business platform augments the intelligence of a digital business by building on its ability to connect and sense, to learn and act automatically, and enables the next stage of your digital transformation.” Here is an example of XMPro as a Digital Business Platform that provides the intelligent integration and recombination capabilities to connect existing systems of transaction with new systems of differentiation to support new disruptive business models and processes. Example of a Digital Business Platform What It Means Digital Business Platforms provide the “plumbing and glue” for Digital Transformation. It means that you can focus on creating new intelligent digital processes that focus on the business outcomes of digital transformation rather than wiring a technology stack together. Aragon Research lists the following benefits of using Digital Business Platforms: Creating new products, services, or business models Accelerating revenue Driving efficiencies through automation Improving profitability Enhancing the customer experience Ensuring compliance Linking to ecosystems for revenue or operational improvement It also means that the transformational insights that we gain from emerging technologies and advanced analytics can easily be integrated into our new digital business models to deliver transformational outcomes. It provides the agility to do (and change) Business @ The Speed of Thought. Automating this adaptable business with a Digital Business Platform is like having a smart GPS that provides not just situational awareness of where you are, but also the real-time actions and guidance needed to beat your competition to the goal post. Here are just three examples of use cases that can be deployed on the XMPro Digital Business Platform: New revenue opportunities through new products and services from real-time IoT data to disrupt traditional Industrial, Healthcare, Transportation & Logistics and Insurance markets; New customer experience management for Retail, Banking and consumer markets with new products and services; and New Operational Risk management capabilities for Investment and Retail Banking to reduce risk profile and increase profitability Contact us for a demonstration of the XMPro Digital Business Platform."
  },
  "docs/resources/faqs/external-content/blogs/2018/xmpro-at-bpmnext-2018-watch-the-presentation.html": {
    "href": "docs/resources/faqs/external-content/blogs/2018/xmpro-at-bpmnext-2018-watch-the-presentation.html",
    "title": "XMPro at bpmNEXT 2018: Watch The Presentation | XMPro",
    "summary": "XMPro at bpmNEXT 2018: Watch The Presentation Blog, CEO'S Blog XMPro at bpmNEXT 2018: Watch The Presentation Posted on April 18, 2018 by xmpro Learn how to Turn IoT Technology into Operational Capability in this presentation by XMPro’s CEO, Pieter van Schalkwyk. In this presentation Pieter covers: The challenges of operationalizing IoT How to integrate IoT into existing business workflows A brief demo of XMPro’s IoT and BPM capabilities This talk was recorded at the 2018 bpmNEXT conference in Santa Barbara, California."
  },
  "docs/resources/faqs/external-content/blogs/2018/xmpro-iot-operational-capability-survey-results-2018.html": {
    "href": "docs/resources/faqs/external-content/blogs/2018/xmpro-iot-operational-capability-survey-results-2018.html",
    "title": "XMPro IoT Operational Capability Survey Results 2018 | XMPro",
    "summary": "XMPro IoT Operational Capability Survey Results 2018 Blog, CEO'S Blog XMPro IoT Operational Capability Survey Results 2018 Posted on March 29, 2018 by Pieter van Schalkwyk In a recent survey, we asked users in Industrial IoT to rank their current IoT capability challenges. The results correlated well with a similar study conducted by McKinsey in July 2017. IoT Capability Challenge Rank Managing all the data we collect 1 Integrating IoT Solutions into existing business workflows 2 Applications and platforms that support my specific use cases 3 Extracting data from sensors and machines 4 End-to-end prototyping of connected products 5 Interoperability between different vendor machines and applications 6 Identifying use cases and applications for IIoT 7 Connectivity and device management 8 Finding AI or machine learning models and algorithms for my use cases 9 We don’t even know where to start with IoT 10 (respondents n = 40) It is interesting that managing data and integrating IoT technology with existing business workflows is ranked higher than connectivity, device management and extracting data from sensors and machines. It seems that organizations have moved on from small-scale POCs that demonstrate connectivity and the “blinking LED on the Raspberry PI” to real projects that require designs that go beyond five pumps in a controlled environment to the real processing plant on a mine or a refinery for example. A survey conducted by Cisco in May 2017 with 1845 IoT decision makers shows that 60 percent of IoT initiatives stall at the PoC stage. Furthermore, only 26 percent of companies have had an IoT initiative that they considered a complete success. Digital Transformation through IoT is not something done as a POC where we are out to prove technical concepts like integration to a business system or data historian, for example. There is enough evidence now to show we can connect devices, get data from them and provide real-time operational intelligence. The question becomes “what we do with this intelligence to improve business outcomes and effectively transform our businesses.” We want to align with new business models and transform to be relevant in a competitive digital landscape. We furthermore want to do it at scale. POCs don’t provide an answer to that. Finding the business processes that will deliver most value from all the IoT data is the key. The McKinsey study found that 70% of respondents cited “integrating IoT solutions into existing business work flows” as a major capability challenge. “For instance, 70 percent of respondents stated that companies have not yet integrated IoT solutions into their existing business work flows – in other words, they are not using enterprise IoT to optimize day-to-day tasks”. Starting with business challenges and the supporting processes leads to the analytics that is needed to find and address those challenges. Starting with the problem will lead to an understanding of the operations intelligence that makes operations more predictable. Once we know what analytics and intelligence we need, we know what data to collect, analyze and potentially store. The XMPro I2OC (IoT to Operational Capability) framework provides practical guidance on this approach. It is published in the Industrial Internet Consortium’s Journal of Innovation and you can download a copy here. The IoT market is maturing and companies are starting to see the business value, not just the blinking lights. We are glad to see “no clue” at the bottom of this list. XMPro’s focus is on the top 2 issues and the XMPro IoT platform provides both IoT data management capabilities as well as integration to existing (and new digital) business processes and workflows. Contact us here to see it in action."
  },
  "docs/resources/faqs/external-content/blogs/2019/copy-me.html": {
    "href": "docs/resources/faqs/external-content/blogs/2019/copy-me.html",
    "title": "| XMPro",
    "summary": "Blogs: 2019 My Digital Twin: Digital Twin Applications for Real-time Operations (Like Me)"
  },
  "docs/resources/faqs/external-content/blogs/2019/index.html": {
    "href": "docs/resources/faqs/external-content/blogs/2019/index.html",
    "title": "2019 Blogs | XMPro",
    "summary": "2019 Blogs Articles from the XMPro blog published in 2019. Articles copy-me My Digital Twin: Digital Twin Applications for Real-time Operations (Like Me)"
  },
  "docs/resources/faqs/external-content/blogs/2019/my-digital-twin-digital-twin-applications-for-realtime-operations-like-me.html": {
    "href": "docs/resources/faqs/external-content/blogs/2019/my-digital-twin-digital-twin-applications-for-realtime-operations-like-me.html",
    "title": "My Digital Twin: Digital Twin Applications for Real-time Operations (Like Me) | XMPro",
    "summary": "My Digital Twin: Digital Twin Applications for Real-time Operations (Like Me) Blog My Digital Twin: Digital Twin Applications for Real-time Operations (Like Me) Posted on June 19, 2019 by Pieter van Schalkwyk Everyone is talking about digital twins. But I find so many different definitions that I thought I’d use myself as an example and explain how digital twin applications for real-time operations can help organizations improve asset performance and help find those unexpected events that typically increase risk and cost. Is This My Digital Twin? A couple of years ago at a Microsoft Inspire event, they had a great setup where you could stand on a rotating disc surrounded by kinetic cameras. As the disc spun, it would take a 3D image of you. Now I have a 3D model of myself, which I could potentially use for several different use cases. I could print a 3D model of myself. I could fit some clothes on the 3D model and see what brands would suit me or how different colors would look. Or Is My Smartphone My Digital Twin? Looking at how digital twins have evolved over the last couple of years, I concluded that my smartphone is probably a better representation of my digital twin than the 3D model. If you look at the history of digital twins, it typically started as 3D or CAD-based models where we tried to create a visual representation or digital representation of a physical object. As digital twins are evolving, we’re starting to see that it’s more than just a 3D model. It encapsulates a lot of the things that I do. It knows a lot about me in many different areas, and the smartphone is an excellent example of this. Several applications on my phone know quite a lot about me. If you look at the photos, my phone knows where I’ve been, what people I’ve been in contact with, and who my family is. If it looks at Google Maps data, it will know where I’ve traveled. Likewise, with Instagram or Skype, it’ll know my conversations. Well, hopefully, it doesn’t know all the detail of the conversation, but it’s a bit creepy that when I get in my car, it even knows where I’m going. Google tells me you’re heading here, and it’ll take you this long to drive there. In terms of the predictive capability of digital twins, it shows the future of where things are going. My phone knows a lot more about me than anyone else, including my wife and close family. The Past, Present and Possibly the Future Let’s look at a couple of examples around my health. There are many different health apps, and each one has different sets of data that it collects either real-time or when I’m using that specific app. Likewise, with finances, it knows everything about my investments and banking. It knows my bank account, what I’m spending on my credit card and my stock investments. In terms of my travel, it knows where I’ve been and which trips I have planned. If you’re looking at the phone as a repository or as a single front end, it’s not one database with all the information. Each app has its own data source, analytics, and use case. It’s all accessible from one single user interface, and it’s got the best representation of knowing who I’m talking to socially, where I’ve been, what my financial state is, and also what my health is. If I look at health in a little bit more detail, for example, the Apple Health app. It will show you your current performance in terms of activity, health records, etc. There could be different places where all this data comes from, but this is the single place where I can see that data. The Apple Health app is the best digital representation of my health. It knows more about my health than anyone else in one single area. Use Case: Triathlon Heart Rate Training I’m an aspiring triathlete, and I use Strava to track my training. One of the things I do is modify my workouts to keep my heart rate at a certain level. That’s real-time monitoring and information that I need to know. This digital twin shows me where I’ve been. It gives me the historical data for a specific activity. I can also see my data in real time. I can even have it on a different user interface like an Apple Watch where I can actually interact with it as I’m doing my exercise. The other use case for the same information is for my family. When I’m at a race, my family can monitor me in real time without looking at the physical me. They look at my twin or proxy on the Internet. It uses the same sensors that feed the different UIs through the same APIs. Each app doesn’t have to have its own sensor data. Many different applications use the GPS on my phone, but it’s the same GPS location. The same goes for any of the other sensors in the phone. What Happens When I Buy a New Garmin? The other interesting thing with this is that I can swap out the underlying sensing equipment. I can change my Fitbit or Garmin or have a new model of the latest gadget with sensor-based technology that I use to collect this data, and it doesn’t even blip on the app. The app doesn’t even know, because it’s just collecting the data through a standard integration user interface. If I swap out the underlying data source, the application and what I use this for, in this example, my heart rate training, it does not even know that I’ve swapped out the underlying equipment. My smartphone is the best representation of me from a digital twin perspective. Again, it knows everything about me in terms of my physical health, financial health, my social life, who I know, what conversations I’m in, and who I’m connected to. Digital Twin for My Factory, Plant or Mine: How Is This Different? If we look at our factory, plant or mine, how is this different than having my smartphone as the place that serves all the applications? Well, it isn’t actually different. With XMPro, you can build real-time applications that each serve a specific use case. For equipment maintenance, your apps would be similar to what you have on your personal health app. You can create a centrifugal pump condition monitoring or predictive maintenance app. The apps serve different purposes, but they can use the same underlying data source. How to Set This Up Inside XMPro You can use XMPro’s visual designer to create a data stream to wire up your digital twin from different data sources. In this instance, it’s getting data from OSIsoft and OPC UA, going through some logic and applying analytics. The Google Maps app on my phone does the same when it predicts where I’m heading. In this instance, we’re predicting that a particular piece of equipment is going to fail, and then we are creating work orders, sending our recommendation and putting it on a dashboard for you to see. I can swap out the OPC UA or OSIsoft data source with a different data source, and it wouldn’t change what I see at the end. You can publish your data to a dashboard similar to what you’ve seen on Strava. XMPro dashboards can have historical data and real-time status. The historical elements could be looking at historical records, similar to what you’ve seen on the health app. Having this digital twin application means I don’t have to go out to the physical asset for an inspection because I can look at the asset health in my app. I can show other information from systems like from SAP EAM or IBM Maximo. I can see the current maintenance we’ve got planned and what we did recently. You can bring in transactional information like you would with health records on your phone. Conclusion The goal is not to build one mega digital twin. The digital twin actually connects to the data that sits in different places. It’s not replacing it. It’s also not replicating it. It is merely showing it to you in the context of the application that you are trying to use it in. There’s no difference between how I use my smartphone and how you can set up digital twin applications to manage your operations in real-time. Click here to learn more about industrial digital twins for real-time operations. Or send an email to [email protected] if you would like to find out how we can help you get started."
  },
  "docs/resources/faqs/external-content/blogs/2020/copy-me.html": {
    "href": "docs/resources/faqs/external-content/blogs/2020/copy-me.html",
    "title": "| XMPro",
    "summary": "Blogs: 2020 Lean Digital Twin: Part 2 Digital Twin: Your Most Productive Remote Worker From the Control Room to the Bedroom Lean Digital Twin: Part 3"
  },
  "docs/resources/faqs/external-content/blogs/2020/digital-twin-your-most-productive-remote-worker.html": {
    "href": "docs/resources/faqs/external-content/blogs/2020/digital-twin-your-most-productive-remote-worker.html",
    "title": "Digital Twin: Your Most Productive Remote Worker | XMPro",
    "summary": "Digital Twin: Your Most Productive Remote Worker Blog, CEO'S Blog Digital Twin: Your Most Productive Remote Worker Posted on March 17, 2020 by Pieter van Schalkwyk Remote work is becoming the new normal for many businesses, and it requires a new way of monitoring and managing for executives. Many decision-makers still rely on historical reporting and business intelligence dashboards to make day-to-day operational decisions. For many of these decision-makers in asset-intensive industries, it also meant that you walk the factory floor, let experts inspect equipment to monitor its condition, and talk to machine operators to find out what is happening. This approach worked for 80 percent of businesses that are either average or below-average performers. But this all changed as the recent Coronavirus (COVID-19) pandemic blindsided these organizations. Governments implemented “social distancing” measures and these “average” organizations were not prepared to have a remote workforce that couldn’t walk the plant to gather intelligence and make situationally aware decisions. Their Digital Twins were comprised of historical dashboards and spreadsheets with stale data about past performance. It is kind of like having your parents reminisce about “the good old days”. In contrast, executives are glued to real-time newsfeeds on the pandemic. They watch stock market updates and look for signals that will impact their business survival as never seen before events happen. They subscribe to real-time feeds and collaborate remotely using live chat and video conferencing on the best next steps or recommendation actions. They are not using BI dashboards to analyze what happened. They are looking for insight and foresight, not hindsight. Complacent Business Intelligence practices lead to low levels of situational awareness. Complacency is a major cause of chronic laziness, and for business, this is no different. Situational laziness and “Management By Dashboards” numbs the organizational senses to what is happening right now, what is likely to happen, and what is an appropriate response. In the past, the focus was more on “what were the sales per store last month/year”. They now require a new level of intelligence on “what is the real-time demand at each store and do we have too much/too little, just the right level” to optimize current conditions or events. The pandemic will pass, but it will change how executives will revisit their strategic readiness for real-time events. We have access to vast volumes of real-time data coming into businesses at sub-second intervals. But we never built strategic capabilities around this, until now. This brings me to the most productive remote worker in the workforce of the future, the real-time, event-aware Digital Twin, or Event Twin. This Digital Twin can virtually walk around your business, collect real-time data or “signals”, find the key events that will impact business operations, and pass it to decision-makers at the right time, in an unbiased, forward-looking way. An Event Twin not only provides insights into what is happening right now, but it can also predict what is likely to occur with the right analytics. It is like running a “Lights Out” factory for your whole business. Decision-makers can work remotely and “see” what is happening in real-time through the eyes of the Event Digital Twin. Event Intelligence through event-aware Digital Twins will be the most valuable decision support investment organizations will make post the pandemic. [ultimate_spacer height=”30″ height_on_tabs=”20″ height_on_tabs_portrait=”20″ height_on_mob_landscape=”10″ height_on_mob=”10″] So what does an Event Twin look like? It needs to harvest information from a range of real-time data sources including business applications, web-services, and more recently, the influx of information from the Internet of Things (IoT) with sensor-based or smart device machine-born data. It has strong integration and orchestration capabilities and knows what analytics to apply when creating intelligent data streams. It also knows how to pass data on to other systems where you want to take action, like creating a work order in your ERP solution. It can handle a large volume of varied data at scale. It is masterful at finding key business events that provide opportunity and risk. It has a high work rate with a high level of accuracy. It is the perfect team member to know what is happening or likely to happen right now. Digital Twin Data Stream harvest data through listeners and turn it into real-time intelligence. The Eventboard in an Event Digital Twin is different from a historical dashboard. It shows key business events that are happening right now based on recommendation rules that alert you when a specified event or sequence of events occur. It only shows Event Intelligence and provides the right decision support to make the best decision quickly. Companies will build these event rules into the Digital Twin, and it will be a critical competitive advantage in future. Example of an Eventboard where real-time event data from a Digital Twin Data Stream on a pump trigger a recommendation and a notification. Recommendation rules are set up to detect and report operational events that are happening or likely to happen. It is easy to define new event rules for the Event Twin to detect and respond in real-time. What if we offered to help you hire a real-time Event Digital Twin solution that works on our Event Intelligence platform: – that connects to the real-time data sources in and around your business in a matter of a few days – and provide you with unbiased, right-time information and recommended actions – so that you can track leading indicators for key events in your business and make the best decisions quickly. Would you take me up on that offer? If you’re interested in learning more, send an email to [email protected]."
  },
  "docs/resources/faqs/external-content/blogs/2020/from-the-control-room-to-the-bedroom.html": {
    "href": "docs/resources/faqs/external-content/blogs/2020/from-the-control-room-to-the-bedroom.html",
    "title": "From the Control Room to the Bedroom | XMPro",
    "summary": "From the Control Room to the Bedroom Blog From the Control Room to the Bedroom Posted on April 17, 2020 by Pieter van Schalkwyk How we went from “going to work” to “being at work” overnight As part of their daily operations, executives, managers and operators in industrial businesses are used to working with data and information from systems like: DCS (Distributed Control System) SCADA (Supervisory Control and Data Acquisition) MES (Manufacturing Execution Systems) EAM (Enterprise Asset Management) APM (Asset Performance Management) PLM (Product Lifecycle Management) Many of these systems have been around for a long time and operate on legacy platforms that require complex IT infrastructure. Executives and operators use information from these systems in reports, BI dashboards and graphs to make critical operational decisions that impact their production, safety, cost, and supply chain to mention a few. They also collaborate on the information through secure corporate networks and IT infrastructure that house all of these systems and information. Morning meetings and production planning meetings are critical coordination points for these businesses, and everyone has the real-time information at hand on corporate laptops, tablets and even mobile devices. It is all nicely ring-fenced in closed corporate networks in systems that were designed for people to work from plants, factories and sites. It was perfect for the “factory worker” style of work where managers and operators where on-site, in close proximity of the DCS or control room. But that all changed with the world-wide COVID-19 pandemic with “stay at home” orders that forced executives and managers across most industries to run operations from their bedrooms now. Office blocks at factories, mines, and oil fields are suddenly empty with small numbers of operational staff rotating on shifts to keep operations going. The impact of delayed Digital Transformation initiatives Control Rooms are suddenly empty and Distributed Control Systems or DCSs were suddenly not so distributed. Managers needed to make decisions without access to all the systems they had inside the firewall of the corporate network before. Virtual meetings helped with collaboration, but access to real-time operational intelligence remains a challenge for the many organizations that didn’t invest in developing real-time digital twins for their operations. They now suddenly find that they don’t have remote access to data about key events happening in their business and they don’t have the remote data acquisition capabilities to direct their operations, maintenance and 3rd party personnel the way they could do from their traditional control room. Moving from the control room to the bedroom exposed those organizations that were slow to adopt Event Intelligence, Digital Twins and other IoT-style capabilities. These organizations find themselves with greater operational, safety and financial risk than before and executives are blindsided by business events because they don’t have any insight (or foresight) to make good decisions fast. Smart businesses invested in “Work from Wherever” Event Intelligence But there are those organizations that implemented remote event intelligence solutions that give them access to real-time operational and business data, alerts on business events that may impact their operations, the recommendations to address those events, and the feedback on the effectiveness of the business actions. They didn’t invest in it as a result of COVID-19, but they did it strategically beforehand. It just turns out that their decisionmakers now have access to event digital twins of their operations that give them the “digital eyes” not to fly blind while working from their bedrooms. These organizations understood early on that their businesses are exposed to more and more internal and external events that need to be responded to in real-time. They also know that these business events can come from: the actions of people in their business; the actions of their competitors, customers, legislators or suppliers; equipment breakdown, process failures, weather events; the Operational Intelligence that they gather from their business applications, data sources and web services; and more recently, the influx of information from the Internet of Things (IoT) with sensor-based or smart device machine-born data (IoT Platforms). They know their businesses are forced to operate in real-time to remain relevant, compliant and competitive as every(thing) is becoming more connected in digital business. They are also aware that the value of real-time, event knowledge loses its value (depreciates) quickly with a small window to exploit it. They found that traditional request-driven applications, integration and architectures don’t support their event-driven business requirements. Their business users (subject matter experts) want solutions to easily create complex real-time applications with analytics that provide situational awareness, are always on, and prescribe and orchestrate appropriate actions. Their managers and executives want to know that these business events are responded to and want to monitor the effectiveness of their decisions and actions. These organizations invested in Event Intelligence capabilities that give their executives, managers and operators remote access to critical business and event data to make good decisions fast that increase revenue or throughput, reduce risk and cost, and keep workers safe. XMPRO’s Event Intelligence platform connects to most of the traditional systems and combines the data to create new event intelligence that didn’t exist before. XMPRO brings data and information from traditional legacy systems and well as new IoT solutions together in a secure, resilient and reliable way. Eventboards (event dashboards) provide critical real-time insights into key events that are happening or likely to happen. And XMPRO Recommendations and Workflows ensure that the appropriate action gets taken in a timely manner. Watch this demo to see how we take the Control Room to the Bedroom and contact us at [email protected] to find out how your team can create the event intelligence that you need when you “Work From Wherever”."
  },
  "docs/resources/faqs/external-content/blogs/2020/index.html": {
    "href": "docs/resources/faqs/external-content/blogs/2020/index.html",
    "title": "2020 Blogs | XMPro",
    "summary": "2020 Blogs Articles from the XMPro blog published in 2020. Articles copy-me Digital Twin: Your Most Productive Remote Worker From the Control Room to the Bedroom Lean Digital Twin: Part 2 Lean Digital Twin: Part 3"
  },
  "docs/resources/faqs/external-content/blogs/2020/lean-digital-twin-part-2.html": {
    "href": "docs/resources/faqs/external-content/blogs/2020/lean-digital-twin-part-2.html",
    "title": "Lean Digital Twin: Part 2 | XMPro",
    "summary": "Lean Digital Twin: Part 2 Blog Lean Digital Twin: Part 2 Posted on August 28, 2020 by Pieter van Schalkwyk In part 1 of this series, we described the fundamentals of the Lean Startup approach and how it can be applied to creating digital twins. In this article, we’ll look at the practical application of this approach to develop a Lean Digital Twin. The first phase of the approach minimizes development effort as it focuses on identifying key business issues that can be addressed with a digital twin by describing the overall solution in an easy-to-understand manner. It is referred to as the problem/solution fit phase of the Lean Startup methodology. The second phase of the approach defines a minimum viable digital twin (MVDT) based on the problem/solution statement from the previous phase. The MVDT is used to validate and verify assumptions and hypotheses made during the problem/solution assessment. The MVDT may undergo multiple iterations to demonstrate a digital twin/business fit. This is derived from the product/market fit in the Lean Startup approach. This is best accomplished with agile development tools that allows subject matter experts to quickly change elements of the MVDT. Once an MVDT hypothesis has been validated and verified, the digital twin can be scaled for full production applications and lifecycle. These first two phases are focused on validated learning based on iterations and potentially pivoting the digital twin application as new learning emerges. It is best to construct the lean digital twin in a series of consecutive steps that are outlined below. Step 1: Find a Problem Worth Solving (Understand the Problem) The prioritization approach described here is used in two iterations to 1) rank multiple initiatives in an organization that could benefit from Digital Twins, and then 2) rank assets or processes that collectively operate as a system where use cases for the system are ranked. The first prioritization exercise focuses on prioritizing which system to focus on. The latter exercise provides guidance on the prioritization of use cases for an asset grouping that could be serviced by a single digital twin. Examples include a packing line in FMCG (fast moving consumer goods), a well in Oil & Gas, a robot assembly cell in manufacturing, a main line conveyor in materials handling or a processing plant in mining. Both exercises follow the same approach and the initial prioritization matrix can be omitted if the business is clear on the system that could benefit from a Digital Twin. It is, however, recommended to do the initial prioritization exercise to ensure that the real business challenges are addressed. It is the authors’ experience that organizational biases often influence the selection of Digital Twin candidates and the Prioritization Matrix approach assists in identifying impactful projects. The objective is to establish a falsifiable hypothesis to test around the business problem that the digital twin will address. “Cyclone pumps are responsible for 30 hours of downtime per month” is an example of such a hypothesis that can be tested in product/solution fit interviews and workshops. The prioritization process assesses both business impact and technical readiness of a Digital Twin project. The high-level business outcomes in the prioritization framework are the basis for scoring and agreeing on the business impact of a specific use case. The prioritization process starts with a list of potential Digital Twin use cases and ranks them based on their business impact for each desired business outcome. The business impact metrics are chosen to align with the strategic objectives of the organization. These are often referred to as the business drivers in digital transformation programs. To avoid analysis paralysis, a simple high, medium, and low scoring methodology is used in setting up a ranking matrix. This is best done with the business (operations), IT and OT representatives in a working session. Once the business impact is scored for each scenario the technical feasibility (or complexity) is assessed for each scenario, again without over-analyzing or getting into too much technical detail. It is a top down approach and even though information from reliability engineering practices like FMEA can be helpful indicators, it is important to guard that this becomes a technical feature or requirements design session. The impact assessment is done based on the strategic drivers of the business such as Safety, Down Time, Quality, Throughput, Cost etc. In this example, the following technical assessment criteria is used: (1) OT complexity, (2) IT complexity, (3) analytics, (4) system complexity, and (5) project readiness. Technical assessment criteria can be adjusted to fit the requirements of the business, but for this example the criteria are for a typical industrial installation. OT and IT complexity are described in terms of availability, accuracy, latency, and geographical location. Analytics is described in terms of maturity, sophistication (predictive and cognitive analytics) and application of business rules or physical models. System complexity is based on deployment infrastructure (edge, local, and cloud) and geographical constraints. Project readiness is assessed based on availability of subject matter experts and technical resources. It is generally useful to define “order of magnitude” financial measures to agree on the high-level impact of each new state or scenario. The objective is not to be accurate in estimating the value of a business case, but to get high-level agreement between the different stakeholders on the potential impact of each scenario. In this example a scale of (1) greater than $100k, (2) greater than $1m, or (3) greater than $10m is used. This “order of magnitude” is visually represented in a bubble chart with the business impact and technical readiness scores as the two major measures. The weighted average values of each of the measures are placed on the graph which is divided into four quadrants. The size of the bubble is determined by the value of the economic impact. The four quadrants represent the business readiness for each of the Digital Twin scenarios. The “Do Now” quadrant represents high business impact and a high level of technical readiness. Opportunities on the far right of the quadrant with the biggest bubble size often represent Digital Twin projects with the highest likelihood of success for all stakeholders. This approach provides a common understanding of the expected business outcomes and potential technical challenges in achieving this goal. It provides the basis for more detailed analysis of those projects with a high likelihood of success. A downloadable copy of the Prioritization Matrix is provided in the Lean Digital Twin Kit linked at the end of the article. In using an iterative approach where Digital Twin scenarios are first done at an overall business level, the ranking will provide guidance on the highest priority process or system that in turn gets broken down into sub-systems, components or assets that are ranked based on the same process. In an FMCG scenario the initial ranking may be done for business areas such as raw materials handling, production processes, filling and packing, and shipping. If filling and packing is identified as the best “Do Now” opportunity then the follow-on exercise could rank digital twin scenarios for filling/sealer, labelling, cartooning, packing, and palletizing. This will identify digital use cases with the highest likelihood of successfully addressing pressing business issues. These sessions should be limited to 90min as more detail will be required in later analysis, but the objective is to be lean and reduce waste. The outcome of step 1 is a prioritized list of Digital Twin scenarios for problems worth solving. Did you enjoy part 2 of this series? Click here to read part 3 Get the Free Lean Digital Twin Resource Kit Includes the PDF version of 3-part series, the Lean Digital Twin Canvas Template and the Prioritization Matrix Get My Resource Kit"
  },
  "docs/resources/faqs/external-content/blogs/2020/lean-digital-twin-part-3.html": {
    "href": "docs/resources/faqs/external-content/blogs/2020/lean-digital-twin-part-3.html",
    "title": "Lean Digital Twin: Part 3 | XMPro",
    "summary": "Lean Digital Twin: Part 3 Blog Lean Digital Twin: Part 3 Posted on September 2, 2020 by Pieter van Schalkwyk This is the final article in a 3-part series on Lean Digital Twins. Get started with part 1 and part 2 or sign up at the bottom of the article to get the full PDF version. Step 2: Document The Plan – Lean Digital Twin Canvas (Define the Solution) Once one or two digital twin candidates are identified, a single page solution description is created for each candidate. This single page solution description is based on the Lean Canvas described in the section on the Lean Startup approach. The canvas is adapted for the Lean Digital Twin process and is referred to as a Lean Digital Twin Canvas. It describes all the key elements of the problem/solution fit. The proposed sequence for completing the Lean Digital Twin Canvas is described below. It is recommended to do the initial draft in a 60min session following the prioritization ranking exercise. Sequence for completing the canvas in a workshop session: (Problem) Define top 3 problems that your twin will address based on the prioritization matrix. (Customer Segments) Who are the target users that will benefit from the solution? (digital twin) (Digital Twin UVP) What makes this digital twin different from what you are already doing? (Solution) What are the top 3 features of the digital twin? (AI, real-time, decision support etc.) (External Challenges) What are the external red flags for the digital twin? (security, data access, connectivity) (ROI Business Case) How will this digital twin deliver ROI? (Key Metrics) How will the digital twin be measured (quantitatively)? (Integration) What are the key integrations required to make it work? (Costing) What is the projected costing? It follows a logical sequence that starts with the problem and the users rather than jumping to product features. This holistic approach considers all aspects required to deliver a successful digital twin project. The initial workshop session is aimed at completing as much as possible, but the Lean Digital Twin Canvas is updated continuously (versioned) as we test, validate and learn. One major benefit of the canvas is that it is easy to complete in a workshop session and it provides a one-page business plan for the digital twin that is easy to communicate to both end-users and project sponsors. It is presented as a single slide in sponsor meetings. The completion of the Lean Digital Twin Canvas concludes the product/market fit phase of the Lean Digital Twin approach. The next phase focuses on validating and verifying your hypothesis and assumptions. This is done by developing a minimum viable digital twin or MVDT similar to the product/market fit phase of the lean startup methodology. Step 3: Decide what goes into V1 of the Minimum Viable Digital Twin (MVDT) The same prioritization process that was used for choosing MVDT candidates is used to determine what features to include in the initial release or MVDTv1. The only difference is that digital twin features are now assessed on business impact and technical feasibility rather than business applications of the Digital Twins. Typical features in an industrial application include, but are not limited to: real-time equipment data from sensors and devices time series data from historians and automation systems machine learning algorithms such as anomaly detection predictive algorithms such as classification and regression models production data from enterprise systems physics-based or engineering models that describe equipment behavior simulation models Digital twins are typically composed of combinations of the above features. Ranking these features in their ability to address the problem statement and solution identifies the two or three key features in the “Do in V1” quadrant. This approach identifies key capabilities in the initial release that will provide the best guidance on validating and verifying the solutions capability to address the business challenge. It is important to note that the minimum viable digital twin features are still implemented as fully featured solution components, but the scope of the overall MVDT is limited to the two or three features identified in this process. Step 4: Create lightweight MVDT to validate hypothesis Mocking up an MVDT that includes the three features chosen in the previous step provides a basis for validated learning. This mockup for a proposed well digital twin uses real-time event information for reservoir data from a subsurface data store, a well location map from a GIS data source, and production data from an operations database. The information is not yet integrated to the real-time data, but the mockup provides a realistic view of the lean digital twin and the decision support that it will provide users including operations data, recommendations based on descriptive and prescriptive analytics, and open tasks associated with this entity. Digital twin application tools that support an agile approach have this capability built in, but these mockups can also be done in simple tools such as Microsoft PowerPoint. The mockup typically goes through multiple iterations during a two-week period of review by users and stakeholders. Mini presentations and interviews with stakeholders provide immediate feedback that is easy to incorporate in the mockups. The outcome of this process provides a validated basis for creating a live version of MVDT that will be used to verify that it solves the problem that it set out to do. Step 5: Create an operational MVDT Once the mockup is agreed upon, the actual MVDT is built out, preferably in agile development toolset, and integrated to operational data sources. This can range from a few days to 3-4 weeks depending on the tools that are used. The benefit of using a Lean Digital Twin approach is that the feature set is limited which means users can be trained in a relatively short period of time to use the digital twin application. The objective of this step is to verify the assumptions and hypotheses made along the way and to identify areas where the MVDT must be improved or changed. This can be done for between 4 and 12 weeks based on the complexity and impact of the solution. This phase is also used to validate and clean up data sources, verify algorithms, and check calculations to improve the quality of the decision support the digital twin provides. The results from this step concludes the product market fit review and serves as the basis to deploy and scale the digital twin to production. Conclusion The Lean Digital Twin approach is well suited to organizations that are starting a digital twin journey and want to use an iterative approach to discover requirements in a systematic way while demonstrating business value at the same time. The approach requires top-down support and bottom up commitment. It is a collaborative approach that provides the guardrails for managing the process as it is not prescriptive. It does, however, require multiple iterations or pivots to find the best digital twin/business fit. [ultimate_spacer height=”30″ height_on_tabs=”30″ height_on_tabs_portrait=”30″ height_on_mob_landscape=”20″ height_on_mob=”20″][ultimate_spacer height=”30″ height_on_tabs=”30″ height_on_tabs_portrait=”30″ height_on_mob_landscape=”20″ height_on_mob=”20″] Get the Free Lean Digital Twin Resource Kit Includes the PDF version of 3-part series, the Lean Digital Twin Canvas Template and the Prioritization Matrix Get My Resource Kit [ultimate_spacer height=”30″ height_on_tabs=”30″ height_on_tabs_portrait=”30″ height_on_mob_landscape=”20″ height_on_mob=”20″]"
  },
  "docs/resources/faqs/external-content/blogs/2021/copy-me.html": {
    "href": "docs/resources/faqs/external-content/blogs/2021/copy-me.html",
    "title": "| XMPro",
    "summary": "Blogs: 2021 The Value of a Composable Digital Twin"
  },
  "docs/resources/faqs/external-content/blogs/2021/index.html": {
    "href": "docs/resources/faqs/external-content/blogs/2021/index.html",
    "title": "2021 Blogs | XMPro",
    "summary": "2021 Blogs Articles from the XMPro blog published in 2021. Articles copy-me The Value of a Composable Digital Twin"
  },
  "docs/resources/faqs/external-content/blogs/2021/the-value-of-a-composable-digital-twin.html": {
    "href": "docs/resources/faqs/external-content/blogs/2021/the-value-of-a-composable-digital-twin.html",
    "title": "The Value of a Composable Digital Twin | XMPro",
    "summary": "The Value of a Composable Digital Twin Blog The Value of a Composable Digital Twin Posted on March 11, 2021 by Pieter van Schalkwyk What is a Composable Digital Twin 2020 will go down in history as one of the most disruptive, uncertain, and transformative years for people and organizations worldwide. Or, as one of my learned friends explained, “overnight it exposed the systemic fragility in the operating capabilities of enterprises and organizations”. Many experts have commented on the unprecedented acceleration of digital technology adoption and the behavioral transformation in business that was activated during the pandemic. Five years of Digital Transformation happened in less than five months, according to industry analysts. Some organizations were able to adjust quickly and exploit opportunities from the turmoil. In contrast, others are still paralyzed by rigid business systems and processes that are typical of traditional enterprise architecture approaches. The future reality is that the rate of change will only increase. Professor Peter Frisk, who specializes in “innovative business future”, says that we will see more change in the next 10 years than in the last 250. Traditional enterprise models were designed when computerized business was still in its infancy. IT was a department that supplied and supported servers, desktops, and notebooks to business users. IT improved business efficiency, but it wasn’t the platform for business. Technology was slow-changing and siloed organizational structures reflected the rigid, hierarchical approach to organizing people and work. But the pace of business changed in correlation with the rate of change as Peter Frisk’s graph points out. The average lifespan of companies has fallen from 75 years in 1950 to 15 years today; 52% of the Fortune 500 in 2000 were gone by 2020. The traditional enterprise has not kept up with the demands of fast-paced emerging technologies like Digital Twins, Internet of Things (IoT), Distributed Ledgers, AI, Quantum Computing, and a smorgasbord of high-impact technologies. These innovative technologies drive changes in new products, services and the new business models needed to support them. The pace of change requires agile and resilient enterprise architectures that enable multi-disciplinary teams to be highly responsive in exploiting business events in an innovative way. It still, however, needs to have guardrails to protect the organization from unnecessary risk. The keynote at Gartner’s 2020 ITxpo introduced the concept of a Composable Enterprise that enable teams that combine business and IT to compose new “fit for purpose” applications in a fraction of the time that it takes in traditional software development processes. Gartner predicts that “by 2023, organizations that have adopted an intelligent composable approach will outpace the competition by 80% in the speed of new feature implementation.” [1] The Composable Enterprise is powered by Composable Business Applications. Digital Twins, in turn, are a class of this Composable Business Applications that deliver specific high-value business capabilities in the Composable Enterprise. Digital Twins can compose and package these high-value capabilities in what Gartner refers to as PBCs or Packaged Business Capability. To better understand Composable Digital Twins (CDT) and their value to the organization, it is worth exploring Composable Business Applications in more detail. What is a Composable Business Application? Traditional 3-tiered business applications were hierarchically structured with a data layer at the base, an application logic layer in the middle, and a user interface layer at the top. The introduction of service-oriented architecture (SOA), APIs and micro-services provided some flexibility, but traditional business applications still reflect this rigid design pattern. Changes to business logic are compiled into the larger codebase of the application and rely on specialized developers to make changes at any of the three layers of the business application. The reference model describes the key components necessary to create Composable Business Applications for your Composable Enterprise. I highly recommend that you read the reference model document by Gartner, but I will highlight a few elements related to composing digital twins using this approach. Reference Model for Intelligent Composable Business Applications (https://www.gartner.com/document/3991699) Composable business applications use a modular approach to compose and recompose applications quickly to address a specific problem at that point in time. An Application Composition Platform composes Packaged Business Capabilities (PBC) around specific user Application Experiences that address a particular business need. It uses PBCs as building blocks to create a highly relevant application without superfluous capabilities that never get used. Think of your ERP or CRM solutions and how many of the features actually get used. Composable Business Applications use Application, Data, and Analytics PBCs through a Low Code Application Composition Platform to integrate, compose, orchestrate and provide a tailored user experience that focus on the critical business outcomes for the application. Applications can be composed and recomposed quickly by subject matter experts. This provides a high level of agility and flexibility to address new business challenges. A reliability engineer can, for example, create a predictive maintenance solution around a key asset that is suddenly impacting production capability due to a change in operating conditions or supply chain intelligence. The solution can be composed in a matter of days, in stark contrast to the traditional model that may take months or even years to deploy a solution like this. PBCs enable the engineer to use sophisticated business capabilities without being an expert in data integration, predictive analytics, or digital twins. These capabilities are pre-packaged and available in a low code environment to drag-and-drop onto an application experience canvas. PBCs wrap integration, business logic, and analytical capabilities into reusable elements. It’s like Lego bricks that kids use to build (compose) creations that are only limited by their imagination. The range of modular bricks enable the kids to, for example, create motion experience through packaged wheels, electric motors, gears and a range of other capabilities that take a few seconds to assemble. They have the option to follow the vendor’s assembly plans and build it as shown on the outside of the box. Alternatively, they only use elements of the original design and create their own unique instance that they can play with for a while before reconfiguring it into a new creation. PBCs provide similar building blocks for the Composable Enterprise and bring built-in resilience, scalability and expertise. Reference Model for Intelligent Composable Business Applications (https://www.gartner.com/document/3991699) Since a PBC is built around business capabilities, it is easily recognized by a business user. This is different to a JSON file, for example, with comprehensive code and information that is only understood by an experienced developer. Gartner describes PBCs in the reference framework as follows: PBCs are encapsulated software components that represent a well-deﬁned business capability, recognizable as such by a business user. Well-designed PBCs are: Modular: Partitioned into a cohesive set of components. Autonomous: Self-sufﬁcient and with minimal dependencies to ensure ﬂexibility in composition. Orchestrated: Packaged for composition to assemble process ﬂows or complex transactions through APIs, event interfaces or other technical means. Discoverable: Designed with semantic clarity and economy to be accessible to business and technical designers, developers, and active applications. Digital twins are a specific type of Application PBC in the framework for Composable Business Applications. Composable Digital Twins follow a similar development and usage patterns as the applications that they serve. The Composable Digital Twin Business Capability Now that we know what a Composable Business Application is, we can turn our attention to Composable Digital Twins. We mentioned above that it is an application-based “Packaged Business Capability”. Digital Twin PBCs are used to create unique application experiences to address target opportunities or challenges. In “Digital Twins: The Ultimate Guide” I describe the 6 types of data and models that we can combine in different configurations to provide the real-time intelligence and decision support that we to reusable compose Digital Twin capabilities. The 6 data types and models are: Physics-based models (FEM, Thermodynamic, Geological) Analytical Models (Predictive Maintenance) Sensor and Time series data (IoT platforms and Historians) Transactional data (ERP, EAM) Visual Models (CAD, AR, VR, BPM, BIM, GIS, and GEO) Master Data (EAM, AF, BPM) The different configurations are “packaged” to create reusable business capabilities based on the underlying technologies of the 6 types. This now makes it accessible to a business user as a citizen developer, without needing to be a technology specialist. Examples of these Packaged Business Capabilities (PBC) for Digital Twins are: Time Series data ingestion, validation, anomaly detection and forecasting as an integration PBC Real-time data updates from time-series data sources to a gaming engine or 3D visualization model Integrating real-time sensor data and context data from a maintenance system to a Python scheduling and optimization algorithm Integrating real-time IoT sensor data with GIS technology to create track-and-trace with geofencing capabilities. These PBCs can now be used to compose tailored Digital Twins quickly and securely through a low-code composition and orchestration platform. The Value of a Composable Digital Twin The business value of a Composable Digital Twin (CDT) is in the speed, resilience, and agility that it enables. Organizations need to respond quickly to business events that bring opportunity and risk. It is the backbone of the Continuous Intelligence capability, which is the business’ ability to respond to business events from: the actions of people in your business the actions of your competitors, customers, legislators or suppliers external events such as pandemics and extreme weather equipment breakdown, process failures, environmental events the operational intelligence that you gather from your business applications, data sources and web services; and more recently, the influx of information from the Internet of Things (IoT) with sensor-based or smart device machine-born data (IoT Platforms). The World Economic Forum’s “Value at Stake” framework for Digital Transformation Initiatives (DTI) provides a model for measuring the value of a CDT in terms of the financial impact across multiple dimensions. Typical Value at Stake calculation for Composite Digital Twins The advantage of the Value at Stake (VaS) framework is that it measures the benefits of CDTs in financial indicators that are understood by executives, boards, and shareholders. These numbers can impact balance sheets and share prices and go beyond the “warm and fuzzy” metrics that are often used to describe technology benefits. The VaS model is based on the practical measurements and quantification of the following elements: The value of empowering your subject matter experts with low code composition tools to create their own CDTs without being an experienced developer They can address the business problems firsthand without being a technology expert, but leverage state-of-the-art capabilities when composing CDTs They can create new solutions in an iterative way, by composing and recomposing minimum viable Digital Twins until they have a problem/solution fit. See our 3-part series on the Lean Digital Twin Your subject matter experts can compose new Digital Twin capabilities in days, rather than weeks or months, to address opportunities and risks in a quick and timely manner A resilient composable architecture reduces the brittleness of traditional system and business application complexity, and the resulting cost of ownership It removes bloatware or features that you pay for, but don’t use in traditional business and OT applications It enables you to compose Digital Twins or your assets, processes, supply chains, or organization from the bottom up. You can compose Digital Twins based on business problems and aggregate the solution as you gain the benefits of each incremental step A marketplace of preassembled CDTs based on PBCs reduces the complexity of composing Digital Twins Custom CDT capabilities of modern composition platforms allow you to develop your own CDTs and PBCs that address the unique requirements of your use case Create your own library of reusable CDTs with your own organizational IP as a competitive advantage for capabilities that make your organization unique. Use generic CDTs for capabilities that provide no competitive advantage. CDTs maximize the opportunities from “Easy Wins” vs the risk of the “Big Bets”. CDTs maximize the opportunities from “Easy Wins” vs the risk of the “Big Bets”. XMPro Digital Twin Composition Platform Creating Packaged Business Capabilities (PCB) like Composable Digital Twins (CDT) typically require a low-code Application Composition framework to deliver quick time to Value at Stake. XMPro’s Low Code solution provide the end-to-end capabilities for a Digital Twin Composition Platform. These capabilities include integration, composition, orchestration, development, and UX development in a single solution to create Digital Twin PBCs and more comprehensive Composed Digital Twin Experiences. XMPro manages the design and operation of Digital Twins across the lifecycle and can be compared to a composer who is also the conductor in an orchestra. XMPro Digital Twin Composition Platform The XMPro Data Stream Designed (DS) lets you visually design the data flow and orchestration for your Digital Twin compositions. Our drag & drop connectors provide reusable PBCs that makes it easy to bring in real-time data from a variety of sources, add contextual data from systems like EAM, apply native and third-party analytics and initiate actions based on events in your data. XMPro Data Streams Designer with Packaged Business Capabilities Examples of drag-and-drop Packaged Business Capabilities in XMPro DS The XMPro App Designer is a low/no-code application and UX composition platform. It enables Subject Matter Experts (SMEs) to create and deploy real-time Digital Twins applications without being programmers. XMPRO’s visual page designer enables you to compose custom page designs by dragging blocks from the toolbox onto your CDT, configuring their properties and connecting to your data sources, all without having to code. It provides integration, composition, orchestration and UX development capabilities in the XMPro Digital Twin Composition platform. From drag-and-drop composition to production Digital Twins The XMPRO App Designer integrates real-time data from XMPRO Data Streams with other business, operational or 3rd party data sources. Each block in your page designs can be connected to a different data source, enabling you to build apps that provide your team with comprehensive decision support from multiple systems. After publishing your app, your pages will update with live data. XMPRO Recommendations in the App Designer, are advanced event alerts that combine alerts, actions, and monitoring. You compose recommendations based on business rules and AI logic to recommend the best next actions to take when a specific event happens. You can monitor the actions against the outcomes they create to continuously improve your decision-making. Composable recommendation rules from real-time event data The XMPro Digital Twin Composition platform provides an end-to-end capability to create, deploy, operate and maintain Composable Digital Twins at scale. The green process blocks show the data integration, validation, and storage requirements for real-time Digital Twins. The blue process blocks represent the analytics requirements of Composable Digital Twins, whereas the purple elements show the user interaction and actionable capabilities of XMPro’s Composable Digital Twins. End-to-end Digital Twin development process in XMPro Digital Twin Composition Platform References Gartner, Use Gartner’s Reference Model to Deliver Intelligent Composable Business Applications, Natis et al., 14 October 2020. Disclaimer Gartner does not endorse any vendor, product or service depicted in its research publications, and does not advise technology users to select only those vendors with the highest ratings or other designation. Gartner research publications consist of the opinions of Gartner’s research organization and should not be construed as statements of fact. Gartner disclaims all warranties, expressed or implied, with respect to this research, including any warranties of merchantability or fitness for a particular purpose."
  },
  "docs/resources/faqs/external-content/blogs/2022/7-trends-for-industrial-digital-twins-in-2022.html": {
    "href": "docs/resources/faqs/external-content/blogs/2022/7-trends-for-industrial-digital-twins-in-2022.html",
    "title": "7 Trends for Industrial Digital Twins in 2022 | XMPro",
    "summary": "7 Trends for Industrial Digital Twins in 2022 Blog 7 Trends for Industrial Digital Twins in 2022 Posted on January 12, 2022 by Pieter van Schalkwyk 2021 has been an excellent year for increased awareness of the application and benefits of industrial digital twins. Organizations such as the Digital Twin Consortium (DTC) and the Industrial Digital Twin Association (IDTA) have also seen an increase in membership activity and collaboration. I recently co-authored a book on Industrial Digital Twins with Shyam Nath and thought I would take what we’ve seen in 2021 and extrapolate that to 7 trends I believe will accelerate the adoption of Digital Twins in 2022. Trend 1 – Composable, Capability-Driven In 2021 XMPro created the concept of “Composable Digital Twins” based on Gartner’s Composable Applications, and this remains a key trend for 2022. Composable Digital Twins are described in our blog post on “The Value of a Composable Digital Twin”, and we presented it as a public webinar in the Digital Twin Consortium (DTC). End-user organizations looking to leverage digital twins are starting to realize that the composable approach is the only way to scale complex use cases or applications. These end-users also focus on capability requirements rather than technology solutions hyped up by vendors. Packaged Business Capabilities (PBCs) are the building blocks of Composable Digital Twins, and in 2022 we will see the focus on the packaging of capabilities that make up these PBC’s. XMPro created the Digital Twin Capabilities Periodic Table in the Digital Twin Consortium’s Natural Resources Working Group to assist end-users with identifying and choosing the right capabilities for their Digital Twin use cases. The periodic table will enable a capability-driven approach to defining the requirements for a Composable Digital Twin. It will accelerate the requirements definition process and ensure that the focus remains on capability delivery rather than the “Shiny New Object” syndrome. We envisage that the composable, capability driven approach to developing and managing digital twins will go from strength to strength in 2022. XMPro’s 2022 product roadmap and solution delivery approach will remain capability-driven to ensure that we focus on the problem that needs to be solved rather than the underlying technology that supports it. Trend 2 – BlockTwin Emerging In 2022 we will see high-impact industrial use cases where digital twins leverage Blockchain technology. It will signal the emergence of the BlockTwin. Web 3.0 blockchain technology will enable autonomous digital twins that transact with other digital twins to exchange value through transparent smart contracts. Web 3.0 provides the decentralized, networked infrastructure to deploy distributed applications (or dApps in blockchain terminology) for digital twin uses cases that span beyond the boundaries of a single enterprise. These digital twin use cases will require secure, tamperproof, immutable records of transactions and the ability to execute smart contract dApps in a distributed ecosystem. This is the basis for the emerging BlockTwin and will allow these digital twins to interact autonomously with each other and business applications. BlockTwin use cases will be based on two usage patterns. The first is the familiar “traceability” use case pattern where a Digital Thread (or breadcrumb) is stored as an immutable audit trail of relevant IoT and transactional information. This is the primary use case pattern in supply chain solutions. For example, the temperature, humidity, and geo-location of fresh produce are captured and stored on a blockchain to prove the origin and exposure conditions as it is transferred across multiple nodes in a supply chain from the producer to the consumer. This is essentially a supply chain digital twin where the transactions are captured and locked on a blockchain. The second usage pattern will increase the application of Digital Twins in more advanced, distributed deployment or Web 3.0 applications. The BlockTwin will not just store the transactional information described in the first usage pattern but will also store digital twin models and instances on blockchain technology. This will allow digital twins to interact with each other across domains and organizational boundaries through smart contracts and oracle services. This usage pattern will allow for physical assets to be created as NFTs. A BlockTwin NFT will become the digital avatar of the physical asset in the emerging metaverse and enable the asset to interact and exchange value with other assets in the metaverse. This usage pattern is highly relevant for transferable assets, ESG applications, and Digital Twins that transact autonomously through a smart contract with a cryptocurrency or token. A simple example of a transferrable asset scenario is the process of buying a used vehicle. The benefits of putting an aircraft maintenance logbook on the blockchain is a well-known use case for distributed ledger technology, but it doesn’t contain any information on the operational use and condition of the aircraft. Imagine an NFT of the vehicle based on a standard digital twin model from the original manufacturer. This will contain all the relevant information such as VIN number, manufacture date, and verified model information. It will also contain proof of ownership, along with all previous owner information. These are standard characteristics of NFTs. The second set of digital twin information is the maintenance records stored on a blockchain by the various service providers that have provided maintenance and repair services. It will also contain any accident records and damage reports. The third data set is operational, time-series information from IoT sensors that may include coolant and transmission fluid temperatures, fuel usage, odometer readings, acceleration and braking profiles, and off-road geo-tagged locations. The vehicle seller can present the NFT in a metaverse marketplace to potential buyers, each with their own set of smart contracts that will autonomously derive a buy price from the immutable, tamperproof NFT records. The price can be presented back to the seller. Suppose it exceeds or matches the seller’s list price. In that case, it can automatically buy the vehicle, transfer ownership of the NFT and physical asset, and pay for the transaction in a cryptocurrency that the seller can convert back to fiat currency through another smart contract if desired. We can create a decentralized marketplace to buy and sell used vehicles based on a series of transparent and open smart contracts that use BlockTwin NFTs to transfer physical assets. This is just a straightforward example. And this use case pattern will apply in many industrial scenarios, including managing carbon credits, transferring responsibility in nuclear waste containers or LACT (Lease Automatic Custody Transfer) in Oil and Gas, and transferring certified equipment such as pressure vessels in a merger or acquisition. At XMPro, our BlockTwin applications for 2022 will focus on providing composable capabilities to deploy both usage scenarios in a no-code environment. BlockTwin™ powered by XMPro, provides capabilities to oraclize industrial data and manage all three data types described in the example on a blockchain. Sign up for our newsletter or follow us on LinkedIn to receive the latest updates on this emerging new trend for industrial digital twins. Trend 3 – Edge to Cloud Continuum During the last six months of 2021, we have seen an increased number of requirements for digital twins that can operate across a deployment spectrum, from the edge to the cloud. There is often a debate whether solutions should run at the edge or in the cloud and there are pros and cons to both deployment approaches. The pros and cons are well documented and are not argued here, but we are seeing an increased interest in scenarios where digital twins can be deployed in a distributed manner where specific capabilities are needed at the edge and other capabilities in the cloud. We call this the Edge to Cloud Continuum (E2C2). We predict that we will see more of this trend in 2022 where digital twins will be deployed across the Edge to Cloud Continuum to get the best of both worlds. The emergence of a fog node at the edge, or what is in reality an on-premise data center, is another pattern that is becoming a requirement in many industrial digital twin scenarios. XMPro’s Data Streams are designed to operate across the spectrum of E2C2, and as the previous trend highlighted, this capability is also available for cloud-based blockchain networks. Trend 4 – Dataverse As digital twins advance in the metaverse, we will also see the “dataverse” evolving to support the complexity and scale of these new use cases. The dataverse that we are referring to is not the open-source project or the rebranded Microsoft Common Data Services, but rather the data infrastructure to support the next generation of digital twin use cases. Digital twins are ideally suited to graph-based relationship management, and this is best supported with graph databases. For example, Azure Digital Twin is built on a graph model, and we will see more of this pattern emerge in 2022. The underlying benefit of a graph-based model is that it codifies the network impact of digital twins. It subscribes to Metcalfe’s law that states that “the value of a network is proportional to the square of the number of connected users.” The value and benefit from digital twins will increase exponentially as more instances and use cases are connected by a TwinGraph. Traditional hierarchical asset structures and frameworks will be replaced with graph-based solutions such as Azure Digital Twin, modelled with Digital Twin definition language (DTDL) that describes the relationships. An example of this is the work that we at XMPro are doing on converting the ISO 14224 asset model to DTDL and Azure Digital Twins. We will also see the rise of cloud historians to support use cases where data from Operational Technology (OT) is combined with Information Technology (IT) to remove the siloed traditional historian structure and boundaries. This will enable business technologists such as engineers with data science skills to use Digital Twins that solve challenges that are not easily done with traditional historian and analytics separation. Trend 5 – Intelligence Smarter digital twins built on AI and ML are the holy grail for use cases such as predictive maintenance and autonomous operations. Unlike consumer digital twins and standardized application-based predictive models such as CRM or customer churn in telecommunications, machine-borne data for IoT sensors and control systems is proving to be a much more significant challenge for industrial AI use cases. The data is not as clean, consistent, and well formed as in the examples mentioned above. The impact of a decision made on an industrial predictive model is potentially much higher that of a CRM churn model, for example. A 95% model accuracy may be great for identifying a customer that is potentially looking to change service provider and offering them a better deal, compared to a 95% accurate failure prediction model for the cooling water pump system in a nuclear reactor. Intelligent digital twins will advance in three main areas in 2022 to address some of the challenges described above. Firstly, more digital twins with virtual sensor agents will be used to train AI and ML models like anomaly detection, for example. This will improve the quality of the models as they will be trained on better quality data. Secondly, enhanced data management and wrangling in the emerging dataverse as described in trend 5 will further provide better training data sets. The third and most crucial area that will impact the development of intelligent digital twins is the ability for business technologists to embed the models in operational digital twins. A business technologist is a subject matter expert, like a reliability engineer, who also has the analytical and technical skills to develop a model in Python, for example. Embedding these knowledge models in digital twins requires a no-code development approach. It allows the business technologist to focus on the smarts of the solution while the no-code platform provides the operational platform to deliver a digital twin application. XMPro’ Application Composition Platform is aimed at enabling business technologists to create intelligent digital twins without being a developer or a cloud deployment specialist. As an example, the screen in figure 5 shows how you can create new instances in Azure Digital Twins, upload DTDL models and set up new resources without leaving the XMPro interface. Trend 6 – Security Digital Twins will come under increased security attacks in 2022. IoT security is rapidly maturing through the work of organizations such as the Industry IoT Consortium (IIC) with their Trustworthiness framework. Digital Twins have the added complexity of managing many models or prototypes that often house an organization’s intellectual property. They will be the target of competitive intelligence attacks. And having access to the digital twin instances for these models will provide a fully operational competitive intelligence scenario that can be exploited in different ways. It can be used to monitor a competitor, and in extreme cases it can be used in physical and cyber-attacks. 2022 will see an increased awareness of digital twins security across the lifecycle of a digital twin to preserve its trustworthiness and integrity. It will remain a foundational pillar for XMPro in 2022 and beyond. Trend 7 – Portfolio Management The final trend for digital twins in 2022 emerges as organizations move from POCs and pilots to full-scale production. Testing digital twins in a POC or pilot focuses on proving that a digital twin can address a specific challenge. It doesn’t require model management, configuration management, version control, and many other system management capabilities of large-scale complex systems. But as organizations are rolling out our digital twins at scale across many facilities and geographic locations, portfolio management is becoming increasingly important. Some analysts predict that large organizations may have more than 100,000 models and more than 1,000,000 instances in the next few years. 2022 will see Digital Twin Portfolio management become an essential requirement in the enterprise architecture models of digital twins user organizations. Together with trustworthiness and scalability, this remains foundational for XMPro’s product roadmap for 2022. We are excited for what lies ahead in the world of digital twins and in the words of Buzz Lightyear, our digital twin vision for 2022 is “To Infinity, And Beyond!”"
  },
  "docs/resources/faqs/external-content/blogs/2022/copy-me.html": {
    "href": "docs/resources/faqs/external-content/blogs/2022/copy-me.html",
    "title": "| XMPro",
    "summary": "Blogs: 2022 Create a Common Operating Picture of Your Operations with XMPro 7 Trends for Industrial Digital Twins in 2022 How to Build a Digital Twin + 60 Use Cases By Industry What are composable digital twins in the metaverse?"
  },
  "docs/resources/faqs/external-content/blogs/2022/create-a-common-operating-picture-of-your-operations-with-xmpro.html": {
    "href": "docs/resources/faqs/external-content/blogs/2022/create-a-common-operating-picture-of-your-operations-with-xmpro.html",
    "title": "Create a Common Operating Picture of Your Operations with XMPro | XMPro",
    "summary": "Create a Common Operating Picture of Your Operations with XMPro Blog Create a Common Operating Picture of Your Operations with XMPro Posted on July 29, 2022 by Wouter Beneke Changing from a reactive to a prescriptive way of working is one of the biggest challenges for large, complex asset-intensive businesses. Enabling subject matter experts to create an integrated common operating picture that is strategically aligned with your business objectives is the only way to manage your operations in real-time at scale. In this video we will cover: How to evolve from reactive operations to prescriptive. How to empower your subject matter experts to pull the value levers that align with all levels of your organization. How to compose an integrated Common Operating Picture that closes the loop on event response. How to reduce the risk of being blindsided by crucial business events that are happening or are likely to happen. How to improve accountability and create more visibility and opportunity for learning. XMPro’s No Code Application Development Platform enables subject matter experts to easily create real-time applications that provide situational awareness, are always on, and prescribe and orchestrate appropriate actions. Visit https://xmpro.com to learn how you can get more insights from your real-time data with XMPro."
  },
  "docs/resources/faqs/external-content/blogs/2022/how-to-build-a-digital-twin--60-use-cases-by-industry.html": {
    "href": "docs/resources/faqs/external-content/blogs/2022/how-to-build-a-digital-twin--60-use-cases-by-industry.html",
    "title": "How to Build a Digital Twin + 60 Use Cases By Industry | XMPro",
    "summary": "How to Build a Digital Twin + 60 Use Cases By Industry Blog, Guides / How To's, News How to Build a Digital Twin + 60 Use Cases By Industry Posted on December 8, 2022 by Wouter BenekeTABLE OF CONTENTSIntroductionWhat is a Digital Twin?Define the problem and scopeHow to build a Digital Twin?Using a Composable Approach to Digital TwinsDigital Twin Capabilities FrameworkThe 5 quick winsEnable your subject matter expertsWhat happens when you get this right?XMPro : The No-Code Digital Twin Composition Platform60 Digital Twin use-cases by industry INTRODUCTION In recent years, the word “Digital Twin” has become quite a buzzword. Many industries have concluded that the future of operational excellence will be dictated by successful implementation of a composable digital twin strategy. A successful composable digital twin strategy will not only give COOs a real-time view of assets, processes, and workflows but also help to create a common operating picture at the strategic, tactical, and operational level. The word “Common Operating Picture” is a military term for a single identical display of relevant (operational) information (e.g. position of own troops and enemy troops, position and status of important infrastructure such as bridges, roads, etc.) shared by more than one command. A COP facilitates collaborative planning and combined execution and assists all echelons to achieve situational awareness. Business leaders have realised that through an effective Digital Twin strategy, they can create a real-time COP that helps them: Avoid being blindsided by critical operational events Minimise asset down-time Increase asset output and lifespan. Improve operational safety & security Synchronize communication at a strategic, tactical and operational level Most industries including mining, oil & gas, natural resources, and manufacturing are still in the very early stages of technology adoption – few know where to start, or exactly what a composable digital twin strategy looks like. What is A digital Twin? According to the Digital Twin Consortium – Digital Twins are virtual representations of real-world entities and processes, synchronized at a specified frequency and fidelity. Digital twins use real-time and historical data to represent the past and present and simulate predicted futures. Digital twin systems transform business by accelerating holistic understanding, optimal decision-making, and effective action. Digital twins use real-time and historical data to represent the past and present and simulate predicted futures. Digital twins are motivated by outcomes, tailored to use cases, powered by integration, built on data, guided by domain knowledge, and implemented in IT/OT systems. Digital Twin Example : Smart Manufacturing In this example, real-time data is used for anomaly detection in a robotic arm. Define The Problem & Scope Before you can start to think about building a Digital Twin, you will firstly need to properly define the problem that you are trying to solve, including the scope, severity, and impact. Many companies have fallen into the trap of building a Digital Twin without a proper problem and scope definition – this is simply planning to fail as the Digital Twin will lack the sufficient capabilities to solve the required business problem. You will also need to think about how the ROI of your digital twin can be measured and benchmarked as it will be vital for the justification of the expansion of your business’ Digital Twin strategy, which should culminate in a real-time Common Operating Picture. How to Build A Digital Twin The very first step in building a digital twin is the same as with building anything – you need to make sure you have the right tools! You wouldn’t build a house without making sure you have the right tools ready to get the job done, the same is true with digital twins. Many companies jump in headfirst, trying to build their digital twin through an in-house point solution. This is a bit like trying to build a house by hand… In addition to spending over 50% of project budget on integration capability, there are a few critical issues with this approach: Point solutions are usually designed with only one use case in mind. Point solutions are usually not interoperable or scalable. Point solutions are resource intensive, and do not guarantee success. “Okay, so point solutions won’t do for Digital Twins, what are the right tools that I need to build a Digital Twin strategy for my business?” The key to building an effective Digital Twin strategy is found in the word “composability” USING A COMPOSABLE APPROACH TO DIGITAL TWINS Composable Digital Twins is an application development approach for Digital Twins that is based on an agile, composable enterprise architectural pattern. Using a composable approach to building digital twins is often a neglected topic, but is vital to the scalability of digital twin solutions within an organization. Composable digital twin solutions can be reused within new applications utilizing a “plug and play” fashion. It is the key ingredient to cementing competitive advantage over competitors. Most businesses in asset-intensive industries are held hostage to large scale monolithic technology support platforms such as complex ERP’s and CRM. The only way a digital twin strategy can be implemented at scale is through composable digital twins that easily integrates across the board. Composable Digital Twins focus on faster time to value, service-based orchestration, and reusing packaged business capabilities to develop and adapt highly scalable industrial applications for complex systems​. DIGITAL TWIN CAPABILITIES FRAMEWORK Digital twins can be composed to solve a plethora of business use cases using packaged business capabilities. The Digital Twin Consortium defines these capabilities in 6 different categories: Data Services Integration Intelligence UX Management Trustworthiness The 5 quick wins At XMPro, we recommend that businesses identify the top 5 “quick wins” where initial Digital Twin development can create quick time-to-value, which in turn justifies the expansion of the digital twin environment within the organisation. In order to do this, we use the XMPro prioritization matrix to assesses both business impact and technical readiness of a Digital Twin project. We go into more depth regarding this prioritization matrix in our article “Lean Digital Twin : Part 2” – read more… ENABLE YOUR SUBJECT MATTER EXPERTS Okay, so you have identified your first 5 “quick wins” where digital twins can make a real difference, the next step is to involve your subject matter experts in the solution design process. Your SME’s understand the problem better than anyone else, and will be the most effective at designing a digital twin solution to address it. XMPro’s No Code Digital Twin Composition Platform allows SME’s to easily compose their own digital twin solutions using blueprints and templates for quick time-to-value. WHAT HAPPENS WHEN YOU GET THIS RIGHT As an example, this is what happens when a digital twin strategy is effectively implemented: XMPro - The World's Only No-Code Digital Twin Composition Platform With XMPro, you can compose your digital twin solutions without any code, and easily integrate with your current systems using our integration library. The XMPro application designer also features prebuilt agents that allow users to fully leverage the pre-packaged business capabilities mentioned in the Digital Twin Consortium periodic table. Okay, so you are ready to build your first Digital Twin – Why not make sure you have the right tools for success, and give XMPro a go! We have helped many fortune 500 companies, including a fortune 20 business build real-time, Common Operating Pictures on XMPro’s Composable Digital Twin Platform that delivers >10x ROI, without kicking off a complex, long, and expensive IT project. START FREE TRIAL 60 DIGITAL TWIN USE CASES BY INDUSTRY There are countless use-cases for digital twins in multiple industries. We are only scratching the surface of Digital Twin potential. The only limit for this technology is the knowledge and creativity of your subject matter experts, as well as the tools you use to compose your digital twins. XMPro Solutions See how XMPro can help your business unlock it's full operational potential. Explore by use case, technology or industry. Search for:Search Button AEROSPACE DIGITAL TWIN USE-CASES AEROSPACE The aerospace industry is inherently complex, with decision making in design and engineering often having a mission critical impact if neglected. Digital twins play a vital role throughout the life-cycle of a project, from design and manufacturing, through to implementation and real-time operations monitoring. USE CASES: Project Design & Development. Project Simulation & Onboarding. Predictive Maintenance & Prescriptive Operations Safety Monitoring ARCHITECTURE DIGITAL TWIN USE-CASES ARCHITECTURE The architecture industry can leverage Digital Twins to render and model designs in real-time using AR overlays. USE CASES: Augmented Reality Design Review Augmented Reality Onboarding & Training Smart Building Design Principles AUTOMOTIVE DIGITAL TWIN USE-CASES AUTOMOTIVE The Automotive industry uses Digital Twins to simulate and test new design concepts as well as identify issues before they happen. USE CASES: 3D Product Development Human-machine Interfaces (HMI) Autonomous driving simulation Training & Guidance for Mechanics 3D configurators CONSTRUCTION DIGITAL TWIN USE-CASES CONSTRUCTION Supply chain data has become essential for the construction industry to make critical business decisions. Through Digital Twins, the construction industry can better visualise projects from design through to operations and maintenance. USE CASES: Real time Asset Security Predictive Maintenance Worker Safety Zones Real-Time Design Collaboration Immersive AR safety training Real-Time Project Timeline Visualisation ENERGY DIGITAL TWIN USE-CASES ENERGY Energy companies are increasingly outfitted with Industry 4.0 technology such as IoT sensors, AI-enabled cameras and real-time 3D modelling. The application of these tools in the construction of Digital twins can help this industry to make better decisions in real-time, increase production, improve maintenance as well as safety & security concerns. USE CASES: Augmented Reality Field Servicing Real-Time Remote Performance Management Asset Security through Geo Location Perimeter Monitoring. infrastructure DIGITAL TWIN USE-CASES INFRASTRUCTURE Modern city planners can use interactive models and live IoT data to simulate mobility pattern, traffic flow, climate impact and even energy efficiencies. Digital twins are exponentially expanding in this space, from individual systems through to comprehensive smart cities that cover key infrastructure such as airports, transportation networks and even entire Smart Building CBD’s with integrated IoT sensors. USE CASES: Real-time traffic flow monitoring 3D City planning and expansion simulation Real-time parking meterage monitoring Real-time safety monitoring 3D visual incident reporting and analytics Real-time crime prevention Common Operating Picture for operations management Building carbon footprint management MILITARY DIGITAL TWIN USE-CASES MILITARY / DEFENCE We have seen the development of AI-enabled Digital Twin technology accelerate, especially in the defence and intelligence sectors of government. This cutting-edge tech is being applied to a variety of applications, but the most notable of these would be AI-enabled real-time vehicle and asset monitoring for situational awareness in high-risk environments such as foreign warzones. This not only allows command to pre-empt imminent risks, but take action based on AI-recommendations. USE CASES: Real-time COP of high risk environments (warzones) Weapon and vehicle modelling and simulation Augmented Reality weapons and environment training Forensic Auditing of foreign environments Real-time security of high-risk munitions Artillery and munition simulation Real-time asset health and vitals monitoring MINING DIGITAL TWIN USE-CASES MINING The mining industry, in many ways, is at the forefront of digital twin development. Not only do we see sophisticated real time operations management in action, but advanced AI based decision making has grown in leaps and bounds. Some players have successfully implemented real-time operating pictures at the operational, tactical and strategic levels. USE CASES: AI-enabled Predictive Maintenance Geo-location Underground Safety Monitoring Mission Critical Asset Monitoring Systems Immersive Augmented Reality Safety Training Improve Machine Output Performance Anomaly detection OIL & GAS DIGITAL TWIN USE-CASES Oil & Gas The oil & gas industry has been working with dynamic software models for years. In a natural progression, cloud computing, advanced simulation, virtual system testing, virtual/augmented reality and machine learning has merged into fully fledged digital twins which feature packaged digital twin capabilities necessary for real-time event intelligence and operational awareness. USE CASES: Offshore Well Integrity Digital Twin EPC Digital Twin Virtual commissioning, training & startup. Simulation twin for cost-weight estimations Gas Turbine twin for predictive maintenance Optimum output simulation twin MANUFACTURING DIGITAL TWIN USE-CASES MANUFACTURING We are seeing an emerging trend amongst the manufacturing industry. There seems to be a consensus that digital twins will be the catalyst that ushers in the fourth industrial revolution, much like previous revolutions, players who fail to keep up with advancing technology will be left behind. Digital twins will transform the entire product lifestyle. This includes revolutionary product design and immersive planning simulations, to faster product development and product functionality. A key aspect of smart manufacturing that is still largely untapped, is Digital Twin technology on demand as a way for Original Equipment Manufacturers to differentiate their offering. By offering a digital twin alongside their product as a “Software As A Service” OEM’s can create new value for customers, while exploring new avenues of monetization. USE CASES: Smart Factory Layout Design Real-time Robotics Monitoring Prescriptive Maintenance Scheduling “Golden Batch” OEE monitoring Smart Product design and simulation Real-time product data feeds for R&D SUPPLY CHAIN DIGITAL TWIN USE-CASES SUPPLY CHAIN If the pandemic has taught the supply chain industry anything, it is that the value of real-time data decreases significantly as time moves on. As Freight charges skyrocketed, it was the companies with the best real-time data that could determine the best course of action to navigate through the tumultuous time that was 2020-2021. USE CASES: Leverage AI to predict performance of packaging materials Optimize warehouse design Real-time Logistics Network Visualization AI-enabled stock ordering and logistics. Geolocation based stock integrity tracking Real-time warehouse operations safety monitoring"
  },
  "docs/resources/faqs/external-content/blogs/2022/index.html": {
    "href": "docs/resources/faqs/external-content/blogs/2022/index.html",
    "title": "2022 Blogs | XMPro",
    "summary": "2022 Blogs Articles from the XMPro blog published in 2022. Articles 7 Trends for Industrial Digital Twins in 2022 copy-me Create a Common Operating Picture of Your Operations with XMPro How to Build a Digital Twin + 60 Use Cases By Industry What are composable digital twins in the metaverse?"
  },
  "docs/resources/faqs/external-content/blogs/2022/what-are-composable-digital-twins-in-the-metaverse.html": {
    "href": "docs/resources/faqs/external-content/blogs/2022/what-are-composable-digital-twins-in-the-metaverse.html",
    "title": "What are composable digital twins in the metaverse? | XMPro",
    "summary": "What are composable digital twins in the metaverse? Blog, Explainer Series What are composable digital twins in the metaverse? Posted on December 22, 2022 by Wouter Beneke Composable digital twins in the metaverse are virtual representations of real-world objects, systems, or processes that can be composed, or connected, with other digital twins to create a comprehensive, interactive model of a particular environment or situation. These digital twins can be used in a variety of applications, such as virtual reality training simulations, architectural design, or even in the planning and management of complex systems like smart cities or transportation networks. One of the key benefits of composable digital twins is their ability to be connected and integrated with other digital twins to create a more comprehensive and accurate model. This allows for more effective collaboration and decision-making, as well as the ability to simulate and test various scenarios and scenarios in a virtual environment before implementing them in the real world. In the metaverse, composable digital twins can be used to create immersive, interactive experiences that allow users to explore and interact with virtual representations of real-world objects and systems. This can be especially useful in fields such as architecture, engineering, and urban planning, where visualizing and interacting with complex systems can help improve design and decision-making processes."
  },
  "docs/resources/faqs/external-content/blogs/2023/accelerate-your-ai-workflow-the-3-key-business-advantages-of-xmpro-notebook.html": {
    "href": "docs/resources/faqs/external-content/blogs/2023/accelerate-your-ai-workflow-the-3-key-business-advantages-of-xmpro-notebook.html",
    "title": "Accelerate Your AI Workflow: The 3 Key Business Advantages of XMPro Notebook | XMPro",
    "summary": "Accelerate Your AI Workflow: The 3 Key Business Advantages of XMPro Notebook Blog Accelerate Your AI Workflow: The 3 Key Business Advantages of XMPro Notebook Posted on August 8, 2023 by Wouter Beneke By Jaun van Heerden, XMPro Strategic Solutions Engineer As the AI landscape rapidly evolves, organizations worldwide are on the lookout for scalable and cost-effective solutions to fuel innovation and experimentation. Making the right strategic decisions by integrating AI can be a pivotal factor in the future trajectory of businesses, propelling them towards success or leaving them adrift in the ever-advancing digital age. In this context, XMPro emerges as this crucial catalyst for success, with its recently unveiled AI Capability set to revolutionize the arena of Digital Twins. By leveraging the transformative potential of XMPro AI and the impressive capabilities of XMPro Notebooks, businesses can usher in a new era of AI strategies, marked by unprecedented impact and potential. The dynamic combination provides a versatile environment for scalable innovation, simulations, and real-time visualizations. With XMPro Notebooks, organizations can explore diverse configurations, predict outcomes, and identify optimal parameters to enhance efficiency, productivity, and profitability. In this blog post, we will delve into three key advantages of XMPro Notebooks and Intelligent Digital Twins that accelerate growth and promote innovation – propelling businesses towards their AI-driven success. 1.Built-in Authorization and Streamlined Access Management : XMPro Notebook is designed with built-in robust authorization that complies with corporate IT policies and minimizes potential risks. It leverages containerization, simplifying setup processes and minimising the need for extensive custom configurations or installations. This approach not only provides an environment that is secure but also seamlessly scalable, integrating effortlessly with an organization’s existing IT infrastructure. This innovative design promotes a culture of compliance and enhances overall project efficiency by reducing the administrative overhead typically associated with implementing new technologies. With XMPro Notebooks, the focus is kept firmly where it should be—on accelerating your AI workflow and driving innovation. 2. Integrated XMPro-Specific Libraries and Real-Time Data Access : AI development in XMPro Notebooks is supercharged by offering pre-integrated, specific libraries that elevate functionality while simplifying the AI development journey. These libraries encompass various data science tools, exclusive XMPro functions, and cutting-edge AI capabilities like ChatGPTmagics. The breadth and depth of these resources provide an enriched toolkit for developers to navigate the complexities of AI with greater ease and efficiency. A standout feature embedded into the platform is the ability to tap into real-time data streaming from XMPro Data Streams. This feature eliminates the traditionally cumbersome process of manual CSV file imports and paves the way for automated retraining. With real-time data at their fingertips, users can work with fresher, more relevant data sets that result in more precise modelling and accurate results. By offering advanced tools and seamless real-time data access, XMPro Notebooks accelerate innovation and drive the successful delivery of AI projects. 3. Advanced Data Management and Pre-processing : Another key strength of XMPro Notebooks lies with the capacity for advanced data management, performing crucial tasks before the data even arrives in the Notebook. This proactive approach transforms data cleansing and wrangling from a traditionally labor-intensive process into a streamlined, automated one. From data imputation to handling missing values and managing offline or broken sensors, XMPro Notebooks offer a comprehensive suite of features for maintaining the quality and integrity of data. By addressing these issues at the earliest stage, it ensures that the data entering your AI models and analysis is of the highest quality. Furthermore, this capacity for early-stage data management relieves the data science teams of time-consuming data pre-processing tasks. This allows them to focus more on deriving insights and developing innovative AI solutions. The result? More reliable models, more insightful analyses, and ultimately, a greater capacity for innovation and success in your AI initiatives. Adopting XMPro Notebooks for your AI projects can yield significant business benefits. From robust authorization and access management to the convenience of pre-loaded libraries and advanced data management, XMPro Notebooks enhance team productivity, accelerate AI development, and ensure compliance with industry standards. In essence, XMPro Notebook is a tool that empowers organizations to unlock the full potential of AI, fostering innovation and enabling the achievement of tangible business outcomes. And now, XMPro Notebook is available on our Freemium Trial Product. Sign up today to experience these unique benefits and more."
  },
  "docs/resources/faqs/external-content/blogs/2023/copy-me.html": {
    "href": "docs/resources/faqs/external-content/blogs/2023/copy-me.html",
    "title": "| XMPro",
    "summary": "Blogs: 2023 How to master Predictive Analytics using Composable Digital Twins Accelerate Your AI Workflow: The 3 Key Business Advantages of XMPro Notebook The Roadmap to Intelligent Digital Twins What is edge computing, and how can digital twins utilize this technology? THE TOP 5 USE CASES FOR COMPOSABLE DIGITAL TWINS IN RENEWABLES + HOW TO SUPERCHARGE RESULTS WITH AI The Technology Behind Predictive Maintenance (PdM) : Hardware & Software The Benefits of Using Digital Twins in Smart Manufacturing XMPro I3C Intelligent Digital Twins Strategy Framework The TOP 5 use cases for composable digital twins in mining – and how to use AI to supercharge results The TOP 5 use cases for Composable Digital Twins in the Oil & Gas industry Why Decision Intelligence with Digital Twins is “kinda like” DCS for Automation and Control XMPro becomes an NVIDIA Cloud-Validated partner From Reactive to Predictive : Introduction to Predictive Maintenance Microsoft Azure Digital Twins : Everything You Need To Know Unlocking Efficiency: The Right Time & Strategy to Launch Your Digital Twin for Enhanced Asset Management Revolutionize Your Supply Chain: How Digital Twins Can Boost Efficiency and Cut Costs"
  },
  "docs/resources/faqs/external-content/blogs/2023/from-reactive-to-predictive--introduction-to-predictive-maintenance.html": {
    "href": "docs/resources/faqs/external-content/blogs/2023/from-reactive-to-predictive--introduction-to-predictive-maintenance.html",
    "title": "From Reactive to Predictive : Introduction to Predictive Maintenance | XMPro",
    "summary": "From Reactive to Predictive : Introduction to Predictive Maintenance Blog From Reactive to Predictive : Introduction to Predictive Maintenance Posted on October 31, 2023 by Wouter Beneke Introduction to Predictive Maintenance In the ever-evolving landscape of industrial operations, the transition from reactive to predictive maintenance marks a significant paradigm shift. This shift is not just a change in maintenance practices but a strategic move towards operational excellence. Predictive maintenance (PdM) emerges as a beacon of innovation, offering a multitude of benefits to forward-thinking companies like XMPro. This comprehensive guide delves into the essence of predictive maintenance, its myriad advantages, and the evolution of maintenance strategies from reactive to predictive. What is Predictive Maintenance? At its core, predictive maintenance is a proactive maintenance strategy that stands in stark contrast to traditional, reactive approaches. It’s a sophisticated method that employs data analysis tools and advanced diagnostic techniques to detect anomalies and predict equipment failures before they occur. Unlike traditional maintenance approaches that rely on scheduled or reactive measures, predictive maintenance is based on the actual condition of the equipment, using real-time data and analytics to anticipate potential issues. This approach allows for timely interventions, preventing failures before they happen. It’s particularly beneficial in industries where equipment uptime is critical, and unexpected downtime can lead to significant financial and operational setbacks. Benefits of Predictive Maintenance Cost Savings: One of the most compelling benefits of predictive maintenance is the potential for significant cost savings. By identifying problems early, predictive maintenance helps avoid the need for expensive repairs or replacements that often result from catastrophic failures. Addressing issues before they escalate not only saves on the direct costs associated with major breakdowns but also reduces the indirect costs related to downtime, lost production, and the impact on customer satisfaction. Increased Equipment Lifespan: Regular monitoring and maintenance based on predictive data can significantly extend the life of machinery and equipment. By detecting and addressing potential issues promptly, predictive maintenance prevents the wear and tear that can lead to major damages. This proactive approach ensures that equipment operates at optimal conditions for longer periods, thereby maximizing the return on investment for expensive machinery. Reduced Downtime: Downtime in industrial operations can be incredibly costly, not just in terms of repair costs but also in lost productivity and revenue. Predictive maintenance plays a crucial role in reducing unplanned downtime. By predicting failures before they happen, maintenance activities can be scheduled during non-peak times or during planned shutdowns, ensuring that operations run smoothly without unexpected interruptions. This leads to more reliable production schedules and improved customer satisfaction. Enhanced Safety: Workplace safety is a paramount concern in any industrial setting. Predictive maintenance enhances safety by preventing equipment failures that could lead to accidents or hazardous situations. By maintaining equipment in optimal condition and addressing potential issues before they become critical, predictive maintenance reduces the risk of accidents and injuries, promoting a safer workplace for employees. Improved Efficiency: Predictive maintenance enables companies like XMPro to optimize their maintenance schedules, ensuring that resources are used efficiently. By focusing maintenance efforts on areas that need attention, rather than following a one-size-fits-all schedule, resources are allocated more effectively. This leads to improved overall operational efficiency, higher productivity, and better resource utilization. The Evolution of Maintenance Strategies Reactive Maintenance: Reactive maintenance, also known as “run-to-failure” maintenance, is the most basic form of maintenance. In this approach, actions are taken only after a failure has occurred. While it may seem cost-effective in the short term, reactive maintenance often leads to higher costs in the long run due to unexpected breakdowns, emergency repairs, and the associated downtime. It’s a strategy that is largely unpredictable and can lead to significant disruptions in operations. Preventive Maintenance: Preventive maintenance represents a more proactive approach than reactive maintenance. It involves regular maintenance activities based on a predetermined schedule, regardless of the actual condition of the equipment. While it’s more proactive than reactive maintenance, it can lead to unnecessary maintenance activities and associated costs, as it doesn’t consider the real-time condition of the equipment. Preventive maintenance is based on the assumption that all equipment degrades at a similar rate, which is not always the case. Predictive Maintenance: The most advanced form of maintenance, predictive maintenance, uses real-time data and analytics to predict when a piece of equipment is likely to fail. This approach allows for maintenance to be performed just in time, preventing unnecessary interventions and reducing downtime. Predictive maintenance is data-driven and condition-based, focusing on the actual needs of the equipment. It represents a significant leap forward in maintenance strategies, offering a more efficient, cost-effective, and reliable approach to maintaining industrial equipment. In conclusion, the shift from reactive to predictive maintenance represents a monumental advancement in maintenance strategies. For enterprises like XMPro, adopting this approach can result in substantial cost savings, extended equipment lifespan, reduced downtime, and overall enhanced operational efficiency. As technology continues to evolve, predictive maintenance is poised to become an increasingly integral component of industrial operations, propelling efficiency and productivity to unprecedented levels. Keywords: Predictive Maintenance, XMPro, Industrial Operations, Equipment Maintenance, Cost Savings, Reduced Downtime, Enhanced Safety, Operational Efficiency, Maintenance Strategies, Real-Time Data, Advanced Analytics, Proactive Maintenance."
  },
  "docs/resources/faqs/external-content/blogs/2023/how-to-master-predictive-analytics-using-composable-digital-twins.html": {
    "href": "docs/resources/faqs/external-content/blogs/2023/how-to-master-predictive-analytics-using-composable-digital-twins.html",
    "title": "How to master Predictive Analytics using Composable Digital Twins | XMPro",
    "summary": "How to master Predictive Analytics using Composable Digital Twins Blog, Guides / How To's How to master Predictive Analytics using Composable Digital Twins Posted on January 13, 2023 by Wouter Beneke Part 1 Predictive analytics is a powerful tool that can help businesses make more informed decisions and stay ahead of the competition. One of the most promising ways to master predictive analytics is through the use of composable digital twins. A digital twin is a virtual representation of a physical object or system that can be used to simulate and analyze its behavior. Composable digital twins take this concept a step further by allowing users to create and manipulate multiple digital twins in a single environment. This allows for more complex simulations and more accurate predictions. To master predictive analytics using composable digital twins, it is important to first understand the basics of how digital twins work. A digital twin is created by collecting data from sensors and other sources, and then using that data to create a model of the physical object or system. This model can then be used to simulate the behavior of the object or system under different conditions, and to make predictions about its future behavior. One of the key benefits of using composable digital twins for predictive analytics is that they allow for more complex simulations. By combining multiple digital twins in a single environment, users can create simulations that take into account the interactions between different systems and objects. This can lead to more accurate predictions and a better understanding of how the different systems and objects affect each other. Another benefit of composable digital twins is that they can be used to create a more complete picture of a system or object. By combining data from multiple sources, users can create a more detailed and accurate representation of the object or system. This can lead to more accurate predictions and a better understanding of how the object or system behaves. In Part 2 of the article, we will discuss the steps required to create and use composable digital twins for predictive analytics. This will include a discussion of the tools and technologies required, as well as best practices for creating and using digital twins. Part 2 In Part 1 of this article, we discussed the basics of how composable digital twins can be used for predictive analytics and some of their key benefits. In Part 2, we will dive deeper into the steps required to create and use composable digital twins for predictive analytics. Step 1: Collect Data The first step in creating a composable digital twin is to collect data from the physical object or system that you want to model. This data can be collected using sensors, cameras, and other devices, and can include information on the object’s or system’s physical characteristics, as well as its behavior and performance. It’s important to ensure that the data collected is accurate and of high quality. Step 2: Create a Model Once you have collected the data, the next step is to create a model of the object or system. This can be done using a variety of tools and technologies, such as computer-aided design (CAD) software or simulation software. The model should be as detailed and accurate as possible, taking into account all of the data that has been collected. Step 3: Combine Digital Twins Once you have created a model, the next step is to combine it with other digital twins to create a composable digital twin. This can be done using a variety of tools and technologies, such as simulation software or digital twin platforms. It is important to ensure that the digital twins are compatible and can be easily combined to create a single, cohesive simulation. Step 4: Simulate and Analyze Once you have created a composable digital twin, the next step is to use it to simulate and analyze the behavior of the physical object or system. This can be done using a variety of tools and technologies, such as simulation software or digital twin platforms. It is important to ensure that the simulations are as accurate as possible and take into account all of the data that has been collected. Step 5: Make Predictions Once you have simulated and analyzed the behavior of the physical object or system, the next step is to make predictions about its future behavior. This can be done using a variety of tools and technologies, such as machine learning algorithms or artificial intelligence. It is important to ensure that the predictions are as accurate as possible and take into account all of the data that has been collected. In conclusion, mastering predictive analytics using composable digital twins can be a powerful tool for businesses to make more informed decisions and stay ahead of the competition. By following these steps, it is possible to create and use composable digital twins that provide accurate and detailed simulations and predictions. It’s important to remember that the process of creating and using composable digital twins is an ongoing process and requires regular updates and maintenance. Part 3 In Part 1 and 2 of this article, we discussed the basics of how composable digital twins can be used for predictive analytics, the steps required to create and use them, and some of their key benefits. In Part 3, we will discuss some best practices for creating and using composable digital twins for predictive analytics. Best Practice 1: Keep it Simple When creating a composable digital twin, it’s important to keep the model as simple as possible while still being accurate. A complex model with too many variables can be difficult to understand and can lead to inaccurate predictions. By keeping the model simple, you can ensure that it is easy to understand and can be used to make accurate predictions. Best Practice 2: Use Real-World Data When collecting data for a composable digital twin, it’s important to use real-world data that is representative of the physical object or system being modeled. This will ensure that the digital twin is an accurate representation of the physical object or system, and that predictions made using the twin will be as accurate as possible. Best Practice 3: Continuously Monitor and Update A composable digital twin is not a one-time creation, it’s an ongoing process that requires regular monitoring and updating. As new data becomes available, or as the physical object or system changes, it’s important to update the digital twin accordingly. This will ensure that the twin remains an accurate representation of the physical object or system, and that predictions made using the twin will continue to be accurate. Best Practice 4: Collaboration and communication Creating a composable digital twin involves different teams and departments, it’s important to have a clear communication and collaboration process in place. This includes regular meetings and updates, as well as a shared platform where everyone can access and update the digital twin. This will ensure that everyone is aware of the progress and can contribute to the creation and maintenance of the digital twin. In conclusion, mastering predictive analytics using composable digital twins can be a powerful tool for businesses to make more informed decisions and stay ahead of the competition. By following best practices such as keeping the model simple, using real-world data, continuously monitoring and updating, and collaborating and communicating effectively, it’s possible to create and use composable digital twins that provide accurate and detailed simulations and predictions. Part 4 In Part 1, 2 and 3 of this article, we discussed the basics of how composable digital twins can be used for predictive analytics, the steps required to create and use them, best practices and some of their key benefits. In Part 4, we will discuss some of the potential applications and real-world use cases of composable digital twins in predictive analytics. Predictive Maintenance: By using composable digital twins, businesses can create detailed simulations of their equipment and systems, and use them to predict when maintenance is needed. This can help businesses to reduce downtime and improve the efficiency of their operations. Supply Chain Optimization: Composable digital twins can be used to create simulations of entire supply chains, including logistics and transportation networks. These simulations can be used to predict bottlenecks and inefficiencies, and to optimize the flow of goods and materials. Predictive Quality Control: Composable digital twins can be used to create simulations of manufacturing processes, and to predict the quality of the final products. This can help businesses to improve the quality of their products, and to identify and correct problems before they occur. Predictive Energy Management: Composable digital twins can be used to create simulations of energy systems, such as power grids and renewable energy sources. These simulations can be used to predict energy usage and to optimize the use of energy resources. Predictive Environmental Impact: Composable digital twins can be used to create simulations of environmental systems, such as ecosystems and weather patterns. These simulations can be used to predict the impact of human activities on the environment, and to identify potential problems before they occur. CONCLUSION In this article, we have explored how composable digital twins can be used for predictive analytics, the steps required to create and use them, best practices and some of their key benefits and real-world use cases. By creating detailed simulations of equipment and systems and using them to predict when maintenance is needed, optimize their supply chain, improve the quality of their products, manage energy resources, and predict the environmental impact of their operations, businesses can improve the efficiency of their operations, reduce downtime and stay ahead of the competition. One way to take the next step in utilizing the power of composable digital twins for predictive analytics is by using a platform such as XMPro. XMPro is an industrial IoT platform that allows businesses to create, manage and analyze digital twins, automate workflows and optimize their operations. It also provides a collaborative environment for different teams and departments to work together and share information. By using XMPro, businesses can easily create, manage and analyze composable digital twins for predictive analytics and gain a competitive advantage."
  },
  "docs/resources/faqs/external-content/blogs/2023/index.html": {
    "href": "docs/resources/faqs/external-content/blogs/2023/index.html",
    "title": "2023 Blogs | XMPro",
    "summary": "2023 Blogs Articles from the XMPro blog published in 2023. Articles Accelerate Your AI Workflow: The 3 Key Business Advantages of XMPro Notebook copy-me From Reactive to Predictive : Introduction to Predictive Maintenance How to master Predictive Analytics using Composable Digital Twins Microsoft Azure Digital Twins : Everything You Need To Know Revolutionize Your Supply Chain: How Digital Twins Can Boost Efficiency and Cut Costs The Benefits of Using Digital Twins in Smart Manufacturing The Roadmap to Intelligent Digital Twins The Technology Behind Predictive Maintenance (PdM) : Hardware & Software The TOP 5 use cases for composable digital twins in mining – and how to use AI to supercharge result THE TOP 5 USE CASES FOR COMPOSABLE DIGITAL TWINS IN RENEWABLES + HOW TO SUPERCHARGE RESULTS WITH AI The TOP 5 use cases for Composable Digital Twins in the Oil & Gas industry Unlocking Efficiency: The Right Time & Strategy to Launch Your Digital Twin for Enhanced Asset Manag What is edge computing, and how can digital twins utilize this technology? Why Decision Intelligence with Digital Twins is “kinda like” DCS for Automation and Control XMPro becomes an NVIDIA Cloud-Validated partner XMPro I3C Intelligent Digital Twins Strategy Framework"
  },
  "docs/resources/faqs/external-content/blogs/2023/microsoft-azure-digital-twins--everything-you-need-to-know.html": {
    "href": "docs/resources/faqs/external-content/blogs/2023/microsoft-azure-digital-twins--everything-you-need-to-know.html",
    "title": "Microsoft Azure Digital Twins : Everything You Need To Know | XMPro",
    "summary": "Microsoft Azure Digital Twins : Everything You Need To Know Blog, Explainer Series Microsoft Azure Digital Twins : Everything You Need To Know Posted on January 30, 2023 by Wouter Beneke Updated Feb 06 2024 Microsoft Azure Digital Twins : Everything You Need To Know What are Microsoft Azure Digital Twins? Microsoft Azure Digital Twins is an advanced IoT platform that empowers developers to replicate physical environments digitally, fostering a deeper understanding through simulation, analysis, and automation of connected ecosystems. It provides essential APIs, tools, and pre-built templates for efficient digital twin model creation and management. This platform stands out by offering in-depth insights into physical environments, enabling informed decision-making and addressing real-world challenges through precise modeling and analysis capabilities. How does Microsoft Azure Digital Twins differ from other digital twins? Azure Digital Twins is one of several platforms available for creating and managing digital twin models, but it has several features that set it apart from other options: Azure Digital Twins is built on top of the Azure IoT platform, which means it has native integration with other Azure services like Azure IoT Hub, Azure Stream Analytics, and Azure Data Explorer. This can make it easier to connect digital twin models to physical devices and systems, and to analyze and visualize data from those devices and systems. Azure Digital Twins includes a set of pre-built templates and sample models that can be used as starting points for creating digital twin models. This can help developers get started quickly and reduce the time and effort required to create a digital twin from scratch. Azure Digital Twins is designed to be highly scalable and can handle large numbers of digital twin models and devices. This makes it suitable for use in large-scale IoT projects and for use in industries such as smart buildings and smart cities. Azure Digital Twins allows for multiuser access and collaboration, this allows for different teams to work on different parts of the model and also allows for different stakeholders to access the same model with different level of permissions. One lesser-known fact about Azure Digital Twins is that it includes support for spatial intelligence. This means that it can be used to create 3D models of physical spaces, and to analyze and visualize data in a spatial context. This can be useful for a variety of applications, such as creating digital twin models of buildings and other physical structures, and for analyzing and visualizing data from sensors and other devices that are spatially distributed. The spatial intelligence feature allows for more accurate representation of the real world, and allows for more detailed analysis of the data collected from IoT devices. What are the top use cases for Microsoft Azure Digital Twins? Azure Digital Twins is well suited for a variety of use cases, but some of the most common use cases include 1.Smart Buildings: Azure Digital Twins can be used to create digital twin models of buildings and other physical structures, and to analyze and visualize data from sensors and other devices in those buildings. This can be used to optimize building performance, improve energy efficiency, and enhance the occupant experience. 2.Smart Cities: Azure Digital Twins can be used to create digital twin models of entire cities and to analyze and visualize data from a wide range of sensors and devices that are distributed throughout the city. This can be used to optimize traffic flow, improve public safety, and enhance the overall livability of the city. 3. Industrial IoT: Azure Digital Twins can be used to create digital twin models of industrial environments, and to analyze and visualize data from sensors and other devices that are distributed throughout those environments. This can be used to optimize production, improve equipment performance, and enhance the overall efficiency of the facility. 4.Supply Chain Optimization: Azure Digital Twins can be used to create digital twin models of supply chain networks, and to analyze and visualize data from sensors and other devices that are distributed throughout the supply chain. This can be used to optimize logistics, improve inventory management, and enhance the overall efficiency of the supply chain. 5.Remote Monitoring and Control: Azure Digital Twins can be used to create digital twin models of remote environments, such as offshore platforms and remote mining sites, and to analyze and visualize data from sensors and other devices that are distributed throughout those environments. This can be used to monitor and control the environment, improve safety and reduce downtime. These are just a few examples of the many use cases for Azure Digital Twins. It’s a versatile platform that can be used in many different industries, and for many different applications. How can companies use XMPro to supercharge Microsoft Azure Digital Twins? XMPro iDTS (Intelligent Digital Twin Suite) is a sophisticated software platform designed for the creation and management of digital twins, focusing on asset and process optimization across various industries. It offers real-time insights and predictive analytics for proactive decision-making, enhancing operational efficiency by identifying inefficiencies and optimizing workflows. XMPro iDTS supports informed strategic decision-making through comprehensive data analytics, thereby improving risk management and operational planning. The platform integrates seamlessly with existing systems, ensuring scalability and flexibility for businesses of all sizes, and is tailored to enhance asset lifecycle management by facilitating predictive maintenance to extend asset life and minimize downtime. Through its focus on asset and process digital twins, XMPro iDTS enables organizations to achieve significant cost savings and maintain a competitive edge in the market. XMPro iDTS not only offers a comprehensive suite for managing digital twins but also includes several key modules designed to enhance its capabilities and provide users with a robust set of tools for optimizing their operations. These modules include the Datastream Designer, XMPro AI, App Designer, and Recommendation Manager, each contributing to the platform’s flexibility and power in unique ways. Datastream Designer: This module allows users to easily configure and manage the flow of data from various sources into the digital twin environment. It supports the integration of real-time data, enabling organizations to create accurate and dynamic representations of their assets and processes. The Datastream Designer is essential for ensuring that digital twins are always up-to-date and reflective of current conditions. XMPro AI: At the heart of predictive analytics and decision support within the platform, XMPro AI leverages machine learning algorithms to analyze data and predict outcomes. This module enhances the platform’s ability to forecast future conditions, identify potential issues before they arise, and suggest optimal courses of action, making it a crucial tool for proactive management and maintenance strategies. App Designer: The App Designer module provides users with the tools to create custom applications within the XMPro iDTS environment. These applications can be tailored to meet specific business needs, enabling personalized dashboards, reports, and interactive tools that support decision-making and operational oversight. This flexibility ensures that organizations can maximize the value of their digital twin data. Recommendation Manager: This module complements the predictive capabilities of XMPro AI by not only identifying potential issues but also suggesting actionable recommendations. The Recommendation Manager uses data-driven insights to advise on maintenance schedules, operational adjustments, and other strategies to improve performance and reduce risk. It serves as a bridge between data analysis and practical application, facilitating informed decision-making across the organization. Together, these modules enhance the XMPro iDTS platform’s ability to provide comprehensive asset and process optimization. By integrating real-time data analytics, AI-driven predictions, customizable application development, and actionable recommendations, XMPro iDTS delivers a powerful solution for organizations looking to leverage digital twin technology for improved efficiency, reduced costs, and strategic advantage in their operations. Benefits of deploying XMPro on Azure 1.Scalability For Real-Time Data Orchestration Ingesting large volumes of data at high velocity requires scalable cloud computing resources. By deploying your XMPRO Apps and Data Streams on Microsoft Azure, you can leverage their flexible, secure and robust cloud computing platform to provide your team with real-time decision support. A powerful cloud platform becomes especially important when you rely on data coming in every second from thousands of assets across multiple plants. As an example, one of our customers ingest 77 million events per day through their XMPRO Data Streams. 2.Leverage Pre-Built Connectors To Azure Services Azure provides a suite of products you can leverage in your XMPRO Data Streams and Applications, from storing telemetry data in Azure Data Explorer (ADX) to creating digital twins in Azure Digital Twins. And our no-code approach makes these services even more accessible for users who don’t have an IT background. As an example, the screen above shows how you can create new instances in Azure Digital Twins, upload DTDL models and set up new resources without leaving the XMPRO interface. 3.Integrate Machine Learning Into Your Apps XMPRO makes predictive analytics from services like Azure Machine Learning consumable for end users in their day-to-day work, enabling them to make data-driven decisions. But running these models to analyze data in real-time requires powerful and reliable cloud computing resources, which makes Azure an ideal deployment platform for AI-driven applications. 4.Manage Assets and Hierarchies in Real-Time Integrate Azure Data Explorer (ADX) time series data and analytics with Azure Digital Twins with XMPRO’s No Code Application Composition toolset for a complete web-based historian and industrial analytics solution. Create and manage an asset hierarchy with an XMPRO Asset Blueprint based Azure Digital Twins services. Configure event triggers to create automated recommendations with prescriptive actions as well as event frames for future analysis of real-time events. 5.Simplify The Procurement & Billing Process By purchasing your XMPRO license through the Azure Marketplace, you’ll be able to manage your XMPRO license and billing as part of your Azure account. You’ll get a single, consolidated monthly bill from Microsoft that includes both your XMPRO and Azure charges. How The XMPro Agent for Azure DT Abstracts Complexity and Provides Additional Features For companies with business-critical assets, creating apps to solve problems such as condition monitoring, real-time visualization, optimization, and simulation is crucial. However, when using Azure DT as the service for your digital twins and asset hierarchy, companies may run into a problem. Azure DT is a developer-focused tool, which can be complex and challenging for companies to use effectively. That’s where XMPro Agent for Azure DT comes in. The XMPro Agent abstracts the complexity of Azure DT and provides additional features that make it easier for companies to use the platform effectively. Some of the key features of the XMPro Agent include the following: Simplified Resource Provisioning With the XMPro Agent, companies don’t need Azure certification or knowledge of the Azure Portal to create Azure DT services. This is important because Azure DT is a developer-focused tool, and without a strong understanding of the platform, companies may struggle to create and manage their digital twins and assets. The XMPro Agent handles all the resource provisioning for you. This saves companies time and effort, and reduces the need for specialized skills and knowledge. Automated Instance Discovery Another benefit of the XMPro Agent is that it automates the process of instance discovery. This is important because it saves companies time and effort, and reduces the risk of errors or omissions when creating digital twins. With the XMPro Agent, companies can simply set up the integration and let the Agent discover and create the digital twins automatically. This makes it easier for companies to get up and running with Azure DT and focus on solving their business problems. Centralized Model Management The XMPro Agent also provides companies with centralized model management, which makes it easier to manage and update their digital twin models over time. With the XMPro Agent, companies can register new models and twin definitions in a single, unified location. This gives companies a complete and centralized view of their digital twins, and makes it easier to manage and update their models as needed. Holistic Relationship Management The XMPro Agent also provides companies with the ability to manage relationships in the asset hierarchy. This is important because it helps companies understand the relationships between their assets and devices, and makes it easier to monitor and manage their systems. With the XMPro Agent, companies can see a holistic view of their connected devices and systems, and make informed decisions based on this data. Real-time Data Integration The XMPro Agent also integrates with data sources and ensures that the digital twin is updated in near real-time. This is important because it provides companies with accurate and up-to-date information about their assets and systems, and helps ensure that their digital twins accurately reflect the state of their devices and systems. With the XMPro Agent, companies can have confidence that their digital twins are always up-to-date and accurately reflect the state of their assets and systems. Time-series Logging Azure DT doesn’t handle time-series data, but the XMPro Agent can log telemetry into a cloud historian like ADX. This is important because it provides companies with a complete picture of their devices and systems over time, and helps them make informed decisions based on historical data. With the XMPro Agent, companies can see how their systems have been performing over time, and use this data to make informed decisions about their assets and devices. In conclusion, the XMPro Agent for Azure DT provides companies with a simplified and more efficient way to use the Azure DT platform. With its additional features and benefits, companies can focus on solving their business problems and leave the technicalities to the XMPro Agent. MORE ABOUT XMPRO & AZURE DIGITAL TWINS In addition to the benefits listed above, companies can use XMPro to supercharge Microsoft Azure Digital Twins in several ways: Automating processes: XMPro allows companies to create custom workflows and decision trees to automate processes and make decisions based on data from the digital twin model. This can be used to automate the control of the physical environment, trigger alarms or notifications, and perform other actions in response to changes in the digital twin model. Enhancing decision-making: XMPro provides a visual interface to create, manage and monitor the workflows, this can help non-technical users to participate in the management of the digital twin and make better decisions based on the data from the digital twin. Creating a holistic view: XMPro allows for the integration of various data sources, such as IoT devices, ERP systems, CRM systems, and SCADA systems, to create a holistic view of the environment. This can help in making more accurate decisions and getting a better understanding of the environment. Streamlining operations: XMPro can automate repetitive tasks, such as data collection and analysis, which can save time and resources for companies and streamline their operations. Improving scalability: With the out of the box integration with Azure Digital Twins, XMPro can leverage the scalability and integration capabilities of Azure, which can help companies to handle a large amount of data and devices and easily scale their digital twin solution as their needs grow. By integrating XMPro with Azure Digital Twins, companies can take advantage of the powerful capabilities of both platforms to automate processes, enhance decision-making, create a holistic view, streamline operations, and improve scalability."
  },
  "docs/resources/faqs/external-content/blogs/2023/revolutionize-your-supply-chain-how-digital-twins-can-boost-efficiency-and-cut-costs.html": {
    "href": "docs/resources/faqs/external-content/blogs/2023/revolutionize-your-supply-chain-how-digital-twins-can-boost-efficiency-and-cut-costs.html",
    "title": "Revolutionize Your Supply Chain: How Digital Twins Can Boost Efficiency and Cut Costs | XMPro",
    "summary": "Revolutionize Your Supply Chain: How Digital Twins Can Boost Efficiency and Cut Costs Blog, Guides / How To's Revolutionize Your Supply Chain: How Digital Twins Can Boost Efficiency and Cut Costs Posted on January 23, 2023 by Wouter Beneke Introduction Digital twins are quickly becoming the buzzword in supply chain management, but why all the hype? Simply put, a digital twin is a virtual representation of a physical asset, process, or system. It is created using data and information from the physical world, and it can be used to simulate and analyze the performance of the physical asset, process, or system. Imagine being able to detect and address potential issues with your equipment before they occur, or optimize the routing of your goods for maximum efficiency and cost savings. This is where digital twins come in. By creating a virtual replica of your physical assets, processes, and systems, you can gain a deeper understanding of how they function and perform, allowing you to make informed decisions that can improve your supply chain. In this blog post, we’ll dive deeper into the various ways that digital twins can improve supply chain efficiency and reduce costs. We will cover transportation route optimization, supply chain network design, and production planning. Transportation Routing Optimization Transportation Routing Optimization Transportation route optimization is a subcomponent of supply chain network design, which specifically focuses on determining the most efficient routes for moving goods from one location to another. This includes decisions about how to transport goods (e.g., by truck, train, ship, or airplane), which carrier or carrier to use, and how to consolidate shipments to reduce costs. By creating a virtual model of a transportation network, digital twin technology allows companies to simulate and analyze different transportation routes. This can be done by analyzing data such as traffic patterns, weather conditions, and vehicle capacity. As an example, a large international logistics company has been using digital twin technology to analyze traffic patterns and weather conditions to predict which transportation routes would be the most effective. Another example is a manufacturer of freight and passenger locomotives, using digital twin technology to optimize the scheduling of its trains, resulting in reducing fuel consumption and maintenance costs. By reducing transportation costs, companies can increase profitability and provide more competitive pricing to their customers. Additionally, improved delivery times can lead to increased customer satisfaction and improved relationships with suppliers. Optimized transportation routing can also lead to improved utilization of resources, such as vehicles and warehouse space. In our next section we will discuss how digital twins can impact supply chain networking design to further improve efficiency. Supply Chain Network Design Supply Chain Networking Design Supply chain network design is the process of determining the most efficient and cost-effective way to organize and operate a company’s supply chain. This includes decisions about the location and capacity of production facilities, distribution centers, and warehouses, as well as the selection of suppliers and transportation routes. The goal of supply chain network design is to optimize the flow of materials and information throughout the supply chain in order to meet customer demand while minimizing costs and risks. Digital twin technology can be used to create a virtual model of a supply chain network, allowing companies to simulate and analyze different network designs. By analyzing data such as demand patterns, production capacity, and inventory levels, digital twin technology can help companies identify the most efficient and cost-effective network design. Using digital twin technology, companies can optimize their warehouse locations, transportation routes, and inventory management to reduce costs and improve efficiency. Furthermore, companies can leverage AI in their digital twins to supercharge their chain network design in several ways: Predictive modeling: AI can be used to create predictive models that forecast customer demand, inventory levels, and production schedules. These models can be integrated into the digital twin to help companies anticipate and plan for changes in the supply chain. Optimization algorithms: AI-powered optimization algorithms can be used to identify the most efficient and cost-effective solutions for organizing and operating a company’s supply chain. These algorithms can be integrated into the digital twin to help companies make better decisions about the location and capacity of production facilities, distribution centers, and warehouses, as well as the selection of suppliers and transportation routes. Anomaly detection: AI can be used to monitor the performance of the supply chain in real-time and detect anomalies or issues that might impact the overall performance of the supply chain. Machine learning: Machine learning algorithms can be used to learn from historical data and identify patterns that can be used to improve the supply chain. Natural language processing: AI-powered natural language processing (NLP) can be used to process unstructured data such as customer feedback, social media posts, and news articles to gain insights about customer preferences and trends. As we have seen, digital twin technology can be used to optimize transportation routing and supply chain network design, the next step is to see how it can be used to optimize production planning in order to further improve efficiency. Production Planning Production Planning Optimizing production planning is crucial for companies to reduce costs and improve efficiency. Digital twin technology can be used to create a virtual model of the production process, allowing companies to simulate and analyze different production scenarios. By analyzing data such as demand patterns, production capacity, and inventory levels, digital twin technology can help companies identify the most efficient and cost-effective production plan. Just some applications include production scheduling, inventory management, and equipment utilization to reduce costs and improve efficiency. In addition, companies can also leverage AI in digital twins to achieve the following outcomes in production planning: Predictive modeling: AI can be used to create predictive models that forecast customer demand, inventory levels, and production schedules. These models can be integrated into the digital twin to help companies anticipate and plan for changes in the production schedule. Optimization algorithms: AI-powered optimization algorithms can be used to identify the most efficient and cost-effective solutions for production planning. These algorithms can be integrated into the digital twin to help companies make better decisions about the production schedule, resources allocation, and inventory management. Machine learning: Machine learning algorithms can be used to learn from historical data and identify patterns in production schedules, such as seasonality, trends, or fluctuations in customer demand. These insights can be used to optimize the production schedule and improve the accuracy of demand forecasting. Anomaly detection: AI can be used to monitor the performance of the production process in real-time and detect anomalies or issues that might impact the overall performance of the production. Predictive maintenance: AI can be used to predict when machines or equipment are likely to fail, allowing companies to schedule maintenance before breakdowns occur, thus reducing downtime and increasing efficiency. Conclusion In this blog post, we have explored how digital twin technology can revolutionize supply chain management by optimizing transportation routing, supply chain network design, and production planning. Through the use of digital twin technology, companies can reduce costs and improve efficiency, leading to increased profitability and improved customer satisfaction. As the use of digital twin technology in supply chain management is still in its early stages, the future of this technology is very promising. With advancements in technology, we can expect to see digital twin technology becoming more sophisticated and widely adopted by companies in the industry. However, the implementation of digital twin technology can be a daunting task for many companies. It’s important to have the right tools and software to effectively implement digital twin technology in your supply chain operations. One option that can help companies in this task is XMPro. XMPro is a powerful No-Code Digital Twin Composition Platform that allows companies to easily compose their own Digital Twins."
  },
  "docs/resources/faqs/external-content/blogs/2023/the-benefits-of-using-digital-twins-in-smart-manufacturing.html": {
    "href": "docs/resources/faqs/external-content/blogs/2023/the-benefits-of-using-digital-twins-in-smart-manufacturing.html",
    "title": "The Benefits of Using Digital Twins in Smart Manufacturing | XMPro",
    "summary": "The Benefits of Using Digital Twins in Smart Manufacturing Blog, CEO'S Blog The Benefits of Using Digital Twins in Smart Manufacturing Posted on January 20, 2023 by Wouter Beneke Introduction In today’s fast-paced and ever-evolving manufacturing industry, companies are constantly looking for ways to improve efficiency, productivity, and quality. One innovative solution that has been gaining traction in recent years is the use of a digital twin. But what exactly is a digital twin, and how can it benefit smart manufacturing? A digital twin is a virtual replica of a physical product, process, or system. It can be used to simulate and analyze various aspects of the manufacturing process, including design, testing, and performance. By creating a digital twin, companies can gain valuable insights into how their products and processes will perform in the real world, without the need for costly and time-consuming physical experimentation. The use of a digital twin in smart manufacturing can bring numerous benefits, such as improved design and testing, increased efficiency and productivity, improved communication and collaboration, and many more. In this blog post, we will explore these benefits in more detail, and how they can help companies stay ahead of the competition in today’s manufacturing landscape. BENEFIT 1 IMPROVED DESIGN AND TESTING Benefit 1 : Improved design and testing One of the most significant benefits of using a digital twin in smart manufacturing is the ability to improve the design and testing of products and processes. With a digital twin, companies can simulate and analyze various aspects of the manufacturing process before it is physically implemented. This allows for virtual experimentation and testing, which can save time and money in the long run. For example, a company that manufactures cars can create a digital twin of their assembly line. By simulating the assembly process, the company can identify potential bottlenecks and inefficiencies before they occur in the real world. This can help them optimize the assembly process, reducing downtime and increasing productivity. Additionally, a digital twin can be used to simulate and optimize the manufacturing process. Companies can use a digital twin to analyze data from various sensors, such as temperature and pressure, to identify areas where improvements can be made. This can help companies increase efficiency, reduce waste, and improve the overall quality of their products. In short, the use of a digital twin in smart manufacturing can help companies improve the design and testing of their products and processes, resulting in a more efficient and cost-effective manufacturing process. BENEFIT 2 INCREASED EFFECIENCY AND PRODUCTIVITY Benefit 2 : Increased Efficiency and Productivity One of the primary benefits of using a digital twin in smart manufacturing is the ability to increase efficiency and productivity. With a digital twin, companies can monitor and analyze performance in real-time, identify and solve problems quickly, and reduce downtime and maintenance costs. For example, a digital twin can be used to monitor and analyze data from various sensors, such as temperature and pressure, to identify potential issues with equipment before they become critical. This can help companies plan for maintenance and repairs, reducing downtime and increasing productivity. Furthermore, a digital twin can be used to identify and solve problems quickly. By analyzing data in real-time, companies can identify issues and take corrective action before they become critical. This can help companies reduce downtime, increase efficiency, and improve the overall quality of their products. Additionally, the use of a digital twin in smart manufacturing can also result in cost savings. By reducing downtime and maintenance costs, companies can save money and increase competitiveness in the manufacturing industry. In summary, the use of a digital twin in smart manufacturing can help companies increase efficiency, productivity and reduce costs, which can lead to a more competitive and profitable manufacturing process. BENEFIT 3 IMPROVED COMMUNICATION & COLLABORATION Benefit 3 : Improved Communication and Collaboration Communication and collaboration is essential for any manufacturing environment. With a digital twin, companies can share and access data across different departments and teams, making it easier to make informed decisions based on accurate and up-to-date information. For example, a digital twin can be used to share information between the design and manufacturing departments. By sharing data, the design department can ensure that the product is designed to meet the requirements of the manufacturing process, reducing the risk of errors and delays. Additionally, a digital twin can be used to improve collaboration among different teams. By providing real-time data and analysis, teams can work together more effectively to identify and solve problems, improve efficiency, and increase productivity. Furthermore, the use of a digital twin in smart manufacturing can also improve customer service. Companies can use a digital twin to provide real-time updates on production progress and delivery schedules, respond quickly to customer requests and concerns, and improve overall customer satisfaction. In conclusion, the use of a digital twin in smart manufacturing can help companies improve communication and collaboration among different departments and teams, resulting in a more efficient and responsive manufacturing process. BENEFIT 4 PREDICTIVE MAINTENANCE Benefit 4 : Predictive Maintenance Another key benefit of using a digital twin in smart manufacturing is the ability to perform predictive maintenance. Predictive maintenance is a technique that uses data and analysis to predict when equipment will need maintenance and plan accordingly. This can help companies reduce downtime and increase productivity. For example, a digital twin can be used to monitor the performance of equipment in real-time. By analyzing data from various sensors, such as temperature and vibration, a digital twin can identify potential issues with equipment before they become critical. This can help companies plan for maintenance and repairs, reducing downtime and increasing productivity. Additionally, the use of a digital twin in smart manufacturing can also help companies identify potential safety hazards. By simulating and analyzing the manufacturing process, companies can identify potential hazards and take corrective action before they occur in the real world. This can help companies improve safety and reduce the risk of accidents. Overall, the use of a digital twin in smart manufacturing can help companies perform predictive maintenance, reduce downtime, improve safety, and increase productivity. In summary, the use of a digital twin in smart manufacturing can help companies perform predictive maintenance, which can lead to a more efficient and cost-effective manufacturing process. BENEFIT 5 IMPROVED QUALITY CONTROL Benefit 5 : Improved Quality Control Another significant benefit of using a digital twin in smart manufacturing is the ability to improve quality control. With a digital twin, companies can monitor and analyze production data in real-time, identify and address quality issues quickly, and track and trace products throughout the production process. For example, a digital twin can be used to monitor and analyze data from various sensors, such as temperature and pressure, to identify potential issues with equipment before they become critical. This can help companies address quality issues quickly and reduce the risk of defects or nonconformities. Additionally, a digital twin can be used to track and trace products throughout the production process. By monitoring and analyzing data in real-time, companies can ensure that products are manufactured to the correct specifications, reducing the risk of errors and defects. Furthermore, the use of a digital twin in smart manufacturing can also lead to cost savings. By reducing the risk of defects and nonconformities, companies can save money and increase competitiveness in the manufacturing industry. In summary, the use of a digital twin in smart manufacturing can help companies improve quality control, reduce defects and nonconformities, and increase competitiveness in the manufacturing industry. BENEFIT 6 ENHANCED FLEXIBITY AND SCALABILITY Benefit 6 : Enhanced Flexibility and Scalability Another benefit of using a digital twin in smart manufacturing is the ability to enhance flexibility and scalability. With a digital twin, companies can adapt to changing market conditions and customer demands, and scale production up or down as needed. For example, a digital twin can be used to simulate and analyze the manufacturing process, allowing companies to identify areas where improvements can be made. By making these improvements, companies can increase efficiency and reduce waste, which can help them adapt to changing market conditions and customer demands. Additionally, a digital twin can be used to scale production up or down as needed. By monitoring and analyzing data in real-time, companies can identify areas where production needs to be increased or decreased. This can help companies respond quickly to changes in demand, which can lead to increased efficiency and reduced costs. Furthermore, the use of a digital twin in smart manufacturing can also lead to cost savings. By increasing efficiency and reducing waste, companies can save money and increase competitiveness in the manufacturing industry. In summary, the use of a digital twin in smart manufacturing can help companies enhance flexibility and scalability, increase efficiency, reduce costs, and increase competitiveness in the manufacturing industry. BENEFIT 7 IMPROVED SAFETY Benefit 7: Improved Safety Another important benefit of using a digital twin in smart manufacturing is the ability to improve safety. With a digital twin, companies can simulate and identify potential safety hazards, and monitor and enforce safety protocols in real-time. For example, a digital twin can be used to simulate the manufacturing process, allowing companies to identify potential hazards, such as heavy machinery or hazardous materials. By identifying these hazards, companies can take corrective action before they occur in the real world, improving safety and reducing the risk of accidents. Additionally, a digital twin can be used to monitor and enforce safety protocols in real-time. By analyzing data from various sensors, such as temperature and pressure, companies can ensure that safety protocols are being followed, which can help them improve safety and reduce the risk of accidents. Furthermore, the use of a digital twin in smart manufacturing can also lead to cost savings. By improving safety, companies can reduce the risk of accidents, which can lead to lower insurance costs and reduced downtime. In conclusion, the use of a digital twin in smart manufacturing can help companies improve safety, reduce the risk of accidents, and lead to cost savings. BENEFIT 8 IMPROVED DATA MANAGEMENT Benefit 8: Improved Data Management Another benefit of using a digital twin in smart manufacturing is the ability to improve data management. With a digital twin, companies can collect, store, and analyze data from various sources, making it easier to make informed decisions based on accurate and up-to-date information. For example, a digital twin can be used to collect data from various sensors, such as temperature and pressure, and store it in a central database. This can help companies analyze data in real-time, identify issues and take corrective action, and improve overall efficiency and productivity. Additionally, a digital twin can be used to analyze data from various sources, such as customer feedback and market research. By analyzing this data, companies can make informed decisions about product development, marketing, and other areas of the business. Furthermore, the use of a digital twin in smart manufacturing can also improve data security. By collecting and storing data in a central database, companies can protect it from unauthorized access and ensure compliance with data privacy regulations. In summary, the use of a digital twin in smart manufacturing can help companies improve data management, make informed decisions, and improve data security. Conclusion In conclusion, the use of a digital twin in smart manufacturing can bring numerous benefits to companies, including improved design and testing, increased efficiency and productivity, improved communication and collaboration, predictive maintenance, improved quality control, enhanced flexibility and scalability, improved safety and improved data management. By using a digital twin, companies can gain valuable insights into how their products and processes will perform in the real world, without the need for costly and time-consuming physical experimentation. This can lead to cost savings, increased competitiveness, and improved overall efficiency in the manufacturing industry. Companies that are looking to stay ahead of the competition should consider implementing a digital twin in their manufacturing process. In addition to the benefits outlined above, smart manufacturers should consider using XMPro’s No Code Digital Twin Composition Platform as a way to easily and quickly create and manage digital twins. The platform allows for the creation of digital twins without the need for coding or programming knowledge, making it accessible to a wide range of users. It also allows for real-time monitoring and analysis of data, as well as the ability to connect with various sensors, systems and third-party applications. Using XMPro’s platform, manufacturers can streamline their digital twin creation process, and easily manage and update their digital twins as needed. This can lead to faster implementation of digital twin technology and improved efficiency in the manufacturing process."
  },
  "docs/resources/faqs/external-content/blogs/2023/the-roadmap-to-intelligent-digital-twins.html": {
    "href": "docs/resources/faqs/external-content/blogs/2023/the-roadmap-to-intelligent-digital-twins.html",
    "title": "The Roadmap to Intelligent Digital Twins | XMPro",
    "summary": "The Roadmap to Intelligent Digital Twins Blog, CEO'S Blog The Roadmap to Intelligent Digital Twins Posted on April 12, 2023 by Pieter van Schalkwyk The Roadmap to Intelligent Digital Twins Starting on the journey towards Intelligent Digital Twins (IDTs) has become a reality for industry leaders and innovators worldwide, as the democratization of AI has made advanced technologies such as ChatGPT readily accessible to engineers and subject matter experts. This shift empowers professionals across various domains to harness the power of AI, accelerating the adoption and implementation of IDTs in diverse industries. In this blog, I will explore what Intelligent Digital Twins are and how they differ from traditional Digital Twins. The most significant benefit of an Intelligent Digital Twin (IDT) is its ability to actively and continuously augment human decision-making processes. By being active, online, goal-seeking, and anticipatory, IDTs can provide real-time insights and predictions to improve operational efficiency, optimize processes, and minimize the use of physical resources. This ultimately leads to better, data-driven decision-making, reduced costs, and enhanced overall performance throughout the entire product lifecycle. Dr Michael Grieves (regarded as the father of Digital Twins) provides the roadmap toward Intelligent Digital Twins in his 2022 article [i] “Intelligent digital twins and the development and management of complex systems”. Figure 1 – Evolution of Digital Twin: Dr M. Grieves 2022 Digital Twin Consortium Member Meeting, Orlando, Florida The Intelligent Digital Twin (IDT) is an advanced version of the traditional digital twin, which has been largely passive in nature. IDTs are characterized by their active, online, goal-seeking, and anticipatory nature. They are designed to assist and augment human intelligence, rather than replace it. Figure 2 – Traditional Digital Twins versus Intelligent Digital Twins Here is my summary of Dr. Grieves’ description of Intelligent Digital Twins: Active vs Passive: As active entities, IDTs are constantly scanning their physical counterparts and their environments, evolving from passive repositories to proactive, always-online agents. Traditional digital twins are passive repositories of product information, where users search for the information they need. In contrast, Intelligent Digital Twins (IDTs) are active, assisting and providing information as needed to augment human intelligence, not replace it. Offline vs Online: Traditional digital twins communicate with their physical counterparts and environments in an offline, passive manner, waiting for the physical system to initiate an action. IDTs, however, are online, actively scanning their physical twins and environments. The digital twin evolves from a passive repository to a proactive, constantly online agent. Goal-given vs Goal-seeking: The goal-seeking aspect of IDTs is shared between them and their human users, with goals defined throughout the different phases of a product’s lifecycle. IDTs aim to assist humans in reaching these goals, while focusing on minimizing resource usage. Additionally, IDTs are anticipatory in nature, running continuous simulations to model and predict future events, helping to prevent human biases from tainting decision-making processes. In the traditional digital twin model, humans provide all the goal-seeking. With IDTs, goal-seeking is shared between the digital twin and its human users, with goals defined throughout the product lifecycle. The intent is to assist and augment humans in reaching their goals, not to change the goals themselves. Predictive vs Anticipatory: Traditional digital twins do not anticipate future events or adjust actions to meet future goals. IDTs, however, are anticipatory, constantly running simulations of the product’s performance and predicting future adverse events. This allows IDTs to provide valuable assistance to humans by preventing biases from tainting decision-making processes and running complex calculations to model and simulate physical events. Front Running Simulation (FRS) is an example of the anticipatory capabilities of IDTs, which uses the digital twin of complex products to run constant simulations. FRS relies on the digital twin’s ability to manipulate time (by simulating faster than real-time) and predict adverse events, allowing humans to assess risk. By processing all available data and information in an accurate and unbiased manner, IDTs can help humans explore a wide variety of scenarios, calculate probabilities, and provide estimates of outcomes based on current conditions and actions. An Example of Front Running Simulations in Formula 1 Formula 1 (F1) teams utilize Front Running Simulations (FRS) with digital twins of cars and tracks combined with real-time telemetry data to optimize race planning and gain a competitive edge. This advanced technology helps teams make strategic decisions during races, including tire choices, pit stops, and energy management. McLaren, for example, uses a digital twin of their F1 car to gain real-time data insights and make faster decisions during races. The digital twin can process over 100 GB of data per race. (Digital Twin: Advancing Real-time Data Insights in F1 Racing and Beyond (brighttalk.com)) The process begins by creating highly detailed digital twins of both the cars and tracks. These digital twins are accurate representations of the physical components, taking into account factors such as aerodynamics, suspension setup, powertrain, and tire behavior. Additionally, digital twins of tracks capture the specific characteristics of each circuit, such as elevation changes, corner types, and track surfaces. During the race, real-time telemetry data is collected from the car’s sensors, including information on tire wear, fuel consumption, brake temperatures, and more. This data is combined with the digital twin models, as well as other factors like weather conditions, to create accurate simulations of the race. FRS then use these digital twins and telemetry data to create “faster than real-time” scenarios. These simulations run continuously, anticipating the car’s performance and the evolving race conditions. This predictive approach allows teams to evaluate various strategies and assess their potential impact on the race outcome. These simulations can run multiple scenarios, such as different pit stop strategies, tire choices, or energy management tactics, enabling teams to make informed decisions on the fly. By using FRS with digital twins and real-time telemetry data, F1 teams can optimize their race planning and react quickly to changing conditions. This advanced technology ultimately helps teams improve their chances of success on the track and adapt to unforeseen circumstances during the race. Just imagine what FRS can mean for your business in the same way it changed Formula 1 racing. Embracing this vision of Intelligent Digital Twins, XMPro has developed a comprehensive framework that transforms this concept into a tangible and actionable implementation plan. In our next blog post we will introduce our I3C Digital Twin Strategy Framework, designed to guide organizations through the process of adopting and integrating Intelligent Digital Twins seamlessly and effectively. (Link to blog post 2) [1] https://digitaltwin1.org/articles/2-8 Continue Reading Part 2: XMPro I³C Intelligent Digital Twins Strategy Framework"
  },
  "docs/resources/faqs/external-content/blogs/2023/the-technology-behind-predictive-maintenance-pdm--hardware--software.html": {
    "href": "docs/resources/faqs/external-content/blogs/2023/the-technology-behind-predictive-maintenance-pdm--hardware--software.html",
    "title": "The Technology Behind Predictive Maintenance (PdM) : Hardware & Software | XMPro",
    "summary": "The Technology Behind Predictive Maintenance (PdM) : Hardware & Software Articles, Blog The Technology Behind Predictive Maintenance (PdM) : Hardware & Software Posted on November 1, 2023 by Wouter Beneke The technology behind predictive maintenance Welcome to our exploration of the fascinating world of predictive maintenance. In this blog article, we’re diving into the technological backbone that makes predictive maintenance not just a possibility, but a game-changer in various industries. We’ll unravel the intricate details of the hardware and software components that are essential for implementing predictive maintenance effectively. Whether you’re a professional in the field, a curious learner, or someone interested in the intersection of technology and industry, this video will provide valuable insights into how predictive maintenance works and why it’s becoming increasingly important in our tech-driven world. So, let’s get started and uncover the technology behind predictive maintenance! Predictive maintenance is a revolutionary approach that is transforming how industries manage equipment and machinery maintenance. Unlike traditional maintenance methods that rely on scheduled or reactive measures, predictive maintenance utilizes real-time data and advanced analytics to predict when maintenance should be performed. This proactive approach is based on the actual condition of the equipment, rather than predetermined schedules or unexpected breakdowns. The primary goal of predictive maintenance is to anticipate potential failures before they occur, thereby reducing downtime, extending equipment life, and optimizing maintenance resources. This is achieved by continuously monitoring the condition and performance of equipment through various sensors and data collection methods. The collected data is then analyzed to identify patterns and anomalies that could indicate potential issues or failures. Industries such as manufacturing, aviation, energy, and transportation are increasingly adopting predictive maintenance. In these sectors, equipment downtime can lead to significant financial losses and safety risks. By implementing predictive maintenance, companies can not only save costs but also enhance operational efficiency and safety. The effectiveness of predictive maintenance hinges on the seamless integration of both hardware and software components. The hardware is responsible for collecting and transmitting critical data, while the software plays a crucial role in analyzing this data and providing actionable insights. Together, they form the foundation of a predictive maintenance system that can significantly improve maintenance strategies and outcomes. Hardware Components in Predictive Maintenance Sensors Sensors Sensors are the cornerstone of any predictive maintenance system. They act as the eyes and ears of machinery, continuously monitoring various parameters that indicate the equipment’s health. Here are the key aspects of sensors in predictive maintenance: Types of Sensors : There are several types of sensors used in predictive maintenance, each designed to monitor specific aspects of machinery. Common types include the following: Vibration Sensors: These sensors detect abnormal vibrations in machinery, which can indicate issues like misalignment or imbalance. Temperature Sensors: These are used to monitor the temperature of equipment. Overheating can be a sign of friction, wear, or electrical issues. Pressure Sensors: These sensors measure the pressure within systems, which is crucial in industries like oil and gas or hydraulics. Acoustic Sensors: These can detect changes in noise levels, which might indicate leaks, cracks, or other mechanical failures. Ultrasonic Sensors: These are used for detecting flaws or changes in material properties. Role in Data Collection Sensors continuously collect data from equipment during operation. This data can include readings on temperature, pressure, vibration, and more, depending on the type of sensor used. The frequency and accuracy of data collection are crucial for effective predictive maintenance. Monitoring and Early Warning The primary function of sensors is to provide real-time monitoring of equipment. They can detect even the slightest changes in performance, which might be indicative of a developing problem. This early warning capability is essential for taking preemptive action before a minor issue turns into a major failure. Durability and Reliability Sensors used in predictive maintenance need to be durable and reliable, especially in harsh industrial environments. They should be able to withstand extreme temperatures, pressures, and other challenging conditions while providing accurate data. Sensors are an indispensable part of the hardware setup in predictive maintenance. Their ability to provide detailed and real-time data about equipment health is what enables predictive maintenance systems to anticipate and prevent potential failures, ensuring smoother and more efficient operations. Data Acquisition Systems Data Acquisition Systems Data acquisition systems play a pivotal role in predictive maintenance by serving as the bridge between the raw data collected by sensors and the analysis that leads to maintenance decisions. Here are the key aspects of data acquisition systems in predictive maintenance: Data Collection and Transmission: Data acquisition systems are responsible for collecting the data from various sensors attached to the equipment. They not only gather this data but also format and transmit it for further analysis. This involves converting sensor signals, which are often analog, into digital data that can be processed by computers. Real-Time Data Acquisition: One of the critical features of these systems is the ability to acquire data in real time. This means that as soon as a sensor detects a change or anomaly, the data acquisition system captures and processes this information instantly. Real-time data is crucial for timely decision-making in predictive maintenance. Integration with Multiple Sensors: In complex machinery, multiple sensors measuring different parameters are often used. Data acquisition systems are designed to integrate inputs from these various sensors, providing a comprehensive view of the equipment’s condition. Data Quality and Filtering: These systems also play a role in ensuring the quality of the data. They can filter out noise or irrelevant data, ensuring that only meaningful and accurate information is passed on for analysis. This is important for preventing false alarms and ensuring the reliability of predictive maintenance decisions. Scalability and Flexibility: Data acquisition systems in predictive maintenance need to be scalable to accommodate additional sensors or equipment. They should also be flexible enough to adapt to different types of machinery and varying data collection requirements. Data acquisition systems are a critical hardware component in predictive maintenance. They ensure that the data collected by sensors is accurately and promptly captured, transmitted, and prepared for analysis. Without effective data acquisition systems, the ability to predict and prevent equipment failures would be significantly hindered. Connectivity Devices Connectivity Devices Connectivity devices are essential in predictive maintenance for ensuring the seamless transmission of data from the machinery to the analysis systems. Here are the key aspects of connectivity devices in predictive maintenance: Role in Data Transmission: Connectivity devices are responsible for transmitting the data collected by sensors and processed by data acquisition systems to the central analysis software or cloud storage. This transmission can occur over various mediums, including wired networks, Wi-Fi, or cellular networks. Internet of Things (IoT) Integration: Many predictive maintenance systems leverage the Internet of Things (IoT) to enhance connectivity. IoT devices can communicate with each other and with central systems, creating a network of interconnected devices that share data in real-time. Network Reliability and Security: It’s crucial that connectivity devices provide a reliable and secure network for data transmission. Any interruption in connectivity can lead to delays in data analysis and potentially missed maintenance opportunities. Additionally, the data transmitted often contains sensitive information, making security a top priority to prevent unauthorized access or cyber attacks. Wireless and Remote Monitoring: In many cases, connectivity devices enable wireless and remote monitoring of equipment. This is particularly useful in hard-to-reach or hazardous environments. It allows for continuous monitoring without the need for physical proximity, enhancing safety and efficiency. Edge Computing Capabilities: Some connectivity devices come equipped with edge computing capabilities. This means they can process and analyze data at the source, reducing the need for constant data transmission to a central system. This can lead to faster response times and reduced network load. In conclusion, connectivity devices are a vital hardware component in predictive maintenance systems. They ensure that the data flow from the machinery to the analysis systems is uninterrupted, secure, and efficient. By enabling reliable and real-time data transmission, these devices play a crucial role in the effectiveness of predictive maintenance strategies. Software Components in Predictive Maintenance Data Analytics Software Data analytics software is the brain of predictive maintenance systems, turning raw data into actionable insights. Here are the key aspects of data analytics software in predictive maintenance, with insights into how XMPro addresses each: Data Analysis and Pattern Recognition: The primary function of data analytics software is to analyze the vast amounts of data collected from various sensors and equipment. It identifies patterns, trends, and anomalies that might indicate potential issues or impending failures. XMPro excels in this area, effectively analyzing data to pinpoint potential problems. Machine Learning and AI Algorithms: Advanced data analytics software often employs machine learning and artificial intelligence algorithms. These algorithms can learn from historical data, improve over time, and make increasingly accurate predictions about equipment maintenance needs. XMPro utilizes these advanced algorithms to enhance its predictive capabilities. Visualization Tools: Data analytics software typically includes visualization tools that present data in an easily understandable format. Dashboards, graphs, and heat maps help maintenance teams quickly grasp the condition of equipment and make informed decisions. XMPro offers such visualization tools, aiding in the clear presentation of data. Predictive Alerts and Notifications: One of the critical features of this software is its ability to provide predictive alerts and notifications. When potential issues are detected, the software can alert maintenance personnel, allowing them to take preemptive action before a failure occurs. XMPro incorporates this feature, ensuring timely alerts and notifications. Integration with Other Systems: Effective data analytics software can integrate with other systems such as Enterprise Resource Planning (ERP) and Computerized Maintenance Management Systems (CMMS). This integration allows for a more holistic approach to maintenance management. XMPro supports such integrations, enhancing its effectiveness in maintenance strategies. Customization and Scalability: Different industries and equipment types may have unique requirements. Therefore, data analytics software in predictive maintenance should be customizable to meet specific needs. It should also be scalable to accommodate growing data volumes and additional equipment. XMPro is designed with both customization and scalability in mind, catering to diverse industry needs. Data analytics software is a crucial component of predictive maintenance. It provides the intelligence needed to interpret data accurately, predict potential failures, and guide maintenance decisions. By leveraging advanced algorithms and providing intuitive visualizations, software like XMPro plays a pivotal role in transforming raw data into meaningful insights that drive predictive maintenance strategies. Predictive Modelling Predictive modeling is a fundamental software component in predictive maintenance, enabling the prediction of future equipment failures based on historical and real-time data. XMPro, as an example of such software, addresses these key aspects: Creation of Predictive Models: Predictive modeling involves developing mathematical models that can forecast potential equipment failures. These models are created using historical data, which includes records of past failures, maintenance activities, and operational conditions. XMPro excels in creating these predictive models, utilizing comprehensive historical data for accuracy. Use of Historical and Real-Time Data: Predictive models utilize both historical data and real-time data from sensors. The historical data helps in understanding past trends and failure patterns, while real-time data provides current insights into equipment condition. XMPro effectively combines both data types to enhance prediction quality. Machine Learning Techniques: Many predictive models employ machine learning techniques, which allow the models to learn from data, identify patterns, and improve their predictions over time. Techniques such as regression analysis, classification, and neural networks are commonly used. XMPro incorporates these advanced machine learning techniques to refine its predictive models. Accuracy and Reliability: The effectiveness of predictive maintenance heavily relies on the accuracy and reliability of predictive models. The models must be rigorously tested and validated to ensure they can provide trustworthy predictions. XMPro prioritizes the accuracy and reliability of its predictive models, ensuring they meet high standards. Continuous Improvement and Updating: Predictive models are not static; they need to be continuously updated and refined as more data becomes available. This ongoing improvement helps in adapting to changes in equipment behavior and operational conditions. XMPro supports this continuous improvement, constantly refining its models with new data. Customization for Specific Equipment: Different types of equipment may have unique operational characteristics and failure modes. Therefore, predictive models often need to be customized for specific equipment types to ensure accurate predictions. XMPro offers customization options to cater to different equipment types and operational nuances. Integration with Maintenance Schedules: Predictive models are often integrated with maintenance management software to ensure that the predictions are effectively translated into maintenance actions and schedules. XMPro seamlessly integrates with maintenance schedules, ensuring that its predictions lead to timely and effective maintenance actions. Predictive modeling is a critical software component in predictive maintenance. It provides the capability to forecast equipment failures, allowing maintenance teams to act proactively. The accuracy, reliability, and continuous improvement of these models, as exemplified by XMPro, are essential for the success of predictive maintenance programs. Maintenance Management Software Maintenance management software is a vital software component in predictive maintenance, serving as the operational hub for managing and implementing maintenance activities. XMPro, as an example of such software, addresses these key aspects: Integration of Predictive Maintenance Data: This software integrates the insights and predictions derived from data analytics and predictive modeling. XMPro excels in translating these insights into actionable maintenance tasks, ensuring that the predictions lead to effective maintenance actions. Scheduling and Planning: Maintenance management software facilitates the scheduling and planning of maintenance activities. XMPro helps in prioritizing tasks based on the urgency and importance of the predicted maintenance needs, ensuring optimal allocation of resources and minimizing downtime. Work Order Management: The software streamlines the creation, assignment, and tracking of work orders. When a predictive model, like those in XMPro, indicates a potential issue, the software can automatically generate a work order, assign it to the appropriate personnel, and track its progress. Inventory and Spare Parts Management: Effective maintenance often requires the availability of spare parts and tools. XMPro aids in managing inventory, ensuring that necessary parts are available when needed for predictive maintenance tasks. Record Keeping and Documentation: The software maintains comprehensive records of all maintenance activities, including predictive maintenance tasks. XMPro’s documentation capabilities are crucial for tracking the effectiveness of maintenance strategies, compliance with regulations, and future decision-making. Performance Analysis and Reporting: Maintenance management software often includes tools for analyzing maintenance performance and generating reports. XMPro provides reports that can offer insights into the effectiveness of the predictive maintenance program, areas for improvement, and cost savings achieved. User-Friendly Interface and Accessibility: A user-friendly interface is essential for efficient use of the software. Additionally, the software should be accessible on various devices, including computers, tablets, and smartphones. XMPro ensures ease of use and accessibility, allowing maintenance teams to access information and manage tasks on the go. Maintenance management software is an indispensable component in predictive maintenance. It serves as the operational platform that turns predictive insights into organized and effective maintenance actions. By streamlining scheduling, work order management, inventory control, and performance analysis, software like XMPro plays a pivotal role in the successful implementation and management of predictive maintenance strategies. Integration of Hardware & Software The integration of hardware and software components is crucial for the success of predictive maintenance systems. XMPro exemplifies how these components work together to create a cohesive and effective predictive maintenance strategy. Seamless Data Flow: The foundation of successful predictive maintenance lies in the seamless flow of data from hardware components like sensors and connectivity devices to software components such as data analytics and maintenance management systems. XMPro ensures this uninterrupted flow, enabling accurate data collection, transmission, analysis, and action. Real-Time Monitoring and Analysis: The integration allows for real-time monitoring of equipment and immediate analysis of data. Sensors collect data, which is then transmitted through connectivity devices to data analytics software like XMPro. This software analyzes the data in real-time, providing timely insights for maintenance decisions. Predictive Alerts and Maintenance Actions: When the data analytics software, such as XMPro, identifies a potential issue, it triggers predictive alerts. These alerts are integrated with maintenance management software, which then generates and schedules maintenance tasks. XMPro ensures that predictive insights lead to prompt and organized maintenance actions. Feedback Loop for Continuous Improvement: The integration of hardware and software creates a feedback loop. Data from completed maintenance tasks is fed back into the system, allowing for continuous improvement of predictive models and maintenance strategies. XMPro leverages this feedback loop for adapting to changing conditions and improving the accuracy of predictions. Customization and Scalability: Effective integration, as seen in XMPro, allows for customization to meet specific industry or equipment needs. It also ensures scalability, enabling the predictive maintenance system to grow and adapt as more equipment is added or as operational requirements change. User Interface and Accessibility: The integration provides a user-friendly interface that consolidates information from various sources. XMPro offers an interface that is accessible to maintenance teams, allowing them to easily understand and act on the information provided by the system. Challenges and Solutions: Integrating hardware and software components can present challenges such as compatibility issues, data overload, and cybersecurity concerns. XMPro addresses these challenges through careful planning, selection of compatible and secure systems, and effective data management strategies. The integration of PdM hardware and PdM software is a critical aspect of predictive maintenance. It ensures that each component works harmoniously to provide a comprehensive and effective maintenance solution. Software like XMPro is key to transforming raw data into actionable maintenance strategies, ultimately enhancing the efficiency and reliability of maintenance programs. Challenges & Considerations While predictive maintenance offers numerous benefits, implementing such a system comes with its own set of challenges and considerations. Addressing these effectively, as XMPro demonstrates, is crucial for the successful adoption and operation of predictive maintenance strategies. Data Privacy and Security: With the increasing amount of data being collected and transmitted, data privacy and security become paramount. XMPro prioritizes protecting sensitive information from cyber threats and ensures compliance with data protection regulations, addressing these critical challenges. System Compatibility and Integration: Integrating new predictive maintenance technologies with existing systems can be challenging. XMPro is designed to minimize compatibility issues, offering careful planning and seamless integration capabilities, even with legacy systems. Cost and Return on Investment (ROI): Implementing predictive maintenance can be costly, especially for small and medium-sized enterprises. XMPro helps organizations carefully consider the initial investment and ongoing costs against the potential ROI, which includes reduced downtime, extended equipment life, and improved efficiency. Skill Gaps and Training: The successful implementation of predictive maintenance often requires specialized skills. XMPro provides support and resources to bridge skill gaps, and its user-friendly interface reduces the need for extensive training. Data Overload and Analysis Paralysis: The vast amount of data generated by predictive maintenance systems can lead to data overload. XMPro offers strategies to manage, filter, and prioritize data, avoiding analysis paralysis and ensuring actionable insights. Reliability and False Positives: Ensuring the reliability of predictive maintenance systems is crucial. XMPro focuses on reducing false positives, ensuring that the system’s predictions are trustworthy and lead to necessary maintenance actions. Customization and Scalability: Predictive maintenance systems need to be customized to specific industry and equipment requirements. XMPro is both customizable and scalable, accommodating future growth and changes in the operational environment. Cultural and Organizational Change: Adopting predictive maintenance often requires a cultural shift within an organization. XMPro supports this transition, offering change management strategies to move from reactive or scheduled maintenance to a predictive approach. While predictive maintenance offers significant advantages, it’s important to carefully consider and address the various challenges and considerations. Software like XMPro plays a pivotal role in ensuring a smooth transition and maximizes the benefits of predictive maintenance for the organization. Conclusion In conclusion, predictive maintenance represents a transformative approach in equipment management, offering substantial benefits in efficiency, cost savings, and equipment longevity. Implementing this strategy, however, requires careful consideration of various factors, from data security to system integration. Software solutions like XMPro play a crucial role in addressing these challenges, offering seamless integration, data management, and user-friendly interfaces. By effectively leveraging such advanced tools, organizations can not only overcome the hurdles associated with predictive maintenance but also fully harness its potential, leading to a more proactive, data-driven, and efficient maintenance landscape."
  },
  "docs/resources/faqs/external-content/blogs/2023/the-top-5-use-cases-for-composable-digital-twins-in-mining--and-how-to-use-ai-to-supercharge-results.html": {
    "href": "docs/resources/faqs/external-content/blogs/2023/the-top-5-use-cases-for-composable-digital-twins-in-mining--and-how-to-use-ai-to-supercharge-results.html",
    "title": "The TOP 5 use cases for composable digital twins in mining – and how to use AI to supercharge result | XMPro",
    "summary": "The TOP 5 use cases for composable digital twins in mining – and how to use AI to supercharge result Blog, Guides / How To's The TOP 5 use cases for composable digital twins in mining – and how to use AI to supercharge results Posted on January 11, 2023 by Wouter Beneke THE TOP 5 USE CASES FOR COMPOSABLE DIGITAL TWINS IN MINING PLUS: HOW TO SUPERCHARGE RESULTS WITH AI Introduction Digital twins, virtual representations of physical assets, have become increasingly popular in a variety of industries. In the mining industry, digital twins can be used to optimize operations and improve safety by providing a detailed, real-time view of the mine’s physical assets and processes. In particular, composable digital twin technology, which allows different digital twins to be combined and integrated to create a more complete and accurate representation of the mine, has several key use cases. The TOP 5 USE CASES Predictive maintenance: By creating a digital twin of the mine’s equipment, such as haul trucks and excavators, mining companies can use sensor data to monitor the health of these assets and predict when maintenance is needed. This can help to reduce downtime and prolong the life of the equipment, ultimately leading to cost savings. Automation and optimization: Digital twins can also be used to optimize mining processes, such as ore processing and transportation. By simulating different scenarios and analyzing the results, mining companies can identify bottlenecks and improve efficiency. Additionally, digital twins can be used to control and monitor automated mining systems, such as driverless trucks and excavators. Safety and risk management: Digital twins can be used to create detailed, 3D models of the mine, including the locations of personnel and equipment. This can be used to track the movement of workers and vehicles, ensuring that they are in safe areas and avoiding collisions. Additionally, digital twins can be used to simulate and plan for emergency scenarios, such as mine collapses, allowing for more effective response in the event of an emergency. Environmental monitoring: Digital twins can be used to monitor and control the environmental impact of mining operations, including air and water quality, deforestation and soil erosion. They can provide better visibility into the environmental performance of the mine, enabling more effective management and compliance with environmental regulations. Collaboration and decision-making: Digital twins can provide a common, real-time view of the mine, which can be accessed and manipulated by different teams and stakeholders. This can improve collaboration and decision-making, enabling teams to make more informed decisions faster. How to supercharge results with AI Artificial intelligence (AI) is becoming increasingly important in the mining industry, as it has the potential to significantly improve the capabilities of digital twins. By incorporating AI into composable digital twin technology, mining companies can achieve even greater levels of automation, optimization, and safety. Here are a few key ways that AI can be used to supercharge digital twin use cases in mining: Predictive maintenance: One of the most significant benefits of digital twins is the ability to predict when equipment will need maintenance. By incorporating AI, digital twins can become even more effective in predicting equipment failures, allowing mining companies to take preventative measures and avoid costly downtime. AI-powered digital twins can analyze sensor data and identify patterns that indicate when equipment is likely to fail. Additionally, Machine Learning models such as Random Forest, Gradient Boosting, and Neural Networks can be used to predict equipment failures and monitor their health status. Automation and optimization: AI can be used to optimize mining processes by simulating different scenarios and identifying the most efficient solutions. For example, an AI-powered digital twin could be used to optimize the movement of ore from the mine to the processing plant, taking into account factors such as traffic congestion and weather conditions. Furthermore, AI can also be used to control and monitor automated mining systems, such as driverless trucks and excavators, and adapt their behaviour in real-time to the changing conditions of the mine. Safety and risk management: AI can be used to improve safety and risk management by providing real-time monitoring of the mine’s physical assets and personnel. AI-powered digital twins can detect anomalies in sensor data and alert workers to potential hazards, such as equipment malfunctions or unsafe working conditions. Additionally, AI-powered digital twins can be used to simulate and plan for emergency scenarios, such as mine collapses, allowing for more effective response in the event of an emergency. Computer Vision models can also be used to detect and alert personnel and equipment in hazardous zones. Environmental monitoring: AI can be used to monitor and control the environmental impact of mining operations, such as air and water quality, deforestation, and soil erosion. AI-powered digital twins can analyze sensor data in real-time and automatically adjust mining operations to minimize the environmental impact. Also, image analysis and Satellite data analysis could be used for monitoring and reporting the environmental impact. Collaboration and decision making: AI can be used to improve collaboration and decision making by providing real-time insights into the mine’s operations, enabling teams to make more informed decisions faster. For example, an AI-powered digital twin could be used to automatically identify opportunities for cost savings or increased efficiency. Additionally, Natural Language Processing can be used to extract knowledge from unstructured data and enable better communication and collaboration among teams. In conclusion, incorporating AI into composable digital twin technology can help mining companies to achieve even greater levels of automation, optimization, and safety. AI-powered digital twins can predict equipment failures, optimize mining processes, improve safety and risk management, monitor and control the environmental impact of mining operations, and improve collaboration and decision making. By leveraging the power of AI, mining companies can achieve new levels of efficiency, productivity, and safety, ultimately leading to cost savings and improved environmental stewardship. It’s important to note that AI implementation in mining requires a proper data management strategy, reliable and accurate data collection, and robust models that are able to handle the complexity and uncertainty of mining operations. However, by overcoming these challenges, mining companies can harness the power of AI to supercharge their composable digital twin use cases."
  },
  "docs/resources/faqs/external-content/blogs/2023/the-top-5-use-cases-for-composable-digital-twins-in-renewables--how-to-supercharge-results-with-ai.html": {
    "href": "docs/resources/faqs/external-content/blogs/2023/the-top-5-use-cases-for-composable-digital-twins-in-renewables--how-to-supercharge-results-with-ai.html",
    "title": "THE TOP 5 USE CASES FOR COMPOSABLE DIGITAL TWINS IN RENEWABLES + HOW TO SUPERCHARGE RESULTS WITH AI | XMPro",
    "summary": "THE TOP 5 USE CASES FOR COMPOSABLE DIGITAL TWINS IN RENEWABLES + HOW TO SUPERCHARGE RESULTS WITH AI Guides / How To's THE TOP 5 USE CASES FOR COMPOSABLE DIGITAL TWINS IN RENEWABLES + HOW TO SUPERCHARGE RESULTS WITH AI Posted on February 10, 2023 by Wouter BenekeTable of contentsIntroductionUse Case 1: Asset Performance MonitoringUse Case 2: Predictive MaintenanceUse Case 3: Grid Integration and OptimizationUse Case 4: Renewable Energy ForecastingUse Case 5: Decentralized Energy Management5 Less Well-Known Use CasesConclusion Introduction Composable digital twins are revolutionizing the way organizations approach renewable energy management. Players that can successfully implement a composable digital twin strategy in the next 12 – 24 months will cement a decisive competitive advantage thanks to the benefits offered by digital twins. In this article we will discuss the top 5 use cases for composable digital twins in the renewables sector, their associated benefits, and how companies can supercharge results using AI. As a bonus we will also look at 5 lesser known use cases A. Definition of Composable Digital Twin A composable digital twin is a flexible and scalable digital twin that can be created and reconfigured to represent different components, systems, or processes. Unlike traditional digital twins , a composable digital twin allows for the creation of a virtual environment that mirrors the real-world operations of a system, providing a unified view of all relevant data, processes, and systems. This enables organizations to gain a comprehensive understanding of the system’s behaviour and identify areas for improvement. By utilizing AI technology, organizations can supercharge the insights and benefits provided by composable digital twins, making it a powerful tool in the renewable energy sector. B. Importance of Digital Twin in renewables The use of composable digital twins in the renewable energy sector brings numerous benefits, including: Improved decision-making: By providing real-time data and insights into system performance, composable digital twins can help organizations make informed decisions about the operation and maintenance of their renewable energy systems. Enhanced efficiency: By modelling the behaviour of complex renewable energy systems, organizations can identify inefficiencies and optimize their operations, leading to improved performance and increased energy output. Predictive maintenance: With the ability to monitor the performance of components and systems, composable digital twins can provide early warning of potential issues, allowing organizations to perform proactive maintenance and prevent costly downtime. Increased transparency: By providing a unified view of all relevant data, processes, and systems, composable digital twins increase transparency, enabling organizations to more easily identify and address issues. Reduced operational costs: By optimizing the performance of renewable energy systems, organizations can reduce operational costs and increase their return on investment. Use Case 1: Asset Performance Monitoring A. Overview of the use case A composable digital twin can provide you with real-time data on the performance of your renewable energy assets, enabling you to keep a close eye on any potential issues. This use case is all about optimizing performance and maximizing the lifespan of your assets. With the help of AI, you can take asset performance monitoring to the next level. Predictive analytics algorithms can detect potential issues before they occur, reducing downtime and maintenance costs. Data visualization tools provide improved monitoring, enabling you to make informed decisions quickly. By using composable digital twin for asset performance monitoring, you’ll have a complete, up-to-date view of your renewable energy assets, making it easier to ensure they are working optimally and delivering the results you need. B. Benefits of using Composable Digital Twin for Asset Performance Monitoring Benefits of using Composable Digital Twin for Asset Performance Monitoring: Early detection of potential issues: With predictive analytics, you can identify potential problems before they happen, reducing downtime and maintenance costs. Increased efficiency and cost savings: AI-driven optimization algorithms can help you get the most out of your assets, improving efficiency and reducing costs. Improved monitoring and decision-making: Real-time data visualization enables you to monitor performance and make informed decisions quickly, helping you to stay on top of things. Accurate data: By using a composable digital twin, you can be sure that you’re working with accurate, up-to-date data on the performance of your assets, helping you to make better decisions. Improved asset lifespan: Regular monitoring and maintenance can help to extend the lifespan of your assets, ensuring you get the most out of your investment. With these benefits, it’s easy to see why using a composable digital twin for asset performance monitoring is an excellent choice for any renewable energy organization. By supercharging your results with AI, you can take your monitoring to the next level and stay ahead of the competition. C. Supercharging Results with AI As we’ve seen, the Composable Digital Twin has numerous benefits for Asset Performance Monitoring. But what if we took it a step further and utilized the power of AI to supercharge our results? Here’s how: AI-powered Predictive Analytics: AI algorithms can be used to analyze data from the Composable Digital Twin to detect potential issues early on, before they become major problems. This allows for proactive maintenance and reduced downtime, improving overall asset performance. AI-driven Optimization Algorithms: AI algorithms can also be utilized to optimize asset performance, leading to increased efficiency and cost savings. By analyzing real-time data and making predictions, AI can help identify areas for improvement and make recommendations for optimizing performance. AI-powered Data Visualization: Lastly, the Composable Digital Twin’s data can be visualized using AI-powered tools, allowing for improved monitoring and decision-making. The data can be transformed into interactive, easy-to-understand visualizations that provide actionable insights into asset performance. AI-powered anomaly detection can help organizations identify and address issues before they escalate, improving the reliability and performance of their assets. In conclusion, the combination of the Composable Digital Twin and AI leads to a powerful solution for Asset Performance Monitoring. With the ability to detect issues early, optimize performance, and provide clear, actionable insights, organizations can ensure that their assets are performing at their best and minimize downtime. Predictive Maintenance Use Case 2: Predictive Maintenance A. Overview of the use case In the context of renewable energy, predictive maintenance involves using data and analytics to predict when equipment is likely to fail, and performing maintenance before it actually does. This helps organizations reduce downtime, improve equipment reliability, and save on maintenance costs. By using a Composable Digital Twin in predictive maintenance, organizations can access real-time, accurate data about the performance and condition of their equipment. This data, combined with advanced analytics and AI, can be used to predict when equipment is likely to fail, allowing organizations to perform maintenance before a failure occurs. Predictive maintenance helps organizations improve equipment reliability, reduce downtime, and save on maintenance costs, ultimately leading to improved overall efficiency and performance. B. Benefits of using Composable Digital Twin for Predictive Maintenance Real-time condition monitoring: The Composable Digital Twin allows organizations to monitor the performance and condition of their equipment in real-time, providing a more accurate and complete picture of the equipment’s health. Improved maintenance planning: Predictive maintenance allows organizations to schedule maintenance before a failure occurs, reducing downtime and improving equipment reliability. Lower maintenance costs: By performing maintenance before equipment fails, organizations can save on the costs associated with emergency repairs and replacements. Improved equipment reliability: Predictive maintenance helps organizations improve equipment reliability by reducing the frequency of unplanned downtime and by performing maintenance before a failure occurs. By using a Composable Digital Twin in predictive maintenance, organizations can unlock these benefits, leading to improved efficiency, reliability, and cost savings. C. Supercharging Results with AI Real-time Equipment Health Monitoring: AI-powered monitoring tools are changing the game for predictive maintenance. With real-time, accurate assessments of equipment health, organizations can identify potential issues before they become critical problems. Smarter Maintenance Schedules: By incorporating AI into predictive maintenance, organizations can optimize maintenance schedules, reducing downtime and cutting costs. Say goodbye to guesswork and hello to data-driven decision making! Streamlined Efficiency: AI-enabled optimization of maintenance schedules can take your predictive maintenance efforts to the next level. With AI, organizations can streamline their maintenance processes, leading to even greater cost savings and improved equipment reliability. AI-driven Predictive Failure Analysis: Proactive Issue Resolution. AI is revolutionizing the way we approach predictive maintenance, and this includes taking proactive measures to avoid failures altogether. Predictive failure analysis uses AI algorithms to analyze data and predict when an issue might arise. This allows you to take preventative measures before the problem becomes critical, which reduces downtime, improves efficiency, and saves money. With composable digital twins, you can take advantage of AI-driven predictive failure analysis to keep your renewable energy systems running smoothly, avoid costly downtime, and supercharge your results. Incorporating AI into predictive maintenance efforts is a no-brainer. Whether it’s through real-time equipment health monitoring, smarter maintenance schedules, or streamlined efficiency, organizations can unlock the full potential of predictive maintenance with AI. Grid Integration & Optimization Use Case 3: Grid Integration and Optimization A. Overview of the use case Renewable energy is becoming increasingly prevalent, and the ability to seamlessly integrate it into the grid is critical. Composable Digital Twin technology provides a powerful solution for grid integration and optimization. With the ability to provide real-time monitoring and analysis, this technology can help optimize the flow of energy and improve the stability and efficiency of the grid. In this section, we’ll explore the benefits of using Composable Digital Twin for grid integration and optimization. B. Benefits of using Composable Digital Twin for Grid Integration and Optimization The benefits of using the digital twin for grid integration and optimization are numerous: Improved grid stability: By using AI-powered demand response algorithms, operators can balance supply and demand in real-time, ensuring grid stability. Increased efficiency: AI-driven algorithms can optimize energy distribution, reducing waste and increasing efficiency. Enhanced decision-making: The real-time monitoring and visualization provided by the digital twin enable operators to make informed decisions about energy distribution, improving the overall performance of the grid. With these benefits, the Composable Digital Twin provides an innovative solution for grid integration and optimization, enabling renewable energy to reach its full potential. C. Supercharging Results with AI By incorporating AI, the results of grid integration and optimization are supercharged. Here’s how: AI-powered demand response algorithms: With AI-powered demand response algorithms, the energy grid can respond to changes in demand in real-time, leading to improved stability and efficiency. AI-driven real-time grid monitoring: AI-driven monitoring allows for improved decision-making and reaction times, ensuring the smooth operation of the grid. AI-based grid optimization algorithms: AI-based algorithms can optimize the flow of energy, leading to increased efficiency and cost savings for both the energy providers and consumers. Renewable Energy Forecasting Use Case 4: Renewable Energy Forecasting A. Overview of the use case Forecasting renewable energy production is critical for effective grid management and energy trading. Digital twins offer a new way to approach this challenge, providing real-time, accurate data on renewable energy production. With Composable Digital Twins, energy producers can optimize their operations, reduce costs, and maximize their return on investment. In this section, we’ll explore the use case of renewable energy forecasting and how it can be supercharged with AI. B. Benefits of using Composable Digital Twin for Renewable Energy Forecasting Accurate forecasting is critical to the success of renewable energy projects. The use of a digital twin in this context offers several benefits, including: Improved Forecasting Accuracy: A digital twin enables the integration of real-time weather and climate data, allowing for more accurate forecasting of renewable energy production. Real-Time Forecasting and Optimization: Predictive algorithms can be integrated into a digital twin to provide real-time forecasting, allowing for real-time optimization and improved decision-making. Data Visualization: A digital twin provides an easy-to-use, visual representation of renewable energy production, making it easier to analyze data and make informed decisions. In summary, the use of a digital twin in renewable energy forecasting can help to improve forecasting accuracy, optimize renewable energy production, and provide a clear picture of renewable energy production, enabling better decision-making. C. Supercharging Results with AI Renewable energy forecasting is crucial for the success of renewable energy projects, as it helps to optimize the energy production and distribution process. The use of Composable Digital Twin technology in this field can supercharge the results, making the forecasting process even more accurate and efficient. Here’s how AI can enhance the process: AI-powered weather and climate data analysis: AI algorithms can process vast amounts of weather and climate data, providing more accurate and up-to-date information for forecasting. AI-based predictive algorithms: AI can use this data to make real-time forecasting and optimization decisions, providing a more precise and efficient process. AI-driven visualization tools: The use of AI-driven visualization tools can help to analyze and interpret the data, making it easier to make informed decisions. These tools can also provide real-time updates, allowing energy managers to make quick and informed decisions. AI-enabled integration of multiple data sources for improved forecasting accuracy: The integration of multiple data sources can greatly enhance the accuracy of renewable energy forecasting. AI algorithms can be leveraged to combine data from a variety of sources, including satellite imagery, weather forecasts, and historical data, to provide a more comprehensive picture of expected energy generation. With AI, the integration process is automated and optimized, reducing manual effort and increasing the speed at which data can be analyzed. This results in improved forecasting accuracy, which can inform decision-making processes related to grid management, energy storage, and energy trading. With the help of AI, renewable energy forecasting can be transformed into a more reliable, efficient, and cost-effective process, providing significant benefits for renewable energy projects. Decentralized Energy Management Use Case 5: Decentralized Energy Management A. Overview of the use case The energy sector is undergoing a major transformation as the world moves towards a more decentralized and sustainable energy future. Decentralized energy management is a key component of this shift, allowing for more efficient and cost-effective energy distribution and usage. A composable digital twin can play a crucial role in supporting this transition by providing real-time monitoring, management, and optimization of decentralized energy systems. This use case can help organizations achieve greater energy efficiency, cost savings, and overall sustainability in their energy management practices. B. Benefits of using Composable Digital Twin for Decentralized Energy Management In the energy sector, decentralization is becoming increasingly popular as it offers more control and flexibility over energy production, distribution and consumption. This approach is especially relevant for renewables, as it allows for more efficient use of local resources and reduces the dependence on large centralized energy sources. Composable Digital Twin technology plays a crucial role in enabling decentralized energy management, by providing real-time monitoring, control, and optimization of energy systems at a local level. With the use of Digital Twin, energy management becomes more efficient, with improved decision-making capabilities, optimized energy distribution and reduced energy waste. By using Digital Twin, energy providers can better understand local energy consumption patterns, allowing them to optimize energy production and distribution, leading to increased energy efficiency, cost savings, and reduced greenhouse gas emissions. C. Supercharging Results with AI In decentralized energy management, Composable Digital Twin supercharges results by utilizing AI in various ways. Some examples include AI-powered demand response algorithms: These algorithms help improve energy balancing and efficiency by using real-time data and machine learning algorithms to optimize energy distribution. AI-based real-time energy monitoring and management: With AI-powered monitoring and management, decision-making in decentralized energy management becomes more informed, timely and effective. AI-enabled optimization of energy distribution: The integration of AI algorithms in decentralized energy management helps optimize energy distribution, resulting in increased efficiency and cost savings. AI-enabled integration of multiple data sources: By integrating multiple data sources such as weather forecasts, energy usage patterns, and grid capacities, AI-enabled systems can improve the accuracy of energy forecasting and optimize energy distribution. 5 Less Well-Known Use Cases A. Microgrids Microgrids are small-scale energy systems that are independent of the main grid, providing power to communities and businesses. They offer a solution for communities who are looking for more control over their energy supply, increased reliability, and reduced costs. With a Composable Digital Twin, microgrids can take advantage of AI-powered optimization algorithms to increase energy efficiency, improve energy management, and minimize costs. B. Hybrid Renewable Energy Systems Hybrid renewable energy systems are becoming increasingly popular, combining the benefits of multiple energy sources to meet energy demands. A composable digital twin can be a powerful tool in optimizing and managing these systems, bringing together data from a variety of sources, including wind, solar, and energy storage systems. The digital twin allows energy professionals to monitor and manage the performance of each energy source in real-time, ensuring efficient and cost-effective energy production. With the help of AI, the digital twin can analyze data and predict potential issues, allowing for proactive maintenance and optimization. This leads to improved energy production, increased efficiency, and reduced costs. By using a composable digital twin for hybrid renewable energy systems, energy professionals can stay ahead of the curve and ensure that their systems are operating at their maximum potential. C. Energy Storage Systems Composable Digital Twin can help in effectively managing hybrid renewable energy systems by providing a unified view of the entire system. The digital twin can provide real-time monitoring and control of the system, allowing for optimized energy distribution and improved efficiency. With the help of AI algorithms, the digital twin can analyze data from multiple sources and make predictions on energy production and consumption. This information can be used to make informed decisions on energy distribution, improving the overall performance and efficiency of the hybrid renewable energy system. D. Building-Integrated Renewables Building-integrated renewables refer to the integration of renewable energy sources into a building’s design and construction. This not only reduces the building’s carbon footprint but also makes it more energy-efficient. Composable Digital Twin technology can be used to monitor and optimize building-integrated renewables, such as rooftop solar panels or wind turbines. With AI-powered monitoring and optimization algorithms, building-integrated renewables can be managed more efficiently and effectively. This not only helps to reduce the building’s energy costs but also improves its overall sustainability. Real-time monitoring of energy production and consumption allows building owners and managers to make data-driven decisions on energy usage and make any necessary adjustments to ensure maximum efficiency and cost savings. E. Electric Vehicle Charging Using composable digital twin technology, energy providers can better manage and optimize electric vehicle charging networks. By integrating real-time data on vehicle battery levels, charging station availability, and energy usage patterns, energy providers can better plan and coordinate charging activities. With AI-powered algorithms, energy providers can predict energy demand, dynamically adjust charging speeds, and reduce the risk of grid overloading. This leads to improved energy efficiency, reduced costs, and a more seamless electric vehicle charging experience for drivers. Conclusion A. Recap of the Top 5 Use Cases and 5 Less Known Use Cases In this blog post, we’ve explored the many ways that composable digital twins can be used to revolutionize the renewable energy industry. From asset performance monitoring and predictive maintenance to grid integration and optimization, renewable energy forecasting, and decentralized energy management, we’ve highlighted some of the most important and impactful use cases for digital twins in renewables. But that’s not all! We’ve also touched upon five less well-known use cases, such as microgrids, hybrid renewable energy systems, energy storage systems, building-integrated renewables, and electric vehicle charging. These innovative applications showcase the versatility and potential of composable digital twins to drive progress in the renewables sector. In conclusion, we hope this overview has provided valuable insights into how composable digital twins are set to change the face of the renewable energy industry, and how AI can supercharge the results of these use cases. B. How to get started with Composable Digital Twins Building a composable digital twin can seem like a daunting task, but it doesn’t have to be. The key to success is finding the right tools and platforms to help you get started. One platform that stands out in this regard is XMPro, the world’s only No-Code Digital Twin Composition Platform. XMPro is designed to make it easy for organizations of all sizes to build and manage composable digital twins. With XMPro, you can create a digital twin of your assets, processes, and systems in a matter of minutes, without the need for complex coding or IT skills. The platform is user-friendly, intuitive, and offers a wide range of features and capabilities that are designed to help you quickly and easily build, integrate, and optimize your digital twin. Some of the key benefits of using XMPro to build your composable digital twin include No-Code Composition: With XMPro, you don’t need to know how to code to build a digital twin. The platform’s drag-and-drop interface and intuitive workflows make it easy for anyone to build a digital twin, even if you have limited technical skills. Wide Range of Integrations: XMPro offers a wide range of integrations with other tools and platforms, making it easy to bring in data from multiple sources and integrate it into your digital twin. Advanced Analytics and AI: XMPro includes advanced analytics and AI capabilities, making it easy to monitor and analyze your digital twin in real-time. You can use the platform’s predictive analytics and AI-driven insights to make data-driven decisions and improve your operations. Scalability: XMPro is a scalable platform, making it easy to start small and grow as your needs change. Whether you’re a small organization or a large enterprise, XMPro can help you build a composable digital twin that meets your needs. In conclusion, XMPro is a powerful, yet user-friendly platform that can help you build a composable digital twin quickly and easily. With its no-code composition capabilities, wide range of integrations, advanced analytics, and AI capabilities, XMPro is the perfect solution for organizations looking to build a composable digital twin. So if you’re ready to get started, give XMPro a try today!"
  },
  "docs/resources/faqs/external-content/blogs/2023/the-top-5-use-cases-for-composable-digital-twins-in-the-oil--gas-industry.html": {
    "href": "docs/resources/faqs/external-content/blogs/2023/the-top-5-use-cases-for-composable-digital-twins-in-the-oil--gas-industry.html",
    "title": "The TOP 5 use cases for Composable Digital Twins in the Oil & Gas industry | XMPro",
    "summary": "The TOP 5 use cases for Composable Digital Twins in the Oil & Gas industry Blog, Guides / How To's The TOP 5 use cases for Composable Digital Twins in the Oil & Gas industry Posted on January 13, 2023 by Wouter Beneke The top 5 use cases for composable digital twin in OIL & GAS Introduction Digital twins have revolutionized the way the oil and gas industry operates. A digital twin is a virtual representation of a physical asset, such as a drilling rig or a pipeline, that allows for real-time monitoring and optimization of operations. Composable digital twins take this concept a step further by allowing for the integration of data from multiple sources and the creation of a holistic view of the asset. Here are the top 5 use cases for composable digital twins in the oil and gas industry: The top 5 use cases Predictive Maintenance: Composable digital twins allow for the integration of data from sensors and other sources to predict when maintenance is needed on an asset. This can help to reduce downtime and increase efficiency by allowing for proactive maintenance rather than reactive maintenance. Asset Optimization: Composable digital twins can be used to optimize the performance of an asset by analyzing data from multiple sources and identifying areas for improvement. For example, a composable digital twin of a drilling rig could be used to optimize the drilling process by analyzing data on drilling parameters, weather conditions, and other factors. Safety and Compliance: Composable digital twins can be used to ensure that assets are in compliance with safety and regulatory standards. This can be done by analyzing data from sensors and other sources to identify potential hazards and take proactive measures to address them. Remote Monitoring: Composable digital twins can be used to remotely monitor assets and make decisions based on real-time data. For example, a composable digital twin of a pipeline could be used to monitor the flow of oil and make adjustments to the pumping rate as needed. Supply Chain Optimization: Composable digital twins can be used to optimize the supply chain by analyzing data on inventory levels, transportation routes, and other factors. This can help to reduce costs and improve efficiency by identifying areas for improvement and taking proactive measures to address them. Supercharge results with AI Artificial intelligence (AI) has the potential to revolutionize the oil and gas industry by providing new ways to optimize operations and improve efficiency. One of the key ways that AI can supercharge results in the industry is through the use of composable digital twins. Here is a closer look at how AI can supercharge results for the top 5 use cases for composable digital twins in the oil and gas industry: Predictive Maintenance: AI algorithms can be used to analyze data from sensors and other sources to predict when maintenance is needed on an asset. This can help to reduce downtime and increase efficiency by allowing for proactive maintenance rather than reactive maintenance. With the help of AI, the system can learn from the patterns and data, it can predict the failure even before it happens, thus providing ample time to schedule maintenance and avoid any unplanned downtime. Asset Optimization: AI can be used to optimize the performance of an asset by analyzing data from multiple sources and identifying areas for improvement. For example, an AI-powered digital twin of a drilling rig could be used to optimize the drilling process by analyzing data on drilling parameters, weather conditions, and other factors. The system can learn from the historical data and optimize the process for better performance. Safety and Compliance: AI can be used to ensure that assets are in compliance with safety and regulatory standards. This can be done by analyzing data from sensors and other sources to identify potential hazards and take proactive measures to address them. AI-powered systems can detect anomalies and alert the operations team for necessary actions, it can also learn from the historical data to predict any potential hazards. Remote Monitoring: AI can be used to remotely monitor assets and make decisions based on real-time data. For example, an AI-powered digital twin of a pipeline could be used to monitor the flow of oil and make adjustments to the pumping rate as needed. The AI algorithms can analyze data from multiple sources, including weather conditions, to optimize the flow of oil and reduce costs. Supply Chain Optimization: AI can be used to optimize the supply chain by analyzing data on inventory levels, transportation routes, and other factors. This can help to reduce costs and improve efficiency by identifying areas for improvement and taking proactive measures to address them. AI-powered systems can predict the demand, optimize the supply chain for better performance, and reduce costs. In conclusion, AI can supercharge results for the top 5 use cases for composable digital twins in the oil and gas industry. By providing new ways to analyze data and make decisions, AI can help to optimize operations, reduce downtime, and increase efficiency. With the help of AI, the oil and gas industry can become more efficient, safer and more profitable."
  },
  "docs/resources/faqs/external-content/blogs/2023/unlocking-efficiency-the-right-time--strategy-to-launch-your-digital-twin-for-enhanced-asset-managem.html": {
    "href": "docs/resources/faqs/external-content/blogs/2023/unlocking-efficiency-the-right-time--strategy-to-launch-your-digital-twin-for-enhanced-asset-managem.html",
    "title": "Unlocking Efficiency: The Right Time & Strategy to Launch Your Digital Twin for Enhanced Asset Manag | XMPro",
    "summary": "Unlocking Efficiency: The Right Time & Strategy to Launch Your Digital Twin for Enhanced Asset Manag Articles, Blog, CEO'S Blog Unlocking Efficiency: The Right Time & Strategy to Launch Your Digital Twin for Enhanced Asset Management Posted on December 13, 2023 by Pieter van Schalkwyk Unlocking Efficiency: The Right Time & Strategy to Launch Your Digital Twin for Enhanced Asset Management As providers of digital twin solutions, we at XMPro have the unique opportunity to observe the impact of digital technology on asset management. We witness how some companies harness these innovations to achieve a staggering 10-fold return on investment within the first year, while others remain ensnared in the ‘pilot purgatory,’ unable to scale. Fortunately, the difference isn’t due to some elusive ‘magic’ button available only to the fortunate few. Instead, it boils down to a straightforward, robust framework that’s repeatable, scalable, and accessible to all. This framework isn’t a well-kept secret; it’s a set of best practices that any organization can implement to successfully transform its asset performance management. Make Data Meaningful The ultimate objective for sectors reliant on heavy assets is the implementation of AI-driven Predictive Maintenance (PdM) systems that not only forecast potential issues but also suggest specific corrective actions, ideally with ready-to-use models for various asset types. While this is an aspirational goal, it’s different from where one should begin when establishing a solid PdM framework. In truth, despite the abundance of industrial data available, a significant portion of this data requires further processing and contextual refinement to become truly serviceable for PdM. Data in quantity is not enough; the priority lies in refining this data into a dependable, applicable, and insightful format. Start Bottom Up, One Failure Mode at a time The cornerstone of a truly effective Predictive Maintenance (PdM) strategy lies in adopting a focused approach rather than an overwhelming ‘top-down’ methodology. Begin by dissecting a single root cause for each type of failure, delve into the analytics that can detect or predict its occurrence, and determine the specific data required for these analytics. This process typically starts with a condition monitoring system. As you accumulate and scrutinize sufficient data regarding equipment condition and failure patterns, you can progress to the predictive phase. At this juncture, the data you’ve curated becomes invaluable, allowing you to make informed and actionable recommendations. Track Leading Indicators Starting with a ‘Bad Actor’ analysis paves the way for a straightforward strategy. Focus on pinpointing the top 10 factors that most significantly impact downtime or asset performance degradation in your facility. Break down the failure modes leading to these inefficiencies and uncover their root causes. From there, determine leading indicators that could serve as benchmarks for asset health and track these metrics in real time. Archive these valuable insights systematically, as they will form the foundation for the development of more advanced predictive models in the future. Add More Failure Modes Gradually incorporate additional failure modes and enhance your asset’s digital twin by integrating various use cases related to those failure modes. Adopting a layered, ‘onion-ring’ method ensures that your asset’s digital twin is fully equipped to handle all critical failure modes. This ‘bad actor’ strategy also guarantees continuous focus on the most significant failure mode causing downtime. It’s a practical application of the ‘theory of constraints,’ which suggests that solving one problem will naturally shift attention to the next most pressing issue. Think Big, Start Small, Scale Fast Our firsthand experience has shown us the value of this methodology; it mitigates the risks associated with initial digital twin ventures, offers rapid value realization, and showcases the transformative power of digital technology when applied effectively. Moreover, this strategy resonates with the intrinsic nature of reliability engineers who excel at addressing issues sequentially, one problem at a time. At XMPro, we specialize in this strategic approach that balances low risk with high potential rewards. Our clients see a return of more than 10X in the first year with this simple, but effective approach. We invite you to reach out to us for a consultation to explore how this method can be tailored to benefit your organization. Want to know more? I would like to:(Required) Contact a Technical / Solutions Specialist Register For Monthly Webinar Register for a Product Demo Access The Free Trial Join The XMPro NewsletterName(Required) First LastEmail(Required)Company(Required)DepartmentITOperationsExecutiveEngineeringAcademicOtherJob RoleMessage: Δdocument.getElementById( \"ak_js_1\" ).setAttribute( \"value\", ( new Date() ).getTime() ); /* <![CDATA[ */ gform.initializeOnLoaded( function() {gformInitSpinner( 11, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery('#gform_ajax_frame_11').on('load',function(){var contents = jQuery(this).contents().find('*').html();var is_postback = contents.indexOf('GF_AJAX_POSTBACK') >= 0;if(!is_postback){return;}var form_content = jQuery(this).contents().find('#gform_wrapper_11');var is_confirmation = jQuery(this).contents().find('#gform_confirmation_wrapper_11').length > 0;var is_redirect = contents.indexOf('gformRedirect(){') >= 0;var is_form = form_content.length > 0 && ! is_redirect && ! is_confirmation;var mt = parseInt(jQuery('html').css('margin-top'), 10) + parseInt(jQuery('body').css('margin-top'), 10) + 100;if(is_form){form_content.find('form').css('opacity', 0);jQuery('#gform_wrapper_11').html(form_content.html());if(form_content.hasClass('gform_validation_error')){jQuery('#gform_wrapper_11').addClass('gform_validation_error');} else {jQuery('#gform_wrapper_11').removeClass('gform_validation_error');}setTimeout( function() { /* delay the scroll by 50 milliseconds to fix a bug in chrome */ }, 50 );if(window['gformInitDatepicker']) {gformInitDatepicker();}if(window['gformInitPriceFields']) {gformInitPriceFields();}var current_page = jQuery('#gform_source_page_number_11').val();gformInitSpinner( 11, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery(document).trigger('gform_page_loaded', [11, current_page]);window['gf_submitting_11'] = false;}else if(!is_redirect){var confirmation_content = jQuery(this).contents().find('.GF_AJAX_POSTBACK').html();if(!confirmation_content){confirmation_content = contents;}jQuery('#gform_wrapper_11').replaceWith(confirmation_content);jQuery(document).trigger('gform_confirmation_loaded', [11]);window['gf_submitting_11'] = false;wp.a11y.speak(jQuery('#gform_confirmation_message_11').text());}else{jQuery('#gform_11').append(contents);if(window['gformRedirect']) {gformRedirect();}}jQuery(document).trigger(\"gform_pre_post_render\", [{ formId: \"11\", currentPage: \"current_page\", abort: function() { this.preventDefault(); } }]); if (event && event.defaultPrevented) { return; } const gformWrapperDiv = document.getElementById( \"gform_wrapper_11\" ); if ( gformWrapperDiv ) { const visibilitySpan = document.createElement( \"span\" ); visibilitySpan.id = \"gform_visibility_test_11\"; gformWrapperDiv.insertAdjacentElement( \"afterend\", visibilitySpan ); } const visibilityTestDiv = document.getElementById( \"gform_visibility_test_11\" ); let postRenderFired = false; function triggerPostRender() { if ( postRenderFired ) { return; } postRenderFired = true; jQuery( document ).trigger( 'gform_post_render', [11, current_page] ); gform.utils.trigger( { event: 'gform/postRender', native: false, data: { formId: 11, currentPage: current_page } } ); gform.utils.trigger( { event: 'gform/post_render', native: false, data: { formId: 11, currentPage: current_page } } ); if ( visibilityTestDiv ) { visibilityTestDiv.parentNode.removeChild( visibilityTestDiv ); } } function debounce( func, wait, immediate ) { var timeout; return function() { var context = this, args = arguments; var later = function() { timeout = null; if ( !immediate ) func.apply( context, args ); }; var callNow = immediate && !timeout; clearTimeout( timeout ); timeout = setTimeout( later, wait ); if ( callNow ) func.apply( context, args ); }; } const debouncedTriggerPostRender = debounce( function() { triggerPostRender(); }, 200 ); if ( visibilityTestDiv && visibilityTestDiv.offsetParent === null ) { const observer = new MutationObserver( ( mutations ) => { mutations.forEach( ( mutation ) => { if ( mutation.type === 'attributes' && visibilityTestDiv.offsetParent !== null ) { debouncedTriggerPostRender(); observer.disconnect(); } }); }); observer.observe( document.body, { attributes: true, childList: false, subtree: true, attributeFilter: [ 'style', 'class' ], }); } else { triggerPostRender(); } } );} ); /* ]]&gt; */"
  },
  "docs/resources/faqs/external-content/blogs/2023/what-is-edge-computing-and-how-can-digital-twins-utilize-this-technology.html": {
    "href": "docs/resources/faqs/external-content/blogs/2023/what-is-edge-computing-and-how-can-digital-twins-utilize-this-technology.html",
    "title": "What is edge computing, and how can digital twins utilize this technology? | XMPro",
    "summary": "What is edge computing, and how can digital twins utilize this technology? Blog, Explainer Series What is edge computing, and how can digital twins utilize this technology? Posted on January 2, 2023 by Wouter Beneke Edge computing is a distributed computing paradigm that brings computation and data storage closer to the edge of a network, where data is generated and used. The goal of edge computing is to reduce the amount of data that needs to be transmitted over the network, and to improve the speed and efficiency of data processing. Digital twins can utilize edge computing in several ways. For example, a digital twin of a manufacturing plant could be implemented at the edge of the network, allowing it to process and analyze data from sensors and other devices in real-time, without the need to transmit the data to a centralized server for processing. This can help improve the responsiveness and accuracy of the digital twin, and allow it to make more informed decisions. Edge computing can also be used to support the development and deployment of digital twins in resource-constrained or remote environments, where access to the cloud or other centralized resources may be limited. In these cases, the edge computing platform can provide the necessary computing and storage resources to support the digital twin, without the need for a dedicated server or other infrastructure. Overall, edge computing can help digital twins operate more efficiently, respond more quickly to changing conditions, and provide more accurate and reliable insights and predictions."
  },
  "docs/resources/faqs/external-content/blogs/2023/why-decision-intelligence-with-digital-twins-is-kinda-like-dcs-for-automation-and-control.html": {
    "href": "docs/resources/faqs/external-content/blogs/2023/why-decision-intelligence-with-digital-twins-is-kinda-like-dcs-for-automation-and-control.html",
    "title": "Why Decision Intelligence with Digital Twins is “kinda like” DCS for Automation and Control | XMPro",
    "summary": "Why Decision Intelligence with Digital Twins is “kinda like” DCS for Automation and Control Blog, CEO'S Blog, Explainer Series Why Decision Intelligence with Digital Twins is “kinda like” DCS for Automation and Control Posted on June 22, 2023 by Pieter van Schalkwyk Why Decision Intelligence with Digital Twins is “kinda like” DCS for Automation and Control In this article: Industrial Digital Twins, combine Operational Technology (OT), Information Technology (IT), and Engineering Technology (ET). Digital twins leverage AI and advanced analytics to provide insights from extensive datasets, enabling decision support, augmentation, and automation through a Distributed Intelligence System . DIS revolutionizes business process automation, similar to how Distributed Control Systems (DCS) automate plant or factory operations. The implementation of DIS based on digital twin architecture streamlines processes, enhances decision-making, and paves the way for intelligently automated operations. Industrial Digital Twins exist at the confluence of Operational Technology (OT), Information Technology (IT), and Engineering Technology (ET), forming a unique triad of interconnected functionalities. They bring many benefits, one of the most significant being their capability to facilitate superior decision-making grounded in real-time data and context-specific information. The initial phase of decision support delivered by digital twins equips business users with comprehensive data visualizations through dashboards and sophisticated business intelligence tools. These users then process this detailed, up-to-the-minute data using their professional acumen to make informed operational decisions. However, the role of digital twins is continually evolving. They’re not just being used to offer information, but now, with the incorporation of AI and advanced analytics, they can further enrich the information by revealing hidden insights from colossal datasets that would be challenging (or even impossible) for business users to process manually in real time. This capability translates into decision augmentation, a process that yields prescriptive recommendations, providing a course of action from which the user can choose. As we move into the future, the ambit of digital twins will extend beyond decision augmentation to decision automation. They will operate within a safe framework, making strategic decisions based on AI, analytics, and established business rules. This leap forward will enable ‘lights-out’ operations, driving an algorithmic business model. Figure 1 – Decision Support Models for Decision Intelligence Collectively, these three models of decision enablement – supporting, augmented, and automated – lay the foundation for a Distributed Intelligence System. “DIS” This system, enabled by digital twins, functions similarly to how Distributed Control Systems (DCS) provide the intelligence for automating plant or factory operations. This seamless integration of technologies propels us toward a future of digitized, intelligent, and automated decision-making in the industrial sector. The DIS for business process is “kinda like” a DCS for automation control. Distributed Control System “DCS” in Industrial Automation A Distributed Control System “DCS” is a digital automation system that uses geographically distributed control loops throughout a factory or plant. Each control loop manages a specific part of the process, such as controlling the temperature in a furnace or the speed of a conveyor belt. The DCS allows for a high degree of automation, reducing the need for human intervention and increasing the efficiency and reliability of the process. It also provides a central control room where operators can monitor the entire process, make adjustments as needed, and respond quickly to any issues or alarms. It is coordinated through Supervisory Computers that serve as Supervisory Control and Data Acquisition (SCADA) systems in many organizations. This feeds into Production Control systems, MES, MRP, and various other business production scheduling systems. Figure 2 – ISA 95 – Purdue Model for shopfloor automation and control Distributed Intelligence System “DIS” in Business Process Automation A Distributed Intelligence System “DIS” performs a similar function for operational business processes. Instead of controlling physical processes in an industrial setting, it controls and optimizes business processes and decision-making. In a DIS based on decentralized, edge-based business rules and a composable digital twin architecture, each business process or operation has its own digital twin, complete with its own data, rules, and decision-making algorithms. These digital twins are located at the edge, near where the data is generated, and the decisions are made, or in the cloud for centralized management and data. This architecture allows each business process or operation to be managed independently, with its own localized decision-making capabilities. The digital twins communicate and coordinate with each other, creating a network of distributed intelligence throughout the organization. Figure 3 – DCS vs DIS Both SCADA and DIS systems provide a framework for monitoring, controlling, and optimizing operations, albeit in different contexts: SCADA for industrial processes and DIS for business operations. SCADA typically deals with physical parameters and controls hardware devices. In contrast, DIS deals with business parameters and controls software processes or decisions. One fundamental difference is the way control is distributed. SCADA systems often centralize high-level control and decision-making in master stations, while DIS systems distribute decision-making across local units to enable faster, more context-aware decisions. In a DIS based on a composable digital twin architecture, the system can quickly adapt to changes in the business environment. Each digital twin can be updated or replaced independently, providing flexibility and agility. This is crucial for businesses that need to respond rapidly to market dynamics. We can create a new layer of strategic intelligence by merging the control intelligence from the Distributed Control Systems “DCS” and the operations intelligence derived from the Decision Intelligence System “DIS”. This unique amalgamation paves the way for developing a Common Operating Picture, essentially acting as an executive control tower, offering senior decision-makers unparalleled, comprehensive oversight and control. Figure 4 – Influence operational decisions by adjusting strategic value levers With this system in place, executives gain the ability to manipulate strategic value levers directly influencing business process rules and logic. This influence, in turn, modifies the decision support, augmentation, or automation provided by individual digital twins. Consequently, this creates a transparent and scalable pathway from strategic planning to operational execution, streamlining the entire organizational decision-making process. Why does your business need a DIS? A Distributed Intelligence System “DIS” will serve as a robust and dynamic repository for managing a broad spectrum of digital rules and models. It will house essential elements of hyperautomation, including diverse rulesets, artificial intelligence models, and other emerging digital meta-models and artifacts. Acting as the central nervous system of digital operations, the DIS will orchestrate and streamline various automated processes and decision-making capabilities. It will empower organizations to respond promptly and intelligently to changing business conditions. In this ecosystem, data will play the role of lifeblood, powering every function and decision. Seamless dataflow will be the critical enabler, facilitating real-time communication, synchronization, and coordination among various components. As data courses through the system, it brings vitality and intelligence, enabling the business to operate efficiently and adaptively. How do you start with a DIS? The foundation or Level 0 of the Decision Intelligence System “DIS” involves the acquisition of data via Internet of Things (IoT) and from Operational Technology (OT), Information Technology (IT), and Engineering Technology (ET) systems. This applies to various industrial environments, including factories, mines, power utilities, etc. This crucial data acquisition and management task is effectively and efficiently handled by XMPro Data Streams, which integrate and orchestrate the flow of information to the respective digital twins at the next level of the framework. The next level, Level 1, in the Decision Intelligence System “DIS” framework, comprises the specific applications or use cases of Digital Twins. Each use case addresses a business challenge or seizes an opportunity within the overarching value chain. These use cases are divided into eight universal categories consistently relevant across various industries, a focal point for XMPro’s solutions. To accelerate the development and deployment of these various digital twin use cases, XMPro offers ‘Blueprints.’ These blueprints encompass Data Streams, AppDesigner, AI, and Recommendations, forming a robust toolkit for swift and efficient implementation. Additionally, XMPro provides a prioritization framework for use cases, assisting clients in selecting the most suitable and impactful candidates to kickstart their digital twin journey. Figure 5 – Manufacturing Use Cases Value Chain Matrix The strength of a Digital Twin platform, such as XMPro, lies in its capacity to generate numerous use cases within the same framework, providing a unified operational view across the entire value chain and diverse business focus areas. Contrasting with traditional point solutions, a digital twin platform shuns the creation of isolated information silos. Instead, it enables the construction of aggregate digital twins on top of foundational operational twins. This shift from traditional standalone point solutions enhances decision intelligence and provides a composite view enriched with novel insights that previously couldn’t be interconnected. Moreover, it achieves these benefits with remarkable efficiency – executing tasks with greater speed, enhanced quality, superior intelligence, and unparalleled agility, significantly outperforming traditional stand-alone point solutions. Level 2 of the Decision Intelligence System framework supports tactical planning by cultivating a comprehensive view of multiple operational digital twins, emphasizing more tactical planning use cases. These use cases have a longer decision impact horizon and focus more on managing constraints. In future iterations of this framework, they will gain the ability to guide lower-level operational digital twins, leveraging the recommendations and business rules derived from the rules and model control at Level 3. Figure 6 – Decisions at different levels of the organization Level 3 functions as a centrally managed repository for digital twin models, AI models, and business rules, utilized by the Supervisory Twins to orchestrate and manage operations in a transparent and highly scalable manner. Drawing parallels with DevOps and MLOps methodologies, Level 3 introduces the concept of TwinOps. This mechanism supports continuous integration (CI) and continuous delivery (CD), automating the integration and deployment of digital twin models and business rules. This level offers a streamlined, automated approach, ensuring constant updates and smooth functioning of the digital twin ecosystem. Level 4 functions as an Operations Intelligence Control Tower, equipping Operations Managers with the tools to monitor and manage operational Key Performance Indicators (KPIs). By aligning these KPIs with strategic directives from executive leadership, managers can fine-tune rules and thresholds at level 3. This grants Operations Managers a ‘control tower’ perspective across their segment of the value chain, as well as across all eight functional areas. Achieving this integrated, comprehensive view is only possible with a robust dlike XMPro iDTS, our intelligent Digital Twin Suite. It offers an all-encompassing Distributed Intelligence System that transforms complex data into actionable insights, fostering superior decision-making and strategic alignment across the organization. Integrating intelligence from both the Distributed Control Systems (DCS) and the Distributed Intelligence System DIS empowers executive leadership with a Common Operating Picture at Level 5. This concept, drawn from military and disaster response decision-making models, leverages the same data as the operational and tactical digital twins, but reframes it through a strategic lens. This comprehensive perspective enables leadership to decide on strategic alterations to value levers, which can then be communicated at level 4 and implemented by modifying rules, thresholds, and guidelines at level 3. These changes are then disseminated to the appropriate operational digital twins in a process that is transparent, scalable, and managed through TwinOps. This systematic approach ensures that strategic decisions seamlessly influence operational processes, maintaining coherence and consistency across all organizational levels. Figure 7 – Common Operating Picture with XMPro intelligent Digital Twin Suite (iDTS) In the constantly evolving landscape of industrial technology, the intersection of Operational Technology (OT), Information Technology (IT), and Engineering Technology (ET) forms a powerful triad: Industrial Digital Twins. These twins, more than just information providers, have begun to augment and even automate decision-making, harnessing the power of AI and advanced analytics to reveal hidden insights in extensive datasets. In doing so, they are progressively driving ‘lights-out’ autonomous operations and an algorithmic business model. The digital twin architecture encapsulates three key models of decision enablement – decision support, augmentation, and automation, all under a robust umbrella, known as a Decision Intelligence System DIS. DIS revolutionizes business process automation by drawing parallels to how Distributed Control Systems (DCS) provide intelligence for automating plant or factory operations. Fueled by XMPro’s effective digital twin management tools, this framework offers diverse use cases across industries. It creates a network of distributed intelligence throughout the organization, responding dynamically to the changing business environment. Moreover, DIS goes beyond connecting the dots between different operational levels. It builds an executive-level Common Operating Picture that reframes operational and tactical data through a strategic lens. This comprehensive view allows leadership to influence business process rules, seamlessly translating strategic planning into operational execution. In conclusion, the evolution and implementation of Decision Intelligence Systems based on digital twin architecture not only streamlines processes and augments decision-making but also promises a future of intelligently automated operations. By redefining the boundaries of control and decision-making, from shop floor automation to executive strategic planning, these systems stand as testament to the groundbreaking strides technology continues to make toward an optimized, digitally transformed future."
  },
  "docs/resources/faqs/external-content/blogs/2023/xmpro-becomes-an-nvidia-cloudvalidated-partner.html": {
    "href": "docs/resources/faqs/external-content/blogs/2023/xmpro-becomes-an-nvidia-cloudvalidated-partner.html",
    "title": "XMPro becomes an NVIDIA Cloud-Validated partner | XMPro",
    "summary": "XMPro becomes an NVIDIA Cloud-Validated partner Blog XMPro becomes an NVIDIA Cloud-Validated partner Posted on November 2, 2023 by Sarah Danks By Daniel King, XMPro Development Manager This week XMPro announced that it has become an NVIDIA Cloud-Validated partner. By using the NVIDIA AI platform, we are better positioned to support customers to bridge the gap between data flow and operational AI in the cloud. To get here, we completed a rigorous test plan to validate and showcase the XMPro Suite running efficiently on NVIDIA’s technology stack. This included building an agent to process a sample AI workload using NVIDIA GPUs (via CUDA) to showcase how the XMPro suite runs optimally on the NVIDIA stack. And how did we do this, you ask? The following steps outline the process we used to test XMPro on an NVIDIA GPU. Step One: Build an Agent – Follow our docs to build and package an Agent. Import your preferred CUDA NuGet package when building your agent. In this example we used ILGPU. You can find a list of popular libraries in NuGet. Step Two: Provision an NVIDIA GPU service in the cloud – Provision your preferred NVIDIA Virtual Machine Image and cloud provider. Step Three: Install Stream Host – Follow the docs to install the XMPro Stream Host on Ubuntu 20.04 using the provisioned virtual machine in step two. Step Four: Add the Agent to a Stream – Import an agent, add to a stream, configure the agent and publish the stream. This is an example of an Agent that calculates the number of prime numbers in a given number using concurrent processing on an NVIDIA GPU running in AWS. It is implemented in a stream that increases the number every second by 500,000. The primary objective of NVIDIA cloud validation is to help customers easily identify and adopt validated NVIDIA-based solutions. XMPro is now part of the NVIDIA Accelerated Applications Catalog, which features world-class GPU- and DPU-accelerated solutions. About Daniel King: Daniel King is Development Manager for XMPro, with 19 years of experience in development and solution architecture. He designs simple solutions to complex problems that deliver business value."
  },
  "docs/resources/faqs/external-content/blogs/2023/xmpro-i3c-intelligent-digital-twins-strategy-framework.html": {
    "href": "docs/resources/faqs/external-content/blogs/2023/xmpro-i3c-intelligent-digital-twins-strategy-framework.html",
    "title": "XMPro I3C Intelligent Digital Twins Strategy Framework | XMPro",
    "summary": "XMPro I3C Intelligent Digital Twins Strategy Framework Blog, CEO'S Blog XMPro I3C Intelligent Digital Twins Strategy Framework Posted on April 12, 2023 by Pieter van Schalkwyk XMPro I3C Intelligent Digital Twins Strategy Framework Introducing XMPro’s I3C Intelligent Digital Twin framework, a cutting-edge solution designed to help organizations harness the power of Intelligent Digital Twins (IDTs) in their operations. Building upon the concepts of traditional digital twins and inspired by Dr. Michael Grieves’ vision (The Roadmap to Intelligent Digital Twins), our I3C framework aims to empower organizations with a strategic roadmap for the seamless adoption and integration of IDTs. By leveraging the active, online, goal-seeking, and anticipatory nature of IDTs, businesses can unlock unprecedented levels of operational efficiency, optimize processes, and minimize resource usage. Figure 3- XMPro I3C Intelligent Digital Twin Framework: Integrated, Intelligent, Interactive, and Composable\\ In this blog post, we delve into the four foundational pillars of Intelligent Digital Twins that set them apart from traditional solutions: Integrated, Intelligent, Interactive, and Composable. We’ll explore how these aspects work together to create a powerful, cohesive platform, empowering organizations to harness real-time data, leverage advanced analytics, make data-driven decisions, and rapidly adapt to evolving business landscapes. Integrated: Real-time and Contextualized Integrated Digital Twins unite data from diverse, heterogeneous sources, creating a cohesive, common operating picture that enhances decision-making capabilities beyond traditional siloed data approaches. An Integrated Digital Twin approach can reduce integration costs and time by 30%-50% over the lifecycle of digital twin applications. This is possible by following the three principles: Figure 4 -XMPro I3C Intelligent Digital Twin Framework: Integrated\\ Standards-based APIs and Packaged Business Capabilities: Organizations should adopt a standards-based API approach when integrating data from heterogeneous sources for digital twins, leveraging XMPro Agents in data streams for several compelling reasons: Interoperability: By adhering to standardized APIs, XMPro Agents can communicate and exchange data with various systems, regardless of the underlying technology or vendor. This seamless integration reduces compatibility issues and fosters collaboration between different platforms. Simplified Integration: Utilizing standards-based APIs, XMPro Agents streamline the process of connecting diverse data sources, providing a consistent and well-documented interface for data exchange. This approach simplifies the complexity of integrating multiple systems, saving time and resources. Improved Data Quality: Standardized APIs, implemented by XMPro Agents, enforce data consistency and validation rules, maintaining the quality and accuracy of the exchanged data. Accurate, real-time data is crucial for digital twins to generate insights and drive decision-making. Faster Deployment: XMPro Agents using standards-based APIs enable organizations to rapidly deploy digital twin solutions, as the standardized interface allows for quicker integration of new data sources or updates to existing ones. This accelerates time-to-value for digital twin implementations. Scalability and Adaptability: A standards-based API approach, combined with XMPro Agents, provides a flexible and modular foundation for digital twin solutions, making it easier to scale and adapt the system as the organization’s needs change. This flexibility ensures the digital twin solution remains relevant and effective over time. Future-proofing: Adopting standards-based APIs with XMPro Agents helps future-proof digital twin implementations, as standardized interfaces are more likely to be supported by new technologies or vendors entering the market. This reduces the risk of obsolescence and ensures a longer lifespan for the digital twin solution. Packaged Business Capabilities (PBCs): PBCs allow organizations to encapsulate specific functionalities or processes as modular, reusable components. XMPro Agents can easily integrate these components into the digital twin architecture to address various business needs and requirements. This approach enables organizations to rapidly deploy and scale digital twin solutions while maintaining flexibility and adaptability to changing business environments. Model-driven development and integration: Model-driven development and integration approaches offer several key benefits when building composable digital twins: Abstraction and Simplification: Model-driven development allows developers to focus on high-level business logic and functionality by abstracting away the underlying complexities of the system. This simplification enables quicker and more efficient development of digital twin components. Reusability and Modularity: With a model-driven approach, digital twin components are designed as reusable and modular building blocks. This allows organizations to easily compose, reconfigure, and extend their digital twin solutions to meet changing requirements or address new use cases, enhancing flexibility and adaptability. Consistency and Standardization: Model-driven development promotes consistency and standardization across the digital twin solution by providing a unified methodology and framework for designing components. This ensures that the various parts of the digital twin system are compatible and can work together seamlessly. Improved Quality and Maintainability: Model-driven development can lead to improved software quality and maintainability by enforcing best practices and reducing the potential for human error. Additionally, the use of high-level models can make it easier to understand, troubleshoot, and modify the digital twin system as needed. Enhanced Collaboration: A model-driven approach fosters better collaboration between various stakeholders, such as domain experts, developers, and system architects. By providing a common, visual representation of the digital twin system, model-driven development enables all parties to more effectively communicate, align their efforts, and work together towards a shared goal. Bi-directional active connection between physical and digital twin Bi-directional active connections between physical and digital twins offer several key benefits that enhance the overall effectiveness and value of digital twin technology: Real-time Data Synchronization: A bi-directional connection ensures that the digital twin continuously receives real-time data from the physical asset, enabling it to accurately reflect the asset’s current state and performance. Conversely, the physical asset can receive updates or adjustments from the digital twin, enabling more dynamic and responsive interactions between the two. Improved Decision-Making: With up-to-date and accurate data, digital twins can generate more reliable insights and recommendations, empowering decision-makers to make well-informed choices based on the current state of the physical asset. This leads to better outcomes and more efficient use of resources. Enhanced Predictive and Prescriptive Capabilities: Bi-directional connections enable digital twins to learn from the physical asset’s behavior and continuously refine their predictive models. This results in more accurate predictions and prescriptions, which can help prevent issues, optimize performance, and extend the asset’s lifespan. Faster Response to Changes: The active connection allows digital twins to rapidly detect and respond to changes in the physical asset’s conditions or performance. This enables organizations to address potential issues or opportunities more quickly, reducing downtime and mitigating risks. Closed-Loop Control: Bi-directional active connections enable closed-loop control, where the digital twin can not only monitor the physical asset but also directly influence its operation. This allows for more precise control, automation, and optimization of asset performance, further improving efficiency and reducing costs. Better Integration with Business Systems: The active connection between the digital twin and the physical asset facilitates seamless integration with other business systems, such as ERP or CRM, enabling organizations to leverage the insights from digital twins across their entire operation. Continuous Improvement: Bi-directional connections foster a continuous feedback loop between the digital and physical twins, enabling ongoing improvement and adaptation as the physical asset and its operating environment evolve over time. This helps ensure that the digital twin remains relevant, effective, and aligned with the organization’s objectives. Intelligent: Analytics and Simulation By consolidating data into a common operating picture, digital twins provide the ideal foundation for harnessing the power of AI and machine learning. This enables the generation of novel insights and recommendations previously unattainable with isolated point solutions. As a result, intelligent digital twins unlock new possibilities for data-driven decision-making and propel organizations toward greater efficiency and innovation. Intelligence in Digital Twins can result in 10x effectiveness improvement in Composable Digital Twin applications. The following three mechanisms for leveraging AI and intelligence enables these business outcomes: Figure 5 -XMPro I3C Intelligent Digital Twin Framework: Intelligent\\ Executable AI and Machine Learning for Algorithmic Business Processes Embedding XMPro AI Agents in XMPro Data Streams enables Executable AI and Machine Learning for Algorithmic Business Processes, which in turn, enhances the capabilities of operational digital twins. This integration offers several key benefits: Real-time Analytics: By incorporating AI Agents into Data Streams, digital twins can process and analyze data in real-time, generating immediate insights and recommendations. This allows organizations to make informed decisions and respond to changing conditions more quickly, improving operational efficiency and agility. Continuous Learning: AI Agents embedded in Data Streams can continuously learn from the data they process, refining their models and algorithms over time. This ongoing improvement enables digital twins to deliver increasingly accurate predictions and recommendations, helping organizations optimize asset performance and proactively address potential issues. Automation of Complex Processes: Executable AI and Machine Learning enable digital twins to automate complex, data-driven business processes, streamlining operations and reducing the need for manual intervention. This automation can lead to significant cost savings, increased productivity, and better resource allocation. Personalized and Adaptive Solutions: AI Agents embedded in Data Streams can adapt their algorithms to the specific needs and context of each digital twin, delivering personalized and adaptive solutions that cater to the unique requirements of individual assets and processes. This customization enhances the effectiveness of digital twins and drives better outcomes. Integration of AI into Core Business Processes: By baking AI into operational digital twins, organizations can seamlessly integrate advanced analytics and machine learning capabilities into their core business processes. This deep integration allows for more holistic decision-making and drives greater value from AI investments. MLOps: Incorporating MLOps practices into the AI Agent lifecycle ensures that the development, deployment, and maintenance of machine learning models in digital twins are efficient, scalable, and reliable. This leads to faster time-to-value, improved model performance, and better alignment between AI capabilities and business objectives. Innovation AI for Experimentation and Front Running Simulation XMPro Intelligent Digital Twins offer a powerful platform for facilitating innovation and experimentation in AI by incorporating XMPro Notebooks based on fully embedded and integrated Jupyter Notebooks. XMPro Notebooks provide an interactive environment that allows subject matter experts (SMEs) to experiment with data, algorithms, and models in real time. Rapid Experimentation: XMPro Notebooks enable subject matter experts (SMEs) to quickly test ideas, algorithms, and models, promoting a faster innovation cycle and reducing time-to-market for new solutions. Collaboration and Knowledge Sharing: The interactive environment of XMPro Notebooks (Jupyter Hub) allows SMEs to collaborate and share insights easily, fostering cross-functional teamwork and enhancing the overall decision-making process. Accessible AI for Non-Technical Users: By embedding Jupyter Notebooks within the Intelligent Digital Twin platform, XMPro empowers SMEs without deep technical expertise to harness the power of AI, democratizing advanced analytics and promoting innovation across the organization. Optimized Processes and Decision-Making: Through Front Running Simulations, XMPro Notebooks enable SMEs to make data-driven decisions, optimizing processes, reducing operational costs, and improving overall efficiency. Continuous Improvement: The iterative nature of XMPro Notebooks allows SMEs to refine their digital twins and AI models constantly, ensuring that they remain relevant, accurate, and effective as business environments and requirements evolve. Augmented AI for Self-learning Digital Twins XMPro Augmented AI for Self-learning Digital Twins harnesses the power of artificial intelligence and machine learning to enhance the decision support and automation capabilities of Digital Twins built on the XMPro platform. Real-time Anomaly Detection: XMPro Augmented AI utilizes AI and Machine Learning techniques to monitor real-time and historical recommendation data, identifying unusual patterns or deviations from expected behavior. This allows organizations to detect anomalies early, enabling rapid response and mitigation of potential issues in their Digital Twins. Pattern Discovery: By analyzing XMPro real-time and historical recommendation data, Augmented AI can uncover hidden patterns and trends, helping organizations understand complex relationships within their Digital Twins. This deeper understanding of the underlying dynamics leads to more informed decision-making and improved decision support. Continuous Learning and Adaptation: XMPro Augmented AI leverages self-learning capabilities to continuously refine its models and algorithms as new data is processed. This enables Digital Twins built on the XMPro platform to adapt and evolve over time, ensuring their insights and recommendations remain relevant and effective in a changing environment. Enhanced Decision Automation: By incorporating AI and Machine Learning into the decision-making process, XMPro Augmented AI can automate complex decisions and optimize decision logic within Digital Twins. This not only reduces manual intervention but also drives efficiency, accuracy, and consistency in decision-making across the organization. Performance Optimization: XMPro Augmented AI leverages the power of AI and Machine Learning to identify opportunities for improvement in Digital Twins built on the XMPro platform. By analyzing recommendation data and identifying patterns, Augmented AI can suggest optimizations that enhance the overall performance and effectiveness of the Digital Twins, leading to better outcomes and increased operational efficiency. Interactive: Decisions and Visualization Digital Twins offer invaluable decision support and automation for business users, enabling them to act on the intelligence and recommendations derived from these virtual representations in a multimodal, interactive manner. McKinsey estimates that digital twins can improve worker productivity by 10% to 15%, reduce errors and rework by 10% to 20%, and increase worker safety by 10% to 20% in the manufacturing sector. Interactive Digital Twins based on the following three principles enable those business outcomes: Figure 6 -XMPro I3C Intelligent Digital Twin Framework: Interactive\\ AI-enabled Recommendations for Prescriptive Analytics XMPro Recommendations revolutionizes expert knowledge capture by combining rules-based methodologies with artificial intelligence, empowering digital twins to deliver prescriptive guidance with newfound precision. By leveraging AI-based recommendations, even less experienced users can receive interactive, co-pilot guidance to navigate complex decision-making processes. Here, we outline three key benefits of utilizing prescriptive analytics for recommendations in a digital twin: Enhanced Decision-Making: Prescriptive analytics enables digital twins to provide users with actionable insights and specific recommendations, empowering them to make informed decisions and optimize their operations. Adaptive Learning: As AI-based recommendations continuously learn from historical and real-time data, they become increasingly accurate and relevant, allowing digital twins to adapt and improve their prescriptive guidance over time. Expertise Democratization: By offering co-pilot interactive guidance, prescriptive analytics democratizes expert knowledge, allowing users of varying experience levels to effectively harness the power of digital twins and make well-informed decisions. Generative and Collaborative Multi-experience User Interfaces Generative and Collaborative Multi-experience User Interfaces (UI) offer a transformative approach to designing and interacting with composable digital twins. By incorporating not only traditional desktop and mobile user interfaces, but also embracing emerging technologies such as Augmented Reality (AR) and Virtual Reality (VR), these multi-experience UIs provide a seamless and immersive experience across a wide spectrum of devices and platforms. Here, we highlight three key benefits of Generative and Collaborative Multi-experience User Interfaces for composable digital twins: Enhanced User Engagement: By offering a diverse range of user interfaces, including AR and VR, multi-experience UIs captivate users’ attention and foster deeper engagement with digital twins, resulting in more effective decision-making and improved overall satisfaction. Collaboration and Knowledge Sharing: Multi-experience UIs enable users to collaborate and share knowledge across different platforms and devices, fostering a more connected and informed workforce. This collaborative environment promotes cross-functional teamwork and leads to better, more informed decision-making. Personalized and Context-Aware Experiences: Generative and Collaborative Multi-experience User Interfaces can adapt to users’ preferences, device capabilities, and contextual information, delivering tailored and intuitive interactions that cater to individual needs. By providing a personalized experience, multi-experience UIs ensure that users can effectively harness the full potential of composable digital twins, regardless of their preferred interface or device. Foundation for the Industrial Metaverse Digital twins are the foundation of the Industrial Metaverse because they serve as the bridge between the physical and digital worlds, enabling seamless integration, collaboration, and innovation across various industries. By creating virtual representations of assets, processes, and systems, digital twins allow organizations to harness the power of advanced analytics, AI, and machine learning to gain valuable insights, optimize operations, and drive decision-making. Here are several key reasons why digital twins are essential to the Industrial Metaverse: Data Integration and Analysis: Digital twins enable organizations to aggregate and analyze data from multiple sources, providing a comprehensive view of their assets and processes. This data-driven approach enhances decision-making capabilities and offers a more accurate understanding of the real-world systems they represent. Real-time Insights and Predictive Capabilities: Digital twins offer real-time monitoring and predictive analytics, allowing organizations to proactively identify potential issues, optimize performance, and improve overall efficiency. These capabilities help businesses respond more effectively to changing market conditions and minimize operational disruptions. Collaboration and Innovation: The Industrial Metaverse fosters collaboration among different stakeholders, including manufacturers, suppliers, and customers. This interconnected ecosystem allows organizations to share knowledge, resources, and expertise, driving innovation and enabling the development of new products, services, and business models. Enhanced Simulation and Experimentation: Digital twins provide a virtual environment for testing and simulating various scenarios, reducing the risks and costs associated with physical trials. This enables organizations to experiment with new ideas, strategies, and technologies, accelerating the pace of innovation and growth. Scalability and Flexibility: Digital twins are highly scalable and adaptable, making it easier for organizations to expand their operations, adopt new technologies, and respond to changing market demands. This flexibility ensures that businesses can maintain a competitive edge in a rapidly evolving digital landscape. By serving as the foundation of the Industrial Metaverse, digital twins are transforming the way organizations operate, collaborate, and innovate, unlocking new opportunities for growth and success in the digital era. Composable: No Code Modular Digital Twin Platform and Marketplace Composability in a No Code Modular Digital Twin Platform and a supporting Marketplace are foundational elements of an intelligent digital twin framework because they enable organizations to rapidly design, develop, and deploy digital twin solutions tailored to their unique needs and requirements. Analysts predicts that by 2023, organizations that have adopted an intelligent composable approach will outpace the competition by 80% in the speed of new feature implementation. Composable Digital Twins is a typical implementation of composable business approach. Figure 7 – XMPro I3C Intelligent Digital Twin Framework: Composable\\ By leveraging modular components and pre-built templates, businesses can streamline the development process, foster collaboration, drive innovation across their operations, and enhance security. Here are six supporting reasons to use a Composable Digital Twin platform like XMPro: Rapid Deployment: Composability in a No Code platform such as XMPro allows organizations to quickly assemble and deploy digital twin solutions by combining pre-built modules, templates, and components, significantly reducing development time and accelerating time-to-value. Flexibility and Adaptability: A modular platform enables businesses to easily modify, expand, or reconfigure their digital twin solutions in response to evolving needs or emerging opportunities, ensuring that their digital twin framework remains relevant and effective over time. Collaborative Development: No Code platforms and supporting Marketplaces foster collaboration among various stakeholders, including domain experts, developers, and system architects, facilitating better communication and alignment of efforts towards a shared goal. Reusability and Standardization: Modular digital twin platforms like XMPro, promote reusability and standardization, as organizations can leverage pre-built components and templates across multiple digital twin solutions, ensuring consistency, compatibility, and seamless integration. Security: Composability in a No Code platform ensures that security best practices are consistently applied across all digital twin components, safeguarding sensitive data and protecting against potential threats, while promoting trust and confidence in the digital twin ecosystem. Cost and Resource Efficiency: Composability in a No Code Modular Digital Twin Platform like XMPro, reduces the need for extensive custom development, lowering overall development costs and allowing organizations to allocate their resources more effectively. Composability in a No Code Modular Digital Twin Platform and a supporting Marketplace provide the foundation for an intelligent digital twin framework by enabling rapid deployment, flexibility, collaboration, reusability, enhanced security, and cost efficiency, empowering organizations to unlock the full potential of digital twin technology. XMPro’s I3C Intelligent Digital Twin framework offers a comprehensive, cutting-edge solution that enables organizations to embrace the future of industry and digital transformation. By integrating real-time data and insights, harnessing AI and machine learning capabilities, revolutionizing decision-making and collaboration through interactive digital twins, and leveraging the power of composability with no-code modular platforms and marketplaces, organizations can unlock the full potential of digital twin technology. The I3C framework not only ensures seamless adoption and integration of Intelligent Digital Twins but also empowers businesses to optimize their operations, enhance decision-making, and drive innovation. By adopting the XMPro I3C framework, organizations can confidently navigate the ever-evolving digital landscape and thrive in the era of Intelligent Digital Twins. Don’t miss the opportunity to revolutionize your operations, accelerate growth, and gain a competitive edge. Contact us today to discuss how the framework can be applied to your Digital Twin journey. {See blog post 3 for XMPro AI announcement}"
  },
  "docs/resources/faqs/external-content/blogs/2024/bridging-automation-and-intelligence-xmpros-approach-to-industrial-agent-management.html": {
    "href": "docs/resources/faqs/external-content/blogs/2024/bridging-automation-and-intelligence-xmpros-approach-to-industrial-agent-management.html",
    "title": "Bridging Automation and Intelligence: XMPro’s Approach to Industrial Agent Management | XMPro",
    "summary": "Bridging Automation and Intelligence: XMPro’s Approach to Industrial Agent Management Articles, Blog, CEO'S Blog Bridging Automation and Intelligence: XMPro’s Approach to Industrial Agent Management Posted on November 27, 2024 by Pieter van SchalkwykAgentic Decisions Vs Automations Pieter Van Schalkwyk CEO at XMPRO This article originally appeared on XMPro CEO’s Linkedin Blog, The Digital Engineer With the introduction of Large Language Model (LLM) agent frameworks by Microsoft, AWS, and Salesforce, a common question arises: How does XMPro differ, and why might it be the better choice? These hyperscalers provide agent services designed to automate workflows, manage content, and improve process efficiency, but their focus addresses only part of the broader automation challenge. XMPro’s Multi-Agent Generative System MAGS takes a broader view by combining automation capabilities with intelligent decision-making, creating a system that can both execute tasks and make complex operational decisions. The Evolution of Industrial Decision-Making Industrial operations face three significant challenges today: increasingly complex systems, a widening skills gap in industry, and pressure to improve efficiency with limited resources. These factors have driven the progression from basic decision support to decision augmentation, and now toward decision automation. XMPro has spent more than 15 years developing solutions to address these challenges through data-driven architectures and cognitive frameworks. Decisionmaking is Changing The journey began with decision support through event monitoring and condition tracking. This foundation evolved into real-time decision augmentation using expert systems and predictive analytics, making complex operational data more accessible to users with varying levels of expertise. Today, we stand at the threshold of true decision automation, where AI agents can handle routine tasks while operating within clear operational boundaries. Understanding the Distinction: Automation Agents vs. Decision Agents Automation Agents, provided by major technology vendors, focus on executing predefined tasks within specific parameters. These agents handle important but straightforward processes like document management, data storage, and routine workflow automation. They operate effectively when the rules are clear and the outcomes are predictable. Automation Agents vs Decision Agents Decision Agents, in contrast, address challenges that require contextual understanding and adaptive reasoning. These agents use XMPro’s Observe, Reflect, Plan, Act (ORPA) framework based on this Stanford research paper to process real-time data, consider multiple variables, and make decisions that adapt to changing conditions. It is similar to the familiar OODA loop, but focus on creating executable plans, and actions. It does both System 1 and *System 2 thinking. They excel in situations where simple automation isn’t enough, such as predictive maintenance, resource optimization, and complex operational planning. Cognitive Process of each XMPro MAGS agent The fundamental difference between Automation and Decision Agents lies in their impact on business performance: Automation Agents focus on efficiency (doing things right), while Decision Agents prioritize effectiveness (doing the right things). This distinction matters because improving efficiency in the wrong direction only accelerates movement toward undesired outcomes. Think of it like running a race – you must first ensure you’re running in the right direction before focusing on running faster. Automation Agents, such as those in XMPro DataStreams, establish a foundation by grounding inputs to Decision Agents in first principles engineering and analytical AI. This ensures Decision Agents work with reliable, physics-based observations rather than purely statistical correlations or internal hallucinations. XMPro Decision Agents operate using these grounded observations within objective functions, which are mathematical indicators that measure effectiveness against specific business goals. For example, in manufacturing, an objective function might target overall equipment effectiveness (OEE) rather than just the speed of individual processes. Once Decision Agents establish the correct course of action through these objective functions and grounded observations, Automation Agents can then optimize the execution speed and resource usage of individual tasks. This approach combines the reliability of first principles with the strategic focus of objective functions, ensuring organizations both achieve their strategic goals and execute efficiently at the tactical level. The power of XMPro MAGS lies in its ability to bring these two types of agents together in a coordinated system. By assigning routine tasks to Automation Agents and reserving complex decision-making for Decision Agents, organizations can achieve both efficiency and intelligence in their operations. The Contractor Model: Integrating External Platforms XMPro MAGS implements a contractor model that treats external automation platforms as specialized service providers. This approach allows organizations to l_everage their existing investments in Microsoft, AWS, and Salesforce_ while maintaining central control over decision-making processes. Hiring Automation Agents from other technology vendors if required For example, Microsoft Azure agents can handle enterprise workflows through Logic Apps connectors and enable seamless collaboration via Microsoft Teams. AWS agents can manage cloud infrastructure and computing resources, ensuring optimal performance and scalability. Salesforce agents can, in addition, automate customer relationships and service processes, maintaining consistent communication across channels. XMPro APEX: The Foundation for Agent Management Managing a diverse ecosystem of agents requires sophisticated orchestration capabilities. XMPro APEX provides these capabilities through its AgentOps framework, which treats agents as operational assets that need consistent management, monitoring, and governance. The XMPro APEX Agent \"Control Room\" APEX enables organizations to: Deploy and monitor agents across different platforms and environments Track agent interactions and performance in real-time Apply consistent governance rules across all agent types Scale agent operations based on organizational needs The system implements deontic rules to ensure all agents operate within defined boundaries and comply with organizational policies. This governance framework applies equally to internal Decision Agents and external Automation Agents, maintaining operational consistency. Real-World Applications and Benefits The combination of MAGS and APEX creates practical benefits in industrial settings. Manufacturing companies can use the system to coordinate predictive maintenance, where Decision Agents analyze equipment performance patterns and delegate specific tasks to Automation Agents for execution. Energy companies can employ the system to optimize resource allocation, using Decision Agents to plan operations while Automation Agents handle routine monitoring and reporting. A typical implementation might include the following: Decision Agents monitoring real-time sensor data and identifying potential issues Azure agents managing document workflows and team notifications AWS agents handling data storage and processing Salesforce agents coordinating customer communications APEX ensuring all agents work together efficiently This is an example of the XMPro MAGS team optimizing maintenance schedules for secondary crushers in mining to minimize the risk of production loss due to overlapping liner replacement intervals. XMPro MAGS Agent Team Reducing Crusher Downtime A dedicated team is improving crusher maintenance and performance with an intelligent multi-agent optimization system. This system employs three coordinated agents: a Wear Rate Optimization Agent, a Maintenance Coordinator Agent, and a Performance Monitoring Agent. Together, these agents address critical operational challenges to balance maintenance, performance, and reliability. The Wear Rate Optimization Agent reduces liner deterioration while maintaining optimal crushing conditions to extend equipment life. The Maintenance Coordinator Agent ensures at least five of six crushers are operational by scheduling strategic maintenance. The Performance Monitoring Agent stabilizes throughput, supporting consistent mineral recovery and overall production goals. This system effectively manages competing objectives, such as minimizing unplanned downtime and maximizing choke feeding time. By coordinating maintenance activities, it prevents overlaps that could disrupt operations and impact production. The result is a more predictable and efficient crusher management process, tailored for modern mining operations Building a Future-Ready Operation Organizations can begin their journey with XMPro by focusing on specific operational challenges. The modular nature of MAGS and APEX allows for gradual expansion as needs evolve and capabilities mature. This approach ensures that investments in agent technology align with organizational growth and operational requirements. The Ecosystem with Cognitive Decisionmaking at the core The implementation process typically follows these steps: Identify key operational challenges that require both automation and decision intelligence Deploy initial agents to address specific use cases Monitor performance and gather operational data Expand agent coverage based on proven results Scale operations across additional facilities or processes The Path Forward The integration of automation and decision intelligence represents a significant advance in industrial operations. While task automation remains important, the ability to make intelligent decisions at scale becomes increasingly critical for operational success. XMPro provides a framework for organizations to build this capability while leveraging their existing investments in enterprise platforms and automation tools. XMPro’s approach helps organizations: Reduce response times by combining automated decisions with automated actions Maintain operational consistency through standardized decision-making processes Scale expertise across facilities using AI agents Improve resource allocation through coordinated agent actions Ensure governance and compliance across all agent operations The future of industrial operations lies in the effective combination of automation and intelligence. Organizations that master this integration will be better positioned to address the growing complexity of modern operations while maintaining efficiency and scalability. * System 1 and System Thinking: https://www.linkedin.com/pulse/why-smart-people-so-stupid-decisions-pieter-van-schalkwyk-bfdcc/ Our GitHub Repo has more technical information if you are interested. You can also contact myself or Gavin Green for more information. Read more on MAGS at The Digital Engineer About the Author: Pieter van Schalkwyk is the CEO of XMPro, which focuses on helping organizations bridge the gap between industrial operations and enterprise systems through practical AI solutions. With over thirty years of experience in industrial automation and digital transformation, Pieter leads XMPro’s mission to make industrial operations more intelligent, efficient, and responsive to business needs."
  },
  "docs/resources/faqs/external-content/blogs/2024/content-decision-and-hybrid-the-three-pillars-of-multiagent-systems-in-industry.html": {
    "href": "docs/resources/faqs/external-content/blogs/2024/content-decision-and-hybrid-the-three-pillars-of-multiagent-systems-in-industry.html",
    "title": "Content, Decision, and Hybrid: The Three Pillars of Multi-Agent Systems in Industry | XMPro",
    "summary": "Content, Decision, and Hybrid: The Three Pillars of Multi-Agent Systems in Industry Articles, Blog Content, Decision, and Hybrid: The Three Pillars of Multi-Agent Systems in Industry Posted on August 13, 2024 by Pieter van Schalkwyk Content, Decision, and Hybrid: The Three Pillars of Multi-Agent Systems in Industry Pieter Van Schalkwyk CEO at XMPRO, Author – Building Industrial Digital Twins, DTC Ambassador, Co-chair for AI Joint Work Group at Digital Twin Consortium ChatGPT and other LLM tools like Claude and Perplexity have captured the minds of individuals. Those who are becoming power users are seeing a significant impact in productivity and efficiency for content research, creation, and curation tasks. There are also those who dabbled a bit, attempted a few novelty questions, or tried to use it as a calculator. When they didn’t get the answer they expected, they assumed it was wrong, wrote it off, and moved on. But that’s a topic for a future post. For those of us who now use it as a daily assistant with content-based tasks and are getting more experienced with asking the right questions (prompting), it has increased the efficiency of knowledge tasks by 10x + . It’s “kinda like” having an intern who can help me research faster, take my bullet notes, and turn them into Flesch-Kincaid Grade 10 level material. This material is neatly formatted in 15-20 words per sentence and 3-5 sentences per paragraph for a skeptical engineering audience. Just like the content you are reading now (; This is great at an individual level, but how does this translate to businesses looking for 10x improvements in business process productivity? One popular solution at the moment is to put “Copilot” capability into every business application and productivity tool. This helps create emails faster, brainstorm presentations, and use natural language to query any data source in the business in real time. Users don’t have to wait for IT to write a report in the ERP system to get what they’re looking for. But in a way, this is just a better mousetrap, or in Henry Ford’s words, faster horses. It is very useful, but like many other tools, this provides incremental productivity gains in a business context. Another solution gaining popularity fast is the concept of generative “agents”. Dr. Andrew Ng , the founding lead of the Google Brain team, and many other AI luminaries are highlighting the potential of Agentic AI to change the way business processes work. Is it possible to replicate the “personal use” scenario using virtual employees, aka “agents,” that can perform tasks humans need help with or that humans find challenging to do or really suck at? It turns out that generative agents can do exactly that, and are really good at it. Sergey Malygin , CEO at SODA.auto, an automotive startup out of the UK, shared an example with me. In their process of generating specification documentation, they’re achieving 80% faster shipping of Feature Library documentation with 90% less human work time. Human engineers still review the specification, which they enjoy more than the painstaking process of creating the content. This is a perfect example of a content agent at work. But not all work in business processes is content work. Most industrial processes are not about content creation and curation, but about using large volumes of content to reason and make decisions based on real-time observations, reflections, and context. The tasks are not content tasks, but decision tasks. We call these agents that use LLMs for this reasoning and decision capabilities “Decision Agents“. The foundational research for how this works is out of Stanford. You can find a link in the resources section at the end of this article. The Untapped Potential of Decision Agents in Industrial MAGS While much of the current market focus on Multi-Agent Generative Systems centers on Content Agents, the real game-changer for industrial applications lies in Decision Agents. Content Agents, which create and curate information using Large Language Models (LLMs), have dominated early examples and discussions. However, these applications often limit themselves to hierarchical or procedural processes, missing the true potential of AI in industry. The key to significant productivity improvements in industrial settings lies in leveraging Decision Agents powered by LLMs. These agents excel at tasks that humans typically struggle with, particularly in consuming and processing vast amounts of information for reflection and decision-making. This capability aligns perfectly with the complex, non-deterministic nature of many industrial processes. In industrial applications, the primary challenge often involves observing real-time inputs and making swift, accurate decisions on the next process step. Unlike the procedural workflows where Content Agents shine, industrial processes are frequently dynamic and unpredictable. The task at hand is not so much about creating or curating content, but about making informed decisions in rapidly changing environments. In the next section I will expand on Content Agents, Decision Agents, and also introduce Hybrid Agents for processes that can benefit from agents with both content and reasoning capabilities. The Triad of AI Agents Reshaping Industrial Operations Multi-Agent Generative Systems are reshaping industrial operations by integrating three distinct types of AI agents: Content Agents, Decision Agents, and Hybrid Agents. Each agent type brings unique capabilities to the table, working in concert to optimize complex industrial processes. This article explores the roles and applications of these agents, highlighting their importance in modern industrial settings. Content Agents: The Information Specialists Content Agents excel at gathering, analyzing, and producing information. They sift through vast amounts of data, create reports, and maintain knowledge bases crucial for industrial operations. These agents often serve as researchers, writers, and information curators within the system. In industrial applications, Content Agents might compile maintenance logs, generate safety protocols, or create technical documentation and specifications for equipment. Typical Roles: Research Specialist Content: Gathers, analyzes, and synthesizes information from diverse sources to create comprehensive research reports. Action: Conducts systematic literature reviews, performs data analysis, and compiles findings into structured reports. Content Creator Content: Produces original, engaging content across various formats (text, visual, audio) tailored to specific audiences. Action: Writes articles, designs infographics, records podcasts, and develops multimedia presentations to communicate ideas effectively. Content Curator Content: Organizes and categorizes existing content, creating structured collections and knowledge bases. Action: Tags and categorizes content, creates thematic collections, and develops content taxonomies to improve information accessibility. Decision Agents: The Strategic Thinkers Decision Agents form the backbone of MAGS in industrial settings, typically comprising 60%-70% of the agent team. These agents specialize in analyzing data, evaluating options, and making informed decisions. They use their deep knowledge of processes, quality standards, and operational metrics to guide actions. In a factory setting, Decision Agents might optimize production schedules, manage resource allocation, or determine maintenance priorities based on equipment performance data. Typical Roles: Decision SME (like Quality Engineer) Knowledge: Possesses deep understanding of quality standards, process optimization techniques, and industry best practices. Decision: Evaluates processes against quality standards and makes decisions on corrective actions. Planners Knowledge: Holds comprehensive knowledge of project management methodologies, resource allocation strategies, and risk assessment techniques. Decision: Determines optimal strategies for resource allocation and prioritizes tasks within projects. Managers Knowledge: Maintains broad knowledge of organizational operations, performance metrics, and strategic management principles. Decision: Makes operational decisions based on performance data and aligns team activities with organizational goals. Hybrid Agents: The Versatile Problem-Solvers Hybrid Agents combine the strengths of both Content and Decision Agents. They can both process information and make complex decisions, making them invaluable for tasks that require a holistic approach. In industrial applications, Hybrid Agents often tackle roles such as Design Creators, Decison Curators, or Auditors. They might develop new production methods, optimize knowledge distribution across departments, or manage complex risk assessment processes. Typical Roles: Design Creator Content: Synthesizes diverse information to create detailed proposals and prototypes for innovative solutions. Decision: Evaluates and prioritizes innovation initiatives based on feasibility, market potential, and organizational alignment. Decision Curators Content: Curates and organizes vast amounts of information into accessible knowledge bases and learning materials. Decision: Determines optimal knowledge distribution strategies and prioritizes knowledge gap remediation efforts. Auditors Content: Compiles and analyzes complex data sets to produce comprehensive risk assessment reports and compliance documentation. Decision: Evaluates risk significance and determines appropriate mitigation strategies and policy changes. Composition of Agent Teams in Industrial Applications In typical industrial processes, the composition of agent teams often follows a 20-70-10 split: 20% Content Agents, 70% Decision Agents, and 10% Hybrid Agents. This distribution reflects the critical nature of decision-making in industrial operations. While information gathering and content creation are essential, the ability to make rapid, accurate decisions based on that information is paramount in industrial settings. The Untapped Potential of Decision Agents in Industrial MAGS Decision Agents excel in their ability to observe, reflect, plan, and act on vast amounts of data, far surpassing human cognitive limits. These agents can continuously monitor real-time inputs from industrial processes while simultaneously accessing and interpreting extensive Retrieval-Augmented Generation (RAG) information, such as standard operating procedures, historical data, and best practices. In a chemical plant scenario, a Decision Agent observes real-time data on reaction conditions, product quality metrics, and equipment performance. It then reflects on this information, comparing it against historical trends and RAG data on optimal process parameters. The agent plans adjustments, considering potential outcomes and aligning with safety protocols and production goals. Finally, it acts by implementing precise, real-time modifications to process parameters, optimizing yield and efficiency beyond human capabilities. For predictive maintenance, Decision Agents observe current equipment performance data and environmental conditions. They reflect on this information alongside historical maintenance records and manufacturer specifications retrieved through RAG. The planning phase involves assessing the urgency of maintenance needs, potential failure modes, and optimal timing to minimize disruption. The agent then acts by scheduling maintenance interventions, balancing the risks of equipment failure against unnecessary downtime. In supply chain management, Decision Agents observe global market trends, inventory levels, production capacities, and logistics data. They reflect on this information in conjunction with RAG data on supplier performance, contractual obligations, and demand forecasts. The planning phase involves complex scenario modeling, considering multiple variables to optimize procurement, production, and distribution strategies. The agent then acts by making data-driven decisions on inventory management, supplier selection, and distribution routing, enhancing supply chain resilience and efficiency. This observe-reflect-plan-act cycle, powered by the integration of real-time data and comprehensive RAG information, enables Decision Agents to make nuanced, context-aware decisions in complex industrial environments. Their ability to process and synthesize information from diverse sources allows for more informed and effective decision-making than traditional rule-based systems or human operators alone. As industries look to implement MAGS, they should focus on developing and deploying sophisticated Decision Agents. While Content Agents have their place, the true transformation in industrial productivity will come from AI that can make complex decisions in dynamic environments. By shifting focus to Decision Agents, companies can unlock new levels of efficiency, adaptability, and innovation in their industrial processes. If this excites you and you want to be part of the next evolution in industrial applications, please reach out to me or Gavin Green , VP Strategic Solutions. We are opening limited pilot opportunities for innovative, agile leaders in the industry. PS – My Prompt for improving the readability of this post Rewrite this section of a LinkedIn article on Multi Agent Generative Systems. Do not change the content and meaning, just improve grammar and readability. Audience: Target senior managers and technical readers Aim for 15-20 words per sentence Aim for 3-5 sentences per paragraph Use the Flesch-Kincaid Grade Level to gauge readability. Aim for a Grade Level of 10 for a skeptical engineering Audience Maintain a Professional Tone that avoid marketing jargon. Use an Active Voice and write in the voice of Pieter van Schalkwyk, CEO at XMPro. Avoid words like “realm”, “poised”, “paradigms”, “pivotal”, “delves”, “cutting-edge”, “elevate”, “unprecedented”, “revolutionize”, “enhance” <article> </article> Article Resources SODA Link: https://soda.auto/blog/soda-multi-agent-systems-mags-implementation/ Simulacra Link: https://arxiv.org/abs/2304.03442"
  },
  "docs/resources/faqs/external-content/blogs/2024/copy-me.html": {
    "href": "docs/resources/faqs/external-content/blogs/2024/copy-me.html",
    "title": "| XMPro",
    "summary": "Blogs: 2024 How to Build Multi-Agent Systems for Industry Why Solving the Problem Doesn’t Solve the Problem: The Importance of Scalable Intelligent Operations with XMPro iBOS Content, Decision, and Hybrid: The Three Pillars of Multi-Agent Systems in Industry Revolutionizing Manufacturing with AI and Generative AI: XMPro’s Intelligent Business Operations Suite The Evolution of Skills: Lessons from Agriculture in the GenAI and MAGS Era Part 1: From Railroads to AI: The Evolution of Game-Changing Utilities Part2: The Future of Work: Harnessing Generative Agents in Manufacturing Bridging Automation and Intelligence: XMPro’s Approach to Industrial Agent Management XMPro APEX: Pioneering AgentOps for Industrial Multi Agent Generative Systems Part 5 – Rules of Engagement: Establishing Governance for Multi-Agent Generative Systems How to Achieve Scalable Predictive Maintenance for Industrial Operations Understanding the Difference Between XMPro AI Assistant and AI Advisor Part 3 – AI at the Core: LLMs and Data Pipelines for Industrial Multi-Agent Generative Systems MAGS: The Killer App for Generative AI in Industrial Applications The Importance of Pump Predictive Maintenance for Operational Efficiency Progressing Through The Decision Intelligence Continuum With XMPro The Value-First Approach to Industrial AI: Why MAGS Implementation Must Start with Business Outcomes New Guide – The Ultimate Guide to Multi-Agent Generative Systems The Ultimate Guide To Predictive Analytics Part 4 – Pioneering Progress | Real-World Applications of Multi-Agent Generative Systems Scaling Multi-Agent Systems with Data Pipelines: Solving Real-World Industrial Challenges"
  },
  "docs/resources/faqs/external-content/blogs/2024/how-to-achieve-scalable-predictive-maintenance-for-industrial-operations.html": {
    "href": "docs/resources/faqs/external-content/blogs/2024/how-to-achieve-scalable-predictive-maintenance-for-industrial-operations.html",
    "title": "How to Achieve Scalable Predictive Maintenance for Industrial Operations | XMPro",
    "summary": "How to Achieve Scalable Predictive Maintenance for Industrial Operations Articles, Blog How to Achieve Scalable Predictive Maintenance for Industrial Operations Posted on November 21, 2024 by Wouter Beneke I often get asked, “Why is scalability such a big deal for predictive maintenance?” At XMPro, we work closely with industrial leaders implementing predictive maintenance solutions to streamline operations and reduce costs. A common theme we encounter is this: scaling these systems isn’t as simple as adding more sensors or plugging in more tools. The real challenge lies in managing the complexity that comes with growth—integrating disparate data sources, avoiding bottlenecks, and maintaining efficiency across a larger operational footprint. \uD83D\uDCA1 Scalable predictive maintenance is about building solutions that grow with your business, without sacrificing efficiency or adding unnecessary complexity. This article will unpack the challenges of scaling predictive maintenance, explain why scalability is so essential, and show how XMPro offers a composable, future-proof alternative to traditional bespoke systems. The Growing Importance of Predictive Maintenance Predictive maintenance has quickly become a cornerstone of asset-intensive industries. By analyzing equipment data and identifying potential failures before they occur, it minimizes downtime, extends asset life, and improves safety. But let’s face it: maintenance strategies that worked yesterday aren’t guaranteed to work tomorrow. Here’s why: 1. Industrial operations &#xNAN;are becoming increasingly complex, with more assets, systems, and processes to monitor. 2. Data volumes &#xNAN;are exploding, with IoT devices generating terabytes of information every day. 3. New technologies &#xNAN;like AI and machine learning are reshaping how maintenance decisions are made. To keep pace, businesses need predictive maintenance solutions that don’t just work today but remain effective as operations evolve. What Makes Scaling Predictive Maintenance So Hard? Let’s break down the key hurdles organizations face when trying to scale predictive maintenance. The Patchwork Approach 1. The Patchwork Approach Most predictive maintenance initiatives start small: one tool for one specific use case. This might be vibration monitoring for a critical pump or temperature monitoring for a furnace. Over time, more tools are added for other assets, each with its own software, data format, and operational quirks. The result? A disconnected patchwork of systems that struggle to work together. The Data Silo Problem 2. The Data Silo Problem Each tool in your maintenance ecosystem generates valuable data—but often, that data is locked within the tool itself. For example: A SCADA system might provide insights into production lines but doesn’t share data with your ERP system. An IoT sensor on a motor might detect anomalies, but its data lives in a separate platform. Without integration, it’s impossible to get a full picture of asset health, let alone make coordinated decisions. Integration Fatique 3. Integration Fatigue As operations expand, integrating new assets, processes, or tools becomes a logistical nightmare. Each new addition requires custom coding, manual configuration, or re-engineering of existing systems. This slows progress and increases costs. The Cost Spiral 4. The Cost Spiral Bespoke solutions may seem cost-effective initially, but as complexity grows, so do maintenance and support costs. A single change—like adding a new piece of equipment—can require weeks of work and tens of thousands of dollars. Why Scalability Matters in Predictive Maintenance When we talk about scalability, we’re not just talking about handling more data or assets. We’re talking about systems that adapt and evolve without requiring constant manual intervention or redevelopment. Here are the key reasons scalability is critical: 1. Unified Data Across Assets and Processes Scalable systems eliminate silos by integrating data from multiple sources into a single platform. This allows operators to: Monitor all assets in real-time from one dashboard. Identify cross-functional patterns, like how production rates impact equipment wear. Make data-driven decisions faster. 2. Seamless Expansion Adding new facilities, equipment, or processes shouldn’t feel like reinventing the wheel. A scalable system allows businesses to: Connect new assets with minimal setup. Expand predictive maintenance to new geographies without duplicating efforts. 3. Support for Emerging Technologies IoT devices, machine learning algorithms, and generative AI are becoming essential tools for predictive maintenance. Scalable systems are built to incorporate these technologies, ensuring they remain relevant as innovation continues. 4. Long-Term ROI A scalable predictive maintenance system doesn’t just reduce costs today—it delivers value for years by avoiding the need for expensive overhauls and integrations. The Pitfalls of Bespoke Systems The Pitfalls of Bespoke Systems It’s easy to see why some businesses turn to bespoke solutions. They promise customization and a tailored fit for specific needs. But as organizations grow, these systems can become a liability. Here’s why: Rigid Architectures Bespoke systems are often built for a narrow set of requirements. Adding new features or adapting to changing needs can require starting from scratch, which is time-consuming and expensive. Lagging Insights Disconnected tools create delays in data collection and analysis. Instead of getting real-time insights, teams are left reacting to problems after they occur. Scaling Frustrations Expanding a bespoke system isn’t just difficult—it’s often impractical. Each new element (asset, sensor, tool) adds layers of complexity that slow down operations. How XMPro Solves the Scalability Challenge At XMPro, we designed our platform to address these challenges head-on. Here’s how we make scalability a reality: 1. Composable Architecture With XMPro, you can build modular solutions that adapt as your needs change. Need to monitor a new asset or process? Just drag and drop the components into place—no coding required. 2. Seamless Data Integration Our platform connects to all your existing systems, from SCADA and IoT devices to ERP and CMMS platforms. This creates a unified data stream that eliminates silos and provides a comprehensive view of asset health. 3. Agent-Based Intelligence XMPro’s Multi-Agent Generative Systems (MAGS) use AI to autonomously analyze data, detect anomalies, and recommend actions. This ensures predictive maintenance systems remain efficient and intelligent as they scale. 4. Rapid Deployment Unlike bespoke systems, XMPro can be deployed in weeks, not months. This means faster time-to-value and less disruption to your operations. Data Stream DesignerApp DesignerRecommendation ManagerXMPro AI Real-World Benefits of Scalable Predictive Maintenance 1. Increased Uptime With real-time insights and proactive alerts, businesses can address potential failures before they cause downtime. 2. Cost Optimization Scalable systems reduce operational inefficiencies, lower maintenance costs, and maximize ROI. 3. Future-Proof Operations By embracing scalability, businesses position themselves to leverage emerging technologies and stay competitive. 4. Improved Collaboration Unified data streams and intuitive dashboards make it easier for teams to work together and share insights. Conclusion Predictive maintenance has the power to transform industrial operations—but only if it can scale with your business. Bespoke systems, while effective in the short term, often create long-term challenges that hinder growth and efficiency. XMPro’s composable, scalable solutions are built for the future. By eliminating silos, simplifying integrations, and leveraging the power of AI, we help businesses achieve predictive maintenance at scale—without the headaches. \uD83D\uDCA1 It’s time to move beyond patchwork solutions. Let’s build something that lasts. Ready to Scale Your Operations? Ready to scale your predictive maintenance? Visit XMPro.com to learn more or schedule a demo with our experts today."
  },
  "docs/resources/faqs/external-content/blogs/2024/how-to-build-multiagent-systems-for-industry.html": {
    "href": "docs/resources/faqs/external-content/blogs/2024/how-to-build-multiagent-systems-for-industry.html",
    "title": "How to Build Multi-Agent Systems for Industry | XMPro",
    "summary": "How to Build Multi-Agent Systems for Industry Articles, Blog How to Build Multi-Agent Systems for Industry Posted on November 29, 2024 by Wouter BenekeHow to Build Multi Agent Systems For Industrial Operations How to Build Multi-Agent Systems for Industry I’ve always been intrigued by how technology drives change in industrial operations. Over the years, I’ve seen how the right technology can transform businesses and solve challenges that once seemed impossible. One of the most promising advancements I’ve come across is the rise of Multi-Agent Systems MAS. These systems consist of multiple interacting agents that can autonomously perform tasks, make decisions, and communicate with each other. In this article, I want to share how you can build Multi-Agent Systems for industrial environments, focusing on XMPro’s capabilities as a leader in this field and why scalability is crucial for industrial applications. Latest Agentic AI Articles Bridging Automation and Intelligence: XMPro’s Approach to Industrial Agent Management The Value-First Approach to Industrial AI: Why MAGS Implementation Must Start with Business Outcomes The Value-First Approach to Industrial AI: Why MAGS Implementation Must Start with Business Outcomes This Scaling Multi-Agent Systems with Data Pipelines: Solving Real-World Industrial Challenges In today’s complex industrial landscape, the ability to process and act on data in real XMPro APEX: Pioneering AgentOps for Industrial Multi Agent Generative Systems XMPro APEX: Pioneering AgentOps for Industrial Multi Agent Generative Systems In today’s rapidly evolving industrial Content, Decision, and Hybrid: The Three Pillars of Multi-Agent Systems in Industry Content, Decision, and Hybrid: The Three Pillars of Multi-Agent Systems in Industry ChatGPT and other Part 4 – Pioneering Progress | Real-World Applications of Multi-Agent Generative Systems Part 4 – Pioneering Progress | Real-World Applications of Multi-Agent Generative Systems July 18, 2024 Part 3 – AI at the Core: LLMs and Data Pipelines for Industrial Multi-Agent Generative Systems Part 3 – AI at the Core: LLMs and Data Pipelines for Industrial Multi-Agent Generative Part2: The Future of Work: Harnessing Generative Agents in Manufacturing Part2: The Future of Work: Harnessing Generative Agents in Manufacturing June 26, 2024 – Originallly What Are Multi-Agent Systems? Multi-Agent Systems are designed to solve complex challenges that a single agent might struggle to tackle alone. Each agent in a MAS can represent a different entity, such as a machine, human operator, or software program. These agents work either independently or collaboratively to achieve specific objectives, making them ideal for industrial scenarios where tasks are often interconnected and demand real-time decision-making. I’ve always found that the beauty of MAS is in their ability to reflect the complexity of real-world industrial processes. Think about it—whether it’s a human operator managing a machine or a software program coordinating with other systems, MAS allow for a level of fluidity and adaptability that really matches the fast-paced nature of industry today. Core Components of Multi-Agent Systems Environment: The setting in which agents operate, which can include physical spaces, data streams, or virtual environments. Agents: The basic building blocks of a MAS. Agents can be software programs or physical entities that perceive their environment and act accordingly. Goals: Each agent usually has specific objectives that contribute to the overall goals of the system. Communication: Agents need effective communication to share information and coordinate actions. This can be achieved through various communication protocols. Why Scalability Matters for Industrial Applications When building Multi-Agent Systems for industrial use, scalability is a key factor. As businesses grow, the systems they use must adapt to increasing complexity and data volume. Here are a few reasons why scalability is critical: Handling Large Data Volumes: Industrial operations generate vast amounts of data. A scalable MAS can process and analyze this data efficiently, enabling quick decision-making. Adapting to Business Changes: As requirements evolve, a scalable system can add new agents and features without needing major overhauls. Improving Operational Efficiency: Scalable systems optimize resource allocation and task management, leading to better productivity and reduced costs. Seamless Integration: A scalable MAS can integrate smoothly with existing systems and technologies, ensuring minimal disruptions. One thing I’ve realized while working with industrial clients is that scalability isn’t just a nice-to-have—it’s make-or-break. If a system can’t grow with your needs, you’re setting yourself up for bottlenecks down the road. I recall working with a manufacturing client who faced massive delays because their existing systems couldn’t handle the increased data load as their operations expanded. It became clear that without scalability, growth can actually hinder progress. With XMPro, we focus on making sure that scalability is built into every layer of the MAS solution, so our clients can grow seamlessly. How XMPro Helps Build Multi-Agent Systems XMPro provides a powerful platform for developing and scaling Multi-Agent Systems. Here’s how users can utilize XMPro’s features to build effective MAS for industrial applications. 1. Defining Objectives: Before building a Multi-Agent System, you need to clearly define the objectives. What problems are you trying to solve? What processes need optimization? XMPro helps users define these goals to ensure every agent’s function aligns with the system’s overall objectives. It might sound basic, but I’ve seen so many projects stumble because the objectives weren’t clear from the start. With XMPro, we emphasize defining goals upfront to make sure every agent has a clear role and purpose within the system. 2. Designing the Agent Architecture: XMPro supports a modular architecture, allowing users to design agents tailored to specific tasks. This is crucial in industrial environments, where different processes require unique approaches. Users can create different types of agents, such as: Content Agents: Utilize Large Language Models (LLMs) to manage information, generate reports, and support compliance. Decision Agents: Make real-time decisions based on data analysis, optimizing operations and resource allocation. Hybrid Agents: Combine both content and decision-making capabilities to handle complex tasks requiring both information management and decision-making. Multi-Agent Generative Systems MAGS: These agents are built with advanced cognitive capabilities, allowing them to learn, adapt, and generate content dynamically. MAGS are designed for industrial-grade performance, handling complex operations with real-time adjustments and seamless inter-agent collaboration. One thing I love about XMPro is the modularity. It gives you the freedom to create agents for specific tasks, and as someone who’s worked on several industrial projects, I can tell you that this flexibility is invaluable. 3. Facilitating Agent Communication: Effective communication is essential for a successful Multi-Agent System. XMPro offers a unified data pipeline that allows agents to share information and coordinate actions effortlessly—a must for industrial settings that require real-time responsiveness. In my experience, this kind of seamless communication is what differentiates a mediocre MAS from a game-changing one. If agents can’t communicate effectively, you’re going to lose the whole point of having an interconnected system. 4. Leveraging Data Pipelines: XMPro’s data pipeline capabilities help users process and analyze large volumes of data efficiently. By integrating data streams from various sources, agents get the most up-to-date information. This is particularly important for industries with continuous data generation, such as logistics and manufacturing. 5. Scaling Your System One of XMPro’s standout features is its cloud-native architecture, making it easy to scale Multi-Agent Systems. As operations expand, users can add agents and functionalities without major disruptions, ensuring that the MAS evolves alongside the business. I’ve seen first-hand how the cloud-native approach helps clients avoid the headaches that come with scalability. No one wants to deal with massive overhauls every time there’s a need to grow—XMPro’s architecture makes it seamless. 6. Monitoring and Optimization: Once a MAS is deployed, continuous monitoring and optimization are vital to maintain its effectiveness. XMPro provides tools for tracking agent performance, analyzing data, and making necessary adjustments. This iterative process helps users refine their systems, enhancing efficiency over time. The ability to monitor and optimize is something we put a lot of focus on. It’s not just about setting up a MAS and letting it run—it’s about continuously improving and making sure it adapts to the evolving needs of the business. What Makes XMPro Multi-Agent Generative Systems MAGS Unique? XMPro’s Multi-Agent Generative Systems MAGS bring advanced features to the table, setting them apart from other MAS solutions. Here’s what makes MAGS a top choice for businesses looking to scale Multi-Agent Systems: 1. Industrial-Grade Architecture MAGS are built with a robust architecture designed to support the complexities of large-scale industrial operations. Unlike traditional agent frameworks, MAGS can handle the demands of real-time data processing and decision-making in dynamic environments. 2. Cognitive Capabilities MAGS use advanced cognitive architectures that enable agents to learn and adapt. This ability to adjust based on past experiences and evolving conditions makes them highly effective for tackling complex industrial problems. 3. Seamless Inter-Agent Collaboration MAGS excel at inter-agent interoperability. XMPro makes sure agents communicate and collaborate smoothly, ensuring coordinated actions that lead to optimal outcomes—an essential feature in industries where multiple agents must work together. 4. Generative AI Integration MAGS incorporate generative AI, which allows agents to create and manage content dynamically. This makes agents more efficient in generating reports, maintaining documentation, and handling compliance, ultimately easing the workload of human operators. 5. Scalability and Flexibility With XMPro’s cloud-native architecture, MAGS can easily scale as business needs evolve. Adding new agents and functionalities is seamless, allowing companies to adapt to changing conditions with minimal disruptions. 6. Real-Time Adjustments MAGS are designed to make real-time adjustments based on data and changing environmental factors. This ensures that agents can respond instantly to new information, improving processes and maximizing efficiency. 7. Support for Hybrid Agents XMPro’s MAGS can support hybrid agents—those capable of content generation as well as decision-making. This versatility allows organizations to manage complex tasks requiring both aspects, significantly boosting operational efficiency. Real-World Use Cases for Multi-Agent Systems Multi-Agent Systems have numerous applications across industries. Here are a few ways XMPro’s platform can be used: Manufacturing MAS can optimize production by coordinating machines, robots, and human operators. For example, content agents can handle inventory while decision agents adjust production schedules based on real-time data. Supply Chain Management MAS can boost supply chain visibility and responsiveness by monitoring shipments, tracking inventory, and coordinating with suppliers and customers. XMPro’s data pipeline capabilities help businesses gain critical insights and make timely decisions. Energy Management In energy, MAS can optimize distribution and consumption. Agents monitor energy usage, predict demand, and adjust supply accordingly—particularly important in managing the variability of renewable energy sources. Smart Cities MAS can support smart city development by managing traffic, monitoring environmental conditions, and optimizing public services. By integrating data from sensors and IoT devices, XMPro helps cities run more efficiently and sustainably. Conclusion Building Multi-Agent Systems for industrial use may be complex, but it pays off in operational efficiency and better decision-making. XMPro provides a scalable platform that allows organizations to design, implement, and manage MAS effectively. Success in industrial operations isn’t just about adopting the latest technology—it’s about finding the right approach that adapts and grows with your needs. Scalability ensures that as your business evolves, your systems keep up and help drive growth rather than hold it back. The real power of XMPro’s MAS and MAGS is in their adaptability. Whether it’s dealing with increased data volumes, integrating new functionalities, or responding in real-time to changes in the environment, XMPro provides a framework that evolves alongside your business. Embracing Multi-Agent Systems with XMPro isn’t just about improving today’s operations—it’s about setting yourself up for long-term success in an ever-changing industrial landscape. When I reflect on the projects I’ve been involved with, the real success comes down to creating solutions that don’t just work today but will continue to evolve and improve as businesses grow. XMPro is here to make sure that happens, helping clients build not just systems, but adaptable, intelligent ecosystems ready for whatever the future holds. When I reflect on the projects I’ve been involved with, the real success comes down to creating solutions that don’t just work today but will continue to evolve and improve as businesses grow. XMPro is here to make sure that happens. Wouter Beneke XMPro Marketing Manager"
  },
  "docs/resources/faqs/external-content/blogs/2024/index.html": {
    "href": "docs/resources/faqs/external-content/blogs/2024/index.html",
    "title": "2024 Blogs | XMPro",
    "summary": "2024 Blogs Articles from the XMPro blog published in 2024. Articles Bridging Automation and Intelligence: XMPro’s Approach to Industrial Agent Management Content, Decision, and Hybrid: The Three Pillars of Multi-Agent Systems in Industry copy-me How to Achieve Scalable Predictive Maintenance for Industrial Operations How to Build Multi-Agent Systems for Industry MAGS: The Killer App for Generative AI in Industrial Applications New Guide – The Ultimate Guide to Multi-Agent Generative Systems Part 1: From Railroads to AI: The Evolution of Game-Changing Utilities Part 3 – AI at the Core: LLMs and Data Pipelines for Industrial Multi-Agent Generative Systems Part 4 – Pioneering Progress | Real-World Applications of Multi-Agent Generative Systems Part 5 – Rules of Engagement: Establishing Governance for Multi-Agent Generative Systems Part2: The Future of Work: Harnessing Generative Agents in Manufacturing Progressing Through The Decision Intelligence Continuum With XMPro Revolutionizing Manufacturing with AI and Generative AI: XMPro’s Intelligent Business Operations Sui Scaling Multi-Agent Systems with Data Pipelines: Solving Real-World Industrial Challenges The Evolution of Skills: Lessons from Agriculture in the GenAI and MAGS Era The Importance of Pump Predictive Maintenance for Operational Efficiency The Ultimate Guide To Predictive Analytics The Value-First Approach to Industrial AI: Why MAGS Implementation Must Start with Business Outcomes Understanding the Difference Between XMPro AI Assistant and AI Advisor Why Solving the Problem Doesn’t Solve the Problem: The Importance of Scalable Intelligent Operations XMPro APEX: Pioneering AgentOps for Industrial Multi Agent Generative Systems"
  },
  "docs/resources/faqs/external-content/blogs/2024/mags-the-killer-app-for-generative-ai-in-industrial-applications.html": {
    "href": "docs/resources/faqs/external-content/blogs/2024/mags-the-killer-app-for-generative-ai-in-industrial-applications.html",
    "title": "MAGS: The Killer App for Generative AI in Industrial Applications | XMPro",
    "summary": "MAGS: The Killer App for Generative AI in Industrial Applications Blog, CEO'S Blog MAGS: The Killer App for Generative AI in Industrial Applications Posted on October 30, 2024 by Pieter van Schalkwyk MAGS: The Killer App for Generative AI in Industrial Applications This article was originally posted to XMPro CEO, Pieter Van Schalkwyk’s blog – The Digital Engineer, here In today’s industrial environments, complexity is increasing at an unprecedented pace. With interconnected systems, IoT devices, and advanced digital monitoring, the sheer volume of data generated daily can easily overwhelm even the most experienced operators. At the same time, industries face a “knowledge exodus” as seasoned experts retire, taking years of accumulated insights and best practices with them. Combine this with rising energy costs, stringent regulatory requirements, and growing pressure to operate sustainably, and it’s clear that traditional approaches to managing operations are falling short. This is where Multi-Agent Generative Systems (MAGS) come in. Leveraging the power of Generative AI, MAGS offers a transformative approach to tackling some of the most pressing industrial challenges. Unlike conventional systems, MAGS reduces the cognitive load on operators, functions with the precision of industrial control systems, and delivers continuous, data-driven improvements through machine intelligence. These capabilities make MAGS not only a valuable tool but a “killer app” for Generative AI in industrial settings. Let’s explore this in more detail. Industrial Challenges 1. System Complexity Today’s industrial systems are more interconnected than ever, with IoT devices, digital monitoring tools, and other sophisticated equipment generating vast amounts of data and alerts. Operators often receive thousands of notifications daily, leading to cognitive overload and a drop in operational effectiveness. This constant influx of data demands a sophisticated approach to prioritize and act on critical information, helping operators maintain control without being overwhelmed. 2. Workforce Knowledge Retention As experienced personnel retire, organizations face a “knowledge exodus” where valuable operational insights and best practices are lost. Traditional knowledge transfer methods are often insufficient, especially as technological advancements outpace the training process. This creates a widening knowledge gap that impacts safety, efficiency, and decision-making capabilities across industrial environments, posing a substantial risk to operations. 3. Intense Operational Pressures Industrial organizations are increasingly pressured to deliver peak performance with limited resources while managing rising energy costs and strict regulatory requirements. According to the International Energy Agency (IEA), industrial energy costs are projected to grow by 3% annually due to market volatility and increased consumption. At the same time, environmental regulations, such as the European Union’s Green Deal, require industries to reduce emissions by 55% by 2030. Failing to comply with these standards can lead to substantial financial penalties and reputational damage, and inefficient resource use directly affects profit margins. These operational demands require organizations to achieve higher efficiency, stay compliant, and minimize resource consumption, often with leaner teams. Maintaining productivity under these pressures has become crucial for industrial competitiveness and long-term sustainability. How MAGS Solves These Challenges Multi-Agent Generative Systems (MAGS) provide an impactful application of Generative AI tailored to tackle these critical industrial challenges in three core ways: 1. Reduces Cognitive Load Industrial operators face an overwhelming cognitive burden in today’s complex operations, with systems generating over 1,000 daily alarms per operator—up dramatically from the historical norm of 60-100 alarms. Modern control rooms require operators to monitor up to 7 screens simultaneously while tracking physical equipment and digital systems, creating an unsustainable cognitive load. Research shows operator performance drops significantly after just 25-30 alarms in a 10-minute period, highlighting the critical need for a new approach. A Human Operator Optimizing Maintenance Schedules for Liner replacements on Secondary Crushes in Mining to avoid unplanned downtime due to liner wear MAGS addresses this challenge by deploying teams of virtual workers that autonomously handle routine tasks within defined Safe Operating Envelopes. These AI agents work as a coordinated team to monitor and autonomously manage operational business processes and maintain operational logs—all while strictly adhering to predefined safety parameters. A Three-Agent MAGS Team Optimizing Maintenance Schedules for Liner replacements on Secondary Crushes in Mining to avoid unplanned downtime due to liner wear. ATLAS is the team name and stands for Automated Team for Liner Analysis and Scheduling This transformation allows human operators, planners, and managers to focus their expertise on high-value activities like complex problem-solving, root cause analysis, and strategic planning rather than being overwhelmed by routine operational tasks. 2. Functions Like Industrial Control Systems MAGS implements a sophisticated control loop through the Observe, Reflect, Plan, and Act (ORPA) cycle, mirroring the proven principles of traditional industrial control systems. Each agent operates with specific Objective Functions that define its goals and constraints, much like PID controllers and other industrial control elements. The agents continuously observe real-time data streams, reflect on this data against their defined objectives and historical patterns, plan optimal adjustments using a combination of rule-based and generative AI algorithms, and act autonomously within clearly defined Safe Operating Envelopes. XMPro DataStream with real-time operations data for MAGS Agent in a controlled pipeline This systematic closed-loop approach ensures consistent, measurable performance improvements that can be validated against specific Objective Functions, such as minimizing energy consumption while maintaining production targets. The familiar control system architecture allows automation engineers and subject matter experts to manage complex operational business processes using the same methodologies and rigor they apply to automation and control systems, but now extended beyond traditional equipment control to encompass broader operational optimization. 3. Enables Machine Intelligence (MIMO) MAGS enhances machine intelligence by deploying specialized AI agents that analyze and optimize millions of data points daily through a Machine Intelligence for Manufacturing and Operations (MIMO) framework based on the research at MIT, for example. These agents work as an autonomous team to continuously process millions of monitoring events daily, simultaneously optimizing hundreds of variables across multiple machines and processes – a scale of complexity far beyond human cognitive capabilities. What makes this approach particularly powerful is the agents’ ability to d_iscover novel operational configurations that human operators might never consider_. For example, by analyzing complex interactions between equipment settings, environmental conditions, and production parameters, MAGS can identify counterintuitive but highly efficient operating states that dramatically improve performance while reducing energy consumption and maintenance needs. Human oversight in controlled testing environments can validate these novel new approaches before deploying them to production systems. Interact with Team or individual agents and provide instructions, suggestions, and training These agents autonomously monitor and adjust variables in real-time, managing everything from equipment performance to predictive maintenance and quality control, creating a self-organizing system that enables complex, scalable decision-making and optimization. The result is a continuously optimizing environment that delivers measurable efficiency improvements worth millions in annual savings, adapting to changing conditions without requiring constant human supervision while consistently finding optimal solutions that challenge traditional operating assumptions. Call to Action MAGS represents a transformative leap in industrial operations, delivering three critical advantages: Reduces cognitive load, enabling operators to focus on strategic decisions Provides a control system approach for reliable, measurable performance Drives continuous improvement through machine intelligence and optimized processes As industrial systems grow more complex and the pressure for operational efficiency increases, organizations need innovative solutions to address their operational challenges. MAGS offers a powerful approach that combines proven control system principles with advanced AI capabilities to transform industrial operations. This is why I see MAGS as the killer app for Generative AI in industrial applications. If you face these challenges in your operations and want to explore how MAGS can help, don’t hesitate to get in touch with me to discuss a pilot project. Our GitHub Repo has more technical information if you are interested. You can also contact myself or Gavin Green for more information. Read more on MAGS at The Digital Engineer PS. While I experimented with AI assistance in drafting my articles, I found the MAGS concept too novel for current AI systems to grasp effectively. All ideas and insights shared here are my original work. I used Claude AI for editorial improvements, following my structured guidelines for clarity, readability, and professional tone. Improve this paragraph of the LinkedIn article using: Audience: Target senior managers and technical readers Aim for 15-20 words per sentence Aim for 3-5 sentences per paragraph Use the Flesch-Kincaid Grade Level to gauge readability. Aim for a Grade Level of 10 for a broader business audience. Maintain a Professional, friendly and firm tone like that of Pieter van Schalkwyk, CEO of XMPro Avoid marketing jargon. Avoid words like \"realm\", \"poised\", \"paradigms\", \"pivotal\", \"delves\", \"cutting-edge\", \"elevate\", \"unprecedented\", \"revolutionize\", \"enhance\", \"groundbreaking\" Use bullets or numbers in the texts where appropriate"
  },
  "docs/resources/faqs/external-content/blogs/2024/new-guide--the-ultimate-guide-to-multiagent-generative-systems.html": {
    "href": "docs/resources/faqs/external-content/blogs/2024/new-guide--the-ultimate-guide-to-multiagent-generative-systems.html",
    "title": "New Guide – The Ultimate Guide to Multi-Agent Generative Systems | XMPro",
    "summary": "New Guide – The Ultimate Guide to Multi-Agent Generative Systems Blog, Ebooks, Guides / How To's, Resources New Guide – The Ultimate Guide to Multi-Agent Generative Systems Posted on October 23, 2024 by Wouter Beneke The Ultimate Guide to Multi Agent Generative Systems \uD83D\uDCD8 XMPro Presents: The Ultimate Guide to Multi-Agent Generative Systems (MAGS) Multi-Agent Generative Systems (MAGS) are reshaping how industries solve complex problems in real time. This guide explains the core components of MAGS and how they enable intelligent agents to collaborate and adapt autonomously. Learn how organizations are scaling MAGS in dynamic environments to improve decision-making and optimize workflows. With practical insights on architecture, scalability, and ethical considerations, this guide offers a deep dive into the future of AI-driven operations. Discover XMPro’s unique approach to integrating MAGS into industrial processes. Read the Ultimate Guide #AI #MAGS #DigitalTransformation #IntelligentOperations #XMPro #Automation #DecisionMaking #AgenticOperations #AgentOpsPlatform #MultiAgentGenerativeSystems"
  },
  "docs/resources/faqs/external-content/blogs/2024/part-1-from-railroads-to-ai-the-evolution-of-gamechanging-utilities.html": {
    "href": "docs/resources/faqs/external-content/blogs/2024/part-1-from-railroads-to-ai-the-evolution-of-gamechanging-utilities.html",
    "title": "Part 1: From Railroads to AI: The Evolution of Game-Changing Utilities | XMPro",
    "summary": "Part 1: From Railroads to AI: The Evolution of Game-Changing Utilities Articles, Blog, CEO'S Blog Part 1: From Railroads to AI: The Evolution of Game-Changing Utilities Posted on July 23, 2024 by Pieter van Schalkwyk Part1: From Railroads to AI: The Evolution of Game-Changing Utilities Part1: From Railroads to AI: The Evolution of Game-Changing Utilities Pieter Van Schalkwyk CEO at XMPRO, Author – Building Industrial Digital Twins, DTC Ambassador, Co-chair for AI Joint Work Group at Digital Twin Consortium June 4, 2024 – Originallly posted on Linkedin The technological advancements that have shaped our world can often be traced back to a few pivotal innovations, each transforming society in profound ways. The creation of railroads, for instance, revolutionized transportation and commerce, paving the way for global trade. The advent of electricity powered unprecedented industrial and domestic growth, transforming the way we live and work. The emergence of the internet has interconnected the globe, reshaping economies and revolutionizing communication. Today, we stand on the cusp of another monumental shift with Generative AI. Much like its predecessors, Generative AI is set to redefine industries, create new economic opportunities, and improve the quality of life while also presenting challenges and requiring significant investment. In this article, I explore how Generative AI mirrors the transformative impact of railroads, electricity, and the internet and why it is poised to become the next game-changing utility driving our future. The History of Railroads: Building the Foundation of U.S. Economic Development I had the opportunity to visit the National Railway Museum in York, UK, and I was reminded of the ambition to create rail networks at scale and its impact on the world’s largest economies. The development of the railroad system in North America during the 19th and early 20th centuries laid the critical infrastructure that would drive economic growth and integration across the United States. At the peak of railway expansion, there were hundreds of railroad companies, each contributing to the construction of an extensive network that would eventually consolidate into the major rail systems we recognize today. The “Rocket” in the National Railway Museum in York, UK During this period, the railroad companies were actively involved in building and operating rail lines. These included prominent names like the Atchison, Topeka and Santa Fe Railway, Burlington Northern Railroad, Pennsylvania Railroad, and New York Central Railroad, among many others. The competitive landscape was intense, with numerous smaller and larger companies laying tracks across the vast expanse of North America. This massive undertaking was not without its challenges. The financial requirements were enormous, necessitating innovative financing methods such as issuing stocks and bonds and establishing investment banks. Labor shortages and engineering obstacles, particularly in difficult terrains like the Sierra Nevada mountains, tested the limits of the workforce, which included thousands of immigrant laborers from countries such as Ireland, China, and Italy. Despite these hurdles, the construction of the railroad network profoundly impacted the U.S. economy. It facilitated westward expansion and the settlement of the American West, integrating domestic markets that were previously isolated. This integration allowed for more efficient reallocation of resources, boosting overall productivity and spurring the growth of manufacturing and heavy industry. The railroads also transformed agriculture by enabling farmers to ship products to distant markets, driving growth in the agricultural sector. Furthermore, the railroad system catalyzed the development of the financial system, as the massive capital requirements led to innovations in finance that helped establish modern financial markets. While many of the original railroad companies did not survive due to consolidation and financial difficulties, their legacy is undeniable. The rail network they built became a core utility that enabled the United States to become an economic powerhouse, improving the quality of life for millions and creating new business opportunities and models. The story of the railroads is a testament to the transformative power of infrastructure and innovation, a pattern we now see emerging with Generative AI. The Influence of the Union Pacific-Central Pacific Line: An Analogy to Generative AI Leaders The construction of the Union Pacific-Central Pacific transcontinental railroad line was a monumental achievement that profoundly influenced the development of other railroads across the United States. Midjourney’s representation of Union Pacific-Central Pacific This pioneering project shares several parallels with the contemporary efforts of leading companies like OpenAI, Google, and Anthropic in the field of Generative AI. Here’s how the impact of this historic rail project mirrors the ongoing advancements in Generative AI: Demonstrating Feasibility of Large-Scale Projects The successful completion of the 1,775-mile transcontinental railroad, despite immense challenges, demonstrated that ambitious large-scale infrastructure projects were achievable with the right financing, organization, and labor force. Similarly, the groundbreaking work of OpenAI, Google, and Anthropic showcases the potential and feasibility of large-scale AI projects. Their achievements in developing sophisticated AI models have proven that transformative AI technologies are within reach with sufficient resources and expertise. Spurring Competition and Expansion The Union Pacific-Central Pacific project sparked intense competition among other railroad companies, driving a frenzy of construction to connect to the transcontinental route and access lucrative government subsidies. For Generative AI, the advancements made by leading companies have spurred competition, encouraging other organizations to accelerate their own AI development efforts. This competitive environment fosters innovation and rapid progress in the field. Establishing Technological Standards The transcontinental railroad set important technological standards such as rail gauge, construction methods, and operating procedures that were widely adopted for efficiency and interconnectivity. Similarly, the pioneering work in Generative AI by companies like OpenAI, Google, and Anthropic establishes key technological benchmarks and best practices. These standards guide the broader AI community, ensuring compatibility and fostering collaborative advancements. Facilitating Expansion and Integration By opening up western territories for settlement and economic exploitation, the transcontinental railroad created demand for additional regional rail lines. This facilitated the further expansion of the rail network across the West. In the world of AI, foundational advancements by leading companies create opportunities for further research and development. Their work paves the way for new applications and integrations, expanding the reach and impact of AI technologies. Fueling Economic Growth and Innovation The transcontinental railroad revolutionized transportation, catalyzing economic growth, industrialization, and urbanization across the United States. This created immense demand for more rail infrastructure to support the booming economy. Similarly, Generative AI is poised to drive significant economic growth by transforming industries, creating new business opportunities, and enhancing productivity. The foundational work by leading AI companies is crucial in unlocking this potential. In essence, the Union Pacific-Central Pacific project paved the way for the explosive growth of the railroad industry, knitting together the nation through an expansive rail network. Likewise, the pioneering efforts of OpenAI, Google, and Anthropic in Generative AI are setting the stage for a future where AI becomes an integral utility, driving innovation, economic growth, and societal advancement. The Internet Boom of the 1990s: Building the Digital Railways of Today The Internet boom of the 1990s, often referred to as the “dot-com bubble,” was a period of intense growth and investment in the emerging digital infrastructure. This era saw massive projects to lay the foundational networks of the internet, including extensive fiber optic cable installations, some even spanning oceans. Despite the subsequent market crash that resulted in significant financial losses, the investments made during this time were pivotal in creating the modern internet, which today functions as an essential utility, much like the railroads did in the past. What ChatGPT 4.o thinks of the Internet 1.0 that made the Internet of Today possible Here’s how the internet’s development mirrors the historical growth of the railroads: Massive Infrastructure Projects During the 1990s, companies invested heavily in building the physical infrastructure necessary for a global internet. This included laying thousands of miles of fiber optic cables, both on land and under the sea, to create a network capable of high-speed data transmission across the globe. Similarly, the construction of the railroad system in the 19th century involved extensive track laying, bridge building, and tunneling to connect distant parts of the country. Demonstrating Feasibility and Sparking Investment Just as the completion of the transcontinental railroad demonstrated the feasibility of large-scale transportation projects, the successful implementation of early Internet infrastructure showed that a global digital network was achievable. This spurred further investment and innovation, much like the railroad projects had attracted additional capital and technological advancements. Facilitating Connectivity and Economic Growth The railroads facilitated the movement of goods and people, knitting together previously isolated regions and boosting economic growth. In a similar fashion, the Internet-connected businesses, consumers, and information networks worldwide, leading to a new era of economic expansion and creating entirely new industries such as e-commerce, digital media, and online services. Setting Standards and Encouraging Expansion The railroads established standards in track gauge and operational protocols, which were adopted by other rail companies to ensure interoperability. Similarly, the development of internet protocols (like TCP/IP) and technologies (such as HTTP and HTML) set standards that allowed different networks and devices to communicate seamlessly, encouraging widespread adoption and expansion of the internet. Boom and Bust Cycles Both the railroad expansion and the internet boom experienced speculative bubbles. The rapid growth of railroads in the 19th century and the internet in the 1990s led to overinvestment and speculative financing. The bursting of the dot-com bubble in 2000 and the financial difficulties many early rail companies faced resulted in significant financial losses. However, the infrastructure built during these booms laid the groundwork for long-term growth and development. Lasting Impact and Utility Despite the financial fallout from the speculative bubbles, both the railroads and the internet became critical utilities. The railroad network became the backbone of transportation and commerce in the United States, while the Internet has become an essential utility for communication, information, and commerce worldwide. Both infrastructures have fundamentally transformed how societies function, and economies grow. OpenAI ChatGPT: Opening Our Eyes to the Utility and Impact of Generative AI The advent of OpenAI’s ChatGPT has marked a significant milestone in the artificial intelligence journey, illustrating the transformative potential of generative AI in our daily lives. Much like the historical breakthroughs of the railroads, electricity, and the internet, ChatGPT has shown how generative AI can become a core utility that drives economic growth, enhances the quality of life, and opens up new avenues for innovation. It has made AI accessible and useful, and changed what we are enabled to do now. Generative AI vs. Blockchain: Distinguishing New Utility from Enhanced Functionality I see that Generative AI and blockchain technology are sometimes compared as hyped technologies that won’t deliver on their promises. Here is why I think this is not the case and you can’t compare the two: Generative AI introduces a new way of working by automating creative and productive tasks, personalizing user experiences, and improving decision-making through data analysis and predictive capabilities. It provides instant access to expertise, scales to meet large demands, and automates routine tasks, freeing up time for innovation and driving productivity across various industries. Generative AI enhances human capabilities, opening up new possibilities for creativity and problem-solving, and offers immediate, tangible benefits that transform how we work and interact with technology. It provides utility! In contrast, blockchain primarily enhances existing internet utilities by offering improved security, transparency, and decentralization. It provides secure, trustless transactions and supports decentralized applications and smart contracts, which automate and enforce agreements without intermediaries. However, blockchain’s impact is more targeted, focusing on specific use cases like financial transactions and record-keeping. It tried to enable Web 3.0. There is no new utility, just a better mousetrap. While valuable, blockchain does not offer the broad, transformative utility of Generative AI. Generative AI’s versatility and immediate benefits make it a groundbreaking technology that fundamentally changes productivity and creativity, much like the historical breakthroughs of railroads, electricity, and the internet. Why I’m Excited About Generative AI As I reflect on the transformative potential of generative AI, I am genuinely excited about its profound impact on our future. This new utility promises to improve our quality of life and bring about significant advancements in ways that parallel the revolutionary developments of railroads, electricity, and the internet. Generative AI represents a leap forward in how we interact with technology. It automates previously labor-intensive tasks and enables new forms of creativity and productivity. Its ability to generate human-like text, create art, compose music, and even develop code opens up a world of possibilities that were unimaginable just a few years ago. This technology will streamline workflows, personalize experiences, and provide instant access to vast amounts of information, making our lives more efficient and enriched. Moreover, the economic implications of generative AI are vast. By driving innovation across industries, it will create new business models, enhance productivity, and spur economic growth. Just as the railroads connected distant regions and the internet brought the world closer together, generative AI will break down barriers and foster a more interconnected and dynamic global economy. I fully acknowledge that there will be challenges and setbacks along the way. Many companies will not succeed, and there will be economic losses as the market adjusts to this new technology. However, these failures are part of the growth process. We are laying down the infrastructure that will power our industrial and personal futures, much like the railroads and the internet did in their time. The investments we make today in generative AI will pave the way for future innovations and societal advancements. Beyond its economic impact, generative AI promises significant societal benefits. It will democratize access to knowledge and expertise, offering personalized education, healthcare, and support services that cater to individual needs. This technology has the potential to bridge gaps in education and healthcare, providing equal opportunities and improving the overall quality of life for people around the world. I am excited about generative AI because it represents a new utility that will revolutionize our daily lives, drive economic growth, and bring about meaningful societal improvements. Despite the inevitable challenges and economic losses, the infrastructure we are building today will shape a future where innovation and progress unprecedentedly enhance the human experience. In part 2 – The Future of Work: Harnessing Generative Agents in Manufacturing, I share why I am excited about the utility of Agentic AI based on Generative AI."
  },
  "docs/resources/faqs/external-content/blogs/2024/part-3--ai-at-the-core-llms-and-data-pipelines-for-industrial-multiagent-generative-systems.html": {
    "href": "docs/resources/faqs/external-content/blogs/2024/part-3--ai-at-the-core-llms-and-data-pipelines-for-industrial-multiagent-generative-systems.html",
    "title": "Part 3 – AI at the Core: LLMs and Data Pipelines for Industrial Multi-Agent Generative Systems | XMPro",
    "summary": "Part 3 – AI at the Core: LLMs and Data Pipelines for Industrial Multi-Agent Generative Systems Articles, Blog, CEO'S Blog Part 3 – AI at the Core: LLMs and Data Pipelines for Industrial Multi-Agent Generative Systems Posted on July 24, 2024 by Pieter van Schalkwyk Part 3 – AI at the Core: LLMs and Data Pipelines for Industrial Multi-Agent Generative Systems Part 3 – AI at the Core: LLMs and Data Pipelines for Industrial Multi-Agent Generative Systems Pieter Van Schalkwyk CEO at XMPRO, Author – Building Industrial Digital Twins, DTC Ambassador, Co-chair for AI Joint Work Group at Digital Twin Consortium July 14, 2024 – Originallly posted on Linkedin In Part 1, “From Railroads to AI: The Evolution of Game-Changing Utilities,” I shared my thoughts on why GenAI will become a “utility” like electricity and why we have an opportunity to create “appliances” that will change the way we work and improve the quality of life for everyone. In Part 2, “The Future of Work: Harnessing Generative Agents in Manufacturing,” I explained that skills shortages and gaps, productivity challenges, and the complexity of doing business at the speed of thought is forcing us to look at alternate, novel ways to do work and have a “Technology Enabling Focus” as Michael Carroll so accurately commented. Utilizing technology to foster focus instead of allowing it to become a distraction or creating additive sickness. This means avoiding constant alerts from dashboards and warnings from analytics that divert attention away from meaningful work – Mike Carroll As I was writing Part 3 on the architecture of Multi Agent Generative Systems (MAGS) with examples from applications with XMPro, I realized that I first have to explain “What” the reason is for using agents in the first place, that it is not just writing procedural code with some LLM capabilities baked in. So this became Part 3, with Part 4 following to show the architecture and “How” to build an XMPro MAGS agent team with some examples. I also decided to add Part 5, which will discuss governance, ethical use, trustworthiness, and some thoughts on managing change in organizations planning to use MAGS with the help of Zoran Milosevic Through parts 3 and 4, I explain XMPro MAGS using an Industrial Predictive Maintenance Agent Team and will explore the anatomy of this Reliability and Root Cause Agent as an example. Figure 1 – XMPro MAGS Reliability and Root Cause Agent in a Data Pipeline The following key topics will guide the discussion “Chain of Thought” in this paper: Understanding Industrial Processes: What distinguishes industrial processes from others, and how do the agents within these systems operate differently? Employing LLM-based Agents: How can Large Language Models (LLMs) be utilized within industrial processes? Data Pipelines in Industrial Systems: Why are data pipelines crucial for assembling industrial MAGS? In part 4 “Pioneering Progress: Real-World Applications of Multi-Agent Generative Systems” I will continue and expand on: Anatomy of MAGS: What are the structural and functional components of MAGS? Examples of XMPro MAGS Applications: What are some real-world applications of XMPro MAGS in industrial contexts? In my career, I’ve worked with both industrial and non-industrial operational business processes. Here is why I think industrial processes are different and why their agent teams are different. Understanding the Uniqueness of Industrial Processes Operational processes in industrial settings markedly differ from typical business workflows found in non-industrial environments. The operational impact of industrial processes often results in safety issues or significant loss, and decisions are made on real-time operations intelligence through SCADA, IoT, Digital Twins, and other operational technology applications. Traditional business processes are mostly deterministic, designed to follow a predetermined workflow that ensures consistency and efficiency. Such processes execute tasks based on “If-then-else” logic, where every step is predefined and structured to enable consistent performance, even by the least skilled personnel. This model is great for Robotic Process Automation (RPA), where tasks are clearly defined, and outcomes are predictable. In the image below, I asked Claude to create a Mermaid diagram of the agent process that typical GenAI Agent frameworks like CrewAI, Autogen, and LangChain follow: Figure 2 – Typical Task-based LLM Agent These GenAI Agent frameworks are tailored to support defined workflow tasks by generating content, curating, and reviewing within these deterministic processes. These frameworks enhance efficiency by automating routine content generation and curation processes governed by set rules and procedures. Creating the Mermaid diagram is an example of a typical deterministic content generation task that I can assign to one of these agents to improve my efficiency. “The Decision is the Task” for Many Industrial Processes In contrast, operational business processes in industrial applications rely on dynamic and complex data inputs. They generally use real-time operational and sensor data that frequently changes, requiring integrating this data with recent contextual information. This information is synthesized and presented to Subject Matter Experts (SMEs) such as Operations Managers, Reliability Engineers, or Quality Supervisors. These subject matter experts use detailed data, combined with their skills and available documentation, to make informed decisions. It is most often presented in a dashboard to provide the information and context. Their decision-making further involves causal analysis to determine why (Knowing What Causes What, and When – more on this in the future) before taking any action. We describe these as Emergent Processes, where the next best steps emerge based on the SME’s ability to observe, reflect, plan, and determine actionable options, making these processes fundamentally adaptive and responsive to the changing environment. MAGS is an example of a GenAI Agent framework that mimics these SME actions. It supports emergent, non-deterministic processes that must handle far more complex and dynamic decision-making than its deterministic counterparts. MAGS’ primary focus is to use advanced reasoning processes to recommend actions, and in fully automated industrial environments, potentially execute these actions directly. In such scenarios, the SME assumes a critical oversight role as the ‘Human On The Loop’, ensuring that the automated actions align with strategic objectives and operational safety standards. This shift emphasizes the framework’s ability to operate independently while still under expert governance, enhancing efficiency without compromising control or quality. What is MAGS? Multi Agent Generative Systems (MAGS) are systems that consist of multiple agents, each with a defined role, working together to deconstruct and solve complex problems. These systems mimic the hierarchical structure of enterprises and mirror real-world team dynamics. They can also scale intelligently by adding more agents to handle increased workloads. Each agent in the system _handles modular decision-making steps_that contribute to the overall problem-solving process in a composable way. The agents can further collaborate and cooperate to achieve defined goals without human intervention. Inspired by this foundational research at Stanford, Generative Agents: Interactive Simulacra of Human Behavior, XMPro MAGS enables a collaborative environment where multiple agents share insights, learn from each other, and make more informed and contextually relevant decisions. XMPro MAGS has the ability to observe, reflect, memorize, learn, plan, and act using LLMs to simulate the same behavior human SMEs use to make operational decisions. From Coding to Commanding: Leveraging LLMs for Enhanced Decision-Making in Industrial Applications A common application for existing GenAI Agents, including platforms like CrewAI, Autogen, and LangChain, is optimizing the software development process. These GenAI agents enhance efficiency by specializing in distinct roles within the project workflow. Project Manager Agents break projects into manageable subtasks, Coding Agents handle the actual development of software, Testing Agents are responsible for quality assurance, and Documentation Agents create comprehensive user guides. This specialization streamlines the entire development process, from conception to completion, ensuring each phase meets high standards of quality and efficiency. Here, large language models primarily automate tasks like coding and testing, focusing on individual steps without integrating them into broader decision-making processes. Conversely, Multi-Agent Generative Systems (MAGS) use LLMs for complex decision-making in industrial settings. For example, an Expert OEE MAGS, which is explained in more detail later, actively improves manufacturing efficiency by deploying specialized AI agents focusing on availability, performance, quality monitoring, and predictive maintenance. These agents can reason and decide on actions based on their observations, reflections, memory, and planning, as shown in Figure 3. They dynamically optimize operations and adapt to evolving industrial challenges by continuously analyzing real-time data. Figure 3 – XMPro MAGS Agent Observe, Reflect, Plan, Act In systems like the Expert OEE Optimizer, MAGS monitors equipment performance, analyzes data to uncover patterns, devises optimization strategies, and executes these plans, significantly impacting operational outcomes. The primary value of LLMs in MAGS lies not just in performing tasks, but in driving sophisticated reasoning processes that enhance decision-making and operational efficiency. This contrast highlights significant differences in LLM utilization: Task vs. Decision-Making Focus: In typical non-industrial GenAI agent applications, LLMs enhance task execution. In industrial settings, they support a cycle of observation, reflection, planning, and action, directly influencing strategic, tactical, and operational decisions. Integration and Impact: Non-industrial LLM GenAI agent integration is often compartmentalized, focusing on specific tasks. Industrial LLMs work holistically, with agents interacting and adapting based on collective insights, boosting system intelligence and responsiveness. Scope of Automation: While non-industrial automation targets reducing manual labor in routine tasks (like me drawing the Mermaid diagram), industrial automation through MAGS aims to optimize entire processes, employing GenAI to create effective decision intelligence. Another key distinction between traditional GenAI Agent frameworks for task-based processes and Multi-Agent Generative Systems (MAGS) lies in their development and deployment methodologies. Task-oriented agent frameworks typically employ a procedural, coded approach that defines LLM content tasks and sequences. In contrast, MAGS, particularly those developed with XMPro, favor a visual data pipeline approach. This method enables an agile and flexible composition of work teams and agents, enabling more dynamic interaction and rapid adaptation to changing conditions in the operational environment. Why Data Pipelines are Superior to Traditional Procedural Approaches in MAGS Traditional task-based Generative AI (GenAI) agents primarily employ procedural coding to execute predefined tasks in a sequence defined by a controller or “supervisor,” as shown in the Mermaid flow diagram in Figure 2. Some platforms assign the “CEO” role to the orchestration functions. While effective for straightforward content generation and curation tasks, this approach struggles in dynamic and complex decision-making applications typical of industrial settings. In contrast, data pipelines or data streams offer a transformative approach to building MAGS. Unlike procedural coding, data pipelines utilize a fluid, dynamic method that allows for continuous data integration. This integration ensures decisions are based on the most up-to-date and relevant information, which is critical in fast-paced industrial environments. The flexibility of data pipelines allows for modular changes with minimal disruption to the overall system, facilitating easier adaptation to evolving requirements and conditions. The real power of using data pipelines in MAGS centers on their ability to effectively employ LLMs for complex reasoning and decision-making. Rather than just automating tasks, LLMs in these pipelines can observe data and patterns, reflect, learn, plan, act, and collaborate within a governance framework or “Rules of Engagement”. This capability significantly improves decision-making by allowing the integration of diverse data sources and tools to contextualize decision data. It facilitates the incorporation of physics-based, statistical, and traditional AI models, anchoring MAGS in practical and empirical reality. This approach ensures that decisions are data-driven and realistically applicable, grounded in physics and reality, greatly enhancing the system’s reliability and effectiveness. Furthermore, the use of visual, drag-and-drop interfaces in constructing data pipelines offers substantial advantages in terms of usability and explainability. These interfaces allow users to visually map out and modify data flows, making the system’s operations transparent and comprehensible even to those without deep technical expertise. This visual aspect not only simplifies the monitoring and debugging processes but also helps maintain organizational governance. By implementing data pipelines, organizations can build MAGS that are not only more efficient and adaptive but also easier to manage and scale. The visual building tools and modular, composable design of data pipelines reduce development time and increase consistency across different applications. The ability to visually represent and explain the decision-making process further enhances the value of MAGS, providing clarity and insight into how decisions are derived, which is invaluable for continuous improvement and alignment with business objectives. While traditional GenAI agents are constrained by the limitations of procedural coding, data pipelines provide a robust framework for developing advanced MAGS. This framework supports dynamic data integration, sophisticated reasoning capabilities, and a visual, modular approach to system design, making it ideally suited to the complexities and demands of industrial applications. In part 4, “Pioneering Progress: Real-World Applications of Multi-Agent Generative Systems,” I continue and show the anatomy of an XMPro MAGS agent team that is built and deployed on this data pipeline and use two examples to demonstrate the capabilities of these agent applications. If this excites you and you want to be part of the next evolution in industrial applications, please reach out to me or Gavin Green , VP Strategic Solutions. We are opening limited pilot opportunities for innovative, agile leaders in the industry."
  },
  "docs/resources/faqs/external-content/blogs/2024/part-4--pioneering-progress--realworld-applications-of-multiagent-generative-systems.html": {
    "href": "docs/resources/faqs/external-content/blogs/2024/part-4--pioneering-progress--realworld-applications-of-multiagent-generative-systems.html",
    "title": "Part 4 – Pioneering Progress | Real-World Applications of Multi-Agent Generative Systems | XMPro",
    "summary": "Part 4 – Pioneering Progress | Real-World Applications of Multi-Agent Generative Systems Articles, Blog, CEO'S Blog Part 4 – Pioneering Progress | Real-World Applications of Multi-Agent Generative Systems Posted on July 24, 2024 by Pieter van Schalkwyk Part 4 – Pioneering Progress | Real-World Applications of Multi-Agent Generative Systems Part 4 – Pioneering Progress | Real-World Applications of Multi-Agent Generative Systems Pieter Van Schalkwyk CEO at XMPRO, Author – Building Industrial Digital Twins, DTC Ambassador, Co-chair for AI Joint Work Group at Digital Twin Consortium July 18, 2024 – Originallly posted on Linkedin Welcome to Part 4 of our series, where we explore the real-world applications of Multi-Agent Generative Systems (MAGS) in industrial settings. This installment, “Pioneering Progress | Real-World Applications of Multi-Agent Generative Systems,” shifts our focus from theory to the tangible implementation of these advanced systems. Before we dive deeper, let’s briefly revisit where we’ve been In Part 1, “From Railroads to AI The Evolution of Game-Changing Utilities,” I discussed how Generative AI (GenAI) is becoming as fundamental as electricity in our lives, creating opportunities for innovative applications across various sectors. Read Part 1 Here Part 2, “The Future of Work Harnessing Generative Agents in Manufacturing,” addressed the critical skills shortages and productivity challenges that are reshaping how work is conducted in manufacturing environments. Read Part 2 Here In Part 3, “AI at the Core LLMs and Data Pipelines for Industrial Multi-Agent Generative Systems,” I covered the architecture of MAGS, focusing on why and how these systems use agents to enhance industrial processes. Read Part 3 Here Now, let’s delve into how XMPro leverages its established infrastructure to deploy MAGS effectively, enhancing operational efficiency and adaptability in complex industrial environments. The Anatomy of an XMPro MAGS Agent XMPro stands at the forefront of industrial MAGS development, leveraging our existing, robust data pipeline infrastructure. This same foundation, which powers our cutting-edge condition monitoring, predictive operations, and event intelligence solutions, now serves as the bedrock for our MAGS implementation. Figure 1 shows an Industrial Predictive Maintenance team structured around XMPro DataStreams. This setup features four distinct agents, each with specific roles and functions, which will be detailed in the subsequent sections of this article. This configuration operates without a central “supervising” agent, highlighting our system’s flexibility. Instead, each agent continuously shares and receives updates—observations, reflections, plans, and actions—enhancing collaborative decision-making. The agents in this example work together to optimize the output of a windfarm while reducing costs and working within resources, time, and budget constraints. This single team can monitor hundreds of wind turbines on the wind farm simultaneously and optimize the overall performance of the system in real-time. Figure 3 shows the “Anatomy” of the Reliability Agent in the wind farm, which is used in the two examples later in this article. It demonstrates the power and flexibility of XMPro’s DataStreams to compose these agents. These agents not only communicate with each other but also actively engage with real-time data streams, integrating these inputs into their ongoing processes as per the standard data pipeline architecture discussed in Part 3. The XMPro DataStream connector framework’s extensibility enables seamless, drag-and-drop integrations across IT, OT, and Engineering systems, providing unparalleled flexibility. Building on this proven architecture, we’ve developed innovative “Agent Brain” components that can be strategically placed within data streams, resulting in fully-featured MAGS agents. Block 1 represents a standard pattern where industrial data is continually ingested from SCADA, IoT, historians, and other engineering sources. The data is contextualized from business systems, such as a Digital Twin in this example. The XMPro Data Stream can combine, wrangle, and transform this data to ensure the data quality with capabilities such as continuously tracking error rates, completeness ratios, and validity scores. In this part of the Agent Data Stream, we can pass all this information to an analytical, statistical, or machine-learning model to provide further insights. In this example, the Reliability Agent runs the information from the DataStream through the Reliability Python library for expert analysis of the data. This is “kinda like” giving it a PhD in Reliability Engineering. This step “grounds” the agent in physics and reality. The output of this step is what XMPro typically sends to a real-time dashboard with a recommendation for a human subject matter expert to decide on the next step. We do this in Block 2 to enable “human on the loop” oversight and guidance, in case we want to monitor how the agent responds to the information from the analysis in Block 1. Blocks 1 and 2 represent a typical Predictive Maintenance data stream that monitors specific equipment, runs a predictive model over it, and present prescriptive recommendations to SMEs and other business users. XMPro has customers that routinely process more than 50 million of these monitoring events per day across a range of complex industrial assets. It is a robust and proven part of the XMPro MAGS agent framework. Block 3 is the XMPro MAGS extension of the data pipeline and represents the “Agent Brain” that takes the output from the operational data and analytics “function block” and adds to it input from other agents in the team, recent memory on the equipment, and the user or task prompt that is configured for this agent when it is set up. All of this is merged and passed to the XMPro MAGS agent that uses this as an observation, combines it with memories, and reflects on it before planning and coming up with a plan of action. In this example, the XMPro MAGS agent runs on a local deployment of a Llama model to ensure privacy and security. XMPro is agnostic to the LLM service and can run both cloud and local models, as well as a hybrid of both. Different agents can run on different models that best suit their objectives. The output of Block 3 determines the actions that the agent can take in Block 4. It could be a recommendation to a human user, but in this example, it updates the preventative maintenance schedules in the Maintenance Management Systems, and it creates a work ticket for a technician to inspect or repair the equipment based on the root causes of failures. This visual, explainable approach makes it easier to understand the agent’s process and logic. It also makes troubleshooting and fine-tuning much easier, as the output from every step in the data stream can be monitored and evaluated. Scalability and Governance The use of a data stream approach further enables scalability and governance that are required for industrial-grade MAGS solutions. The “stream host” architecture of XMPro DataStreams makes it possible to “instantiate” an infinite number of agents based on an “Agent Profile.” This profile is maintained separately and contains the “Rules of Engagement” that include a system prompt with skills, policies, and deontic rules such as obligations, permissions, authorizations, and delegation of authority. These system prompts always override any user or task prompts to ensure the responsible use of XMPro MAGS. This “Separation of Concerns” is designed to ensure that the XMPro MAGS agents are fit for the tasks assigned to them but behave according to company policies, rules, and regulatory requirements. This is a key area that I will address with Zoran Milosevic‘s help in part 5. Figure 9 shows how the XMPro MAGS Predictive Maintenance Team instantiates a team of four agent-based Agent Profiles and then starts observing, reflecting, planning actions, and recoding all this in an XMPro “BrainGraph” memory. The Reliability Engineering and Root Cause Analysis Agent’s planning and task output are used to create actions based on the plans using the XMPro DataStream Action connectors. Each action and result are further added to the memory to enable learning and continuous self-improvement. The XMPro MAGS ‘BrainGraph” is an example of a Predictive Maintenance team collaboratively optimizing asset performance through planning and executing corrective and preventative maintenance tasks. XMPro MAGS Examples I mentioned the Predictive Maintenance and OEE Expert Optimizer examples earlier, and here is a short summary of these solutions. Industrial Predictive Maintenance MAGS Team The Industrial Predictive Maintenance MAGS solution shown in Figure 10 utilizes a team of specialized AI agents working collaboratively to enhance industrial maintenance operations. Each agent plays a pivotal role, ensuring that maintenance is not only reactive but also predictive and strategic, thus minimizing downtime and extending equipment lifespan. Key agents and their roles within this system include Reliability Engineering and Root Cause Analysis Agent This agent analyzes equipment performance and failures to improve reliability and conduct thorough root cause analyses. It monitors equipment data to detect patterns that may indicate potential failures, provides detailed root cause analysis reports, and recommends preventive measures to avoid future breakdowns. This agent is based on the same “Agent Profile” as the “Predictive Maintenance Agent” in the OEE Expert Optimizer team. Maintenance Planning and Scheduling Agent This agent is responsible for efficiently planning and scheduling maintenance activities. It creates predictive maintenance schedules using equipment performance data and historical maintenance records. It aims to optimize these schedules to reduce downtime and improve resource utilization, producing optimized maintenance plans and schedules. Maintenance Management Oversight Agent This agent ensures that maintenance tasks are executed effectively and in compliance with established standards. It oversees maintenance activities, validates their completion and effectiveness, and enforces maintenance protocols, ultimately ensuring all maintenance activities meet required standards. It doesn’t act as a supervisor that delegate tasks but rather as the process quality assurance. Reporting and Feedback Agent Focused on transparency and continuous improvement, this agent collects and analyzes data from maintenance activities and equipment performance. It generates real-time Key Performance Indicator (KPI) reports and provides feedback mechanisms to foster ongoing improvements in maintenance operations. The MAGS workflow of these agents is highly integrated and systematic Data Collection Continuous gathering of equipment performance data, historical maintenance records, and operator input. Reliability Analysis The Reliability Engineering Agent continuously monitors and analyzes equipment to identify and address potential failures. Maintenance Planning The Maintenance Planning Agent develops and refines maintenance schedules based on the analysis. Maintenance Execution Maintenance tasks are carried out as scheduled, and records are duly updated. Management Oversight The Maintenance Management Oversight Agent reviews and validates the maintenance tasks for compliance and efficacy. Reporting and Feedback The Reporting and Feedback Agent provides essential KPIs and actionable insights to ensure continuous optimization of maintenance processes. By employing this structured approach, the Industrial Predictive Maintenance MAGS solution ensures that maintenance operations are proactive, efficient, and aligned with the organization’s strategic goals, significantly enhancing operational reliability and cost efficiency. XMPro OEE Optimization Expert team The Expert OEE Optimizer Multi-Agent Generative System uses an array of specialized AI agents, each tasked with optimizing different facets of manufacturing operations to enhance Overall Equipment Effectiveness (OEE). These agents collaborate in real time, ensuring a highly responsive and adaptive manufacturing environment. Key Agents and Their Roles Availability Monitoring Agent This agent manages equipment uptime and downtime data along with maintenance schedules to maximize machine availability. It provides real-time alerts and actionable recommendations to mitigate downtime risks effectively. Performance Monitoring Agent Responsible for analyzing production speed, cycle times, and operational data, this agent identifies bottlenecks that reduce performance. It offers real-time solutions to enhance production speed and efficiency, thereby optimizing throughput. Quality Monitoring Agent By assessing quality inspection data, defect rates, and rework records, this agent ensures products meet quality standards. It proactively identifies quality issues, offering solutions to improve product quality and reduce defect rates. Predictive Maintenance Agent Utilizing equipment sensor data and maintenance logs, this agent predicts potential equipment failures and schedules preventive maintenance. Its goal is to minimize unplanned downtime and optimize ongoing maintenance efforts. This agent is based on the same “Agent Profile” as the “Reliability and Root Cause Analysis Agent” in the Industrial Predictive Maintenance team. Anomaly Detection and Root Cause Analysis Agent This agent processes historical and real-time operational data to detect anomalies and perform root cause analysis, ensuring swift resolution of performance issues. Simulation and Scenario Analysis Agent Using synthetic data to represent various operating conditions, this agent simulates potential scenarios to forecast their impact on OEE. The insights generated aid in proactive decision-making and operational optimization. The MAGS workflow of these agents is highly integrated and systematic Data Ingestion The system continuously collects data from sensors, maintenance logs, production records, and quality inspections, ensuring a comprehensive data pool for analysis. Agent Processing Each agent processes the ingested data independently, generating specific outputs such as maintenance alerts, quality reports, and performance metrics. Collaboration Agents share their outputs to collaboratively refine and enhance operational strategies, leveraging combined insights for a holistic improvement approach. Solution Presentation The integrated outputs are synthesized into optimized solutions that are presented to human operators and decision-makers, ensuring the implementation of the most effective strategies. Feedback Loop The implemented solutions are monitored, and the feedback obtained is used to continuously train and improve the agents, enhancing their accuracy and effectiveness over time. It is still early days in the development of these collaborative, automated MAGS solutions, but for the OEE Expert Optimizer team, Table 1 shows the types of objectives we are setting and measuring. While the objectives differ from factory to factory, they illustrate the potential benefits of a team that observes, plans, acts, learns, and continuously improves with scarce human resources “on” the loop rather than “in” the loop. By integrating these agents into a cohesive system, the Expert OEE Optimizer not only improves current manufacturing operations but also adapts dynamically to meet future challenges, ensuring sustainable operational excellence and resilience. Join us on the Journey We at XMPro are excited about the opportunity that this capability brings. It enables you to augment current skills and address shortages with a framework that mimics the work of your best SMEs. It is not replacing jobs; it is automating suitable tasks so that you can free up scarce SMEs to focus on value-adding work and enable you to do more with less. In the process, you address the productivity challenge mentioned in Part 2. As systems become more complex, you can use GenAI for what it is good at, processing large volumes of data consistently and helping you to make sense of it, deciding on what action to take next. Fortunately, the journey doesn’t have to start with full multi-agent MAGS. Maybe you want to start with the Reliability Agent and grow into more functionality and capabilities as your confidence grows with the MAGS approach. XMPro MAGS is a flexible approach that allows you to start small and scale fast. If this excites you and you want to be part of the next evolution in industrial applications, please reach out to me or Gavin Green , VP Strategic Solutions. We are opening limited pilot opportunities for innovative, agile leaders in the industry. Continue to Part 5 – Rules of Engagement: Establishing Governance for Multi-Agent Generative Systems"
  },
  "docs/resources/faqs/external-content/blogs/2024/part-5--rules-of-engagement-establishing-governance-for-multiagent-generative-systems.html": {
    "href": "docs/resources/faqs/external-content/blogs/2024/part-5--rules-of-engagement-establishing-governance-for-multiagent-generative-systems.html",
    "title": "Part 5 – Rules of Engagement: Establishing Governance for Multi-Agent Generative Systems | XMPro",
    "summary": "Part 5 – Rules of Engagement: Establishing Governance for Multi-Agent Generative Systems Blog, CEO'S Blog Part 5 – Rules of Engagement: Establishing Governance for Multi-Agent Generative Systems Posted on September 4, 2024 by Wouter Beneke Part 5 – Rules of Engagement: Establishing Governance for Multi-Agent Generative Systems In parts 1 to 4 of the series, we explored the potential of Multi-Agent Generative Systems MAGS to transform industrial processes. We examined how these systems can enhance decision-making, improve efficiency, and adapt to complex operational environments. As we move forward with implementing MAGS for industrial settings, it’s essential to address a fundamental aspect of their deployment: the rules that govern their behavior. In this fifth article in the series, Dr. Zoran Milosevic and I will focus on the concept of “Rules of Engagement” for MAGS. These rules serve as the foundation for the responsible and effective use of AI agents in industrial environments. They ensure that our technological advancements align with organizational goals, legal requirements, and ethical standards. Figure 1 – Position of Rules of Engagement The need for clear rules becomes apparent when we consider the autonomous nature of MAGS. These systems can make decisions and take actions with minimal human intervention. While this autonomy offers significant benefits, it also presents risks if not properly managed. Establishing robust rules of engagement helps mitigate these risks and builds trust in the technology. Our discussion will cover various types of rules, including regulatory, organizational, professional, legal, and ethical guidelines. We’ll explore how these rules apply to different entities within a MAGS ecosystem, including _human operators, organizations, and AI agents_themselves. We’ll also examine practical approaches to implementing these rules. This includes standards-based enterprise modeling and other structuring methods that can define intentions, trust relationships, and interactions between agents and humans. We will explain how these Rules of Engagementare areimplemented in XMPro MAGS. Let’s begin by examining the fundamental concepts of rules and their importance in complex systems like MAGS. Engagement Rules: Why Complex systems consist of diverse actors, each with their own goals, as reflected in their intentions and behavioral preferences. These actors can engage in interactions and collaborations within existing structures, such as organizations, where they accept obligations or commitments associated with their defined roles. They can also form new partnerships with other actors based on mutual agreements. While internal rules guide the behavior of actors as independent entities and their involvement in collaborations, they must also adhere to external regulations. Both internal and external rules contribute to maintaining system stability, predictability, and trust in an uncertain environment. Our earlier articlesdemonstrate how AI agents are becoming an important element of complex systems, addressing the resource scarcity, data complexity and unlocking of new economic values. AI agents, engaged by humans or organizations, are a type of actor whose behavior can simulate or mimic that of humans. However, ultimate accountability lies with the parties that have legal responsibilities, such as human creators or organizational owners. While AI agents may delegate tasks to other agents, the ultimate responsibility remains with the human or organization. The importance of rules in this context is to clearly define responsibility and accountability across both human and AI agent entities. We use the term Rules of Engagement to encompass the various types of rules needed to govern the behavior of actors in complex systems. These rules can be used to define obligations, permissions, and prohibitions for actors involved in collaborative activities. They can originate from internal governance arrangements or from external rules. The latter include r_egulatory compliance, legal requirements, ethical considerations, professional conduct, engineering and security, safety and risk management, resource management, environmental responsibility_, and more. Engagement Rules: What When considering computable approaches for supporting engagement rule expressions, the above considerations suggest a need to express: Organizational context that defines rules, whether the context captures a regulatory domain that specifies controlling rules over members of the domain (both individual actors and organisational structures), or a collaboration structure whose objective drives the rules of collaboration. Legal entities that have their own independent life and identity, regardless of their participation in any collaboration Collaboration structures that can define templates for interactions; these allow for multiple instantiations, such as different parties filling roles in such collaboration structures in different times. Organizational or legal policies that cover constraints over behavior of both the actors (parties and AI agents) and collaboration structures, providing guardrails over their autonomous behavior. Range of organisational or legal policy types, including simpler rules associated with obligations, permissions and prohibitions, but also their derivation, needed for more complex expressions of accountability and responsibility (see Figure). Dynamics of the obligations, permissions and prohibitions as a result of authorisation or delegation of services or responsibilities across actors**, while ensuring clear traceability of responsibility across the actors** Agentic behavior to define possible actions of actors over time, reflecting their objectives while also responding to the actions of others. System policies that can support the variability of system design over time and thus facilitate system evolutions, including the change of organisational or regulative policies. Engagement rules: How When building complex systems, the engagement rules should be articulated in a computer interpretable style, to ensure_consistency, interoperability and evolvability_of the systems, as the operating environment rules change, including the availability of new technologies. Such computable expressions in turn requires formal modelsto ensure that the computable language can be interpreted using suitable tools, including those that check consistency and traceability from business requirements to implementation, in support of conformance and compliance checking. One practical approach to implementing such rules is based on formal models embedded in the concepts specifically developed to design and implement large or complex systems, namely the ISO/ITU-T/IEC open distributed processing systems (ODP) standard [1]. One component of the standard, the ODP Enterprise Language provides [2] much of the semantics needed _semantic foundation_for describing rules of engagement. (* adapted from an upcoming conference publicatiob by Zoran Milosevic ) For example, the ODP concept of party, can be used to model actors thar have broad set of responsibilities derived from some social or legal framework, and these can be natural persons or organisations. Parties may engage automated agents], such as AI agents discussed previously, to perform actions on their behalf, and this involves delegation of their responsibility to address resource issues, either to do with capability specialisation or scalability reasons. The aim here is to be able to trace the way that the rightsand responsibilities of the parties are linked to the individual system actions and their consequences, which is key to ensuring building responsible digital twin systems. Further, the ODP concept of community is useful to define the collaboration structures introduced earlier, which serve as templates for interactions of actors participating in collaborations. A community is essentially a contract specifying how actors can participate in different community roles to fulfil their objectives, and that of the community’s objective. The contract is expressed in terms of obligations, permissions or prohibitionspolicies that apply to the actors fulfilling these roles. These policies, often referred as deontic policies, are intended to capture the rules of the community contract, including those that propagate to the community, from the outer regulatory environment. _Note that the term “Deontic” has roots in philosophy and logic,_specifically in the study of obligation, permission, and prohibition and in recent AI developments they are employed to design systems that adhere to rules and regulations, such as access control mechanisms, legal frameworks, and ethical guidelines for AI [3]. The ODP-EL standard provides a practical implementation path for handling these deontic and accountability concepts, through encapsulating them within objects that can be handed over across the actors in the system, referred to as deontic tokens [2]. Here, the holders of these tokens, are constrained by the nature of policies encapsulated within them. This is similar mechanism as used in many recent security policy approaches such as OAuth 2.0 access tokens, although the constraints here are broader and apply to any holder’s actions rather than to narrower, data access actions, in access control policies. Further, deontic tokens can be associated with some important actions performed by actors, when they result in changes of obligations, permissions or prohibitions associated with the actors. These actions are referred to as speech acts, introduced following the linguistic concepts of speech act, and they can be used to express how certain actions such as requests, orders or promises, change the state of world, from the perspective of obligations or permissions of actors involved. They are powerful way of expressing chain of responsibility across parties and their AI agents. This foundational model for expressing engagement rules as constraints on expected behaviour of agents, also serves as a basis for the inclusion of related modelling concepts/formalism and techniques needed for analysing and reasoning about system properties related to AI agent technologies. One such technique, referred to as Promise Theory [4], is suitable for describing i_ntention of AI agents and how these can be related to their objectives, future actions, and decisions, and thus the deontic formalism mentioned above_. Such decisions can be influenced by agents own objectives and commitments, while allowing for potential choices associated with expected behaviour and trust in other agents with which they can interact and collaborate. How do we implement the Rules of Engagement in XMPro MAGS XMPro’s Multi-Agent Generative System MAGS uses a structured approach to implement Rules of Engagement for AI agents.. This framework ensures AI agents operate within defined boundaries and follow organizational policies. I_t also helps agents make decisions that align with ethical standards and business objectives, which is crucial in complex industrial settings._ Here is an example from the oil and gas industry. A major refinery uses XMPro MAGS to manage its operations, from production planning to maintenance scheduling and safety monitoring. The system includes various AI agents responsible for different aspects of the refinery’s operations, each needing to work in harmony with others while adhering to strict industry regulations. Figure 3 – Agent Profile and Agent Instance Agent Profile System Prompts serve as the foundation for establishing overarching rules in XMPro MAGS. These prompts include **Organizational rules:**These are high-level directives that reflect the company’s values and policies. For example: \"Prioritize safety in all operations\" \"Comply with environmental regulations at all times\" \"Optimize resource utilization to minimize waste\" \"Maintain product quality standards across all production lines\" **Deontic rules:**These specify what agents must, may, or must not do in various situations. Examples include \"Agents must report any safety anomalies immediately\" \"Production agents may adjust output levels within specified ranges\" \"Maintenance agents must not schedule repairs during peak production hours unless critical\" To implement these rules flexibly and dynamically, XMPro MAGS uses a system of Deontic Tokens. These tokens are based on concepts from the ISO/ITU-T/IEC open distributed processing systems (ODP) standard, ensuring a standardized approach to rule implementation. Each token represents a specific rule, including its type, the subject it applies to, and the conditions for its application. The XMPro MAGS Agent Memory Cycle integrates these tokens at every stage of an agent’s decision-making process: **Observation:**Tokens guide what information agents can access and how they interpret it. For instance, a safety monitoring agent might be authorized to access real-time sensor data from all parts of the refinery. **Reflection:**Agents consider applicable tokens when evaluating past actions and forming new insights. This allows them to learn from experience while staying within defined boundaries. **Planning:**Token-based rules influence the creation and prioritization of action plans. A production planning agent might adjust its scheduling strategy based on recent performance data and current operational constraints. **Execution:**Before any action, agents verify compliance with relevant tokens. This ensures that even in dynamic situations, all actions align with organizational rules and regulations. This approach ensures that Rules of Engagement actively shape agent behavior in real-time. It allows the refinery to maintain safe, efficient, and compliant operations while adapting to changing conditions. _For example, system administrators can quickly update the relevant tokens through the Agent Profile if a new environmental regulation is introduced._All affected agents will immediately adjust their behavior to comply with the new rule, without reprograming each agent individually. By implementing Rules of Engagement through Agent Profile System Prompts and Deontic Tokens, XMPro MAGS balances consistent agent behavior with operational flexibility. This method offers several benefits: Ensures AI systems remain aligned with business goals and regulatory requirements Provides a clear audit trail of decision-making, crucial in highly regulated industries Allows for quick adaptation to changing rules or conditions Maintains consistent behavior across different agents and processes Enables fine-grained control over agent permissions and restrictions Facilitates easier updates to system-wide policies without extensive recoding The system also supports hierarchical rule structures, allowing for both broad, organization-wide rules and specific, task-level rules. This hierarchical approach ensures that agents can handle complex, nuanced situations while still adhering to overarching principles. In the refinery example, this might look as follows: Top-level rules about safety and environmental compliance Department-level rules about production targets and quality standards Task-specific rules for individual processes or equipment This layered approach to Rules of Engagement helps organizations implement AI systems that are both powerful and controllable, suitable for complex industrial environments like oil and gas refineries. It provides the flexibility needed to optimize operations while maintaining the strict control necessary in high-risk, highly regulated industries. How to get started with RoE and MAGS As we’ve explored in this article, implementing Rules of Engagement is essential for the successful deployment of Multi-Agent Generative Systems MAGS in industrial environments. These rules, when properly designed and executed, help organizations balance AI autonomy with necessary controls. XMPro’s approach, using the Deontic Token System, offers a practical way to manage AI agent behavior while maintaining operational flexibility. This method allows companies to adapt quickly to changing regulations and business needs without compromising on safety or compliance. For senior managers and technical leaders considering MAGS for their operations, understanding these concepts is crucial. The potential benefits of MAGS are significant, but so are the challenges of implementation. Every organization will have unique requirements for their Rules of Engagement, based on their industry, risk profile, and strategic goals. If you’re interested in exploring how Rules of Engagement and Deontic systems could work for your company, we encourage you to take action. Dr. Zoran Milosevic and Pieter van Schalkwyk are available for in-depth discussions about applying these concepts to your specific situation. Their experience can help you navigate the complexities of AI governance in industrial settings. To arrange a consultation, please reach out through our website or LinkedIn profiles. We look forward to helping you shape the future of your operations with responsible AI implementation. References [1] Linington, P.F., Milosevic, Z., Tanaka, A., Vallecillo, A.: Building Enterprise Sys- tems with ODP: An Introduction to Open Distributed Processing, 1st Edition. Chap- man&Hall/CRC Innovations in Software Engineering and Software Development (2011) [2] ISO/IEC IS 15414, Information Technology – Open Distributed Processing – Enterprise Language 3rd edn, 2015. Also published as ITU-T Recommendation X.911. [3] Z. Milosevic, Ethics in Digital Health: a deontic accountability framework, Proceeding of EDOC2019 conference, https://ieeexplore.ieee.org/abstract/document/8945023 [4] Burgess, M., & Bergstra, J. A. (2014). Promise theory: Principles and applications. Springer. Previous articles Part 1: From Railroads to AI: The Evolution of Game-Changing Utilities Part2: The Future of Work: Harnessing Generative Agents in Manufacturing Part 3 – AI at the Core: LLMs and Data Pipelines for Industrial Multi-Agent Generative Systems Part 4 – Pioneering Progress | Real-World Applications of Multi-Agent Generative Systems"
  },
  "docs/resources/faqs/external-content/blogs/2024/part2-the-future-of-work-harnessing-generative-agents-in-manufacturing.html": {
    "href": "docs/resources/faqs/external-content/blogs/2024/part2-the-future-of-work-harnessing-generative-agents-in-manufacturing.html",
    "title": "Part2: The Future of Work: Harnessing Generative Agents in Manufacturing | XMPro",
    "summary": "Part2: The Future of Work: Harnessing Generative Agents in Manufacturing Articles, Blog, CEO'S Blog Part2: The Future of Work: Harnessing Generative Agents in Manufacturing Posted on July 24, 2024 by Pieter van Schalkwyk Part2: The Future of Work – Harnessing Generative Agents in Manufacturing Part2: The Future of Work: Harnessing Generative Agents in Manufacturing Pieter Van Schalkwyk CEO at XMPRO, Author – Building Industrial Digital Twins, DTC Ambassador, Co-chair for AI Joint Work Group at Digital Twin Consortium June 26, 2024 – Originallly posted on Linkedin In part 1 – From Railroads to AI: The Evolution of Game-Changing Utilities, I explained how the development and adoption of Generative AI mirrors the transformative impact of railroads, electricity, and the Internet, and why it is set to become the next game-changing utility driving our future. Just as we needed machines and appliances to harness the electrical utility, we now need transformative applications to fully leverage the Generative AI Utility. In part 2, I explore how Generative Agents represent the transformative application for GenAI capabilities, ready to revolutionize how we work. Unlike current AI Assistants and Copilots that improve efficiency, Generative Agents will bring a step change—a fundamental shift in “how” we work. This transformation will be similar to how electrical appliances replaced manual tools, enhancing productivity, improving society, and elevating the quality of life for everyone. As we stand on the brink of this shift, it’s important to understand the driving forces behind the need for change, the unique capabilities of Generative Agents, and how they operate. In this article, we will delve into three key focus areas: The compelling reasons why we need to change how we work How Generative Agents are uniquely positioned to facilitate the future of work A deeper dive into the mechanics of Generative Agents and how they function to bring about this transformation. By exploring these areas, I hope to give you a glimpse of Generative Agents’ immense potential for redefining our work landscape. Adapting to the Future: The Case for New Work Approaches The evolving manufacturing landscape demands a significant shift in our work methods. Several key drivers necessitate this change: 1. Skills The current labor market faces multiple challenges: rising labor costs, shortages of skilled workers, an aging workforce, and declining labor participation rates. These issues create a pressing need for businesses to find alternative solutions to maintain productivity and efficiency. Upskilling and reskilling the workforce to adapt to new technologies like AI, robotics, and advanced data analytics are crucial. Traditional manual labor skills no longer suffice to meet the demands of modern manufacturing. 2. Productivity Despite technological advancements such as IoT and automation, manufacturing productivity has not significantly improved in recent years. Initial gains from integrating PCs with PLCs, the advent of the internet, and the introduction of communication standards like OPC DA and UA were substantial. However, productivity has plateaued despite further advancements. This stagnation suggests the need for new strategies and transformative approaches to reignite productivity growth and maximize the potential of current technologies. 3. Complexity of Manufacturing Processes and Systems The increasing complexity of modern manufacturing systems poses significant challenges. Automation, robotics, AI, and IoT integration require advanced technical skills and continuous learning. Effective data management and analysis are essential for process optimization and decision-making. Additionally, achieving interoperability among various systems and technologies, managing global supply chains, and maintaining high-quality control standards demand a shift towards more sophisticated and knowledge-intensive work practices. Let’s explore each of these in more detail. Driver 1 – Navigating the Shift: Addressing Labor Challenges in Manufacturing and Logistics The evolving landscape of manufacturing and logistics is driving a significant shift in the required skill sets for frontline workers. Due to labor shortages, industry analysts project that by 2028, there will be more smart robots than frontline workers in these industries. This transition applies to physical and digital robots or agents that will augment the manufacturing workforce. 1. Labor Challenges and Statistics According to analyst research, labor has become as significant a constraint on operational performance as product availability. The U.S. workforce is expected to grow nearly five times slower than the U.S. GDP over the next decade, with the GDP projected to grow at 2.5% while the overall workforce will grow only 0.5%. Specifically, manufacturing and retail workforces are expected to shrink by 0.1% and 0.4%, respectively. 2. Aging Workforce By 2030, the U.S. Department of Labor expects that one-quarter of the U.S. workforce will be over 65 years of age. This trend is also evident in Europe and parts of Asia, indicating a global challenge. Manufacturing faces an image problem, with many younger generations perceiving these jobs as outdated, boring, and lacking creativity. This negative perception and the widespread labor shortage make it challenging to attract and recruit new talent to fill the positions left by retirees. 3. Declining Labor Participation Rates U.S. labor participation rates are projected to decline from 67% in 2000 to 60.4% in 2030. This decline increases the labor shortages faced by various industries. LNS Research found by comparing 2019 vs. 2023 manufacturing employment data that there is a declining workforce tenure: The average tenure of employees in manufacturing has decreased from 20 years in 2019 to only three years in 2023. This means that manufacturers are dealing with a less experienced workforce. 4. Evolving Skill Requirements The rapid pace of technological change in manufacturing means that the required skills constantly evolve. This makes finding candidates with the right expertise challenging and necessitates continuous upskilling and training efforts. Retaining these upskilled workers is a further challenge in this competitive labor market. LNS further found in their comparison of 2019 and 2023 labor statistics that there is a high employee turnover: 50% of new employees leave manufacturing roles within the first 90 days. The 90-day retention rate in 2019 was 90%. This turnover rate further exacerbates the issue of a less experienced workforce and puts additional pressure on manufacturers to improve onboarding and training processes. Driver 2 – Rethinking Efficiency: Addressing the Productivity Plateau in Manufacturing Despite significant advancements in IoT and automation, manufacturing productivity has not seen substantial improvements in recent years. According to the U.S. Bureau of Labor Statistics, labor productivity in the manufacturing sector, measured as output per hour for all workers, has plateaued. I saw this initially in a LinkedIn article by Michael Carroll. If you want to see how the numbers stack up, you can explore the source for yourself here. https://www.linkedin.com/pulse/looming-copilot-disaster-manufacturing-navigating-complex-carroll-qseye/ Michael marked up the graph with the evolution of manufacturing technologies. It is a great read, and you will find it here. Over the past several decades, the manufacturing sector has witnessed significant shifts in productivity driven by technological advancements. From the 1990s to the mid-2000s, manufacturing productivity saw substantial gains due to the integration of Programmable Logic Controllers (PLCs) with PCs, the advent of the Internet, and the emergence of the Internet of Things (IoT), which improved communication, data collection, and automation. Further advancements included manufacturing intelligence combining data analytics with automation, decreasing sensor costs, and the release of OPC Unified Architecture (UA), which enhanced system interoperability. However, despite these continuous technological advancements, productivity growth has plateaued post-2010, indicating diminishing marginal gains or counteracting factors. 1. Reimagining Workflows for Enhanced Productivity Despite recent innovations, the manufacturing sector has encountered a plateau in productivity growth. This stagnation suggests that the benefits of new technologies are becoming less pronounced or other challenges are counteracting potential gains. To address this issue, we must rethink how we work. Productivity is measured by worker output per hour, and to significantly improve this, we need to enable workers to produce more with less of their own time and resources. Reimagining business processes to include Generative Agents that handle repetitive, high-volume tasks provides a pragmatic solution to the productivity problem. New strategies and breakthroughs are essential to break through this plateau and drive future productivity growth. Driver 3 – Addressing Complexity in Today’s Manufacturing Processes The increasing complexity of manufacturing systems and tasks poses several challenges that require a shift in how work is approached. Here are the main challenges: 1. Automation and Technology Integration Modern manufacturing relies heavily on automated systems, robotics, computer-controlled machinery, and advanced technologies like IoT, AI, and data analytics. This integration requires a workforce with technical skills, problem-solving abilities, and continuous learning capabilities. Traditional manual labor is insufficient to meet these demands. Think of all the screens in a modern control room or smart factory where an operator must monitor multiple process variables to optimize production in real-time. Or all the data from multiple sensors that a reliability engineer needs to analyze in real-time to reduce downtime as the competitive pressure is on businesses to operate “at the speed of thought,” as Bill Gates predicted in 1999. 2. Data Management and Analysis The vast amounts of data generated by sensors, machines, and systems demand robust data management practices and advanced analytics capabilities. Extracting actionable insights for process optimization, forecasting, and decision-making is crucial for maintaining a competitive advantage. 3. Integration and Interoperability Integrating and interoperability between various systems, equipment, and software applications is a significant challenge. Manufacturing operations involve a wide range of modern and legacy systems from different manufacturers with varying data formats and communication protocols. Additionally, the rapid advancement of technology has created a skill gap, with a shortage of professionals proficient in data analytics, cybersecurity, cloud computing, AI, and robotics. Upskilling and reskilling the existing workforce to adapt to technological changes is crucial. The Future of Work: How Generative Agents Can Help A generative agent is an advanced AI entity designed to autonomously recognize patterns, generate predictions, and perform tasks by emulating human reasoning. Unlike traditional computational software, generative agents are trained on extensive data sets to understand context and make informed decisions. They can process large amounts of data, adapt to new information, and optimize workflows, making them valuable for automating complex processes and enhancing productivity across various applications. Generative Agents are set to revolutionize the future of work by leveraging their unique capabilities. To understand their value, consider Generative AI as a utility, similar to electricity, which powers various applications. Generative Agents are like the appliances and machines that use this “electricity” to create products, improve quality of life, and provide the end-user value. Here’s how they can help: 1. Reasonable vs. Computational Human-Like Reasoning Generative Agents are trained to recognize and predict patterns, allowing them to emulate human reasoning with high accuracy. They can make decisions and provide insights that mirror human thought processes, making them more adaptable and intuitive in dynamic work environments. Enhanced Predictions By using “external” computational tools, Generative Agents can enhance the accuracy of their predictions. They combine the strengths of both pattern recognition and computational logic to deliver precise and actionable insights. 2. Speed, Accuracy, and Repeatability Greater Accuracy Generative Agents can process vast amounts of data and identify patterns faster than humans. This capability allows them to provide more accurate predictions and recommendations, improving decision-making and efficiency. Speed With their ability to process information quickly, Generative Agents can offer real-time solutions and responses, enhancing productivity and reducing workflow delays. Repeatability Unlike humans, Generative Agents can perform repetitive tasks with consistent accuracy. This repeatability ensures high-quality outcomes and reduces the risk of errors. 3. Integration with Computational Tools Hybrid Approach Generative Agents and computational machines operate in different approaches but can complement each other. While computational AI and applications excel at executing complex mathematical calculations at scale, Generative Agents excel at pattern recognition and predictive reasoning. Combined Capabilities Organizations can harness the best of both worlds by integrating Generative Agents with computational tools. It further “grounds” GenAI in physics and the laws of nature. It reduces hallucinations as it can be fact-based but with reasoning capabilities. This hybrid approach enhances productivity and enables more sophisticated real-world analysis and problem-solving. Generative Agents are not just another form of computational software. Their ability to emulate human reasoning, combined with computational tools, makes them powerful allies in the future of work. They enhance accuracy, speed, and repeatability, providing a hybrid approach that leverages the strengths of both pattern recognition and computational logic. By doing so, Generative Agents significantly improve the productivity and efficiency of our work output. The Evolution of Agent Approaches in Work and Task Types As work and task types evolve, so do the approaches to implementing AI and automation in business processes. The progression from simple rule-based systems to sophisticated multi-agent generative systems highlights how different levels of AI and automation capabilities align with varying work objectives. XMPro Evolution of Agentic Business Processes Rule-Based Process Automation Before using AI for workflow planning, automation relied on rule-based process automation, which was extended to Robotic Process Automation (RPA), which performs deterministic workflow tasks. These automations are highly structured, following predefined rules to complete repetitive tasks with consistency and accuracy. AI Assistants and Co-Pilots With advancements in AI, the introduction of AI Assistants and Co-Pilots marked a significant shift. Utilizing large language models (LLMs), these systems can handle freestyle chat and more dynamic interactions, providing support and enhancing productivity by assisting with routine tasks and decision-making processes. Task-Based Agentic Workflow As business needs grow more complex, the evolution towards task-based agentic workflows continues. Single generative agents are capable of handling specific tasks autonomously. These generative operator agents can focus on goal-based single objectives, such as optimizing production processes or managing energy resources. Supervisory Control of Agentic Work Teams For more complex scenarios, supervised agentic work teams are used. Directed generative agents operated under a supervisory structure, where a central agent coordinated the efforts of multiple agents working towards deterministic group goals. This setup allows for better management of interdependent tasks and improved efficiency in achieving collective objectives. Multi-Agent Generative Systems (MAGS) The most advanced approach is the deployment of Multi-Agent Generative Systems (MAGS). These systems consist of numerous autonomous agents collaborating within a shared framework to achieve autonomous goal-seeking. MAGS can handle distributed intelligence systems, managing intricate processes like production, energy management, ESG (Environmental, Social, and Governance) compliance, and supply chain operations. Each agent has a defined role, and they work together to deconstruct and solve complex problems, mimicking real-world team dynamics. Tailoring Solutions to Business Process Requirements It’s important to note that not all business processes require the complexity of complete multi-agent systems. Depending on the specific needs and objectives, organizations can implement varying levels of AI and automation: For Routine and Structured Tasks: Rule-based RPA and AI Assistants are often sufficient. For Goal-Based Objectives: Single generative agents can effectively manage individual tasks. For Collaborative Workflows: Directed generative agents with supervisory control provide an efficient solution. For Highly Complex and Distributed Processes: MAGS offers the necessary capabilities to handle large-scale, integrated operations. The evolution from rule-based automation to sophisticated multi-agent generative systems reflects the increasing complexity and diversity of modern business processes. By choosing the appropriate level of AI and automation, organizations can optimize their operations and enhance productivity across various tasks and objectives. What is Multi-Agent Generative Systems (MAGS)? Multi-agent generative Systems (MAGS) represent an advanced integration of computational software agents and large language models (LLMs) designed to simulate and optimize complex industrial processes and interactions. MAGS leverages generative AI to create dynamic, adaptive systems that enhance productivity, efficiency, and decision-making across various operational aspects. Key Characteristics: Integration of LLMs and Agent Technology: MAGS combines the reasoning capabilities of LLMs with the interactive and adaptive nature of software agents. Complex Environment Simulation: They can simulate intricate environments where multiple agents interact, make decisions, and influence each other. Emergent Behavior: Through agent interactions, MAGS generate emergent behaviors that may not be predictable from individual agent characteristics alone. Memory and Reflection: Agents within MAGS can record observations, reflect on past experiences, and use this information to inform future actions. Adaptive Decision Making: Agents can create and modify plans to achieve goals, adapting to changing circumstances in their environment. Multi-modal Capabilities: MAGS are evolving to incorporate multiple modalities, including text, image, audio, and video, enhancing their ability to interact with and understand complex environments. As Multi-Agent Generative Systems (MAGS) evolve, they promise to provide unprecedented insights into complex systems, enabling more accurate predictions, better decision-making, and innovative problem-solving approaches across multiple industries and scientific disciplines. Think of MAGS as the appliance in the electricity utility analogy from the start of this article. MAGS will not simply replace current tasks but expand their use by unlocking new opportunities. Like the seafarers who discovered new lands despite the maps warning “There Be Dragons,” venturing into this new territory brings immense potential to enhance the quality of life for everyone. In Part 3, “AI at the Core: LLMs and Data Pipelines for Industrial Multi-Agent Generative Systems”, we will explore specific examples of MAGS applications with XMPro and practical use cases. If this excites you and you want to be part of the next evolution in manufacturing, please reach out. We are opening limited pilot opportunities for innovative, agile leaders in the industry."
  },
  "docs/resources/faqs/external-content/blogs/2024/progressing-through-the-decision-intelligence-continuum-with-xmpro.html": {
    "href": "docs/resources/faqs/external-content/blogs/2024/progressing-through-the-decision-intelligence-continuum-with-xmpro.html",
    "title": "Progressing Through The Decision Intelligence Continuum With XMPro | XMPro",
    "summary": "Progressing Through The Decision Intelligence Continuum With XMPro Articles, Blog, Uncategorized Progressing Through The Decision Intelligence Continuum With XMPro Posted on August 7, 2024 by Pieter van Schalkwyk Progressing Through the Decision Intelligence Continuum with iBOS In today’s fast-paced and ever-evolving industrial landscape, the journey towards digital transformation can seem daunting. At XMPro, we understand that every organization is at a different point in this journey. That’s why our Intelligent Business Operations Suite (iBOS) is designed to meet you where you are and grow with you, ensuring a seamless and tailored progression through the Decision Intelligence Continuum. 1. Assess Our journey together begins with a comprehensive assessment of your current operational maturity and business goals. We take the time to understand your specific needs, challenges, and objectives. This initial phase lays the groundwork for a customized implementation plan, ensuring that every step we take is aligned with your strategic vision. 2. Implement Once we have a clear understanding of your current state and desired outcomes, we move on to the implementation phase. Our focus here is on achieving quick wins that deliver immediate value. We deploy iBOS with a particular emphasis on enhancing decision support capabilities. This stage is designed to build confidence and demonstrate the tangible benefits of our solution from the outset. 3. Evolve As your team becomes more familiar with iBOS and gains confidence in its capabilities, we gradually introduce more advanced features. This evolution phase is all about augmenting your decision-making processes with intelligent insights and predictive analytics. We ensure that your team is fully supported and trained to leverage these new capabilities, driving continuous improvement in operational performance. 4. Transform The final phase of our approach is transformation. Here, we work closely with you to implement automated decision-making processes where appropriate. Our goal is to create a harmonious balance between human expertise and AI-driven insights. While automation can significantly enhance efficiency and consistency, we ensure that critical decisions always have human oversight. This approach not only boosts operational efficiency but also fosters a culture of trust and collaboration. Why the Decision Intelligence Continuum Matters Progressing through this continuum with XMPro’s iBOS offers several key benefits: Reduce Operational Risks and Costs: By systematically improving decision-making processes, you can minimize errors and reduce the financial impact of operational risks. Improve Decision Quality and Consistency: With intelligent insights at your fingertips, you can ensure that decisions are based on comprehensive, real-time data, enhancing both quality and consistency. Increase Agility and Responsiveness to Market Changes: Our solution empowers you to quickly adapt to changing market conditions, ensuring that your organization remains competitive and resilient. Free Up Skilled Workforce for Strategic Initiatives: By automating routine decision-making tasks, your skilled workforce can focus on high-impact strategic initiatives that drive innovation and growth. With XMPro’s iBOS, you’re not just investing in a software solution – you’re embarking on a journey towards true intelligent autonomous operations. Our phased approach ensures that you can start realizing benefits quickly while laying the foundation for more advanced capabilities in the future. This journey towards continuous improvement and competitive advantage is what sets you apart in your industry. Conclusion At XMPro, we are committed to partnering with you on your digital transformation journey. Our iBOS is designed to evolve with your organization, providing the tools and insights needed to navigate the complexities of modern industrial operations. By progressing through our Decision Intelligence Continuum, you can achieve a seamless transition to intelligent autonomous operations, driving sustained success and innovation. Embark on your journey with XMPro today and discover the power of intelligent business operations."
  },
  "docs/resources/faqs/external-content/blogs/2024/revolutionizing-manufacturing-with-ai-and-generative-ai-xmpros-intelligent-business-operations-suite.html": {
    "href": "docs/resources/faqs/external-content/blogs/2024/revolutionizing-manufacturing-with-ai-and-generative-ai-xmpros-intelligent-business-operations-suite.html",
    "title": "Revolutionizing Manufacturing with AI and Generative AI: XMPro’s Intelligent Business Operations Sui | XMPro",
    "summary": "Revolutionizing Manufacturing with AI and Generative AI: XMPro’s Intelligent Business Operations Sui Articles, Blog, CEO'S Blog Revolutionizing Manufacturing with AI and Generative AI: XMPro’s Intelligent Business Operations Suite Posted on June 3, 2024 by Pieter van Schalkwyk The manufacturing industry is undergoing a significant transformation, driven by the rapid advancements in artificial intelligence (AI) and the emergence of generative AI (GenAI). McKinsey & Co research shows that Industry leaders leveraging AI to power their Industry 4.0 transformations are at the forefront of this revolution. These innovative organizations are achieving remarkable results, including two to three times increase in productivity, 50 percent improvement in service levels, 99 percent reduction in defects, and 30 percent decrease in energy consumption. Composability Makes Time To AI Value Faster One of the key strategies employed by industry leaders is the concept of “assetization,” which involves packaging AI use cases for speed and scale of deployment in a composable architecture approach. By leveraging composable or modular design principles, investing in deployment productivity tools, and including digital upskilling materials as part of the asset package, these leaders are democratizing AI technology to subject matter experts that can apply it to various use cases. This approach empowers engineers and technicians to identify, deploy, and test new AI applications, leading to significant improvements in efficiency and quality. As industry leaders progress towards system-level automation, they are building trust in AI models through closed-loop feedback, integration of safeguards and monitoring mechanisms, and phased deployment alongside human operators. This iterative process ensures that AI recommendations are reliable and accurate, paving the way for true automation. XMPro Operationalize AI in Business Processes XMPro, a leading provider of intelligent business operations solutions, is at the forefront of this AI-driven transformation in manufacturing. The XMPro Intelligent Business Operations Suite (iBOS) offers a comprehensive platform that harnesses the power of AI and GenAI to optimize operations, reduce costs, and drive innovation. XMPro iBOS enables the seamless integration of AI models into business processes, allowing for real-time analytics and executable AI within core operations. XMPro’s enables “RealWorld AI” through the combination of combination of First Principles Engineering Models with Rules-based AI, Traditional AI, Deep Learning, and Generative AI. This unique capability to “chain” various AI capabilities together enables manufacturers to create innovative solutions to complex engineering challenges. XMPro iBOS offers Generative Intelligence for business operations, which augments guidance, enables advanced analysis and prescriptions, fosters innovation and ideation, and supports data-driven decision-making. By synthesizing information from various sources through XMPro’s DataStreams and employing both cloud or local LLM models, XMPro’s generative AI enhances the capability to analyze, suggest, and take action, making it an active participant in the management and optimization of manufacturing systems. Moreover, XMPro’s generative agent AI technology facilitates the intelligent automation of manufacturing processes. By leveraging multi-agent systems, XMPro iBOS enables autonomous agents to collaborate, make decisions, and perform tasks without human intervention. These agents operate within a set of predefined rules, ensuring that their actions are safe, secure, and aligned with operational protocols. Start Small, Scale Fast with XMPro iBOS for AI in Manufacturing The manufacturing industry is on the cusp of a new era, where AI and GenAI are transforming every aspect of the supply chain process. From demand forecasting and asset management to quality control and delivery optimization, AI is driving unprecedented improvements in productivity, quality, and sustainability. As more organizations adopt these technologies and embrace the strategies of industry leaders, we can expect to see a rapid acceleration in the deployment of AI and GenAI solutions, leading to a more efficient, resilient, and innovative manufacturing landscape. XMPro’s Intelligent Business Operations Suite, with its comprehensive AI and GenAI capabilities, is well-positioned to help manufacturers unlock their full operational potential. By delivering a greater than 10x return on investment, reducing maintenance costs, increasing equipment uptime, and decreasing maintenance planning time, XMPro iBOS consistently empowers operations to achieve their full potential and thrive in the era of AI-driven manufacturing."
  },
  "docs/resources/faqs/external-content/blogs/2024/scaling-multiagent-systems-with-data-pipelines-solving-realworld-industrial-challenges.html": {
    "href": "docs/resources/faqs/external-content/blogs/2024/scaling-multiagent-systems-with-data-pipelines-solving-realworld-industrial-challenges.html",
    "title": "Scaling Multi-Agent Systems with Data Pipelines: Solving Real-World Industrial Challenges | XMPro",
    "summary": "Scaling Multi-Agent Systems with Data Pipelines: Solving Real-World Industrial Challenges Blog, CEO'S Blog Scaling Multi-Agent Systems with Data Pipelines: Solving Real-World Industrial Challenges Posted on October 13, 2024 by Pieter van Schalkwyk In today’s complex industrial landscape, the ability to process and act on data in real time is no longer just an advantage—it’s a necessity. XMPro’s DataStreams approach, combined with Multi-Agent Generative Systems (MAGS), offers a powerful solution to this challenge. This integration not only enables real-time data processing but also supports a diverse ecosystem of agent types, making it a versatile tool for solving real-world problems at scale. The Power of Data Pipelines in Industrial Settings Data pipelines, like XMPro DataStreams, form the backbone of modern industrial data processing. They offer several key benefits: Continuous Data Flow: DataStreams ensure a constant flow of information from various sources across the industrial environment. Real-Time Processing: Data is processed as it’s generated, allowing for immediate analysis and action. Scalable Architecture: The pipeline approach can handle increasing data volumes as operations grow. Agent Diversity: DataStreams support a wide range of agent types, from sophisticated AI to simple scripts. These features create a strong foundation for deploying intelligent systems that can keep pace with the speed and complexity of industrial operations. Preparing Real-time Data Input and Engineering Knowledge for a Reliability Agent Enabling MAGS at Scale Multi-Agent Generative Systems represent a significant leap forward in industrial AI. When built on robust data pipelines, MAGS can operate at scales previously unattainable. Here’s how: **Continuous Data Flow:**DataStreams ensure a constant flow of information from various sources across the industrial environment. Real-Time Processing: Data is processed as it’s generated, allowing for immediate analysis and action. Scalable Architecture: The pipeline approach can handle increasing data volumes as operations grow. Agent Diversity: DataStreams support a wide range of agent types, from sophisticated AI to simple scripts. This scalability and flexibility allow MAGS to tackle large-scale industrial challenges that would overwhelm traditional systems. Configuring an XMPro MAGS Agent Instance in XMPro DataStream Designer Versatility in Agent Deployment XMPro DataStreams supports a wide range of agent types, allowing businesses to deploy the right kind of agent for each specific task or problem: XMPro’s Sophisticated Reasoning Agents: Complex decision-making capabilities Ability to form multi-agent teams Continuous learning from real-time data Procedural Python Agents: Custom agents using XMPro’s Python Agent feature as “contractors“ Integration with frameworks like AutoGen and LangGraph Rapid development and deployment for specific tasks Third-Party and Open-Source Agents: Compatibility with various AI technologies Easy integration of specialized industry-specific agents This diverse ecosystem of agents, all operating within the same data pipeline, allows businesses to address a wide range of challenges efficiently and effectively. Solving Real-World Problems The combination of XMPro DataStreams and MAGS addresses several critical industrial needs: Predictive Maintenance: Advanced XMPro agents analyze equipment data in real-time, predicting failures before they occur. Supply Chain Optimization: A mix of sophisticated and simple agents process data from multiple points in the supply chain, identifying bottlenecks and suggesting improvements. Energy Management: MAGS can monitor energy usage across facilities, with different agent types handling various aspects of the process. Quality Control: Python-based agents handle routine inspections, while more complex agents manage overall quality strategies. These real-world applications demonstrate the practical value of integrating diverse agent types within a unified data pipeline system. The XMPro DataStreams Advantage XMPro’s DataStreams approach offers unique benefits for implementing MAGS: No-Code Configuration: DataStreams can be set up without extensive coding, making it easier to deploy and adjust agent systems. Flexible Integration: XMPro easily connects with existing industrial systems and various agent types, allowing for gradual adoption of MAGS. Visual Workflow Design: Complex agent interactions can be mapped out visually, improving understanding and management of the system. Agent Ecosystem Support: The platform accommodates a wide range of agent types, from simple scripts to advanced AI models. These features make XMPro DataStreams particularly well-suited for businesses looking to implement a diverse, scalable MAGS in their operations. XMPro MAGS using real-time data pipelines Overcoming Implementation Challenges While the benefits are clear, implementing MAGS with data pipelines does come with challenges. Here’s how the XMPro approach addresses common issues: Data Quality: DataStreams include built-in data cleansing and validation, ensuring all agent types work with reliable information. System Complexity: The visual nature of XMPro’s platform helps manage the complexity of multi-agent systems, even with diverse agent types. Scalability Concerns: XMPro’s cloud-native architecture allows for easy scaling as data volumes and processing needs grow. Agent Interoperability: The unified data pipeline ensures smooth communication between different types of agents. By addressing these challenges, XMPro makes it feasible for businesses to adopt and scale MAGS technology without overwhelming their existing operations. Looking to the Future As industrial processes become more complex, the need for intelligent, scalable, and flexible systems will only grow. The integration of MAGS with data pipeline approaches like XMPro DataStreams, supporting various agent types, represents a significant step forward. This combination offers the speed, flexibility, and intelligence needed to tackle tomorrow’s industrial challenges. Businesses that adopt this approach now will be well-positioned to: Improve operational efficiency through continuous, AI-driven optimization using the most appropriate agent types for each task. Respond more quickly to market changes and operational issues with a diverse agent ecosystem. Scale their AI capabilities alongside their business growth, easily integrating new agent technologies as they emerge. The integration of Multi-Agent Generative Systems with data pipeline approaches like XMPro DataStreams offers a practical and powerful solution for modern industrial challenges. By enabling real-time data processing, scalable AI deployment, and support for diverse agent types, this approach provides businesses with the tools they need to solve complex, real-world problems. As we move forward, the ability to harness the power of data and AI at scale, while maintaining the flexibility to use the right tool for each job, will become increasingly crucial. Companies that embrace these technologies now will be better prepared to meet the challenges and opportunities of tomorrow’s industrial landscape, with a versatile, efficient, and intelligent operational foundation. I previously wrote more on this in Part 3 – AI at the Core: LLMs and Data Pipelines for Industrial Multi-Agent Generative Systems. Our GitHub Repo has more technical information if you are interested. You can also contact myself or Gavin Green for more information."
  },
  "docs/resources/faqs/external-content/blogs/2024/the-evolution-of-skills-lessons-from-agriculture-in-the-genai-and-mags-era.html": {
    "href": "docs/resources/faqs/external-content/blogs/2024/the-evolution-of-skills-lessons-from-agriculture-in-the-genai-and-mags-era.html",
    "title": "The Evolution of Skills: Lessons from Agriculture in the GenAI and MAGS Era | XMPro",
    "summary": "The Evolution of Skills: Lessons from Agriculture in the GenAI and MAGS Era Articles, Blog, CEO'S Blog The Evolution of Skills: Lessons from Agriculture in the GenAI and MAGS Era Posted on October 29, 2024 by Pieter van Schalkwyk The Evolution of Skills: Lessons from Agriculture in the GenAI and MAGS Era This article was originally posted to XMPro CEO, Pieter Van Schalkwyk’s blog – The Digital Engineer, here At a recent Plants4Space meetup, where I presented Intelligent Digital Twins and Multi-Agent Generative Systems (MAGS), I received a thought-provoking question. Instead of the typical_“Will agents take my job?”_ the question was, “Will we lose essential skills as agents begin automating tasks humans used to perform?” This question goes to the heart of the ongoing technological transformation. It’s a valid concern, especially as we move from automation to autonomous agency, where systems like MAGSwill take on increasingly complex tasks. These systems don’t just automate—they observe, reflect, plan, and act, opening up new questions about the future of human skills. Here’s how I answered, using an analogy from agriculture, a sector that has undergone a series of technological transformations over the past century (and it was topical for the meetup). The Agriculture Analogy: From Oxen to Autonomous Agents Agriculture was one of the first industries to undergo massive transformation through mechanization. Farmers 150 years ago worked behind oxen and single-blade plows, relying on physical skills and experience to efficiently work the land. The more experienced the farmer, the more productive their work. The introduction of mechanical plows, starting with steam-powered tractors, revolutionized farming. By 1917, the Fordson Model F made tractors affordable, allowing even small-scale farmers to adopt mechanized processes. These machines codified many of the skills that farmers once needed—such as maintaining the correct depth of a furrow or the balance of a plow—into the equipment itself. Farmers no longer needed those manual skills; instead, they learned new ones related to operating and maintaining these machines. By the 1930s, tractors saved billions of labor hours annually. Today, farms are managed with fleets of machines that can be operated remotely, drones survey fields, and sensors monitor soil conditions. The skills to manually operate a plow have largely disappeared, but farmers now manage far more advanced systems that produce food for millions, not just local communities. This transition is a mirror for what we see now in many industries with the advent of Multi-Agent Generative Systems (MAGS). Just as tractors transformed farming, MAGS will transform industries by codifying tasks into autonomous agents that observe their environment, reflect on the data, plan their actions, and execute them. My Three Points on Skills in the Age of MAGS Yes, People Will Lose Skills That Can Be Replaced by Autonomous Agents Just as farmers no longer need the skills to balance a plow, many current tasks and skills will be replaced by agents in MAGS that can handle observation, decision-making, and execution autonomously. The agent will “know” how to optimize processes based on codified knowledge, much like how modern farm equipment operates today. No, These Skills Won’t Disappear—They Will Be Codified into Agents The skills and decisions that once required human experience will be embedded into agents that observe, reflect, plan, and act based on real-time data. This isn’t a loss but a shift. The expertise is not gone; it is codified into the Generative AI, Expert Systems, and Knowledge Bases that power MAGS. We Will Develop New Skills to Manage Autonomous Agents The rise of MAGS won’t eliminate the need for human skills—it will shift them. Instead of performing the tasks directly, we will focus on managing, optimizing, and improving the actions of these autonomous agents. The ability to oversee and direct these intelligent systems will become the new essential skill set. Humans will be responsible for guiding these agents to ensure their actions align with larger objectives and strategies, enhancing decision-making across industries. Adapting to Change: From Mechanization to MAGS The transition from manual labor to mechanized systems is now evolving into the rise of autonomous systems like MAGS, which take over not just physical tasks but also cognitive ones. The progression mirrors history: in agriculture, we moved from oxen to steam tractors to automated fleets. Now, in industries like manufacturing, healthcare, and space research, we’re seeing a similar shift—from humans performing tasks to agents taking on those roles with increasing autonomy. MAGS are not just tools; they are systems that can reason, plan, and execute. They operate based on a model that integrates real-time data, past experiences, and complex decision-making frameworks. As with farming, where tractors reduced physical labor but introduced new challenges and opportunities, MAGS will free humans from routine decision-making tasks while demanding new skills to manage this army of agents. In this new world, adaptability remains the key. We won’t need to manage individual machines or even fleets of automated tools; instead, we will manage systems of agents that can observe, reflect, plan, and act. Our role will shift to higher-order thinking, strategy, and guiding these systems to deliver optimal outcomes—whether that’s in a factory on Earth or a farm on Mars. A Future of New Opportunities As we move forward, the quality of life, like that of today’s farmers compared to their predecessors, will improve. Instead of losing skills, we will adapt and develop new ones that align with the demands of the future. MAGS and other autonomous systems will give us the opportunity to focus on more meaningful, strategic tasks that push the boundaries of what we can achieve. Whether it’s improving production efficiency on Earth or growing food in space, the shift towards Multi-Agent Generative Systems represents the next step in a long line of technological advancements that will allow us to work smarter, not harder. About Plants4Space The ARC Centre of Excellence in Plants for Space initiative, led by. Prof Matthew Gilliham at the University of Adelaide explores innovative approaches to food and plant growth in extraterrestrial environments. Prof. Volker Hessel and his team are using XMPro Digital Twin software in their groundbreaking plant research, and we look forward to sharing more results in collaboration with this team soon."
  },
  "docs/resources/faqs/external-content/blogs/2024/the-importance-of-pump-predictive-maintenance-for-operational-efficiency.html": {
    "href": "docs/resources/faqs/external-content/blogs/2024/the-importance-of-pump-predictive-maintenance-for-operational-efficiency.html",
    "title": "The Importance of Pump Predictive Maintenance for Operational Efficiency | XMPro",
    "summary": "The Importance of Pump Predictive Maintenance for Operational Efficiency Articles, Blog The Importance of Pump Predictive Maintenance for Operational Efficiency Posted on November 27, 2024 by Wouter BenekePump Predictive Maintenance Pump Predictive Maintenance: Enhancing Reliability and Efficiency Introduction Did you know that unplanned pump failures can cost industries thousands of dollars in lost productivity and emergency repairs? Pumps are critical components in various industrial processes, and their reliability directly impacts operational efficiency. As organizations strive to minimize downtime and optimize performance, predictive maintenance has emerged as a game-changing strategy. Predictive maintenance offers a proactive solution to ensure pump systems operate reliably and efficiently. By leveraging advanced technologies like IoT sensors, data analytics, and machine learning, organizations can effectively monitor pump health and predict potential issues before they escalate. This approach not only minimizes the likelihood of sudden equipment breakdowns but also significantly reduces maintenance costs and enhances safety across industrial operations. This article explores the value of pump predictive maintenance, highlighting its benefits, real-world applications, challenges, and future trends, as well as best practices for successful implementation. We will also discuss how XMPro’s Intelligent Business Operations Suite (iBOS) can be an ideal solution for implementing effective predictive maintenance strategies. Jump ToThe Role of Pumps in Industrial ProcessesWhat is Predictive MaintenanceBenefits of Predictive Maintenance for PumpsWhy XMPro is Ideal for Predictive MaintenanceChallenges and ConsiderationsFuture Trends in Pump Predictive Maintenanceseveral water pumps with large electric motors The Role of Pumps in Industrial Processes Pumps play an essential role in a wide range of industrial applications, from chemical processing to water treatment and mining operations. They are used to transfer liquids, control pressure, and facilitate cooling and heating processes. Any disruption in pump functionality can lead to severe consequences, including production delays, equipment damage, and even safety hazards for workers. Due to their critical role, ensuring pumps are maintained in optimal condition is of paramount importance. Traditional maintenance approaches, such as reactive and preventive maintenance, have inherent limitations. Reactive maintenance often leads to costly downtime, while preventive maintenance relies on fixed schedules that may not accurately reflect the actual condition of the pump. Predictive maintenance, on the other hand, is a data-driven approach that can address these limitations, providing a more efficient and cost-effective solution for maintaining pump health. Predictive Maintenance in Water Utilities What is Predictive Maintenance? Predictive maintenance is a proactive approach that leverages advanced technologies to anticipate equipment failures before they occur. Unlike reactive maintenance, which focuses on repairing equipment after it breaks down, or preventive maintenance, which involves scheduled servicing based on predefined intervals, predictive maintenance uses real-time data to determine when maintenance is truly needed. By utilizing tools such as IoT sensors, data analytics, and machine learning, organizations can monitor the health of their pumps in real time. This approach allows for timely interventions, reducing the likelihood of unexpected breakdowns and costly repairs. Predictive maintenance provides the insights needed to schedule maintenance activities based on actual equipment conditions rather than arbitrary timelines, ensuring pumps operate at peak efficiency. Key Components of Predictive Maintenance To understand how predictive maintenance works, it’s essential to explore the key components that make this strategy effective: IoT Sensors IoT sensors are the foundation of predictive maintenance. These sensors collect data on various parameters, such as vibration, temperature, pressure, and flow rates, providing insights into pump performance. By continuously monitoring these parameters, organizations can identify deviations from normal operating conditions that may indicate potential issues. Data Analytics Once data is collected from IoT sensors, it must be analyzed to extract meaningful insights. Advanced data analytics tools process the collected data to identify patterns, trends, and anomalies. These analytics help maintenance teams understand the condition of their pumps and predict when maintenance is required. Machine Learning Machine learning algorithms play a crucial role in predictive maintenance by analyzing historical data to improve the accuracy of predictions over time. These algorithms learn from past equipment behavior and use this knowledge to predict future failures. Benefits of Predictive Maintenance for Pumps Predictive maintenance offers several key benefits for pump operations, including cost savings, increased reliability, and enhanced safety. Let’s explore these benefits in detail and see how XMPro can enhance these outcomes: Cost Savings One of the most significant advantages of pump predictive maintenance is its potential for cost savings. By identifying issues early, organizations can avoid the high costs associated with unplanned downtime and emergency repairs. Predictive maintenance enables scheduled maintenance activities, allowing companies to allocate resources more efficiently. XMPro’s platform helps reduce costs by automating the detection of issues and optimizing maintenance schedules. By using XMPro, a manufacturing plant can reduce maintenance costs by up to 25%, thanks to real-time monitoring and efficient resource allocation. Furthermore, predictive maintenance with XMPro helps extend the lifespan of pumps by addressing issues before they cause significant damage. By maintaining pumps in optimal condition, organizations can reduce the need for expensive replacements and spare parts, leading to additional cost savings. Increased Reliability Predictive maintenance enhances the reliability of pumps by ensuring they operate within optimal parameters. Continuous monitoring allows for the early detection of anomalies, such as unusual vibrations or temperature fluctuations. Addressing these issues promptly helps maintain pump performance and extends equipment lifespan. XMPro’s real-time monitoring and alerting capabilities ensure that maintenance teams receive timely notifications of any anomalies. A case study involving a chemical processing facility demonstrated that implementing predictive maintenance using XMPro led to a 40% reduction in pump failures. By continuously monitoring pump conditions and addressing issues before they escalated, the facility significantly improved its operational reliability. Enhanced Safety Safety is paramount in industrial operations, and predictive maintenance contributes to a safer working environment by preventing catastrophic failures that could lead to accidents or injuries. Pumps that malfunction can create hazardous conditions, such as leaks, fires, or explosions. By ensuring that pumps are functioning correctly, organizations can protect their workforce and minimize risks. XMPro provides real-time alerts and automated workflows that ensure maintenance teams can respond quickly to potential safety hazards. For instance, a water treatment plant that adopted XMPro for predictive maintenance reported a decrease in safety incidents related to pump failures. By proactively addressing potential issues, the facility not only improved safety but also enhanced employee morale and confidence in the workplace. Why XMPro is Ideal for Predictive Maintenance XMPro’s Intelligent Business Operations Suite (iBOS) is an ideal solution for implementing predictive maintenance for pumps due to its comprehensive features and capabilities: Advanced Analytics and Machine Learning: XMPro leverages advanced analytics and machine learning models to analyze historical and real-time data, providing accurate predictions of pump failures and maintenance needs. The platform’s ability to learn from historical data helps improve the accuracy of predictions over time. Real-Time Alerts and Automated Workflows: XMPro’s real-time monitoring and alerting system ensures that maintenance teams are promptly notified of any anomalies. The platform also provides automated workflows, enabling quick and efficient responses to potential issues. Integration Capabilities: XMPro integrates seamlessly with existing IoT sensors, SCADA systems, and other data sources, providing a unified view of pump performance. This integration capability ensures a smooth transition to predictive maintenance without disrupting ongoing operations. User-Friendly Interface: XMPro’s user-friendly interface makes it easy for maintenance teams to access and interpret data, empowering them to take proactive action. The platform’s intuitive design ensures that maintenance teams can easily navigate and use its features effectively. Proven Success in Real-World Applications: XMPro has been successfully implemented by various companies across industries, yielding significant improvements in operational efficiency, cost savings, and safety. These success stories demonstrate XMPro’s effectiveness in enhancing predictive maintenance strategies. By using XMPro, organizations can achieve significant improvements in operational efficiency, reduce downtime, and enhance safety. The platform’s comprehensive features make it an ideal choice for implementing and optimizing predictive maintenance strategies for pumps. Challenges and Considerations Common Obstacles While the benefits of predictive maintenance are clear, organizations may face challenges when adopting this strategy. Initial costs for implementing new technologies and integrating them with existing systems can be significant. Additionally, there may be resistance to change within the organization. XMPro helps address these challenges by providing a scalable and flexible platform that integrates with existing systems, minimizing disruption. The platform’s user-friendly interface also helps reduce resistance to change by making it easy for teams to adopt new predictive maintenance practices. Best Practices To overcome these challenges, organizations should consider the following best practices: Start Small: Begin with a pilot program to test predictive maintenance on a few critical pumps before scaling up. XMPro’s platform is scalable, allowing organizations to start small and expand their predictive maintenance initiatives over time. Invest in Training: Provide training for maintenance teams to ensure they understand how to use new technologies effectively. XMPro offers training resources and support to help teams make the most of the platform. Foster Collaboration: Encourage collaboration between maintenance, operations, and IT teams to facilitate a smooth transition. XMPro’s integrated platform makes it easy for cross-functional teams to share insights and best practices, leading to more effective predictive maintenance strategies. Monitor and Adjust: Continuously monitor the performance of predictive maintenance initiatives and make adjustments as needed. XMPro provides tools for tracking performance metrics and making data-driven decisions to refine maintenance strategies. Future Trends in Pump Predictive Maintenance Emerging Technologies The future of predictive maintenance is bright, with emerging technologies poised to enhance its effectiveness. For instance, advancements in artificial intelligence (AI) will enable even more accurate predictions of pump failures. AI algorithms can analyze vast amounts of data and identify complex patterns that may not be apparent through traditional analysis methods. XMPro is at the forefront of these technological advancements, integrating AI and machine learning to provide organizations with cutting-edge predictive maintenance capabilities. The platform’s ability to create digital twins—virtual replicas of physical assets—allows for sophisticated simulations and analyses, optimizing maintenance activities and improving equipment performance. Industry Outlook As industries continue to embrace digital transformation, predictive maintenance will play a crucial role in shaping the future of pump operations. Organizations that adopt these strategies, particularly with the support of platforms like XMPro, will likely see improved efficiency, reduced costs, and enhanced safety. The ability to predict and prevent equipment failures will become a competitive advantage, allowing companies to maintain uninterrupted operations and deliver consistent quality to their customers. Furthermore, as more companies recognize the value of predictive maintenance, the demand for skilled professionals in this field will increase. XMPro’s user-friendly platform helps bridge the gap by providing maintenance teams with the tools and knowledge they need to implement effective predictive maintenance strategies. Conclusion In summary, pump predictive maintenance offers significant benefits for industrial operations, including cost savings, increased reliability, and enhanced safety. By leveraging advanced technologies like IoT sensors, data analytics, and machine learning, organizations can proactively manage their pump maintenance strategies, ensuring operational excellence in an increasingly competitive landscape. XMPro’s Intelligent Business Operations Suite is an ideal solution for implementing predictive maintenance for pumps. With advanced analytics, machine learning, real-time alerts, and seamless integration capabilities, XMPro empowers organizations to optimize their maintenance strategies and achieve better outcomes. Predictive maintenance not only reduces the risk of unplanned downtime but also helps extend the lifespan of critical equipment, leading to long-term cost savings and improved operational efficiency. As the industry evolves, embracing predictive maintenance with the right tools, such as XMPro, will be essential for staying competitive and ensuring that operations run smoothly and safely. Additional Resources Pump Predictive Maintenance in Water Utilities Cyclone Slurry Pump Condition Monitoring Engagement Opportunities We invite you to share your experiences with pump maintenance and predictive strategies in the comments below. How has predictive maintenance impacted your operations? Let’s foster a discussion and learn from each other’s insights. If you found this article valuable, consider sharing it within your network to spread the knowledge and inspire others to embrace predictive maintenance."
  },
  "docs/resources/faqs/external-content/blogs/2024/the-ultimate-guide-to-predictive-analytics.html": {
    "href": "docs/resources/faqs/external-content/blogs/2024/the-ultimate-guide-to-predictive-analytics.html",
    "title": "The Ultimate Guide To Predictive Analytics | XMPro",
    "summary": "The Ultimate Guide To Predictive Analytics Blog The Ultimate Guide To Predictive Analytics Posted on November 14, 2024 by Wouter Beneke The Ultimate Guide To Predictive Analytics 1. Introduction to Predictive Analytics Overview Predictive analytics uses historical data, statistical algorithms, and machine learning to predict future outcomes. Predictive analytics techniques are essential for enhancing decision-making and operational efficiency in various industries. Predictive analytics is becoming increasingly important in industrial operations to enhance decision-making and efficiency, offering predictive analytics benefits that drive competitive advantage. A recent survey found that 76% of asset-intensive companies have adopted predictive analytics to improve operational efficiency and asset management. This shift is largely driven by the need to minimize downtime, optimize resource use, and gain a competitive edge. XMPro’s Perspective XMPro’s predictive analytics platform is tailored to address the unique challenges of asset-intensive industries. By integrating data from sensors, historical logs, and operational systems, XMPro enables organizations to predict issues before they become critical. For example, in mining operations, XMPro’s predictive models, using data from sensors, operational logs, and historical analysis, helped a client reduce unplanned downtime by 25%, leading to significant cost savings and higher productivity. By leveraging sensor data for real-time monitoring and historical data for trend analysis, XMPro was able to provide proactive insights that led to timely maintenance interventions. This tailored approach ensures both operational and strategic decision-making are data-driven. Why Predictive Analytics with XMPro? XMPro’s Intelligent Business Operations Suite (iBOS) offers streamlined implementation of predictive analytics, designed to turn data into actionable insights quickly. The platform’s flexibility allows users to deploy predictive models seamlessly into their existing processes, providing real-time insights for proactive decision-making. This means fewer disruptions, better resource allocation, and improved overall performance. 2. The Predictive Analytics Process with XMPro Data Collection and Preparation XMPro simplifies the process of collecting and preparing data by integrating multiple data sources, including IoT devices, historical databases, and third-party data streams. High-quality data is essential for predictive analytics accuracy, and XMPro’s platform helps ensure data consistency and reliability through automated data cleaning, data quality checks, and validation. This step reduces errors and ensures that predictive models are built on solid foundations. Exploratory Data Analysis (EDA) Exploratory Data Analysis (EDA) is a critical part of the predictive analytics process. XMPro’s visualization tools help users explore data relationships, identify trends, and uncover key variables that influence outcomes. By providing intuitive dashboards and interactive graphs, XMPro allows analysts to quickly understand their data, setting the stage for effective modeling and insight generation. Model Selection XMPro offers a variety of built-in models and the flexibility to integrate advanced machine learning libraries such as TensorFlow and Azure ML. This approach allows users to select the most appropriate predictive model based on the specific use case. XMPro guides users through the predictive analytics model selection process, helping them choose between techniques like regression, classification, and clustering, depending on their goals and the nature of their data. Model Training and Validation Training and validating predictive models are essential to ensuring their accuracy and reliability. XMPro provides tools for splitting data into training and testing sets, which helps in evaluating model performance effectively. By continuously monitoring model outputs and recalibrating as needed through feedback loops and automated monitoring tools, XMPro helps organizations maintain the accuracy of their predictions over time, even as conditions evolve. Model Deployment with XMPro XMPro enables seamless real-time predictive analytics deployment, allowing predictive insights to be delivered directly where they are needed. Integration with existing operational workflows ensures that predictive models are not just theoretical exercises but become practical tools for decision-making. For instance, predictive maintenance alerts can be sent directly to maintenance teams, enabling swift action that prevents costly breakdowns. 3. XMPro’s Key Techniques for Predictive Analytics Statistical Modeling and Machine Learning XMPro supports a range of statistical and machine learning techniques, such as Random Forest and Support Vector Machines, to address diverse predictive analytics needs. Users can apply foundational methods like linear regression for trend analysis or use advanced algorithms such as decision trees and neural networks for more complex predictions. By offering a variety of approaches, XMPro ensures that organizations can tailor predictive models to meet their specific challenges, from simple trend forecasting to detailed anomaly detection. Agentic AI with XMPro XMPro incorporates Agentic AI through its Multi-Agent Generative Systems (MAGS) and APEX AI, allowing systems to react dynamically to changing conditions. These AI-driven agents can autonomously gather data, analyze it, and initiate actions in real time. For example, when equipment in a manufacturing plant begins showing early signs of wear, MAGS can initiate a series of actions—from alerting maintenance teams to adjusting operational parameters—to mitigate potential issues. This proactive, adaptive response capability makes XMPro’s predictive analytics uniquely effective in fast-paced industrial settings. Condition Monitoring and Predictive Maintenance XMPro’s platform excels in condition monitoring and predictive maintenance, providing real-time insights into asset health. For example, in the mining industry, XMPro has been used to monitor the condition of conveyor systems, identifying early signs of wear and potential failures. This has allowed operators to schedule maintenance proactively, resulting in reduced unplanned downtime and increased operational efficiency. By continuously analyzing data from sensors and other sources, XMPro can detect anomalies early, preventing costly equipment failures. Predictive maintenance powered by XMPro helps organizations optimize maintenance schedules, reduce unplanned downtime, and extend the life of critical assets, highlighting predictive analytics benefits for maintenance. The platform’s ability to deliver timely, actionable insights helps organizations maintain high levels of operational efficiency and reliability. Data Sources to Agent 4. Data Sources and Integration with XMPro Unified Data Ingestion XMPro offers seamless data integration for predictive analytics with diverse data sources, ensuring that organizations can consolidate their data into a unified platform. Data can be ingested from a variety of sources, including IoT sensors, SCADA systems, historical databases, and third-party APIs. This comprehensive data integration is crucial for developing accurate predictive models, as it provides a holistic view of operations. By unifying data from multiple sources, XMPro enables real-time insights that lead to more effective decision-making. Industrial IoT Compatibility XMPro’s compatibility with Industrial Internet of Things (IIoT) devices allows for real-time data collection and analysis at scale. XMPro can process vast amounts of sensor data from connected assets, enabling predictive analytics that can identify potential issues before they escalate. For example, by integrating data from temperature sensors, pressure gauges, and vibration monitors, XMPro can provide insights that help optimize equipment performance and prevent failures. This compatibility makes XMPro a powerful tool for industries such as oil & gas, mining, and manufacturing, where real-time monitoring is essential for maintaining productivity and safety. Contextual Data Integration In addition to sensor and operational data, XMPro also integrates contextual data—such as weather information, market conditions, and maintenance logs—that can enhance the predictive analytics process. By incorporating these additional data streams, XMPro allows organizations to consider a broader set of factors when analyzing potential outcomes. This enriched context improves the accuracy of predictions and helps organizations anticipate the impact of external variables on their operations, ultimately leading to more robust and informed decision-making. 5. Predictive Analytics Use Cases with XMPro Manufacturing In manufacturing, XMPro’s predictive analytics capabilities are used to reduce downtime, improve quality control, and optimize production processes. Predictive maintenance helps identify equipment issues before they result in costly breakdowns, ensuring that machinery is maintained efficiently. For instance, a manufacturer using XMPro saw a 30% reduction in unplanned downtime by implementing predictive analytics to monitor machine health. This led to increased operational efficiency and lower maintenance costs. Utilities and Energy For utilities and energy companies, predictive analytics enables real-time grid monitoring, demand forecasting, and proactive asset management. XMPro integrates data from various sources, such as smart meters and grid sensors, to predict demand fluctuations and optimize energy distribution. By forecasting equipment failures, utilities can reduce downtime and improve reliability. A utility company using XMPro was able to cut maintenance costs by 20% through better asset management and predictive insights. Agriculture In agriculture, XMPro helps optimize crop yield, monitor soil health, and improve farm management practices. Predictive analytics allows farmers to make informed decisions regarding irrigation, fertilization, and pest control based on real-time data and weather forecasts. For example, XMPro’s predictive analytics enabled a vineyard to optimize its irrigation schedule based on soil moisture data and upcoming weather conditions, resulting in a 20% reduction in water usage while maintaining crop quality. By integrating sensor data and contextual information, XMPro provides insights that help farmers boost productivity and minimize resource usage. For example, XMPro’s analytics enabled a farm to increase yield by 15% through optimized irrigation strategies. Mining In mining, XMPro’s predictive analytics are used to enhance operational efficiency, safety, and resource management. Predictive maintenance is crucial for reducing equipment failures, such as those involving crushers, conveyors, and haul trucks. By monitoring operational data in real-time, XMPro helps identify early warning signs of equipment issues, allowing for timely interventions that prevent costly breakdowns. In a mining operation, XMPro helped optimize crusher maintenance schedules, reducing unplanned downtime and improving throughput rates. Predictive analytics also aid in optimizing blasting and drilling schedules to maximize resource extraction while minimizing environmental impact. Oil & Gas In the oil & gas industry, XMPro provides predictive insights that support safe and efficient operations, including the monitoring of drilling rigs, pipelines, and refining processes. Predictive analytics help identify anomalies in pipeline pressure and flow, preventing leaks and ensuring environmental safety. XMPro’s predictive maintenance capabilities also help oil & gas companies maintain the integrity of offshore and onshore equipment, leading to reduced risk of operational disruptions. One oil & gas client using XMPro achieved a 25% reduction in unexpected maintenance incidents by leveraging predictive analytics for proactive equipment monitoring. Renewables In the renewables sector, XMPro’s predictive analytics are used to optimize the performance of wind turbines, solar panels, and battery storage systems. By integrating weather data with real-time performance metrics, XMPro helps renewable energy companies predict power generation and proactively manage assets. Predictive analytics also assist in optimizing maintenance schedules for wind turbines, reducing downtime and extending the lifespan of components. For instance, a renewable energy company using XMPro saw a 15% increase in energy production efficiency by optimizing the operation and maintenance of its solar panel installations. Freight & Logistics In freight and logistics, predictive analytics powered by XMPro helps optimize fleet management, improve delivery schedules, and minimize fuel consumption. By analyzing vehicle sensor data, traffic patterns, and weather conditions, XMPro provides insights that enhance route planning and reduce delays. Predictive maintenance of fleet vehicles ensures that trucks and other transport assets are kept in optimal condition, reducing unexpected breakdowns and improving reliability. A logistics company using XMPro achieved a 10% reduction in fuel costs by optimizing delivery routes and using predictive maintenance to keep vehicles running efficiently. Defence In the defence sector, XMPro’s predictive analytics capabilities are applied to asset management, operational readiness, and risk assessment. Predictive maintenance ensures that critical defence equipment, such as aircraft and ground vehicles, are mission-ready by identifying potential failures before they occur. XMPro also supports operational decision-making by analyzing data from multiple sources, providing real-time insights for situational awareness. This enables defence organizations to optimize resource allocation and maintain high readiness levels. For example, XMPro was used to predict maintenance needs for a fleet of military vehicles, reducing downtime and ensuring operational readiness. Smart Cities In smart city initiatives, XMPro’s predictive analytics could help manage urban infrastructure, including traffic systems, energy distribution, and public services. By integrating data from sensors across the city, XMPro could provide insights that improve traffic flow, reduce energy consumption, and enhance public safety. Predictive analytics could also help anticipate infrastructure maintenance needs, reducing disruptions and ensuring the efficient operation of city services. For instance, XMPro could be used in smart city projects to optimize traffic signal timings, potentially reducing congestion during peak hours. Tailored Use Cases with XMPro XMPro provides the flexibility to deliver custom predictive analytics solutions that could meet the unique needs of different industries. Whether it’s healthcare, construction, or public utilities, XMPro’s platform can be customized to deliver relevant insights that could address specific operational challenges. By focusing on industry-specific data sources and analytics models, XMPro aims to provide clients with actionable insights that could be directly applicable to their business context, driving value and improving decision-making. 6. Tools and Technology for Predictive Analytics on XMPro XMPro iBOS Platform XMPro’s Intelligent Business Operations Suite (iBOS) serves as the foundation for predictive analytics, providing a comprehensive and integrated platform for data collection, analysis, and action. The platform’s modular design allows users to select the components they need, ensuring a flexible and scalable solution. With XMPro, companies can manage their data, deploy predictive models, and take action—all from a single platform that is designed specifically for industrial operations. Integrated AI and Machine Learning XMPro seamlessly integrates with machine learning frameworks such as Azure ML, TensorFlow, and Scikit-Learn, enabling users to build and deploy custom predictive models. By leveraging these powerful tools, XMPro empowers users to create advanced analytics solutions that fit their unique requirements. Whether it’s supervised learning models for classification tasks or unsupervised learning for anomaly detection, XMPro supports a wide range of techniques to suit different use cases. Cloud and On-Premise Deployment XMPro offers the flexibility to deploy predictive analytics solutions either on-premises or in the cloud, based on the organization’s specific requirements. Cloud deployment allows for rapid scalability, remote access, and lower infrastructure costs, making it ideal for companies that require quick implementations. Alternatively, on-premises deployment provides better control over data security and compliance, which is crucial for industries dealing with sensitive information. XMPro also supports hybrid deployments, offering the best of both worlds for companies needing both flexibility and control. 7. Implementing Predictive Analytics with XMPro Building an XMPro-Enabled Data Culture Adopting predictive analytics successfully requires building a data-driven culture within the organization, rooted in first principles thinking. By breaking down complex problems into fundamental elements, XMPro helps organizations establish clear objectives and foundational understanding that guides the analytics journey. Training and onboarding programs are designed to ensure that teams understand the core principles of data analytics and are equipped with the knowledge to apply predictive analytics effectively in their daily operations. This foundation helps maintain focus on achieving clear, measurable objectives. XMPro Support and Resources XMPro offers a comprehensive customer success model that includes training, model deployment support, and continuous guidance to maximize ROI. For instance, a manufacturing client faced challenges in adopting predictive analytics due to limited technical expertise. XMPro’s onboarding process provided tailored training sessions and step-by-step deployment assistance, enabling the client to implement predictive models successfully and achieve a 20% reduction in downtime within the first six months. XMPro’s technical support team works closely with clients to ensure that predictive models are effectively implemented and adapted to changing business needs. Resources such as webinars, user guides, and best practices documentation are also available to help users get the most out of the platform and develop deeper insights from their data. Selecting the Right Use Cases with XMPro To achieve meaningful outcomes with predictive analytics, selecting the right use cases is essential. XMPro uses first principles thinking to help clients define the core problems they are trying to solve and establish objective functions that measure success. These objective functions are mathematical expressions that quantify the goals, such as cost savings, operational efficiency, or risk reduction. XMPro’s Use Case Prioritization tool helps clients identify high-impact projects that are feasible and align with their strategic goals. By focusing on objective functions, the tool ensures that selected use cases have a clear, measurable impact on business performance. Overcoming Common Challenges XMPro addresses many common challenges associated with predictive analytics, including data integration, real-time processing, and user adoption, through the lens of first principles thinking. By understanding the fundamental challenges at their core, XMPro provides targeted solutions that align with the organization’s objective functions. User-friendly interfaces and visualization tools help encourage adoption by making predictive analytics accessible to all stakeholders, not just data scientists. This alignment with objective functions ensures that every user, regardless of their role, understands how their actions contribute to the overall effectiveness of the solution. 8. Best Practices for Success with XMPro’s Predictive Analytics Data Quality and Governance Ensuring data quality is essential for the success of predictive analytics initiatives. XMPro addresses common data quality issues, such as missing data, inconsistencies, and data drift, by providing automated tools for data validation and cleaning. This ensures that predictive models are built on reliable data, leading to more accurate and meaningful insights. XMPro enforces data governance by providing tools that maintain data integrity, consistency, and accuracy. Establishing proper data collection standards and protocols ensures that predictive models are built on reliable inputs. Organizations should invest in data validation processes to filter out inaccuracies, ensuring the models produce meaningful and trustworthy insights. XMPro helps streamline this process, allowing organizations to manage and govern their data seamlessly. Iterative Model Refinement Predictive analytics models require continuous refinement to remain effective, especially in dynamic industrial environments. XMPro’s platform supports predictive analytics model iteration based on feedback from real-world performance. By frequently testing and recalibrating models, users can adapt to changing conditions and maintain accuracy over time. Organizations are encouraged to adopt a cycle of regular evaluation, making adjustments where needed to improve model performance and achieve their evolving business goals. Explainability and Transparency Transparency in predictive models is crucial for gaining stakeholder trust and ensuring adoption across all levels of the organization. XMPro focuses on creating explainable predictive analytics models, allowing users to understand how predictions are made. By providing clear explanations of the factors influencing predictions, XMPro helps bridge the gap between data scientists and business users by providing tools like interactive dashboards and automated reports that present complex data in an easily understandable format. This transparency fosters collaboration, ensures regulatory compliance, and helps all stakeholders act confidently on the insights generated. Compliance and Ethical Considerations As industries adopt predictive analytics, it is critical to address compliance and ethical issues, particularly concerning data privacy and the use of AI. XMPro includes features to ensure that predictive models comply with regulatory requirements, maintaining transparency and data security. Organizations should establish guidelines for ethical data use, defining what is and isn’t acceptable when making predictions. XMPro’s platform supports this by enabling data security controls and audit trails, ensuring that analytics initiatives align with both industry regulations and ethical standards. 9. The Future of Predictive Analytics with XMPro Scaling Intelligence and Automation The future of predictive analytics lies in scaling predictive analytics intelligence and automating decision-making across operations. XMPro’s Multi-Agent Generative Systems (MAGS) and APEX AI enable organizations to expand the use of predictive analytics across multiple processes seamlessly. By automating not only data analysis but also decision-making, XMPro allows enterprises to increase both the scope and intelligence of their operations, driving faster responses to dynamic conditions and improving overall efficiency. This scalability ensures that organizations can adapt as their needs grow, applying predictive insights more broadly. Real-Time and Edge Analytics With the rapid growth of the Industrial Internet of Things (IIoT), predictive analytics for IIoT, including real-time and edge analytics, are becoming crucial for maintaining operational efficiency. XMPro supports predictive analytics at the edge, allowing organizations to make informed decisions closer to where data is generated. By processing data in real-time and providing immediate insights, XMPro helps minimize latency and ensures that critical actions are taken promptly. This capability is particularly important for industries like manufacturing and energy, where rapid decision-making can prevent costly disruptions. Adaptive Predictive Models The future of predictive analytics will increasingly rely on adaptive models that can learn and evolve with changing environments. XMPro’s platform supports the creation and deployment of adaptive predictive models that continuously refine themselves based on real-time data and feedback. Before being implemented in live settings, these models are rigorously tested for accuracy and reliability using historical data simulations and validation processes, ensuring they perform well under real-world conditions. This approach allows organizations to remain proactive in the face of uncertainty, as models adjust automatically to new patterns, ensuring sustained accuracy. By deploying adaptive models, organizations can maintain their competitive advantage and respond effectively to fluctuating operational conditions. Emerging Technologies and XMPro’s Vision XMPro is committed to incorporating emerging technologies to enhance its predictive analytics capabilities further. Technologies such as quantum computing, advanced neural networks, and next-generation edge devices are on the horizon, promising to push the boundaries of predictive analytics even further. XMPro’s vision is to leverage these technologies to enable smarter, faster, and more integrated analytics solutions, helping clients stay ahead of industry trends and continue to innovate. By integrating these advancements, XMPro ensures that its clients benefit from the latest developments in predictive analytics, enabling them to maximize their operational potential. 10. Conclusion Why XMPro for Predictive Analytics XMPro offers a unique blend of scalable, customizable, and AI-driven predictive analytics solutions specifically designed for industrial operations, providing predictive analytics benefits for increased efficiency and reliability. By integrating data from multiple sources, automating analysis, and providing actionable insights, XMPro enables organizations to make informed decisions faster and more effectively. This leads to reduced operational disruptions, optimized resource allocation, and improved performance. XMPro’s platform is ideal for organizations looking to transform their operations with real-time, data-driven intelligence. Getting Started with XMPro Organizations interested in harnessing the power of predictive analytics can start by exploring XMPro’s solutions tailored to their industry. XMPro offers a trial and demo options, including a walkthrough of specific use cases and access to selected feature sets, to help companies understand the potential impact of predictive analytics on their operations. Engaging with XMPro’s team allows businesses to receive personalized guidance in identifying high-impact use cases and developing a roadmap for implementation. Additional Resources For further information, XMPro provides a range of resources, including case studies, whitepapers, and webinars, that showcase the power of predictive analytics in action. Organizations can access these materials to learn more about how XMPro’s solutions have driven success across various industries. Additionally, the XMPro blog offers ongoing insights into emerging technologies and best practices for leveraging predictive analytics to achieve business objectives."
  },
  "docs/resources/faqs/external-content/blogs/2024/the-valuefirst-approach-to-industrial-ai-why-mags-implementation-must-start-with-business-outcomes.html": {
    "href": "docs/resources/faqs/external-content/blogs/2024/the-valuefirst-approach-to-industrial-ai-why-mags-implementation-must-start-with-business-outcomes.html",
    "title": "The Value-First Approach to Industrial AI: Why MAGS Implementation Must Start with Business Outcomes | XMPro",
    "summary": "The Value-First Approach to Industrial AI: Why MAGS Implementation Must Start with Business Outcomes Articles, Blog, CEO'S Blog The Value-First Approach to Industrial AI: Why MAGS Implementation Must Start with Business Outcomes Posted on November 14, 2024 by Pieter van Schalkwyk The Value-First Approach to Industrial AI: Why MAGS Implementation Must Start with Business Outcomes This article was originally posted to XMPro CEO, Pieter Van Schalkwyk’s blog – The Digital Engineer, here The promise of artificial intelligence in industrial operations has led many organizations to pursue AI projects with enthusiasm but without clear direction. Multi-Agent Generative Systems (MAGS) represent the next evolution in industrial AI, offering specialized teams of AI agents that can transform operations. However, the rush to implement new technology often overshadows the fundamental question: What business problems are we trying to solve? Why Traditional AI Implementation Often Falls Short Industrial companies frequently approach AI implementation as a technology challenge rather than a business opportunity. This mindset leads to sophisticated solutions, searching for problems to solve. We’ve observed numerous organizations invest significant resources in AI projects that deliver impressive technical capabilities but minimal business impact. The most common implementation mistakes stem from this technology-first thinking: Selecting complex AI solutions before fully understanding operational challenges Pursuing advanced capabilities when simpler approaches would suffice Underestimating total implementation costs Missing opportunities for quick operational wins that could build momentum A Different Path: The Value-First Framework The value-first approach turns traditional AI implementation on its head. Instead of starting with technology capabilities, we begin by mapping your operational landscape and identifying specific areas where MAGS can deliver measurable improvements. This methodical approach ensures that every aspect of implementation ties directly to business outcomes. Our framework builds on years of experience implementing AI solutions in industrial settings. It consists of four integrated steps that guide organizations from initial assessment through to sustained value creation. (1) Business Impact Assessment The foundation of successful MAGS implementation lies in understanding your current operational state. We examine direct operational costs, including equipment performance, process efficiency, and labor productivity. These visible costs tell only part of the story, however. Equally important are hidden costs such as knowledge management, system integration, and change management requirements. Key assessment areas include: Direct Operational Costs – Equipment performance, process efficiency, and resource utilization Hidden Cost Factors – Knowledge preservation, system integration, and training needs Strategic Opportunities – Risk reduction, compliance improvements, and competitive advantages Establishing a baseline from which to work is a key aspect of measuring future value. (2) Agent Team Design With a clear understanding of your operational needs, we design MAGS teams that target specific business objectives. We focus on creating an “Objective Function.” This means that the business problem is broken into a mathematical function that states the objective that needs to be achieved. This makes it a measurable entity for the agent team to achieve. This process involves more than just selecting AI capabilities. We evaluate implementation complexity, time to value, and integration requirements for each potential application. This careful evaluation ensures that we deploy agent teams where they can deliver the quickest and most substantial returns. Each agent receives specific objectives and performance metrics aligned with your business goals. This clarity of purpose ensures that technical capabilities serve business needs rather than driving them. (3) Integration Strategy Successful MAGS implementation requires seamless integration with your existing operations. Our approach focuses on practical implementation steps that minimize disruption while maximizing value capture. We start by connecting with your current systems and data sources using our existing XMPro DataStreams capabilities, building on established processes rather than replacing them. This integration strategy helps maintain operational continuity while gradually introducing new capabilities. It allows your teams to adapt to new tools and processes at a sustainable pace, increasing adoption and effectiveness. However, scaling beyond technology Proof of Concepts presents a critical challenge that many organizations overlook. We frequently observe companies building simple Agentic AI POCs, only to discover these solutions aren’t sustainable at an enterprise level. True scaling requires more than just technical integration – it demands a structured approach to Agent Operations (AgentOps). Implementing frameworks like XMPro APEX should be a key consideration in your organization’s evaluation of Agentic AI solutions. This comprehensive approach to AgentOps ensures your MAGS implementation can scale effectively across the enterprise while maintaining performance, security, and governance standards. (4) Performance Measurement Continuous measurement of both technical performance and business value ensures sustainable results. We establish clear metrics based on the Objective Function that track not only agent performance but also business outcomes. This ongoing measurement helps identify optimization opportunities and validates return on investment. The Human Factor: Key to Sustainable Success Technology implementation succeeds or fails based on human factors. Different roles within your organization have distinct needs and expectations from MAGS implementation: Operations teams need practical tools that reduce routine work and support better decisions. They require systems that integrate smoothly with existing workflows and provide clear, actionable insights. Engineering teams focus on problem-solving capabilities and knowledge preservation. They need tools that enhance their technical expertise rather than replace it, while helping them document and share critical operational knowledge. Supervision teams require clear visibility into performance metrics and resource utilization. They want systems that help them optimize operations while maintaining safety and compliance standards. Implementation Strategy: A Phased Approach Our implementation strategy follows three distinct phases: Phase 1: Foundation – We focus on core capabilities and immediate value capture. This phase establishes baseline performance metrics and builds team confidence through early wins. Phase 2: Expansion – Building on initial success, we introduce specialized functions and expand system integration. This phase validates the value creation model and optimizes performance. Phase 3: Scale – The final phase enables full deployment and cross-process optimization. Here we focus on maximizing value capture and ensuring sustainable results. The Path Forward Organizations that follow this value-first framework position themselves for successful MAGS implementation that delivers real operational improvements. The focus remains on solving business problems rather than implementing technology for its own sake. Three key elements determine success: Clear identification of value opportunities Structured implementation approach Strong focus on human engagement When these elements align, MAGS implementation can transform industrial operations in ways that deliver sustained business value. Our GitHub Repo has more technical information if you are interested. You can also contact myself or Gavin Green for more information. Read more on MAGS at The Digital Engineer About the Author: Pieter van Schalkwyk is the CEO of XMPro, helping industrial organizations implement practical AI solutions that deliver measurable business value."
  },
  "docs/resources/faqs/external-content/blogs/2024/understanding-the-difference-between-xmpro-ai-assistant-and-ai-advisor.html": {
    "href": "docs/resources/faqs/external-content/blogs/2024/understanding-the-difference-between-xmpro-ai-assistant-and-ai-advisor.html",
    "title": "Understanding the Difference Between XMPro AI Assistant and AI Advisor | XMPro",
    "summary": "Understanding the Difference Between XMPro AI Assistant and AI Advisor Articles, Blog Understanding the Difference Between XMPro AI Assistant and AI Advisor Posted on September 9, 2024 by Pieter van Schalkwyk Understanding the Difference Between XMPro AI Assistant and AI Advisor As the industrial sector continues to embrace AI-driven solutions, it is important to differentiate between tools that offer unique capabilities tailored to specific operational needs. XMPro offers two distinct AI-powered solutions: the XMPro AI Assistant (Copilot) and the XMPro AI Advisor. While both tools provide valuable insights and support, they serve different functions and use cases within an organization. Understanding these differences can help you determine how to leverage these technologies for your business best. XMPro AI Assistant: The Interactive Copilot for Operational Support The XMPro AI Assistant acts as an interactive copilot, designed to empower business users with real-time access to data and information. It integrates with XMPro data sources, including recommendations and systems connected to XMPro, providing users with the ability to create natural language queries. User-Friendly Interaction: The AI Assistant allows users to interact with data in a way that feels natural and intuitive. Queries can be made in plain language, making it easier for operators, engineers, and other staff to access relevant information without needing deep technical knowledge. Access to Comprehensive Documentation: The AI Assistant provides access to important business documents such as Standard Operating Procedures (SOPs), product manuals, control system specifications, policy documents, and regulatory guidelines. This ensures that users have the information they need at their fingertips. Tailored Industrial Use Cases: Using specialized Retrieval Augmented Generation (RAG) technology, the AI Assistant is optimized for industrial environments. It provides contextually relevant responses that help users make informed decisions quickly. On-Demand Assistance: Unlike always-on monitoring systems, the AI Assistant is initiated by user queries. This on-demand approach allows business users to get support whenever they need it, reducing downtime and keeping operations running smoothly. Use Cases and Benefits of the XMPro AI Assistant Training and Onboarding The AI Assistant supports new or less experienced staff by providing instant answers to queries about procedures, system operations, and best practices. This is particularly useful in training environments where immediate feedback is critical to learning. Benefit: Reduces the learning curve for new hires and accelerates their path to full productivity. Management of Change The Assistant helps users adapt to changes in technology, processes, or procedures by providing clear guidance based on the latest available information. Whether implementing a new SOP or adjusting a control system, the Assistant helps bridge knowledge gaps. Benefit: Minimizes errors and reduces the impact of changes on operational efficiency. Day-to-Day Operational Support Users can resolve routine queries and minor issues without needing to contact a human SME. This allows operational staff to work more independently while still having access to the support they need. Benefit: Frees up subject matter experts for more complex tasks and improves overall operational flow. Follow-the-Sun Operations In global operations, where SMEs may be located in different time zones, the AI Assistant provides continuous support. This ensures that no matter the time of day, users have access to the information they need. Benefit: Improves response times and operational continuity across time zones. XMPro AI Advisor: The Always-On Expert for Proactive Insights The XMPro AI Advisor goes beyond on-demand assistance by continuously monitoring data streams, recommendations, and events. It acts as an always-on expert that provides in-depth analysis of recommendations and system performance, helping users interpret complex data in real time. Continuous Monitoring and Analysis: The Advisor constantly evaluates data streams and recommendations, providing expert interpretations and actionable insights. This helps users understand what recommendations mean in the context of their operations. Expert Access to Documentation: Like the AI Assistant, the AI Advisor also has access to SOPs, manuals, control system specifications, and policy documents. This enables it to provide expert-level advice based on comprehensive data and documentation. Advanced Scenario Analysis: The Advisor additionally uses domain-specific Python libraries to perform detailed analyses of event data, allowing SMEs to conduct deeper investigations into operational challenges. This facilitates better decision-making and problem-solving. Proactive, Expert Guidance: The Advisor provides ongoing advice on optimizing process configurations, troubleshooting issues, and improving system performance. It is designed to help users stay ahead of potential problems rather than simply reacting to them. Use Cases and Benefits of the XMPro AI Advisor Proactive Monitoring and Alerts The AI Advisor continuously monitors data for anomalies, flagging potential issues and suggesting corrective actions before problems escalate. For instance, it can detect deviations in process parameters and recommend immediate adjustments. Benefit: Increases system reliability and helps prevent costly downtime by addressing issues early. Advanced Analysis of Recommendations The Advisor provides context around complex recommendations, helping users understand not just what action to take but why it is necessary. This expert analysis is particularly valuable in environments where decisions must be made quickly and accurately. Benefit: Enhances the decision-making process by adding expert insights directly into operational workflows. Scenario-Based Expert Guidance SMEs can interact with the AI Advisor for advanced troubleshooting and scenario-specific guidance. This is particularly useful in high-stakes situations where in-depth analysis is required to make the best decisions. Benefit: Enables SMEs to resolve complex problems faster, improving overall process efficiency. Support for Continuous Improvement The Advisor provides ongoing feedback on process performance, helping teams refine their operations over time. By continuously learning from data, it helps optimize configurations and procedures for better outcomes. Benefit: Drives ongoing improvements in efficiency, reducing costs and increasing productivity. Key User Categories and What These Tools Mean for Them The XMPro AI Assistant and AI Advisor are designed to support three main categories of industrial users, each with unique needs and examples of roles within these categories: Operational Users This category includes roles like control room operators and supervisors who are responsible for the day-to-day operation of plant systems. For these users, the AI Assistant provides immediate access to information, helping them troubleshoot issues, follow SOPs, and maintain smooth operations. Impact: Operational users benefit from instant guidance and reduced dependency on SMEs, improving their ability to respond quickly to operational challenges. Technical Users This group includes control engineers, such as those working with PLC, DCS, or SCADA systems. These users rely on precise data and insights to configure and maintain control systems. The AI Advisor’s continuous monitoring and analysis provide them with the expert insights needed to optimize configurations and anticipate problems. Impact: Technical users gain from advanced recommendations and continuous system analysis, allowing them to maintain optimal control system performance without constant manual oversight. Process Improvement Users Roles in this category include process engineers, such as chemical or metallurgical engineers, who focus on enhancing process efficiency and performance. The AI Advisor offers these users deep insights into data streams and expert recommendations for process optimization, enabling targeted improvements. Impact: Process improvement users can make data-driven decisions that enhance process efficiency, reduce waste, and improve overall productivity. Choosing the Right Tool for Your Needs Both the XMPro AI Assistant and AI Advisor provide distinct, valuable benefits that cater to different operational needs. The Assistant is ideal for on-demand support, helping users navigate documents and resolve routine issues with ease. The Advisor, on the other hand, provides continuous, expert-level insights that drive proactive decision-making and continuous improvement. Together, these tools offer a comprehensive approach to enhancing industrial operations, supporting both day-to-day tasks and long-term strategic goals. By understanding the differences and benefits of these AI solutions, organizations can better leverage XMPro’s technology to drive more effective and efficient operations."
  },
  "docs/resources/faqs/external-content/blogs/2024/why-solving-the-problem-doesnt-solve-the-problem-the-importance-of-scalable-intelligent-operations-w.html": {
    "href": "docs/resources/faqs/external-content/blogs/2024/why-solving-the-problem-doesnt-solve-the-problem-the-importance-of-scalable-intelligent-operations-w.html",
    "title": "Why Solving the Problem Doesn’t Solve the Problem: The Importance of Scalable Intelligent Operations | XMPro",
    "summary": "Why Solving the Problem Doesn’t Solve the Problem: The Importance of Scalable Intelligent Operations Articles, Blog Why Solving the Problem Doesn’t Solve the Problem: The Importance of Scalable Intelligent Operations with XMPro iBOS Posted on June 1, 2024 by Wouter Beneke Why Solving the Problem Doesn’t Solve the Problem: The Importance of Scalable Intelligent Operations with XMPro iBOS Watch Video Summary Introduction In today’s fast-paced industrial landscape, companies are under immense pressure to optimize operations and drive efficiency. The demand for real-time insights, seamless integration, and scalable solutions has never been more critical.\\ The Challenge: Industrial companies need to build new Intelligent Operations and Process Solutions to address a variety of needs such as Condition Monitoring, Predictive Maintenance, Process Optimization, Asset Performance Management (APM), Asset Health, Golden Batch production and more.\\ They must manage vast amounts of data from various sources, ensure timely and accurate decision-making, and maintain a competitive edge in a rapidly evolving market. However, achieving these goals is far from straightforward. The complexity of modern industrial operations often results in siloed data, fragmented processes, and a lack of real-time visibility. These issues hinder the ability to make informed decisions and optimize operations effectively.\\ Outdated Approaches: Many businesses still rely heavily on manual, rule-based systems for their operational processes. These traditional methods lead to operational inefficiencies, limited insights, and a competitive disadvantage. The skills shortage in the industry further exacerbates these problems, making it clear that sticking with outdated processes is unsustainable.\\ Additional Challenges: Data Overload: Managing and analyzing massive amounts of data without the right tools. Integration Issues: Complex and time-consuming integration of data from various sources and systems. Real-Time Processing: Struggling to achieve real-time data processing and analytics for timely decision-making. Scalability: Ensuring solutions can scale as companies grow. Security Concerns: Protecting sensitive data and ensuring cybersecurity. Regulatory Compliance: Adhering to industry regulations and standards. Operational Downtime: Minimizing unplanned downtime with predictive maintenance. Energy Efficiency: Improving energy efficiency and reducing environmental impact. IT-OT Disconnect: Bridging the gap between Information Technology (IT) and Operational Technology (OT) systems to ensure seamless data flow and unified operations. The Need for Change: It is evident that companies need a new approach—one that not only addresses immediate operational needs but also provides a scalable, unified framework for long-term success. The complexity and scale of modern industrial challenges demand solutions that integrate advanced AI, automation, and digital twins to empower rapid digital transformation and maintain a competitive edge. By understanding these market insights and the inherent challenges, businesses can better appreciate the need for innovative solutions designed to transform industrial operations with intelligent, scalable, and integrated processes.\\ Alternative Approaches To tackle these challenges, many companies rely on point solutions and specialist consultants to build bespoke systems tailored to specific operational needs. These custom solutions are often designed to address immediate issues, but they come with inherent limitations.\\ One of the primary challenges is the Theory of Constraints. As these bespoke solutions are developed, they frequently solve one bottleneck, only to reveal a new one, potentially leading to even worse performance if the new bottleneck is not addressed. For example, in the mining industry, a company might deploy an advanced predictive maintenance tool to monitor the health of conveyor belts. This tool can predict when a conveyor belt is likely to fail, allowing maintenance to be scheduled proactively, thereby reducing unplanned downtime. However, as the predictive maintenance tool optimizes conveyor belt performance and increases their capacity, a new bottleneck is revealed at the hoists. The hoists, which are not integrated with the predictive maintenance system, become the next point of failure as they cannot handle the increased load from the conveyors. This lack of integration results in fragmented data and disjointed processes, making it difficult to achieve a holistic view of operations and causing new inefficiencies to emerge.\\ Theory Of Constraints Moreover, as new solutions are required, the problem intensifies. Each new system, whether for process optimization, OEE tracking, or environmental monitoring, is typically built independently and lacks interoperability with existing tools. This creates a complex web of disconnected systems that fail to provide cohesive operational insights. Over time, maintaining and scaling these disparate solutions becomes increasingly challenging and expensive. The complexity of managing these fragmented solutions often results in skyrocketing costs. Many man-hours are spent on building and manually integrating bespoke systems, leading to significant financial burdens. The continuous need for customization and manual updates further increases operational costs and resource allocation, diverting attention from strategic initiatives. The disconnect between IT and OT systems further complicates the situation. IT focuses on data processing and cybersecurity, while OT is concerned with physical processes and machinery. This separation can lead to integration challenges, data silos, inconsistent insights, and operational delays, making it difficult to achieve unified, real-time operational visibility. The current approach to building operational and business process solutions is fundamentally flawed. Companies are forced to piece together point solutions and bespoke systems, leading to inefficiencies and a fragmented view of their operations. The reliance on manual, rule-based systems further exacerbates these issues, causing operational inefficiencies and limiting insights. This results in a competitive disadvantage as companies struggle to keep up with more modernized competitors.\\ The Need for a Unified Approach: What is needed is a comprehensive, integrated framework that can seamlessly unify these solutions. It also needs to provide the tools and capabilities for SMEs and consultants to build and compose these solutions in a scalable, integrated, and composable way. XMPro addresses these needs by offering a unified approach that supports both the integration of existing tools and the development of new, intelligent solutions. Our platform enables businesses to transition from outdated, manual processes to advanced, AI-driven operations that are efficient, insightful, and highly integrated.\\ In the Perfect World Imagine a perfect world where your operations are seamlessly integrated, and all your data flows freely across systems, providing real-time insights and a unified view of your business. In this ideal scenario, companies can rapidly advance in their digital transformation journey.\\ Operations are enhanced with advanced analytics, AI, and machine learning, enabling proactive decision-making and predictive maintenance. Companies can anticipate issues before they arise, optimize processes continuously, and make data-driven decisions that improve strategic planning and operational effectiveness. Automation reaches new heights, streamlining workflows and reducing manual intervention. Companies achieve significant efficiency gains, from individual assets to entire factories and enterprise systems. Automation ensures consistent, high-quality outputs and frees up human resources for more strategic tasks. Solutions expand from individual assets to entire systems of systems, creating comprehensive digital twins that offer unparalleled visibility and control over every aspect of operations. This integrated approach ensures that all processes, from simple to complex, are managed effectively and efficiently, providing a holistic view of the entire operation. Silos and fragmentation are eliminated, allowing for seamless integration and optimized processes. Companies can rapidly compose intelligent operations and process solutions that scale with their business needs. Whether deployed on-premise, in the cloud, or using a hybrid approach, XMPro ensures flexibility. Data streams can even run on the edge for real-time analysis, ensuring timely and actionable insights. In this perfect world, companies leverage XMPro’s Co-Pilot and GenAI capabilities to interrogate real-time data and perform in-depth analysis. These advanced tools provide immediate insights, enabling businesses to respond swiftly to operational changes and make informed decisions with confidence. In this perfect world, companies experience enhanced operational efficiency, reduced downtime, increased productivity, and improved competitiveness. They can respond swiftly to market changes, regulatory demands, and customer needs, ensuring sustained growth and success in a rapidly evolving industrial landscape.\\ Introducing XMPro Introducing XMPro iBOS, the world’s only AI-powered suite of tools and framework designed to rapidly compose integrated and intelligent business operations and process solutions at scale. XMPro iBOS addresses the challenges of modern industrial operations by offering a scalable, integrated, and composable framework. With XMPro, you can rapidly build and deploy intelligent operations and process solutions. Our platform enables seamless integration of data from various sources, providing real-time insights and a unified view of your operations. Advanced Analytics and AI: XMPro enhances the intelligence of your operations through advanced analytics and AI. Our platform leverages machine learning, predictive analytics, and generative agents to enable proactive decision-making, anomaly detection, and predictive maintenance. By embedding AI into real-time data streams, XMPro allows you to anticipate and respond to operational challenges before they escalate. Automation Capabilities: Our automation capabilities streamline workflows and reduce manual intervention, leading to significant efficiency gains. XMPro supports event-driven automation and integrates seamlessly with robotic process automation (RPA) technologies, allowing you to automate routine tasks and focus on strategic initiatives. Digital Twin Capabilities: Additionally, XMPro expands the scope of your digital solutions from individual assets to entire systems of systems. Our digital twin capabilities provide comprehensive digital representations of physical assets and systems, enabling real-time monitoring, simulation, and optimization. This holistic view offers unparalleled visibility and control over every aspect of your operations. Flexible Deployment: XMPro can be deployed on-premise, in the cloud, or as a hybrid solution. This flexibility ensures that you can choose the deployment model that best fits your business needs. Moreover, data streams can run on the edge for real-time analysis, providing timely and actionable insights directly where they are needed most. Co-Pilot and GenAI Capabilities: XMPro’s Co-Pilot and GenAI capabilities enable users to interrogate real-time data and perform in-depth analysis effortlessly. These tools provide immediate, actionable insights and recommendations, empowering users to make informed decisions and optimize operations continuously. Key Benefits: By leveraging XMPro iBOS, companies can achieve rapid digital transformation, improving operational intelligence, automation, and the scope of their solutions, all within a unified, scalable framework. This transformative approach ensures that businesses remain competitive and thrive in a rapidly evolving industrial landscape.\\ How Does XMPro Enable Scalable Intelligent Operations? XMPro iBOS stands out by offering a suite of foundational capabilities that are underpinned by a framework that is trustworthy, secure, integrated, interoperable, real-time, and event-driven. These features form the bedrock of our platform, ensuring that your operations are built on a reliable and robust framework.\\ Digital Transformation Across Three Critical Dimensions: Let’s have a look at this use case matrix to illustrate how XMPro empowers companies to grow across three critical dimensions: Intelligence, Automation, and Digital Twin Scope.\\ 1.Intelligence: The first axis, Intelligence, spans from rule-based systems to advanced AI and generative AI. XMPro’s AI capabilities bring intelligence to solution building by leveraging machine learning, natural language processing, and predictive analytics. The three levels of intelligence include the following: Rule-Based Systems: These are foundational systems that follow predefined rules and logic to provide basic automation and decision-making. Augmented AI: This level involves embedding AI models and algorithms into processes to enhance decision-making with real-time data and predictive analytics. It provides deeper insights and supports more complex decision-making scenarios. Generative Agents: The highest level, incorporating advanced AI and machine learning models, including generative AI. These agents provide proactive recommendations, anomaly detection, and predictive maintenance, ensuring optimal operational performance. Key modules that enhance intelligence include the following: Data Stream Designer: Facilitates real-time data processing, allowing for scalable model management and MLOps by embedding models into the data stream. This can include in-house models running on-premise or cloud-based models such as OpenAI GPT variants, enhancing the overall intelligence of operations. This tool also supports granular rule-based recommendations and anomaly detection​. XMPro AI: Leverages machine learning and predictive analytics to enable proactive decision-making, anomaly detection, and predictive maintenance. Features include embedded AI, generative AI, and augmented AI, providing real-time context and insights for decision-making and process optimization​​. App Designer: Visually displays data from various sources as integrated and orchestrated within data streams, supporting comprehensive decision-making and enhancing the intelligence of operations. It features the Co-Pilot capability, which allows users to interrogate real-time data from assets and create custom real-time reports. The Co-Pilot can also notify plant managers of anomalies and predictive insights​​. Recommendation Manager: Provides actionable insights and AI-generated predictive recommendation alerts that are intelligently ranked and actioned by criticality scoring. Recommendations can be triggered by large language models (LLMs) that poll and scan real-time data and results from anomaly detection and machine learning models. This capability is crucial for maintaining operational efficiency and effectiveness by providing timely, data-driven recommendations​​. 2.Automation: The second axis, Automation, covers everything from human workflows to event streams and robotic process automation (RPA), culminating in fully autonomous operations. The three levels of automation include the following: Basic Automation: This level includes simple rule-based automation for routine tasks, where human decision-making is still necessary within the workflow, reducing manual intervention and increasing efficiency. Process Automation: Involves automating complex processes and workflows, integrating with various systems to streamline operations and improve efficiency. Autonomous Operations: The highest level of automation, where advanced AI and machine learning drive fully autonomous operations, minimizing human intervention and optimizing processes in real-time. Key modules that drive automation include the following: Data Stream Designer: Enables real-time data processing and automation of routine tasks, automating data collection, analysis, and response mechanisms. It supports event-driven automation and seamless integration with various data sources, allowing for efficient and streamlined operations​​. XMPro AI: Embeds AI models within data streams to automate decision-making processes and operational workflows, reducing manual intervention. It provides advanced analytics and predictive insights to support automated responses and optimization​. App Designer: Allows users to create custom applications that automate workflows and streamline operations with a drag-and-drop interface for easy deployment of intelligent applications. It enables the visualization of automated processes and real-time monitoring of operations​​. Recommendation Manager: Uses AI logic and business rules to offer prescriptive recommendations, ensuring continuous process improvement and operational efficiency. It provides real-time recommendations and alerts based on data analysis, helping to automate decision-making and optimize processes​​. 3.Digital Twin Scope: The third axis, Digital Twin Scope, ranges from discrete entities to aggregated and complex entities. The three levels of digital twin scope include the following: Discrete Entity: Digital representations of individual assets, providing visibility and control over their operations. Aggregated Entity: Digital twins of combined assets, like a haul truck, enabling the monitoring and optimization of interconnected systems. Complex Entities: Comprehensive digital twins of entire systems, offering a holistic view of complex operations and facilitating advanced simulation and optimization. Key modules that enhance digital twin capabilities include the following: Data Stream Designer: Manages real-time data streams from various sources, ensuring accurate and up-to-date digital representations of physical assets. Deployed by use case, it can be used across thousands of the same asset. Clients have seen great success by starting on use cases at the operational level, focusing on discrete assets where the business solution would have a high impact, then scaling out quickly to tactical solutions and finally strategic solutions. As companies build out datastreams per use case within a discrete asset or component, it is easy to integrate these datastreams into a composite digital twin. The same applies to systems of systems​​. XMPro AI: Integrates AI models into these systems, ensuring continuous improvement and scalability from individual assets to entire interconnected systems. With MLOps and flexibility in model training, including TS foundation model utilization, XMPro AI facilitates rapid AI solution deployment​​. App Designer: A no-code UI builder that allows companies to rapidly build integrated interfaces for their solutions at operational, tactical, and strategic levels. This flexibility ensures that solutions can easily and flexibly scale from discrete digital twins to systems of systems​. Recommendation Manager: Increases scope by allowing sophisticated scoring rules for importance and criticality. Recommendations can be turned into actions that close the loop on event response, including work orders, notifications through email, SMS, collaboration software such as Microsoft Teams, ERP systems, and more. This scales from simple recommendations for a single asset to complex strategic-level views of entire systems of systems, ensuring the most critical recommendation alerts reach the correct employee or team for action​​. Only XMPro enables companies to advance simultaneously across these three dimensions, providing a flexible and scalable framework that adapts to your evolving needs. By integrating intelligence, automation, and digital twin capabilities, XMPro delivers a unified solution that drives rapid digital transformation, optimizes processes, and fosters a comprehensive understanding of your business operations. XMPro iBOS offers flexible deployment options, whether on-premise, in the cloud, or as a hybrid solution. This flexibility allows you to choose the deployment model that best fits your business needs. Additionally, data streams can run on the edge for real-time analysis, ensuring timely and actionable insights​​. XMPro’s Co-Pilot and GenAI capabilities enable users to interrogate real-time data and perform in-depth analysis effortlessly. These tools provide immediate, actionable insights and recommendations, empowering users to make informed decisions and optimize operations continuously​. By advancing across intelligence, automation, and digital twin scope, XMPro iBOS provides a unified, scalable framework that ensures your business remains competitive and thrives in a rapidly evolving market.\\ Case Studies: Successful Implementations of XMPro iBOS 1. $4M Saved – Predictive Maintenance for Underground Long Conveyor In this project, the challenge was to reduce the downtime of underground conveyors by 30%. XMPro introduced intelligence through real-time data monitoring and predictive analytics, monitoring 52 conveyors over 80 km. Automation was achieved by integrating with OSIsoft Historian and Oracle EAM, allowing complex engineering models to execute at 2-second intervals. The scope expanded from monitoring discrete assets (individual conveyors) to a composite digital twin of the entire conveyor system. This comprehensive monitoring and predictive maintenance strategy led to: Preventing approximately 184 hours of downtime. Identifying 44k tonnes of product worth $4M. Achieving a 30% reduction in downtime. Expansion: The success with conveyors allowed the company to scale their use cases, applying the same principles to other assets such as borers, crushers, pumps, fans, and overall management OEE, generating 72 million messages a day. Measures of Success: Time to Value: 30 days to deploy initial release, integration with OSIsoft Historian and Oracle EAM, and execution of complex engineering models at 2-second intervals. Always On, Situational Awareness: Monitoring and analysis of 52 long conveyors every 2 seconds, real-time dashboards for decision support and automation. Expert Knowledge Capture: Continuous analysis and recommendations integrated into predictive maintenance processes. Benefits: Intelligence: Real-time monitoring and predictive analytics. Automation: Integration with existing systems and automated alerting. Scope: From individual conveyors to an interconnected system of various assets. Financial Impact: $4M in additional revenue due to reduced downtime. 2. $8M Savings in First 6 Months This project aimed to increase the predictability of asset operations. XMPro enhanced intelligence by integrating disparate data and systems with field service maintenance, improving data-based decision-making. Automation was introduced to reduce truck rolls and improve asset utilization, with an 18% reduction in field service trips. The scope expanded from discrete assets to a strategic level view of operations. The project resulted in: $8M in documented savings in 6 months. Achieving break-even in less than 4 weeks. Improved asset integrity and utilization. Measures of Success: Time to Value: 3 months to deploy initial release, addressing data integrity issues, achieving break-even in less than 4 weeks. Documented Savings: $8M in 6 months through Lean Sigma look back, reduced truck rolls, and improved asset utilization. Cost and Safety: 18% reduction in field service trips, increased first-time fix rates, and improved safety. Efficiency: 95% reduction in weekly maintenance schedule planning. Benefits: Intelligence: Enhanced data integration and decision-making. Automation: Reduced field service trips and improved efficiency. Scope: From individual assets to a strategic operational view. Financial Impact: $8M in savings in 6 months. 3. 95% Reduction in Noise for Critical Assets Engineers faced the challenge of being flooded with alarms, making it difficult to track critical events. XMPro’s solution introduced intelligence by scoring each event in real-time with company-defined models. Automation was achieved through the integration with OSIsoft Historian and real-time dashboards. The scope expanded from monitoring individual alarms to a system-wide view of critical event logs. The project achieved: A 95% reduction in scored traffic. A 4X improvement in industry staffing ratio. Measures of Success: Time to Value: 30 days to deploy initial release, integration with OSIsoft Historian, and real-time evaluation and scoring of events every 2 minutes. Always On, Situational Awareness: Single view of all critical event logs, auto-filter of irrelevant logs, continuous scoring and ranking of event logs, and real-time dashboards for decision support and automation. Benefits: Intelligence: Real-time scoring and filtering of events. Automation: Continuous monitoring and alerting. Scope: From individual alarms to a comprehensive event log system. Financial Impact: Improved operational efficiency and reduced staffing costs. Conclusion and Next Steps XMPro iBOS offers a unified, scalable framework that addresses critical operational challenges by enhancing intelligence, automation, and digital twin scope. This comprehensive solution empowers businesses to optimize processes, improve decision-making, and achieve substantial cost savings. Next Steps to Get Started: Discovery Call: Schedule a discovery call to discuss your business needs and determine if XMPro is the right fit for your organization. Use Case Exploration: Engage in detailed use case exploration calls to identify specific operational challenges and opportunities. Schedule a Demo: Experience XMPro iBOS in action by scheduling a personalized demo. Explore Pricing Options: Review the XMPro Pricing Page to understand the different packages and find the best fit for your needs. Consult with Experts: Work with XMPro’s team to develop a tailored implementation plan. Pilot Program: This 180 day program includes unlimited data streams, users, and connectors for integration, and is structured into three phases: Design & Configure, Run Pilot, and Review. Additional service/consulting and hosting costs apply. XMPro standard pricing will apply after the pilot/MVP period. More information here XMPro Pricing Page \\"
  },
  "docs/resources/faqs/external-content/blogs/2024/xmpro-apex-pioneering-agentops-for-industrial-multi-agent-generative-systems.html": {
    "href": "docs/resources/faqs/external-content/blogs/2024/xmpro-apex-pioneering-agentops-for-industrial-multi-agent-generative-systems.html",
    "title": "XMPro APEX: Pioneering AgentOps for Industrial Multi Agent Generative Systems | XMPro",
    "summary": "XMPro APEX: Pioneering AgentOps for Industrial Multi Agent Generative Systems Articles, Blog, CEO'S Blog XMPro APEX: Pioneering AgentOps for Industrial Multi Agent Generative Systems Posted on September 29, 2024 by Pieter van Schalkwyk XMPro APEX: Pioneering AgentOps for Industrial Multi Agent Generative Systems Pieter van Schalkwyk CEO at XMPRO, Author – Building Industrial Digital Twins, DTC Ambassador, Co-chair for AI Joint Work Group at Digital Twin Consortium In today’s rapidly evolving industrial landscape, organizations face the challenge of not just implementing intelligent systems but managing them at scale in complex, dynamic environments. At XMPro, we’ve developed a solution that addresses this need: APEX (Agent Platform EXperience) for MAGS(Multi-Agent Generative Systems). This innovation introduces the concept of AgentOps – a new approach to AI in industrial and enterprise settings. Understanding the Components: MAGS, APEX, and AgentOps To fully appreciate the scope of this solution, let’s break down its key components and understand how they work together to create a comprehensive AgentOps ecosystem. MAGS: The Heart of Intelligent Decision-Making At the core of our solution lies MAGS (Multi-Agent Generative Systems), a Decision AI Agent framework designed for complex, large-scale industrial and enterprise environments. MAGS implements a sophisticated agent architecture that brings a new level of intelligence and adaptability to industrial operations. A team of 5 MAGS Agents managing an Antibiotics Fermentation Tank – Plan by the Maintenance Agent Key features of MAGS include Advanced Memory Cycles: Each MAGS agent possesses a memory stream that stores observations, reflections, plans, decisions, and actions. This enables agents to maintain context, learn from past experiences, and make informed decisions based on historical data. Reflection Capabilities: The system allows agents to reflect on their experiences and generate higher-level insights. This meta-cognitive ability enables continuous learning and adaptation, crucial for long-term deployment in dynamic environments. Modular Planning Strategies: MAGS includes a flexible planning system that can employ different strategies based on the situation and agent capabilities. This includes support for standardized planning languages like PDDL (Planning Domain Definition Language), allowing agents to handle a wide range of complex scenarios. Multi-Agent Collaboration: Agents can be organized into teams, enabling collaborative problem-solving and efficient task distribution. This feature is particularly valuable in complex industrial settings where different systems and processes need to work together. Multi-Tool Ecosystem: MAGS provides a diverse toolkit for agents, including SQL query tools, web search capabilities, sentiment analysis, and more. This expandable ecosystem allows agents to tackle a wide array of tasks across various domains. APEX: The AgentOps Platform While MAGS provides the intelligent agents, APEX (Agent Platform EXperience) serves as the comprehensive management platform that oversees their lifecycle, integration, and optimization. APEX brings the concepts of DevOps and MLOps to the world of AI agents, providing a structured approach to managing AI in industrial settings. APEX Agent Profile Management Key capabilities of APEX include OT/IT Integration: APEX acts as a bridge between operational technology and information technology systems, allowing AI agents to process data from OT systems and interface with IT systems for higher-level decision-making and reporting. Governance Framework: The platform incorporates an ethical framework with deontic rules and organizational policies, ensuring that AI agents operate within defined boundaries and align with corporate governance and industry regulations. Advanced Observability: Utilizing OpenTelemetry integration, APEX provides insights into system performance and behavior, enabling comprehensive monitoring and optimization of AI operations. Flexible Communication Infrastructure: Supporting protocols like MQTT and DDS, APEX facilitates efficient and reliable data exchange in distributed, high-performance environments. Extensible Architecture: APEX is designed for adaptability, employing abstract classes and interfaces that allow for customization and extension to meet specific industry requirements. AgentOps: A New Paradigm in AI Management The combination of MAGS and APEX gives rise to AgentOps – a holistic approach to developing, deploying, and managing AI agents in industrial environments. AgentOps brings the agility, efficiency, and continuous improvement principles of DevOps to the world of AI agents. Key aspects of AgentOps include Efficient Agent Deployment: Streamline the process of developing, testing, and deploying AI agents across your organization using flexible agent profiles and instances. Continuous Agent Improvement: Implement feedback loops that allow agents to learn and adapt based on real-world performance, leveraging advanced memory cycles and reflection capabilities. Scalable Agent Management: Efficiently manage and coordinate multiple AI agents across diverse operational contexts, supported by robust data management and communication infrastructure. Human-AI Collaboration: Foster cooperation between human operators and AI agents, optimizing workflows and decision-making processes in complex industrial settings. Responsible AI Practices: Maintain oversight and control over AI agents, ensuring they operate within defined ethical and operational boundaries through deontic rules and organizational policies. Transforming Industrial AI with XMPro APEX for MAGS The combination of MAGS, APEX, and the AgentOps approach offers significant capabilities for organizations looking to leverage AI in complex industrial environments: End-to-End Solution: From data ingestion through XMPro’s Data Streams to AI agent management and interaction with physical systems, APEX for MAGS provides a comprehensive solution for industrial AI. Digital Twin Integration: Leveraging XMPro’s expertise in creating digital twins of physical systems, APEX for MAGS enables AI agents to perform simulation, prediction, and optimization in industrial contexts. Edge Computing Support: With capabilities for edge processing, the platform reduces latency in industrial control applications and supports distributed AI deployments. Real-Time Operational Intelligence: By combining real-time data processing with AI agent capabilities, the platform enables predictive maintenance, process optimization, and adaptive control in industrial settings. Scalable Architecture: The underlying stream architecture allows for horizontal scaling to handle increasing data loads, supporting growing industrial operations. Shaping the Future of Industrial AI XMPro APEX for MAGS represents a significant step forward in how organizations can approach AI in industrial and enterprise settings. By providing a comprehensive framework for developing, deploying, and managing intelligent AI agents, it addresses the unique challenges of complex operational environments. APEX Prompt Administration for MAGS Agents The platform’s focus on memory management, reflection, advanced planning, multi-tool support, and real-time data processing, combined with its flexible and extensible design, makes it well-suited for organizations with intricate OT and IT ecosystems. As AI becomes increasingly integral to industrial operations, XMPro APEX for MAGS offers a structured, scalable, and responsible way to implement and manage intelligent AI agents. It’s not just about deploying AI; it’s about creating an ecosystem where AI agents can learn, adapt, and optimize operations in challenging and dynamic industrial environments. At XMPro, we’re committed to driving operational excellence and responsible AI practices across industries. We believe that XMPro APEX for MAGS will play a key role in shaping the future of industrial AI, enabling organizations to harness the full potential of AI while maintaining control and alignment with business goals. Our GitHub Repo has more technical information if you are interested. You can also contact myself or Gavin Green for more information."
  },
  "docs/resources/faqs/external-content/blogs/index.html": {
    "href": "docs/resources/faqs/external-content/blogs/index.html",
    "title": "Blogs | XMPro",
    "summary": "Blogs Articles from the XMPro blog covering various topics related to digital twins, IoT, and industrial applications. By Year 2024 2023 2022 2021 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010"
  },
  "docs/resources/faqs/external-content/index.html": {
    "href": "docs/resources/faqs/external-content/index.html",
    "title": "External Content | XMPro",
    "summary": "External Content This section contains external content related to XMPro, including use cases, blogs, and YouTube videos. Use Cases Real-world use cases demonstrating how XMPro is used in various industries. View Use Cases"
  },
  "docs/resources/faqs/external-content/use-cases/aging-pipe-predictiv.html": {
    "href": "docs/resources/faqs/external-content/use-cases/aging-pipe-predictiv.html",
    "title": "Aging Pipe Predictive Maintenance in Water Utilities | XMPro",
    "summary": "Aging Pipe Predictive Maintenance in Water Utilities url XMPro Solution for Aging Pipe Predictive Maintenance in Water Utilities Introduction In water utilities, managing aging pipe infrastructure is crucial for ensuring reliable water distribution and minimizing service disruptions. XMPro's solution focuses on predictive maintenance to proactively address the challenges posed by aging pipes. The Challenge Water utilities face several challenges with aging pipe infrastructure: XMPro Solution for Aging Pipe Predictive Maintenance in Water Utilities Introduction In water utilities, managing aging pipe infrastructure is crucial for ensuring reliable water distribution and minimizing service disruptions. XMPro’s solution focuses on predictive maintenance to proactively address the challenges posed by aging pipes. The Challenge Water utilities face several challenges with aging pipe infrastructure: Early Detection of Pipe Degradation: Identifying signs of wear, corrosion, or damage in aging pipes before they lead to failures. Optimizing Maintenance Schedules: Balancing the need for regular maintenance with minimizing service interruptions and managing costs. Resource Allocation: Efficiently allocating resources for maintenance and replacement of aging pipes. The Solution: XMPro’s Predictive Maintenance for Aging Pipes XMPro’s solution employs advanced sensors, data analytics, and machine learning to monitor and maintain aging pipe infrastructure effectively. Key Features Data Integration & Transformation: Utilizing existing sensors to continuously monitor critical parameters such as corrosion, water pressure, flow rate, and pipe vibration. XMPro’s Data Stream Designer integrates this sensor data, providing a comprehensive view of pipe health. Predictive Analytics for Pipe Health: Implementing machine learning algorithms to analyze sensor data and predict potential pipe failures or areas needing maintenance. Predictive insights enable proactive scheduling of maintenance activities, reducing the likelihood of pipe bursts or leaks. Real-Time Monitoring and Alerts: Providing real-time monitoring of pipe conditions, with an alert system that notifies maintenance teams of detected anomalies requiring immediate attention. Customizable Dashboards and Reporting: Offering customizable dashboards that present key data on pipe health, alongside comprehensive reporting features for maintenance planning and regulatory compliance. Figure 1. Real-Time Aging Pipe Network Monitoring Dashboard for Water Utilities Real-Time Aging Pipe Network Monitoring Dashboard This specialized dashboard provides water utility operators with a comprehensive view of their aging pipe network. It features an interactive map that dynamically updates with the condition of the pipe network, including treatment plants, pump stations, and reservoirs, offering a clear visual representation of the water distribution system’s health. Each segment of the pipe network on the map is marked with a color-coded status icon, indicating its current operational state, including active status and any alerts or error messages related to corrosion or other degradation. Overview of Pipe Network Health: The dashboard displays the overall status of the pipe network, highlighting areas with potential or existing issues. It includes critical alerts such as signs of corrosion detected by electrochemical sensors, potential leaks, or significant pressure variations. Corrosion Monitoring and Predictive Alerts: Utilizing data from electrochemical sensors, the dashboard provides real-time insights into the corrosion levels of pipes. It highlights areas where corrosion is reaching critical levels, prompting immediate attention. Maintenance Planning and Scheduling: A detailed graph tracks maintenance and replacement requirements across the pipe network. It prioritizes sections based on their corrosion levels, age, and other health indicators, facilitating efficient and proactive maintenance scheduling. Drill-Down Capability for In-Depth Analysis: Users can explore specific sections of the pipe network for detailed information, including historical corrosion data, recent maintenance activities, and predictive maintenance recommendations. This level of detail enables targeted actions based on the system’s predictive analytics. Customizable Alerts and Recommendations: The dashboard highlights active recommendations generated by the system’s smart rule logic and machine learning algorithms. This includes suggestions for corrosion mitigation, pipe replacement, or other maintenance actions. This Real-Time Aging Pipe Network Monitoring Dashboard is an essential tool for water utility operators, enabling them to effectively monitor and manage the health of their aging infrastructure. By providing real-time data, predictive insights, and actionable recommendations, it ensures informed decision-making and enhances the operational efficiency and reliability of the water distribution system. Figure 2. Asset Class Drill Down View – Aging Pipe Network Asset Class Drilldown View – Aging Pipe Network This dedicated asset view for the aging pipe network in water utilities offers a detailed and comprehensive dashboard, providing key insights into the condition and health of the pipe infrastructure. Alerts Overview: This section visually presents open alerts related to the pipe network’s condition, categorized by severity levels – Low, Medium, and High. This feature is instrumental in quickly identifying pipe segments that require immediate attention, highlighting potential issues like corrosion, leaks, or pressure anomalies. Work Order Status: The dashboard displays the current status of maintenance activities for the pipe network, categorized as Available (no immediate action needed), In Planning (maintenance scheduled), or Waiting (urgent maintenance required). This categorization aids in effective maintenance planning and resource allocation. Performance Metrics (Last 30 Days): It provides a summary of critical metrics related to the health of the pipe network, including new alerts, number of work orders initiated, open work orders, and open work requests. Additionally, it tracks the time elapsed from alert initiation to work order completion, offering a performance comparison with the previous 30 days. Pipe Segment Filtering and Maintenance Information: Users can filter through and select specific pipe segments, accessing detailed information such as the last inspection date, upcoming scheduled maintenance, and due dates. This functionality is essential for planning proactive maintenance and addressing issues before they escalate. Recent Recommendations: This section lists the most recent maintenance and intervention recommendations for the aging pipe network, derived from predictive analysis and real-time sensor data. Detailed information for each recommendation is available, enabling users to take timely and effective actions. XMPro Co-Pilot Integration: The dashboard features the interactive XMPro Co-Pilot, where users can input queries related to pipe network maintenance or operational challenges. The AI model, trained on relevant internal data like historical pipe performance and maintenance records, offers specific guidance for addressing the identified issues. This guidance can be seamlessly integrated into work order requests and triage instructions. This Asset Drill Down View is specifically designed for efficient management of aging pipe networks in water utilities. It empowers operators to quickly access vital information, make informed decisions, and proactively maintain the integrity and reliability of their pipe infrastructure. Figure 3. Asset Analysis View – Pipeline Sector Health Asset Analysis View – Pipeline Sector Health This Asset Analysis View provides in-depth insights into specific sections of the pipeline within the water utility system, focusing on a particular sector identified as P016, with emphasis on the offending pipeline section P016-008455. Comprehensive Pipeline Health Metrics: This section displays vital health indicators for Pipeline Sector P016-008455, including corrosion levels, pressure variability, flow rate, and structural integrity. Real-time data, enhanced with predictive analytics, enables forecasts of potential issues, aiding in proactive maintenance and corrosion management. Interactive 2D and 3D Pipeline Models: The dashboard presents detailed 2D and 3D models of Pipeline Sector P016-008455. Features allow for an expanded view of specific pipeline segments. Areas flagged for potential issues, such as corrosion hotspots or structural weaknesses, are highlighted for quick identification. For instance, sections showing elevated corrosion levels or significant pressure drops are distinctly color-marked. Error Identification and Proactive Recommendations: Clickable sections in the pipeline model lead users to specific error details and associated recommendations. This integration with XMPro’s Recommendation Manager streamlines the process for identifying and addressing issues related to Pipeline Sector P016-008455. Detailed Information on Pipeline Sector P016-008455: The dashboard provides a comprehensive profile of this pipeline sector, including its material composition, installation date, operational history, and recent maintenance activities. This information is crucial for understanding its maintenance needs and predicting future issues. XMPro Co-Pilot Integration: Incorporating XMPro Co-Pilot, this feature utilizes AI, trained on datasets such as historical corrosion data, pressure readings, and maintenance records, to offer specific guidance for issues related to Pipeline Sector P016-008455. This AI-driven assistance supports informed decision-making and enhances the effectiveness of maintenance strategies. This Asset Analysis View is specifically designed to provide a complete picture of the health of Pipeline Sector P016-008455. It combines sophisticated visual models with data-driven insights and AI-powered recommendations, enabling effective management and maintenance of critical pipeline infrastructure in the water utility industry. Why XMPro iDTS? XMPro’s Intelligent Digital Twin Suite (iDTS) offers several unique and innovative solutions for managing aging pipeline infrastructure in water utilities, particularly for a use case like predictive maintenance of a specific pipeline sector (e.g., P016-008455). Here’s how XMPro iDTS can effectively address this challenge: Digital Twin Modeling of Pipeline Infrastructure: XMPro iDTS can create a digital twin of the specific pipeline sector, providing a virtual representation that mirrors its real-world condition. This digital twin allows for continuous monitoring and analysis, helping to identify early signs of degradation such as corrosion or structural weaknesses. Advanced Sensor Data Integration & Transformation: The suite integrates data from various sensors installed along the pipeline, including electrochemical sensors for corrosion monitoring, pressure sensors, and flow meters. This comprehensive data integration is key to monitoring the health of the pipeline and identifying potential maintenance needs. Predictive Analytics for Maintenance Planning: Utilizing machine learning algorithms, XMPro iDTS analyzes sensor data to predict potential pipeline failures or areas requiring maintenance. This predictive approach enables proactive scheduling of maintenance activities, reducing the likelihood of unexpected failures and service interruptions. Maintenance Scheduling Optimization: XMPro iDTS optimizes maintenance scheduling by using predictive analytics and real-time data, enabling proactive, need-based maintenance. This approach maximizes resource efficiency and minimizes costs by reducing unnecessary maintenance activities. Real-Time Monitoring and Alerting: XMPro iDTS provides real-time monitoring of the pipeline’s condition. It generates instant alerts when parameters like corrosion levels or pressure variability exceed predefined thresholds, enabling quick decision-making and response. Customizable Dashboards for Decision Support: The suite includes customizable dashboards that display key pipeline health data in an easy-to-understand format. These dashboards can be tailored to the specific needs of water utility operators, providing them with actionable insights. Scalability and Flexibility – Start Small, Scale Fast: XMPro iDTS offers scalable and flexible solutions, allowing water utilities to start small and expand as needed. Its modular design ensures easy integration and adaptability, facilitating quick deployment and future-proof scalability. Enhanced Safety & Operational Efficiency: XMPro iDTS significantly enhances safety and operational efficiency in water utilities. By providing real-time monitoring and predictive insights, it enables early detection of potential hazards, reducing the risk of accidents and ensuring continuous service delivery. XMPro Blueprints – Quick Time to Value: XMPro Blueprints offer a rapid path to value realization for water utilities. These pre-configured templates are designed for quick implementation, incorporating best practices and industry standards. In summary, XMPro iDTS addresses the aging pipeline predictive maintenance use case by providing a comprehensive, real-time, predictive, and integrated solution. Its capabilities in digital twin technology, advanced sensor data integration, predictive analytics, and effective visualization tools make it a powerful tool for enhancing the reliability and efficiency of pipeline operations in water utilities. Not Sure How To Get Started? No matter where you are on your digital transformation journey, the expert team at XMPro can help guide you every step of the way - We have helped clients successfully implement and deploy projects with Over 10x ROI in only a matter of weeks! Request a free online consultation for your business problem. \"*\" indicates required fields Name*Email*Company / Organization*Job Role*Contact Department*SalesSupportMessage*CaptchaCommentsThis field is for validation purposes and should be left unchanged. Δdocument.getElementById( \"ak_js_3\" ).setAttribute( \"value\", ( new Date() ).getTime() );"
  },
  "docs/resources/faqs/external-content/use-cases/air-quality-monitori.html": {
    "href": "docs/resources/faqs/external-content/use-cases/air-quality-monitori.html",
    "title": "Air Quality Monitoring For Agriculture | XMPro",
    "summary": "Air Quality Monitoring For Agriculture url XMPro Air Quality Monitoring Solution for Agriculture In the realm of agriculture, maintaining optimal air quality is pivotal for ensuring crop health, soil vitality, and overall farm productivity. The XMPro Intelligent Digital Twin solution revolutionizes how air quality is monitored and managed in agricultural settings. The Challenge Agricultural operations are increasingly impacted by air quality XMPro Air Quality Monitoring Solution for Agriculture In the realm of agriculture, maintaining optimal air quality is pivotal for ensuring crop health, soil vitality, and overall farm productivity. The XMPro Intelligent Digital Twin solution revolutionizes how air quality is monitored and managed in agricultural settings. The Challenge Agricultural operations are increasingly impacted by air quality issues, presenting several challenges: Crop Health and Yield: Exposure to air pollutants can significantly affect crop health, leading to reduced yield and quality. Environmental Compliance: Adherence to strict environmental regulations regarding air quality is mandatory to avoid legal and financial repercussions. Soil Degradation: Air pollutants can contribute to soil acidification and nutrient imbalance, adversely affecting soil health. Worker Safety: Ensuring the health and safety of farm workers in environments with potential air quality issues is a paramount concern. The Solution: XMPro Intelligent Digital Twin for Air Quality XMPro’s Intelligent Digital Twin solution offers a dynamic and comprehensive approach to air quality monitoring, leveraging advanced digital twin technology and IoT sensors. Use Case Features: Digital Twin-Enabled Real-Time Monitoring: Deployment of IoT sensors across the farm, integrated with a digital twin model that replicates the farm’s environmental conditions, providing real-time insights into air quality levels. Data Integration and Predictive Analytics: The digital twin aggregates data from various sources, analyzing it to identify trends and predict potential air quality issues before they impact crop health or soil quality. Customizable Dashboards: User-friendly dashboards that display real-time data and predictive insights, customizable to the specific needs of farm managers. Automated Compliance Reporting: The solution automates the generation of compliance reports, ensuring that the farm meets environmental air quality standards. Advantages Proactive Crop Protection: By monitoring and managing air quality, the solution helps protect crops from harmful pollutants, potentially improving yield and quality. Regulatory Compliance: Automated reporting and monitoring facilitate adherence to environmental standards. Soil Health Preservation: Insights into air quality enable actions to prevent soil degradation. Enhanced Worker Safety: Improved air quality management contributes to a safer working environment. Implementation Strategy Farm Assessment and Sensor Deployment: Conducting an in-depth assessment to strategically place IoT sensors, ensuring comprehensive coverage for accurate data collection. Integration and Customization: Seamlessly integrating the digital twin with existing farm management systems. Customizing the digital twin model and dashboards to reflect the unique characteristics and needs of the farm. Training and User Adoption: Providing extensive training to farm staff on utilizing the digital twin platform for maximum benefit.Ensuring user adoption through continuous support and iterative improvements based on user feedback. WHY XMPRO iDTS? XMPro’s Intelligent Digital Twin Suite (iDTS) offers unique and advanced solutions for air quality monitoring in agriculture, addressing the specific challenges of this sector with innovative approaches. Here’s how XMPro iDTS uniquely solves the challenges of air quality monitoring in agriculture: Advanced Simulation with Digital Twin Technology: XMPro iDTS creates a sophisticated digital twin of the agricultural environment, simulating real-world conditions. This digital twin allows for the detailed analysis of various factors affecting air quality and their potential impact on crops and soil. Real-Time Data Integration and Analysis: The solution integrates real-time data from IoT sensors deployed across the farm, including air quality monitors, weather stations, and soil sensors. XMPro iDTS analyzes this data to provide a comprehensive view of the farm’s environmental conditions. Predictive Analytics for Proactive Management: Utilizing machine learning and predictive analytics, XMPro iDTS forecasts changes in air quality and their potential effects. This predictive capability enables farmers to take proactive measures to protect crops and comply with environmental regulations. Customizable Alerts and Automated Responses: The system generates customizable alerts based on specific air quality thresholds. These alerts can trigger automated responses or notify farm managers to take appropriate actions, enhancing the responsiveness to changing air quality conditions. Integration with Farm Management Systems: XMPro iDTS seamlessly integrates with existing farm management systems, ensuring that air quality data is incorporated into broader farm operations. This integration facilitates holistic decision-making and operational efficiency. Compliance and Reporting Tools: The solution includes tools for automated compliance reporting, making it easier for farms to adhere to environmental regulations and standards related to air quality. User-Friendly Interfaces and Dashboards: XMPro iDTS provides intuitive interfaces and customizable dashboards, making it easy for farm staff to monitor air quality data, understand trends, and make informed decisions. Scalability and Flexibility: The digital twin solution is scalable and flexible, capable of adapting to different farm sizes and types. It can be expanded to include additional monitoring parameters or integrate with new technologies as they become available. Enhanced Decision Support with AI: Leveraging AI-driven insights, XMPro iDTS enhances decision support, enabling farmers to optimize their responses to air quality issues and maintain sustainable agricultural practices. In summary, XMPro iDTS addresses the unique challenges of air quality monitoring in agriculture by providing a comprehensive, real-time, predictive, and integrated solution. Its use of digital twin technology, combined with advanced data analytics, AI-driven insights, and effective visualization tools, makes it a powerful tool for enhancing agricultural productivity, sustainability, and compliance with environmental standards."
  },
  "docs/resources/faqs/external-content/use-cases/alarm-management-and.html": {
    "href": "docs/resources/faqs/external-content/use-cases/alarm-management-and.html",
    "title": "Alarm Management and Triage - XMPRO | XMPro",
    "summary": "Alarm Management and Triage - XMPRO url XMPro Alarm Management and Triage Solution for Industrial Plants The Problem Industrial plants often face challenges with the management of numerous level 3 and 4 alarms from DCS (Distributed Control Systems) and SCADA (Supervisory Control and Data Acquisition) systems: Alarm Overload: Operators are frequently overwhelmed by the high volume of lower-level alarms, leading to delayed XMPro Alarm Management and Triage Solution for Industrial Plants The Problem Industrial plants often face challenges with the management of numerous level 3 and 4 alarms from DCS (Distributed Control Systems) and SCADA (Supervisory Control and Data Acquisition) systems: Alarm Overload: Operators are frequently overwhelmed by the high volume of lower-level alarms, leading to delayed or missed responses. Missed Leading Indicators: Many of these alarms can be early indicators of potential operational or equipment failures. Inefficient Alarm Management: Lack of effective triage and prioritization mechanisms for handling these alarms. #text-box-715659814 { width: 60%; } #text-box-715659814 .text-box-content { font-size: 100%; } #banner-2039506122 { padding-top: 398px; } #banner-2039506122 .bg.bg-loaded { background-image: url(https://xmpro.com/wp-content/uploads/2020/04/10.jpg); } The Solution XMPro’s solution focuses on aggregating and grouping level 3 and 4 alarms, making them accessible for detailed review by reliability and process engineers, and creating recommendation rules for potential failure indicators. Key Features: Alarm Aggregation and Grouping: Collecting and categorizing level 3 and 4 alarms for efficient analysis. Expert Review and Analysis: Enabling reliability and process engineers to review aggregated alarms and identify critical issues. Recommendation Rules for Alarms: Developing rules to highlight alarms that are leading indicators of potential failures. Visibility into Leading Indicator Data: Providing insights into critical alarms that may affect operational efficiency and equipment health. Automated Alerts and Prioritization: Implementing a system for prioritizing alarms based on their potential impact. Benefits: Improved Response to Critical Alarms: Enhanced ability to respond to alarms that are indicative of larger issues. Extended Equipment Life: Proactive maintenance and intervention based on alarm data can improve the remaining useful life of older equipment. Operational Efficiency: Better alarm management leads to more efficient plant operations and reduced risk of failures. Why XMPro iDTS? XMPro iDTS offers unique and innovative solutions for Alarm Management and Triage in industrial plants, particularly in handling the influx of level 3 and 4 alarms from DCS and SCADA systems. Here’s how XMPro iDTS can be specifically applied to enhance this solution: Advanced Alarm Aggregation and Analysis: XMPro iDTS can aggregate and categorize alarms from various systems, including DCS and SCADA. It analyzes these alarms in real-time, helping to filter out noise and highlight the most critical alarms that require attention. Digital Twin for Systematic Alarm Monitoring: By creating a digital twin of the plant’s operational systems, XMPro iDTS provides a virtual representation that helps in understanding the interrelations and potential impacts of various alarms. This approach aids in identifying which alarms are leading indicators of potential failures. Customizable Recommendation Rules: XMPro iDTS enables the creation of customizable recommendation rules for alarms. These rules are based on historical data, operational patterns, and expert input, helping reliability and process engineers to prioritize and respond to the most critical alarms effectively. Real-Time Data Integration: Integrating real-time data from multiple sources, XMPro iDTS ensures that the alarm management system is always up-to-date with the latest operational information, enhancing the accuracy of alarm triage and response. Automated Alerts and Prioritization: The system can automatically prioritize alarms based on their severity, potential impact, and the likelihood of leading to equipment or operational failures. This prioritization helps operators and engineers focus on the most urgent issues. Enhanced Visibility and Decision Support: XMPro iDTS provides comprehensive dashboards and visualization tools that offer enhanced visibility into alarm data. These tools support better decision-making by presenting complex data in an easily digestible format. Scalability and Flexibility: XMPro iDTS is scalable and flexible, allowing the solution to be adapted to different sizes of operations and to integrate with various types of industrial control systems. Proactive Maintenance and Operational Efficiency: By enabling proactive responses to critical alarms, XMPro iDTS helps extend the life of equipment and improves overall operational efficiency, reducing downtime and maintenance costs. In summary, XMPro iDTS addresses the unique challenges in alarm management and triage for industrial plants by providing a comprehensive, real-time, and integrated solution. Its capabilities in advanced data aggregation, digital twin technology, customizable recommendation rules, and effective visualization make it a powerful tool for enhancing alarm response, equipment reliability, and operational efficiency."
  },
  "docs/resources/faqs/external-content/use-cases/asset-condition-moni.html": {
    "href": "docs/resources/faqs/external-content/use-cases/asset-condition-moni.html",
    "title": "Asset Condition Monitoring for Surface Processing Plants in the Mining Industry | XMPro",
    "summary": "Asset Condition Monitoring for Surface Processing Plants in the Mining Industry url ASSET CONDITION MONITORING FOR SURFACE PROCESSING PLANTS IN THE MINING INDUSTRY Introduction The mining sector is embarking on a transformative journey with the adoption of smart technologies in surface processing plants. These facilities, responsible for the extraction and processing of minerals, are integrating advanced monitoring systems to enhance the efficiency and safety of their operations. ASSET CONDITION MONITORING FOR SURFACE PROCESSING PLANTS IN THE MINING INDUSTRY Introduction The mining sector is embarking on a transformative journey with the adoption of smart technologies in surface processing plants. These facilities, responsible for the extraction and processing of minerals, are integrating advanced monitoring systems to enhance the efficiency and safety of their operations. The role of asset condition monitoring is becoming increasingly vital, with sophisticated tools needed to manage the intricate machinery that drives production. The Challenge Surface processing plants face multifaceted challenges: Equipment Complexity: A myriad of machines working in unison requires precise coordination and constant monitoring to prevent failures that can disrupt the entire operation. Maintenance and Upkeep: Proactive maintenance strategies are needed to minimize downtime and extend the lifespan of critical equipment. Safety Hazards: The potential for equipment malfunctions poses safety risks to personnel, necessitating vigilant monitoring systems. Operational Optimization: With the push for more output, plants must streamline operations without compromising the quality or safety of their procedures. Technological Adaptability: As new processing technologies emerge, plants must remain agile, integrating innovative solutions to stay competitive. Data-Driven Decisions: Leveraging the data collected from operations to drive improvements poses a significant challenge due to the complexity and volume of the data. The Solution: XMPro iBOS for Asset Condition Monitoring in Surface Processing Plants XMPro’s iBOS is adeptly crafted to meet the complex demands of asset condition monitoring in the mining industry’s surface processing plants. It employs a data-centric strategy to significantly boost the precision, efficiency, and scalability of asset management, essential for upholding rigorous safety and quality standards in the industry. XMPro harnesses state-of-the-art technology to enhance the monitoring process, making it an exceptionally efficient and reliable operation. Key Features Real-time Data Integration and Process Adjustment: XMPro seamlessly connects with plant sensors and systems, gathering immediate data on vital operational parameters. This continuous monitoring is crucial for maintaining ideal conditions, ensuring every component operates within stringent quality parameters. Advanced Analytics for Process Insights: Applying intricate analytics, XMPro deciphers data to reveal underlying patterns and deviations, pinpointing improvement areas and ensuring consistent quality. Predictive Modeling for Optimization: With its predictive modeling prowess, XMPro anticipates various operational outcomes, enabling the fine-tuning of asset performance parameters to bolster efficiency and quality. Automated Optimization and Control: Utilizing both predictive insights and real-time data, XMPro can autonomously adjust operational parameters, keeping the plant process in the best condition, reducing manual oversight, and diminishing error probability. Configurable Dashboards for Centralized Monitoring: XMPro provides customizable dashboards that offer a unified view of the asset conditions, alerting to deviations, and suggesting actionable steps for informed decision-making and sustained operational integrity. Continuous Improvement Loop: Emphasizing continuous enhancement, XMPro learns from each set of operational data, refining its predictive models and optimization tactics for perpetual advancements in the plant’s asset management and performance. By integrating real-time data, advanced analytics, and predictive modeling, XMPro elevates the asset condition monitoring process to a new level of operational efficiency, scalability, and quality assurance. Discover XMPro’s Asset Condition Monitoring Solution For Surface Processing Plants In The Mining Industry. Figure 1. Operational Overview: Surface Processing Plant Monitoring The Surface Processing Plant Overview dashboard is a testament to XMPro’s ability to capture and visualize the complexities of a mining operation. This powerful tool is the cornerstone of a data-driven approach, offering a panoramic view of the plant’s operational health. Key Dashboard Features: Real-time Equipment Status: Interactive schematics provide instant visual feedback on the condition and performance of critical equipment, such as pumps and conveyors, highlighting areas needing attention with color-coded alerts. Operational Safety Intelligence: Detailed reports on potential hazards and recommended control measures, coupled with probability indicators, help prioritize safety efforts where they are most needed. Performance Metrics Visualization: A snapshot of key performance metrics, like operational hours and CO2 emissions, supports environmental and efficiency goals. Recommendations and Resolution Tracking: The dashboard presents actionable recommendations and tracks the resolution times for issues, illustrating the plant’s responsiveness to operational challenges. This comprehensive overview enables site managers to monitor their operations effectively, make informed decisions quickly, and maintain a high level of safety and efficiency within the surface processing plant Figure 2: Pump Health Monitoring Drilldown – Processing Plant The Pump Health Monitoring dashboard provides a detailed drilldown into the operational status of critical pump equipment within the surface processing plant. It presents a unified view of pump health, leveraging real-time data to ensure operational continuity and maintenance optimization. Key Drilldown Features: 3D Equipment Visualization: An interactive 3D model illustrates pump components, highlighting areas of concern and facilitating a deeper understanding of the equipment’s condition. Operational Parameters Tracking: Displays operational hours, electricity consumption, and CO2 emissions to monitor the environmental impact and operational efficiency of the pump. Safety and Risk Indicators: Operational safety intelligence is integrated to describe potential hazards, while risk indicators for various pump components are monitored to preempt failures. Efficiency Metrics: Gauges such as efficiency and degradation loss dials provide at-a-glance information on the pump’s performance, with thresholds set for alerts on deviations. Recommendations and Work Order Management: Suggests actionable maintenance tasks and tracks work order history to ensure timely responses to any identified issues. This level of detail enables maintenance teams to act proactively, addressing potential problems before they escalate, ensuring sustained pump operation, and maintaining safety standards within the plant. Figure 3: Pump Health & Efficiency Monitoring Drilldown (Schematic) – Processing Plant This detailed dashboard for Pump P-78 within a Surface Processing Plant illustrates XMPro’s robust capabilities in asset management and efficiency optimization. Key Drilldown Features: Comprehensive Schematic View: An intricate schematic provides a complete view of the pump’s components, including real-time data on motor current, discharge pressure, and flow rate, enabling precise control and diagnostics. Efficiency and Degradation Loss Gauges: These indicators provide a snapshot of the pump’s operational efficiency and the extent of wear-and-tear, crucial for lifecycle management. Oil Analysis Results: Offering insights into the internal health of the pump, this section presents data on sample ranks, ISO particle counts, and viscosity, which are key to predictive maintenance strategies. Enhanced Real-time View: This includes a more detailed display of real-time operational data, giving an expanded view with additional parameters such as inlet pressure. Health Score: A new metric that gives a quick overview of the overall condition of the pump, summarizing various health and performance factors into a single actionable score. Operational Safety Intelligence: Central to maintaining a safe work environment, this feature outlines potential hazards and control measures, reinforced by a high-probability indicator to ensure worker safety. Risk Assessment Tools: A suite of gauges displays the risk associated with various components, allowing for focused maintenance efforts and risk mitigation strategies. Real-time Performance Data: A live feed of operational data like discharge pressure and flow rate is shown alongside historical trends, offering insights into the pump’s performance over time. Actionable Insights: The dashboard not only diagnoses issues such as a P78 Discharge Exception but also tracks the plant’s response, guiding maintenance teams with clear, actionable steps. This XMPro pump health dashboard serves as an essential tool for maintaining high operational standards, ensuring the reliability and efficiency of critical assets in the Surface Processing Plant. Why XMPro iBOS for Asset Condition Monitoring in Surface Processing Plants? XMPro’s Intelligent Business Operations Suite (iBOS) is specifically engineered to address the complexities of monitoring and optimizing asset conditions in surface processing plants within the mining industry. Advanced Intelligent Digital Twin Modeling: XMPro iBOS advances beyond basic modeling, offering sophisticated digital twin creation that reflects the complex nature of mining operations. It provides a dynamic virtual representation of physical assets for in-depth analysis and scenario planning. Advanced Sensor Data Integration & Transformation: Incorporating live data from sensors on all equipment, XMPro iBOS tracks essential metrics and data. This comprehensive monitoring detects and analyses opportunities for performance enhancement throughout the the plant. Predictive Analytics for Performance Enhancement: Employing predictive analytics to anticipate issues and optimize operational segments, reducing waste and enhancing product quality. Maintenance Scheduling Optimization: XMPro iBOS evaluates performance data to refine maintenance schedules, shifting from a reactive to a predictive maintenance approach. This strategy is vital for synchronizing maintenance tasks across different lines, improving equipment lifespan and minimizing interruptions in operation. Real-Time Monitoring and Predictive Alerting: XMPro iBOS generates automatic recommendations and alerts for equipment based on ongoing and forecasted data analysis. This feature ensures that each component functions at peak performance, greatly diminishing the necessity for manual checks. Configurable and Interactive Dashboards: XMPro iBOS offers adaptable dashboards that give immediate insights into the condition and performance of assembly line equipment. These user interfaces are crafted to be interactive, permitting detailed examination of specific operational elements and aiding centralized decision-making. Scalability and Flexibility – Start Small, Scale Fast: XMPro iBOS is built to scale, supporting the growth of operations without compromising on the agility required to adapt to new challenges and opportunities within the mining sector. Enhanced Safety & Operational Efficiency: XMPro iBOS improves operational safety by pinpointing potential risks and inefficiencies in the process plant sequence, ensuring that machinery works within secure and optimal limits. This leads to a safer workplace and more efficient processing plant processes. XMPro Blueprints – Quick Time to Value: XMPro Blueprints enable fast implementation of battery operations solutions, with templates based on industry best practices for rapid benefits realization. These blueprints ensure swift adoption of digital advancements across mining processing plant operations. XMPro iBOS is specifically tailored to meet the challenges of surface processing plant operations, offering a comprehensive, predictive, and integrated management solution. Its sophisticated operations modeling, coupled with extensive data analytics and personalized dashboards, allows surface processing plants to achieve exceptional operational efficiency, product quality, and safety. Not Sure How To Get Started? No matter where you are on your digital transformation journey, the expert team at XMPro can help guide you every step of the way - We have helped clients successfully implement and deploy projects with Over 10x ROI in only a matter of weeks! Request a free online consultation for your business problem. \"*\" indicates required fields Name*Email*Company / Organization*Job Role*Contact Department*SalesSupportMessage*CaptchaCommentsThis field is for validation purposes and should be left unchanged. Δdocument.getElementById( \"ak_js_3\" ).setAttribute( \"value\", ( new Date() ).getTime() ); /* <![CDATA[ */ gform.initializeOnLoaded( function() {gformInitSpinner( 3, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery('#gform_ajax_frame_3').on('load',function(){var contents = jQuery(this).contents().find('*').html();var is_postback = contents.indexOf('GF_AJAX_POSTBACK') >= 0;if(!is_postback){return;}var form_content = jQuery(this).contents().find('#gform_wrapper_3');var is_confirmation = jQuery(this).contents().find('#gform_confirmation_wrapper_3').length > 0;var is_redirect = contents.indexOf('gformRedirect(){') >= 0;var is_form = form_content.length > 0 && ! is_redirect && ! is_confirmation;var mt = parseInt(jQuery('html').css('margin-top'), 10) + parseInt(jQuery('body').css('margin-top'), 10) + 100;if(is_form){jQuery('#gform_wrapper_3').html(form_content.html());if(form_content.hasClass('gform_validation_error')){jQuery('#gform_wrapper_3').addClass('gform_validation_error');} else {jQuery('#gform_wrapper_3').removeClass('gform_validation_error');}setTimeout( function() { /* delay the scroll by 50 milliseconds to fix a bug in chrome */ }, 50 );if(window['gformInitDatepicker']) {gformInitDatepicker();}if(window['gformInitPriceFields']) {gformInitPriceFields();}var current_page = jQuery('#gform_source_page_number_3').val();gformInitSpinner( 3, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery(document).trigger('gform_page_loaded', [3, current_page]);window['gf_submitting_3'] = false;}else if(!is_redirect){var confirmation_content = jQuery(this).contents().find('.GF_AJAX_POSTBACK').html();if(!confirmation_content){confirmation_content = contents;}setTimeout(function(){jQuery('#gform_wrapper_3').replaceWith(confirmation_content);jQuery(document).trigger('gform_confirmation_loaded', [3]);window['gf_submitting_3'] = false;wp.a11y.speak(jQuery('#gform_confirmation_message_3').text());}, 50);}else{jQuery('#gform_3').append(contents);if(window['gformRedirect']) {gformRedirect();}}jQuery(document).trigger(\"gform_pre_post_render\", [{ formId: \"3\", currentPage: \"current_page\", abort: function() { this.preventDefault(); } }]); if (event.defaultPrevented) { return; } const gformWrapperDiv = document.getElementById( \"gform_wrapper_3\" ); if ( gformWrapperDiv ) { const visibilitySpan = document.createElement( \"span\" ); visibilitySpan.id = \"gform_visibility_test_3\"; gformWrapperDiv.insertAdjacentElement( \"afterend\", visibilitySpan ); } const visibilityTestDiv = document.getElementById( \"gform_visibility_test_3\" ); let postRenderFired = false; function triggerPostRender() { if ( postRenderFired ) { return; } postRenderFired = true; jQuery( document ).trigger( 'gform_post_render', [3, current_page] ); gform.utils.trigger( { event: 'gform/postRender', native: false, data: { formId: 3, currentPage: current_page } } ); if ( visibilityTestDiv ) { visibilityTestDiv.parentNode.removeChild( visibilityTestDiv ); } } function debounce( func, wait, immediate ) { var timeout; return function() { var context = this, args = arguments; var later = function() { timeout = null; if ( !immediate ) func.apply( context, args ); }; var callNow = immediate && !timeout; clearTimeout( timeout ); timeout = setTimeout( later, wait ); if ( callNow ) func.apply( context, args ); }; } const debouncedTriggerPostRender = debounce( function() { triggerPostRender(); }, 200 ); if ( visibilityTestDiv && visibilityTestDiv.offsetParent === null ) { const observer = new MutationObserver( ( mutations ) => { mutations.forEach( ( mutation ) => { if ( mutation.type === 'attributes' && visibilityTestDiv.offsetParent !== null ) { debouncedTriggerPostRender(); observer.disconnect(); } }); }); observer.observe( document.body, { attributes: true, childList: false, subtree: true, attributeFilter: [ 'style', 'class' ], }); } else { triggerPostRender(); } } );} ); /* ]]&gt; */"
  },
  "docs/resources/faqs/external-content/use-cases/bogie-health-monitor.html": {
    "href": "docs/resources/faqs/external-content/use-cases/bogie-health-monitor.html",
    "title": "Bogie Health Monitoring in the Rail Industry | XMPro",
    "summary": "Bogie Health Monitoring in the Rail Industry url Bogie Health Monitoring Solution for the Rail Industry Introduction Maintaining the health of train bogies is crucial for ensuring the safety and efficiency of rail operations. XMPro's Bogie Health Monitoring Solution employs advanced technologies to detect early signs of wear or failure, aiming to enhance safety and minimize unscheduled repairs. The Challenge Rail operators faceTags: Condition Monitoring / Predictive Maintenance Bogie Health Monitoring Solution for the Rail Industry Introduction Maintaining the health of train bogies is crucial for ensuring the safety and efficiency of rail operations. XMPro’s Bogie Health Monitoring Solution employs advanced technologies to detect early signs of wear or failure, aiming to enhance safety and minimize unscheduled repairs. The Challenge Rail operators face several challenges in bogie maintenance: Early Wear and Failure Detection: Identifying early signs of wear or failure in bogie components is essential to prevent accidents and ensure smooth operations. Maintenance Scheduling: Determining the optimal frequency for maintenance activities to maximize safety and minimize disruptions. Unscheduled Repairs: Reducing the occurrence of unscheduled repairs that can lead to operational delays and increased costs. The Solution: XMPro Wheel and Track Wear Monitoring XMPro’s solution leverages data from advanced sensors and machine learning (ML) for anomaly detection, providing a proactive approach to wheel and track maintenance. Key Features Streamlining Sensor Data Integration and Transformation XMPro’s Data Stream Designer efficiently integrates and analyzes data from existing vibration and temperature sensors on train bogies. Leveraging a comprehensive integration library, it transforms diverse sensor data into actionable insights for predictive maintenance, enhancing bogie health monitoring in rail operations. Machine Learning for Anomaly Detection: Implementing ML algorithms to analyze sensor data and detect anomalies indicating abnormal wear. Continuous learning and model refinement based on new data and identified wear patterns. Maintenance Scheduling Optimization: Using data-driven insights to optimize maintenance schedules, shifting from fixed intervals to condition-based maintenance. Real-Time Alerts and Reporting: Providing real-time alerts to maintenance teams about potential issues.Generating detailed reports on wheel and track conditions for maintenance planning and regulatory compliance. Figure 1. Real-Time Rail Asset Overview Dashboard Real-Time Rail Asset Overview Dashboard This comprehensive dashboard provides users with an up-to-the-minute view of their rail assets. It features an interactive map that dynamically updates with the GPS coordinates of trains in motion, offering a clear visual representation of their railway lines. Each asset on the map is marked with a color-coded status icon, indicating its current operational state, including active status and any alerts or error messages. The dashboard comprehensively displays the overall status of various asset categories, such as trains, crossings, tracks, maintenance vehicles, and substations. It also highlights all active recommendations generated by the system’s rule logic. This includes critical alerts like exceeded bogie wear thresholds, ensuring immediate attention to potential issues. Additionally, the dashboard includes a detailed graph that tracks maintenance requirements across assets. It prioritizes assets based on their upcoming service needs, facilitating efficient maintenance scheduling. Each section of the dashboard is designed for deeper exploration. Users can drill down into specific asset and recommendation details, gaining granular insights and enabling targeted actions based on the system’s recommendations. This level of detail ensures that users can make informed decisions quickly and maintain optimal operational efficiency. Figure 2. Asset Drill Down View – Trains Asset Drilldown View – Trains The specific asset view for trains provides users with a comprehensive and informative dashboard. Alerts Overview: Graphical representation of open alerts categorized by severity – no alerts, medium, and high severity. Work Order Status: Displays current status of each asset, categorized as available, in planning, or waiting. Performance Metrics (Last 30 Days): Summarizes key metrics such as new alerts, number of work orders, open work orders, and open work requests. It also tracks the duration from alert initiation to work order completion, comparing it with the previous 30-day period. Asset Filtering and Service Information: Enables filtering across all train assets, showing details like the last service date, upcoming service schedules, and due dates. Recent Recommendations: Lists recent recommendations triggered for train assets, with options for users to view details and take necessary actions. XMPro Co-Pilot Integration: Features an interactive block where users can query the AI model, trained on internal data like train engine manuals, for specific advice on errors, warnings, and issues. This information can be directly linked to work order requests and triage instructions. This dashboard is designed for ease of use, allowing quick access to vital information and efficient management of train assets. Figure 3. Asset Analysis View – Bogie B001 Asset Analysis View – Bogie Health The Asset Analysis View provides in-depth insights into specific bogies, illustrated here with a focus on a particular bogie, identified as Bogie B001. Comprehensive Bogie Health Metrics: This section displays vital health indicators for Bogie B001, including vibration levels, temperature readings, and overall condition assessments. It contrasts real-time health data with predictive analytics to forecast the remaining useful life of the bogie, enhancing maintenance planning. Interactive 2D and 3D Bogie Models: The view features detailed 2D and 3D models of Bogie B001, offering capabilities to ‘explode’ the view for a closer examination of individual components. Critical areas, flagged by predictive analysis for potential wear or failure, are highlighted in the model for quick identification. For example, areas showing abnormal wear patterns are marked in distinct colors. Error Identification and Proactive Recommendations: Users can interact with highlighted areas on the bogie model to access specific error details and associated recommendations. This direct linkage to XMPro’s Recommendation Manager facilitates swift and effective resolution strategies. Detailed Bogie Information: The dashboard provides extensive information about Bogie B001, including its type, model, operational history, and manufacturer details, offering a complete profile of the asset. XMPro Co-Pilot Integration: Integrated with XMPro Co-Pilot, this feature utilizes AI, trained on internal datasets like maintenance records and manufacturer specifications, to provide targeted advice and solutions for issues related to Bogie B001. This AI-driven assistance supports informed decision-making and enhances the efficiency of maintenance processes. This Asset Analysis View is tailored to deliver a comprehensive understanding of Bogie B001’s health, combining advanced visualizations with data-driven insights and AI-powered recommendations for effective bogie management in the rail industry. Why XMPro iDTS? XMPro’s Intelligent Digital Twin Suite (iDTS) is uniquely equipped to address the complexities of Bogie Health Monitoring in the rail industry, utilizing cutting-edge technology and analytics. Here’s how XMPro iDTS excels in this application: Intelligent Digital Twin for Bogie Monitoring: XMPro iDTS allows users to craft an intelligent digital twin of the bogie components, enabling detailed simulation and analysis. This virtual model is crucial for assessing wear patterns and predicting maintenance needs, enhancing the accuracy of anomaly detection. Seamless Data Integration and Transformation: Featuring a robust integration library, XMPro iDTS seamlessly incorporates data from various sensors, including vibration and temperature sensors on bogies. This integration is key to transforming raw data into meaningful insights for predictive maintenance. Advanced Machine Learning for Anomaly Detection: Utilizing machine learning algorithms, XMPro iDTS analyzes sensor data to identify anomalies that indicate abnormal wear. This approach allows for early detection of potential issues that could lead to derailments or other safety hazards. Optimization of Maintenance Schedules: By analyzing wear patterns and predicting maintenance needs, XMPro iDTS helps optimize maintenance schedules. This shift from fixed-interval to condition-based maintenance reduces costs and prevents unnecessary downtime. Real-Time Alerts and Proactive Decision Making: The system provides instant alerts on emerging bogie issues, coupled with decision support tools. This feature aids maintenance teams in prioritizing actions based on the criticality of the detected anomalies. Customizable Dashboards for Enhanced Visibility: XMPro iDTS includes adaptable dashboards that display vital bogie health data, alongside comprehensive reporting features for maintenance planning and compliance purposes. Scalability and Flexibility – Start Small, Scale Fast: Designed for scalability and flexibility, XMPro iDTS can be tailored to various rail network sizes and integrates effortlessly with diverse sensor technologies. Its rapid deployment capability ensures a quick realization of value. Enhanced Safety and Operational Efficiency: By enabling proactive maintenance and early wear detection, XMPro iDTS significantly enhances the safety and operational efficiency of rail systems, reducing accident risks and ensuring reliable service. Quick Time To Value – XMPro Blueprints Utilize XMPro blueprints, pre-configured for bogie health monitoring, to quickly set up the digital twin dashboard. These blueprints integrate industry best practices, ensuring a swift and effective implementation. In essence, XMPro iDTS offers a holistic, real-time, predictive, and integrated approach to bogie health monitoring. Its advanced capabilities in digital twin modeling, data integration, machine learning, and customizable dashboards make it an invaluable asset for ensuring the safety and efficiency of rail operations. Not Sure How To Get Started? No matter where you are on your digital transformation journey, the expert team at XMPro can help guide you every step of the way - We have helped clients successfully implement and deploy projects with Over 10x ROI in only a matter of weeks! Request a free online consultation for your business problem. \"*\" indicates required fields Name*Email*Company / Organization*Job Role*Contact Department*SalesSupportMessage*CaptchaNameThis field is for validation purposes and should be left unchanged. Δdocument.getElementById( \"ak_js_3\" ).setAttribute( \"value\", ( new Date() ).getTime() ); /* <![CDATA[ */ gform.initializeOnLoaded( function() {gformInitSpinner( 3, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery('#gform_ajax_frame_3').on('load',function(){var contents = jQuery(this).contents().find('*').html();var is_postback = contents.indexOf('GF_AJAX_POSTBACK') >= 0;if(!is_postback){return;}var form_content = jQuery(this).contents().find('#gform_wrapper_3');var is_confirmation = jQuery(this).contents().find('#gform_confirmation_wrapper_3').length > 0;var is_redirect = contents.indexOf('gformRedirect(){') >= 0;var is_form = form_content.length > 0 && ! is_redirect && ! is_confirmation;var mt = parseInt(jQuery('html').css('margin-top'), 10) + parseInt(jQuery('body').css('margin-top'), 10) + 100;if(is_form){jQuery('#gform_wrapper_3').html(form_content.html());if(form_content.hasClass('gform_validation_error')){jQuery('#gform_wrapper_3').addClass('gform_validation_error');} else {jQuery('#gform_wrapper_3').removeClass('gform_validation_error');}setTimeout( function() { /* delay the scroll by 50 milliseconds to fix a bug in chrome */ }, 50 );if(window['gformInitDatepicker']) {gformInitDatepicker();}if(window['gformInitPriceFields']) {gformInitPriceFields();}var current_page = jQuery('#gform_source_page_number_3').val();gformInitSpinner( 3, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery(document).trigger('gform_page_loaded', [3, current_page]);window['gf_submitting_3'] = false;}else if(!is_redirect){var confirmation_content = jQuery(this).contents().find('.GF_AJAX_POSTBACK').html();if(!confirmation_content){confirmation_content = contents;}setTimeout(function(){jQuery('#gform_wrapper_3').replaceWith(confirmation_content);jQuery(document).trigger('gform_confirmation_loaded', [3]);window['gf_submitting_3'] = false;wp.a11y.speak(jQuery('#gform_confirmation_message_3').text());}, 50);}else{jQuery('#gform_3').append(contents);if(window['gformRedirect']) {gformRedirect();}}jQuery(document).trigger(\"gform_pre_post_render\", [{ formId: \"3\", currentPage: \"current_page\", abort: function() { this.preventDefault(); } }]); if (event.defaultPrevented) { return; } const gformWrapperDiv = document.getElementById( \"gform_wrapper_3\" ); if ( gformWrapperDiv ) { const visibilitySpan = document.createElement( \"span\" ); visibilitySpan.id = \"gform_visibility_test_3\"; gformWrapperDiv.insertAdjacentElement( \"afterend\", visibilitySpan ); } const visibilityTestDiv = document.getElementById( \"gform_visibility_test_3\" ); let postRenderFired = false; function triggerPostRender() { if ( postRenderFired ) { return; } postRenderFired = true; jQuery( document ).trigger( 'gform_post_render', [3, current_page] ); gform.utils.trigger( { event: 'gform/postRender', native: false, data: { formId: 3, currentPage: current_page } } ); if ( visibilityTestDiv ) { visibilityTestDiv.parentNode.removeChild( visibilityTestDiv ); } } function debounce( func, wait, immediate ) { var timeout; return function() { var context = this, args = arguments; var later = function() { timeout = null; if ( !immediate ) func.apply( context, args ); }; var callNow = immediate && !timeout; clearTimeout( timeout ); timeout = setTimeout( later, wait ); if ( callNow ) func.apply( context, args ); }; } const debouncedTriggerPostRender = debounce( function() { triggerPostRender(); }, 200 ); if ( visibilityTestDiv && visibilityTestDiv.offsetParent === null ) { const observer = new MutationObserver( ( mutations ) => { mutations.forEach( ( mutation ) => { if ( mutation.type === 'attributes' && visibilityTestDiv.offsetParent !== null ) { debouncedTriggerPostRender(); observer.disconnect(); } }); }); observer.observe( document.body, { attributes: true, childList: false, subtree: true, attributeFilter: [ 'style', 'class' ], }); } else { triggerPostRender(); } } );} ); /* ]]&gt; */"
  },
  "docs/resources/faqs/external-content/use-cases/boiler-feed-water-pu.html": {
    "href": "docs/resources/faqs/external-content/use-cases/boiler-feed-water-pu.html",
    "title": "Boiler Feed Water Pumps - XMPRO | XMPro",
    "summary": "Boiler Feed Water Pumps - XMPRO url XMPro Boiler Feed Water Pump Monitoring Solution: Ensuring Efficiency and Reliability in Industrial Boiler Operations The Problem Effective monitoring and optimization of boiler feed water pumps are crucial for maintaining the efficiency and reliability of industrial boiler systems. Challenges include the following: Equipment Wear and Tear: Continuous operation can lead to wear and tear of XMPro Boiler Feed Water Pump Monitoring Solution: Ensuring Efficiency and Reliability in Industrial Boiler Operations The Problem Effective monitoring and optimization of boiler feed water pumps are crucial for maintaining the efficiency and reliability of industrial boiler systems. Challenges include the following: Equipment Wear and Tear: Continuous operation can lead to wear and tear of pump components, risking breakdowns and inefficiencies. Energy Efficiency: Boiler feed water pumps can consume significant energy, making their efficient operation crucial for cost management. Predictive Maintenance: Implementing effective predictive maintenance is challenging due to the critical nature of these pumps in boiler operations. Operational Downtime: Unplanned downtime due to pump failures can cause significant disruptions in industrial processes. Water Quality Management: Ensuring optimal water quality to prevent scaling and corrosion in boilers and pumps. #text-box-1396194393 { width: 60%; } #text-box-1396194393 .text-box-content { font-size: 100%; } #banner-630545068 { padding-top: 500px; } #banner-630545068 .bg.bg-loaded { background-image: url(https://xmpro.com/wp-content/uploads/2020/04/6.jpg); } The Solution XMPro’s Boiler Feed Water Pump Monitoring Solution, utilizing XMPro iDTS, offers a comprehensive approach to monitoring and optimizing the performance and reliability of boiler feed water pumps. Key Metrics Monitored: Pump Vibration and Noise: Monitoring for signs of mechanical issues or imbalances. Flow Rate and Pressure: Tracking operational flow rates and pressures for optimal performance. Energy Consumption: Monitoring energy use to identify inefficiencies. Water Quality: Assessing water quality to prevent scaling and corrosion. Component Wear and Tear: Evaluating the condition of bearings, seals, and other critical components. Goals: Develop predictive maintenance models to anticipate and prevent equipment failures. Optimize pump operation for maximum efficiency and minimal energy consumption. Ensure robust data security and integrity. Provide user-friendly dashboards for real-time monitoring and decision-making. Why XMPro iDTS? XMPro iDTS offers specialized solutions to address the challenges associated with monitoring and optimizing boiler feed water pumps in industrial settings. Here’s how XMPro iDTS can be specifically applied to enhance the Boiler Feed Water Pump Monitoring Solution: Digital Twin for Pump System Simulation: XMPro iDTS can create a digital twin of the boiler feed water pump system, providing a virtual representation that mirrors the physical pump and its operation. This allows for real-time simulation and analysis, enabling operators to visualize the impact of various operational parameters on the pump’s performance and health. Predictive Maintenance for Pump Components: Utilizing advanced analytics and machine learning, XMPro iDTS can analyze data from sensors to predict maintenance needs for critical components of the boiler feed water pump. This predictive approach allows for maintenance to be scheduled based on actual equipment condition, reducing unplanned downtime and extending equipment life. Optimization of Pump Operation: XMPro iDTS can analyze operational data to optimize the efficiency of the boiler feed water pump. This includes adjusting flow rates, pressure settings, and monitoring energy consumption to maximize performance while minimizing energy usage. Real-Time Monitoring of Key Performance Indicators: XMPro iDTS enables real-time monitoring of key performance indicators such as vibration, noise, flow rate, pressure, and energy consumption. By tracking these metrics, operators can identify and address issues promptly to maintain optimal efficiency. Automated Alerts and Recommendations: XMPro iDTS can generate automated alerts and recommendations when potential issues are detected. This feature ensures timely interventions, maintaining operational efficiency and safety. Customizable Dashboards and Reporting: XMPro iDTS provides advanced data visualization tools and customizable dashboards, which are essential for monitoring complex systems like boiler feed water pumps. These tools enable different stakeholders to access relevant information, understand trends, and make informed decisions quickly. Water Quality Monitoring: By continuously monitoring water quality, XMPro iDTS helps in preventing scaling and corrosion in the pump and boiler system, contributing to the longevity and efficiency of the equipment. Scalability and Flexibility: XMPro iDTS is scalable, meaning it can be expanded to accommodate additional sensors or pump systems as operational needs evolve. In summary, XMPro iDTS addresses the unique challenges in boiler feed water pump monitoring by providing a comprehensive, real-time, predictive, and integrated solution. Its use of digital twin technology, combined with advanced analytics, automated guidance, and effective data visualization, makes it a powerful tool for optimizing the performance and reliability of boiler feed water pumps in industrial environments."
  },
  "docs/resources/faqs/external-content/use-cases/casting-guidance---x.html": {
    "href": "docs/resources/faqs/external-content/use-cases/casting-guidance---x.html",
    "title": "Casting Guidance | XMPro",
    "summary": "Casting Guidance url The Problem In the industrial casting industry, several challenges impact the efficiency, quality, and cost-effectiveness of operations: Precision in Casting: Achieving the desired precision in casting processes is crucial for product quality but is often hindered by manual processes and variability in conditions. Resource Utilization: Optimal use of materials and energy resources is essential forTags: Iron & Steel / Solutions The Problem In the industrial casting industry, several challenges impact the efficiency, quality, and cost-effectiveness of operations: Precision in Casting: Achieving the desired precision in casting processes is crucial for product quality but is often hindered by manual processes and variability in conditions. Resource Utilization: Optimal use of materials and energy resources is essential for cost-effective operations, yet difficult to achieve without real-time monitoring and guidance. Quality Control: Ensuring consistent quality in casting processes requires continuous monitoring and adjustment, which can be labor-intensive and prone to human error. Downtime and Maintenance: Unplanned downtime due to equipment failure or process inefficiencies leads to significant productivity losses. Environmental Compliance: Adhering to environmental regulations and minimizing waste and emissions is increasingly important and challenging in the casting industry. The Solution XMPro’s Casting Guidance Solution, powered by our Intelligent Digital Twin Suite (iDTS), revolutionizes the casting process by integrating real-time data analytics, predictive modeling, and automated guidance systems. Key Metrics Monitored: Temperature Control: Monitoring the temperature at various stages of the casting process to ensure optimal material properties. Pressure and Flow Rates: Tracking pressure levels and material flow rates to maintain consistency and quality in the casting. Material Usage: Measuring the amount of raw material used, aiming to optimize usage and reduce waste. Cycle Time: Monitoring the duration of each casting cycle to enhance efficiency and throughput. Equipment Performance: Tracking the performance and condition of casting equipment to predict maintenance needs. Product Quality Metrics: Assessing dimensions, surface finish, and other quality parameters of the cast products. Energy Consumption: Monitoring energy usage to identify opportunities for reducing operational costs. Emission Levels: Tracking emissions and waste to ensure environmental compliance and sustainability. Why XMPro iDTS? XMPro’s Intelligent Digital Twin Suite (iDTS) offers several unique capabilities that can be particularly effective in solving the challenges associated with the casting process. Here’s how XMPro’s iDTS can be applied to the Casting Guidance solution: Digital Twin Modeling for Casting Processes: XMPro’s iDTS can create a digital twin of the entire casting process, providing a virtual representation that mirrors every aspect of the physical process. This allows for real-time monitoring and simulation, enabling operators to visualize the impact of changes in process variables and make informed decisions. Predictive Analytics for Quality and Maintenance: Leveraging advanced analytics and machine learning, XMPro’s iDTS can predict quality outcomes based on current process conditions. It can also forecast equipment maintenance needs, reducing unplanned downtime and extending the lifespan of machinery. Process Optimization through AI-Driven Insights: The iDTS can analyze vast amounts of data to identify optimization opportunities in the casting process. This includes adjusting parameters for energy efficiency, material usage, and cycle times, ensuring optimal operation with minimal waste. Customizable Workflows and Automated Guidance: XMPro’s solution allows for the creation of customizable workflows that can automate decision-making processes. For example, the system can automatically adjust process parameters in real-time based on the data received, ensuring consistent quality and efficiency. Integration with Existing Systems and IoT Devices: The iDTS is designed to integrate seamlessly with existing control systems and IoT devices in the casting environment. This ensures that data from all sources is collected and analyzed cohesively, leading to more accurate insights and decision-making. Enhanced Data Visualization and Reporting: XMPro provides advanced data visualization tools and customizable dashboards, making it easier for operators and managers to understand complex data, monitor key performance indicators, and make informed decisions quickly. Scalability and Flexibility: The iDTS platform is scalable, meaning it can be expanded to accommodate additional sensors, equipment, or even other casting lines as the needs of the facility grow. Environmental Monitoring and Compliance: By continuously monitoring emissions and resource usage, XMPro’s iDTS helps ensure that the casting processes are not only efficient but also environmentally sustainable and compliant with regulatory standards. In summary, XMPro’s Intelligent Digital Twin Suite addresses the unique challenges in the casting industry by providing a comprehensive, real-time, predictive, and integrated solution. Its use of digital twin technology, combined with advanced analytics, automated guidance, and effective data visualization, makes it a powerful tool for optimizing casting operations."
  },
  "docs/resources/faqs/external-content/use-cases/chpp-throughput-loss.html": {
    "href": "docs/resources/faqs/external-content/use-cases/chpp-throughput-loss.html",
    "title": "CHPP Throughput Loss Monitoring - XMPRO | XMPro",
    "summary": "CHPP Throughput Loss Monitoring - XMPRO url XMPro CHPP Throughput Loss Monitoring Solution: Maximizing Efficiency and Productivity in Coal Handling and Preparation The Problem In Coal Handling and Preparation Plants (CHPPs), maintaining optimal throughput is crucial for operational efficiency and profitability. However, several challenges can lead to throughput loss: Equipment Failures: Breakdowns in key equipment like conveyors, crushers, and screens can cause XMPro CHPP Throughput Loss Monitoring Solution: Maximizing Efficiency and Productivity in Coal Handling and Preparation #gap-901767514 { padding-top: 30px; } The Problem In Coal Handling and Preparation Plants (CHPPs), maintaining optimal throughput is crucial for operational efficiency and profitability. However, several challenges can lead to throughput loss: Equipment Failures: Breakdowns in key equipment like conveyors, crushers, and screens can cause significant disruptions. Process Inefficiencies: Suboptimal operation of the plant due to process bottlenecks, poor quality feed, or incorrect process settings. Unplanned Downtime: Reactive maintenance and unexpected equipment failures lead to unplanned downtime, reducing overall throughput. Quality Variations in Feed: Variability in the quality of coal feed can impact the efficiency of processing and preparation. Environmental and Safety Compliance: Ensuring compliance with environmental and safety regulations while maintaining high throughput rates. #text-box-1606417944 { width: 60%; } #text-box-1606417944 .text-box-content { font-size: 100%; } #banner-64805274 { padding-top: 549px; } #banner-64805274 .bg.bg-loaded { background-image: url(https://xmpro.com/wp-content/uploads/2020/04/3.jpg); } #banner-64805274 .bg { background-position: 47% 57%; } The Solution XMPro’s CHPP Throughput Loss Monitoring Solution, utilizing XMPro iDTS, offers a comprehensive approach to identifying, analyzing, and mitigating factors that contribute to throughput loss. Key Metrics Monitored: Equipment Performance: Monitoring the performance and condition of key equipment to anticipate and prevent failures. Process Flow Rates: Tracking the flow rates at various stages to identify bottlenecks. Quality of Feed: Analyzing the quality of incoming coal to adjust processing parameters accordingly. Energy Consumption: Monitoring energy usage to identify inefficiencies. Downtime and Maintenance Records: Tracking downtime incidents and maintenance activities to identify patterns and potential areas for improvement. #gap-262626912 { padding-top: 30px; } Why XMPro iDTS? XMPro iDTS offers specialized solutions to address the challenges associated with throughput loss monitoring in Coal Handling and Preparation Plants (CHPPs). Here’s how XMPro iDTS can be specifically applied to enhance the CHPP Throughput Loss Monitoring Solution: Digital Twin for CHPP Operations: XMPro iDTS can create a digital twin of the entire CHPP, simulating various components like conveyors, crushers, and screens. This allows for real-time visualization and analysis of the entire coal handling and preparation process, enabling operators to identify bottlenecks and inefficiencies. Predictive Maintenance and Equipment Monitoring: Utilizing advanced analytics and machine learning, XMPro iDTS can predict equipment maintenance needs. This predictive approach allows for scheduling maintenance activities based on actual equipment condition, reducing unplanned downtime and maintaining consistent throughput. Process Optimization: XMPro iDTS can analyze real-time data to optimize the efficiency of coal handling and preparation processes. This includes adjusting for varying coal quality, flow rates, and operational parameters to ensure optimal throughput while minimizing energy consumption. Automated Alerts and Process Adjustments: XMPro iDTS can automate the generation of alerts and recommendations for process adjustments when potential issues are detected. This feature ensures timely interventions, maintaining operational efficiency and safety. Integration with Existing Systems: XMPro iDTS is designed to integrate seamlessly with existing CHPP control systems and IoT devices, ensuring a unified platform for data collection and analysis. This integration is crucial for accurate monitoring and effective throughput management. Customizable Dashboards and Reporting: XMPro iDTS provides advanced data visualization tools and customizable dashboards, essential for monitoring complex industrial processes. These tools enable operators and managers to understand trends, monitor key performance indicators, and make informed decisions quickly. Scalability and Flexibility: XMPro iDTS is scalable, meaning it can be expanded to accommodate additional sensors, equipment, or even other CHPPs as operational needs evolve. Environmental and Safety Compliance Monitoring: Continuous monitoring of operational parameters helps ensure that the CHPP operates safely and within environmental compliance. This proactive approach to compliance can prevent accidents and reduce the risk of non-compliance penalties. In summary, XMPro iDTS addresses the unique challenges in CHPP throughput loss monitoring by providing a comprehensive, real-time, predictive, and integrated solution. Its use of digital twin technology, combined with advanced analytics, automated guidance, and effective data visualization, makes it a powerful tool for optimizing the performance and efficiency of CHPP operations."
  },
  "docs/resources/faqs/external-content/use-cases/conveyor-belt-system.html": {
    "href": "docs/resources/faqs/external-content/use-cases/conveyor-belt-system.html",
    "title": "Conveyor Belt System Monitoring and Optimization in Automotive Manufacturing | XMPro",
    "summary": "Conveyor Belt System Monitoring and Optimization in Automotive Manufacturing url XMPro Solution for Conveyor Belt System Monitoring and Optimization in Automotive Manufacturing Introduction In the automotive manufacturing industry, conveyor belt systems are vital for the seamless movement of components and assemblies. Their efficiency directly impacts the overall production process. XMPro's Intelligent Business Operations Suite offers a sophisticated solution for monitoring and optimizing these systems, ensuring XMPro Solution for Conveyor Belt System Monitoring and Optimization in Automotive Manufacturing Introduction In the automotive manufacturing industry, conveyor belt systems are vital for the seamless movement of components and assemblies. Their efficiency directly impacts the overall production process. XMPro’s Intelligent Business Operations Suite offers a sophisticated solution for monitoring and optimizing these systems, ensuring uninterrupted and efficient operation. The Challenge Conveyor belt systems in automotive manufacturing face several significant challenges: Wear and Tear: Continuous operation leads to the deterioration of belts, motors, and rollers, which can result in unexpected failures and costly downtime. Unpredictable Failures: Traditional maintenance schedules based on fixed intervals may not accurately predict the actual wear and tear, leading to either premature maintenance or delayed interventions that can result in equipment failure and production stoppages. Operational Inefficiencies: Inefficiencies in conveyor system operations, such as improper belt alignment or variable speed issues, can go unnoticed without real-time monitoring, affecting the overall productivity and quality of the manufacturing process. Energy Consumption: Conveyor systems can be significant energy consumers in the manufacturing process. Without optimization, they can lead to increased operational costs and a larger carbon footprint. The Solution XMPro’s Intelligent Business Operations Suite (iBOS) offers a comprehensive approach to enhancing the efficiency and reliability of conveyor belt systems in automotive manufacturing: Real-Time Monitoring with IoT Integration: XMPro iBOS integrates existing IoT sensors for continuous monitoring of conveyor system parameters. This proactive approach helps in early detection of issues, preventing unexpected failures and reducing downtime. The key benefit is maintaining consistent production flow by addressing problems before they escalate. Advanced Predictive Analytics: The suite utilizes machine learning to analyze sensor data, predicting potential failures and optimal maintenance times. This shift to predictive maintenance enhances system reliability and extends its lifespan, moving away from less efficient reactive maintenance models. Digital Twin Simulation for Conveyor: XMPro iBOS creates a digital twin of the conveyor system, allowing for failure scenario analysis and operational adjustments in a virtual environment. This capability enables testing and optimization of maintenance strategies without impacting production, leading to improved system performance. Automated Alerts and Maintenance Recommendations: The system generates automated alerts for potential issues and provides targeted maintenance recommendations. This feature ensures quick and effective maintenance responses, improving system uptime and reducing response times for maintenance teams. Maintenance Scheduling Optimization: Maintenance tasks are efficiently scheduled based on urgency and impact, optimizing resource allocation. This approach minimizes downtime and extends the lifespan of conveyor components, ensuring a more effective maintenance process. Operational Efficiency and Energy Consumption Analysis: XMPro iBOS analyzes data to identify improvements in conveyor speed and energy use, enhancing overall system efficiency. This leads to cost savings, reduced environmental impact, and better production efficiency, aligning with sustainable manufacturing practices. Figure 1. Real-Time Conveyor Belt System Overview Dashboard for Automotive Assembly Lines Real-Time Conveyor Belt System Overview Dashboard This advanced dashboard is specifically designed for operators in automotive manufacturing, offering a comprehensive view of conveyor belt system performance in assembly lines. It features an interactive layout of the factory floor, dynamically updating with the operational status of different conveyor belt systems, providing a clear visual representation of their efficiency and health. Each conveyor system is marked with a color-coded status icon, indicating its current operational state, including active status and any alerts or error messages related to performance optimization or maintenance needs. Overview of Conveyor Belt System Health: The dashboard displays the overall performance status of conveyor belt systems, highlighting areas with potential efficiency issues or optimization opportunities. It includes critical alerts such as belt misalignment, speed variances, and maintenance alerts for components like motors and rollers. Performance Optimization Alerts: Utilizing data from integrated sensors and advanced analytics, the dashboard provides real-time insights into optimization opportunities. It highlights conveyor systems requiring adjustments for issues like belt tension discrepancies or speed inefficiencies. Maintenance Planning and Scheduling: A detailed graph tracks maintenance and performance optimization requirements across the conveyor systems. It prioritizes systems based on their needs for maintenance or performance adjustments, facilitating efficient and proactive scheduling. Drill-Down Capability for In-Depth Analysis: Users can explore specific conveyor systems for detailed information, including historical performance data, recent maintenance activities, and predictive maintenance recommendations. This level of detail enables targeted actions based on the system’s predictive analytics. Customizable Alerts and Recommendations: The dashboard highlights active recommendations generated by the system’s smart rule logic and machine learning algorithms. This includes suggestions for enhancing conveyor system performance, addressing wear and tear issues, and other optimization actions. Overall Asset Status Summary: At the bottom of the screen, there’s a summary of the status of different conveyor systems, including the number of active and inactive units across various assembly lines. Search Functionality: A search bar at the top allows users to search for specific data across the platform. This Real-Time Conveyor Belt System Performance Optimization Dashboard is an essential tool for automotive manufacturing operators, enabling them to effectively monitor and optimize the performance of their conveyor belt systems. By providing real-time data, predictive insights, and actionable recommendations, it ensures informed decision-making and enhances the operational efficiency and productivity of the assembly lines. Figure 2. Asset Drill Down View – Conveyor Belt Systems in Automotive Assembly Asset Drilldown View – Conveyor Belt Systems This specialized dashboard for conveyor belt systems in automotive assembly lines offers a comprehensive and actionable overview, essential for maintaining high production standards and efficiency. Alerts Overview The dashboard features a graphical representation of open alerts on conveyor belt systems, categorized by severity (no alerts, medium, high). This categorization is crucial for the immediate identification and prioritization of critical issues. The key benefit is the enhancement of responsiveness to potential problems, preventing their escalation into more significant failures. By addressing medium and high-severity alerts promptly, maintenance teams can resolve issues before they impact production, ensuring a smooth production flow. Work Order Status The current status of each conveyor belt system is displayed on the dashboard, categorized as available, in planning, or waiting. This real-time visibility is vital for facilitating better coordination and planning. The primary benefit is the minimization of downtime and the assurance of continuous production flow. It enables maintenance and operational teams to strategically plan work orders and maintenance activities, ensuring that the conveyor systems are always operational when needed. Performance Metrics (Last 30 Days) The dashboard provides a comprehensive summary of performance metrics for the conveyor belt systems, including new alerts, the number of work orders, open work orders, and open work requests. It also tracks the duration from alert initiation to work order completion, comparing it with the previous 30-day period. This tracking offers critical insights into the maintenance team’s responsiveness and efficiency. Monitoring these metrics over time helps identify trends and areas for improvement, leading to more effective maintenance strategies and enhanced system reliability. Asset Filtering and Service Information Detailed asset filtering is available on the dashboard, showing the last service date, upcoming service schedules, and due dates for all conveyor belt systems. This feature facilitates proactive and strategic maintenance planning. Having a clear overview of service schedules allows maintenance teams to prevent potential issues before they occur, extending the lifespan of the conveyor systems and maintaining consistent production quality. Recent Recommendations The dashboard lists recent recommendations triggered for specific conveyor belt system assets, complete with detailed views and actionable steps. This empowers maintenance teams with data-driven, actionable insights for immediate and future maintenance actions. Such a proactive approach is crucial in addressing minor issues before they escalate into major problems, ensuring high operational efficiency. XMPro Co-Pilot Integration The dashboard integrates interactive AI-assisted queries, providing specific advice on errors, warnings, and issues based on internal data, such as conveyor system manuals. There is also a direct link to work order requests and triage instructions, enhancing the decision-making process for maintenance and operational teams. The key benefit of this integration is that it ensures maintenance and operational decisions are based on comprehensive, real-time data. This leads to more accurate troubleshooting, quicker resolution of issues, and overall improved asset management. This dashboard is designed as a central hub for monitoring and managing the health and performance of conveyor belt systems in automotive assembly lines. By providing real-time data, predictive insights, and actionable recommendations, it plays a crucial role in enhancing operational efficiency, reducing downtime, and maintaining high-quality production standards. Figure 3. Asset Analysis View – Conveyor Belt System CD08 Asset Analysis View – Conveyor Belt Health for CD08 This Asset Analysis View provides a detailed examination of the conveyor belt system CD08 in the automotive assembly line, offering critical insights for maintenance and operational efficiency. Comprehensive Conveyor Belt Health Metrics The dashboard displays vital health indicators for Conveyor Belt System CD08, including belt tension, motor temperature readings, and overall condition assessments. It integrates predictive analytics to contrast real-time health data with forecasts of the remaining useful life (RUL), thereby enhancing maintenance planning. The key benefit of this feature is its enablement of proactive maintenance strategies. By predicting potential issues before they escalate, it reduces unplanned downtime and extends the operational life of the conveyor belt system, ensuring consistent production efficiency. Belt Tension: Currently at 7 N/mm, with an hourly trend line indicating fluctuations outside the optimal range of 5-15 N/mm. Motor Temperature: At 81°C, exceeding the normal range of up to 60°C. Vibration Level: Recorded at 8 mm/s, within the optimal range of 4-12 mm/s. Motor Power Consumption: Measures 15 kW, above the normal range of 10-12 kW. Interactive 2D and 3D Conveyor Belt Models The view features detailed 2D and 3D models of Conveyor Belt System CD08, offering capabilities to ‘explode’ the view for a closer examination of individual components. Critical areas, flagged by predictive analysis for potential wear or failure, such as motor efficiency drop and belt misalignment, are highlighted in the model for quick identification. This functionality facilitates focused attention on high-risk components, streamlining the maintenance and repair processes. Error Identification and Proactive Recommendations Interactive error details allow users to interact with highlighted areas on the model to access specific error information and associated recommendations. This feature is directly linked to XMPro’s Recommendation Manager, facilitating swift and effective resolution strategies. The key benefit here is the assurance of timely and effective maintenance actions, which minimizes the impact of potential failures on production. Alerts: “CD08 – Motor Efficiency Drop Detected” and “CD08 – Conveyor Belt Misalignment Detected,” with the latter marked as HIGH PRIORITY. Detailed Conveyor Belt Information The dashboard provides an extensive asset profile for Conveyor Belt System CD08, including its type, model, operational history, and manufacturer details. This detailed information aids in informed decision-making and tailored maintenance approaches. Belt Type: Dual Belt Model: Dupow-12 Section Length: 30 meters Belt Material: Plastic Belt ID: CD08 XMPro Co-Pilot Integration Integrated with XMPro Co-Pilot, this feature uses AI, trained on datasets like maintenance records, to provide targeted advice and solutions for issues related to Belt Conveyor CD08. This AI-driven assistance supports informed decision-making and enhances maintenance process efficiency. Work Request History A table listing work requests includes details about the maintenance activities performed and their outcomes, providing a history of interventions for Conveyor Belt System CD08. This Asset Analysis View is specifically designed to deliver a comprehensive understanding of Conveyor Belt System CD08’s health, combining advanced visualizations with data-driven insights and AI-powered recommendations. It is an essential tool for effective conveyor belt system management in automotive manufacturing, ensuring operational efficiency and safety. By providing a detailed view of the conveyor belt’s condition and performance, it plays a crucial role in maintaining high-quality production standards. Why XMPro iBOS? for Conveyor Belt System Monitoring and Optimization in the Automotive Industry? XMPro’s Intelligent Business Operations Suite (iBOS) offers a suite of unique solutions specifically tailored for optimizing the performance and maintenance of conveyor belt systems in the automotive industry. Here’s how XMPro iBOS effectively addresses this challenge: Advanced Intelligent Digital Twin Modeling: XMPro iBOS creates sophisticated digital twins of conveyor belt systems, providing a virtual representation that mirrors their real-world conditions. This advanced modeling enables detailed analysis and simulation of the conveyor system’s performance under various operational scenarios. The benefit of this approach is the ability to precisely identify potential issues and test maintenance strategies in a virtual environment, thereby reducing the risk of disruptions in the actual production process. Advanced Sensor Data Integration & Transformation: The suite integrates real-time data from various sensors mounted on the conveyor systems, capturing critical metrics such as belt tension, motor operation, and roller condition. This comprehensive monitoring and analysis allow for the identification of areas that require maintenance or operational optimization. The key benefit here is the provision of a holistic view of each conveyor system’s health, enabling timely interventions and preventing potential failures. Predictive Analytics for Performance Enhancement: Utilizing advanced predictive analytics, XMPro iBOS can forecast potential mechanical issues and identify optimal operational settings for each conveyor system. This predictive approach enables proactive adjustments to be made, enhancing the conveyor systems’ operational efficiency and reducing wear and tear. The primary benefit is the reduction in unplanned downtime and the extension of the conveyor systems’ operational lifespan. Maintenance Scheduling Optimization: By analyzing performance data, XMPro iBOS helps shift maintenance strategies from a reactive to a predictive approach. This optimization of maintenance schedules is based on the actual condition of the equipment, rather than fixed intervals. The benefit is a significant reduction in downtime and an extension of the lifespan of conveyor system components, leading to increased overall efficiency and reduced maintenance costs. Real-Time Monitoring and Predictive Alerting: The platform generates automated recommendations for maintenance actions based on real-time data and predictive insights. This automation ensures that the conveyor systems are always operating at their peak efficiency and reduces the likelihood of unexpected failures. The key advantage is the ability to maintain continuous production flow, with minimal interruptions for maintenance. Customizable and Interactive Dashboards: XMPro iBOS features customizable dashboards that provide real-time insights into the performance of conveyor systems. These dashboards are interactive, allowing operators to drill down into specific aspects of conveyor system operation for a more detailed analysis. The benefit of these dashboards is that they enhance decision-making capabilities and allow for quick responses to emerging issues, ensuring optimal performance at all times. Scalability and Flexibility – Start Small, Scale Fast: XMPro iBOS offers scalable and flexible solutions that are suitable for different sizes of automotive manufacturing operations. This modular design ensures easy integration and adaptability, allowing operations to start small and expand as needed. The benefit here is the ability to scale the solution in line with the growth of the manufacturing operation, ensuring that the predictive maintenance system evolves with the business. Enhanced Safety & Operational Efficiency: The suite enhances operational safety by predicting and mitigating potential risks associated with conveyor system operation. It also improves overall operational efficiency by ensuring that the conveyor systems operate within optimal parameters. The key benefit is the creation of a safer working environment and the reduction of operational risks, leading to a more efficient and productive manufacturing process. XMPro Blueprints – Quick Time to Value: XMPro Blueprints offer a rapid path to value realization for automotive manufacturers. These pre-configured templates are designed for quick implementation, incorporating best practices and industry standards. The benefit of these Blueprints is that they accelerate the deployment process, enabling manufacturers to quickly realize the advantages of the predictive maintenance system. In summary, XMPro iBOS addresses the Conveyor Belt System Monitoring and Optimization use case in the automotive industry by providing a comprehensive, real-time, predictive, and integrated solution. Its capabilities in digital twin technology, advanced data integration, predictive analytics, and interactive dashboards make it a powerful tool for enhancing the performance, safety, and efficiency of conveyor belt systems in automotive manufacturing. Not Sure How To Get Started? No matter where you are on your digital transformation journey, the expert team at XMPro can help guide you every step of the way - We have helped clients successfully implement and deploy projects with Over 10x ROI in only a matter of weeks! Request a free online consultation for your business problem. \"*\" indicates required fields Name*Email*Company / Organization*Job Role*Contact Department*SalesSupportMessage*CaptchaEmailThis field is for validation purposes and should be left unchanged. Δdocument.getElementById( \"ak_js_3\" ).setAttribute( \"value\", ( new Date() ).getTime() ); /* <![CDATA[ */ gform.initializeOnLoaded( function() {gformInitSpinner( 3, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery('#gform_ajax_frame_3').on('load',function(){var contents = jQuery(this).contents().find('*').html();var is_postback = contents.indexOf('GF_AJAX_POSTBACK') >= 0;if(!is_postback){return;}var form_content = jQuery(this).contents().find('#gform_wrapper_3');var is_confirmation = jQuery(this).contents().find('#gform_confirmation_wrapper_3').length > 0;var is_redirect = contents.indexOf('gformRedirect(){') >= 0;var is_form = form_content.length > 0 && ! is_redirect && ! is_confirmation;var mt = parseInt(jQuery('html').css('margin-top'), 10) + parseInt(jQuery('body').css('margin-top'), 10) + 100;if(is_form){jQuery('#gform_wrapper_3').html(form_content.html());if(form_content.hasClass('gform_validation_error')){jQuery('#gform_wrapper_3').addClass('gform_validation_error');} else {jQuery('#gform_wrapper_3').removeClass('gform_validation_error');}setTimeout( function() { /* delay the scroll by 50 milliseconds to fix a bug in chrome */ }, 50 );if(window['gformInitDatepicker']) {gformInitDatepicker();}if(window['gformInitPriceFields']) {gformInitPriceFields();}var current_page = jQuery('#gform_source_page_number_3').val();gformInitSpinner( 3, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery(document).trigger('gform_page_loaded', [3, current_page]);window['gf_submitting_3'] = false;}else if(!is_redirect){var confirmation_content = jQuery(this).contents().find('.GF_AJAX_POSTBACK').html();if(!confirmation_content){confirmation_content = contents;}setTimeout(function(){jQuery('#gform_wrapper_3').replaceWith(confirmation_content);jQuery(document).trigger('gform_confirmation_loaded', [3]);window['gf_submitting_3'] = false;wp.a11y.speak(jQuery('#gform_confirmation_message_3').text());}, 50);}else{jQuery('#gform_3').append(contents);if(window['gformRedirect']) {gformRedirect();}}jQuery(document).trigger(\"gform_pre_post_render\", [{ formId: \"3\", currentPage: \"current_page\", abort: function() { this.preventDefault(); } }]); if (event.defaultPrevented) { return; } const gformWrapperDiv = document.getElementById( \"gform_wrapper_3\" ); if ( gformWrapperDiv ) { const visibilitySpan = document.createElement( \"span\" ); visibilitySpan.id = \"gform_visibility_test_3\"; gformWrapperDiv.insertAdjacentElement( \"afterend\", visibilitySpan ); } const visibilityTestDiv = document.getElementById( \"gform_visibility_test_3\" ); let postRenderFired = false; function triggerPostRender() { if ( postRenderFired ) { return; } postRenderFired = true; jQuery( document ).trigger( 'gform_post_render', [3, current_page] ); gform.utils.trigger( { event: 'gform/postRender', native: false, data: { formId: 3, currentPage: current_page } } ); if ( visibilityTestDiv ) { visibilityTestDiv.parentNode.removeChild( visibilityTestDiv ); } } function debounce( func, wait, immediate ) { var timeout; return function() { var context = this, args = arguments; var later = function() { timeout = null; if ( !immediate ) func.apply( context, args ); }; var callNow = immediate && !timeout; clearTimeout( timeout ); timeout = setTimeout( later, wait ); if ( callNow ) func.apply( context, args ); }; } const debouncedTriggerPostRender = debounce( function() { triggerPostRender(); }, 200 ); if ( visibilityTestDiv && visibilityTestDiv.offsetParent === null ) { const observer = new MutationObserver( ( mutations ) => { mutations.forEach( ( mutation ) => { if ( mutation.type === 'attributes' && visibilityTestDiv.offsetParent !== null ) { debouncedTriggerPostRender(); observer.disconnect(); } }); }); observer.observe( document.body, { attributes: true, childList: false, subtree: true, attributeFilter: [ 'style', 'class' ], }); } else { triggerPostRender(); } } );} ); /* ]]&gt; */"
  },
  "docs/resources/faqs/external-content/use-cases/cooling-tower-fin-fa.html": {
    "href": "docs/resources/faqs/external-content/use-cases/cooling-tower-fin-fa.html",
    "title": "Cooling Tower Fin Fan Monitoring | XMPro",
    "summary": "Cooling Tower Fin Fan Monitoring url XMPro Cooling Tower Fin Fan Monitoring Solution: Real-time Monitoring and Predictive Maintenance for Enhanced Efficiency The Problem In industrial settings, cooling towers are critical for maintaining optimal operating temperatures and ensuring the efficiency of various processes. However, managing these cooling towers poses several challenges: Inefficient Performance Monitoring: Traditional methods of monitoring cooling tower performance areTags: Iron & Steel / Solutions XMPro Cooling Tower Fin Fan Monitoring Solution: Real-time Monitoring and Predictive Maintenance for Enhanced Efficiency The Problem In industrial settings, cooling towers are critical for maintaining optimal operating temperatures and ensuring the efficiency of various processes. However, managing these cooling towers poses several challenges: Inefficient Performance Monitoring: Traditional methods of monitoring cooling tower performance are often manual, time-consuming, and prone to errors, leading to inefficiencies and increased operational costs. Unplanned Downtime: Without real-time monitoring, issues such as fan failures, reduced airflow, or suboptimal cooling can go unnoticed until they cause significant downtime, impacting productivity. Maintenance Challenges: Predicting maintenance needs is difficult without detailed operational data, leading to either excessive preventive maintenance or unexpected breakdowns. Energy Consumption: Cooling towers are energy-intensive, and without proper monitoring, they can consume more energy than necessary, increasing costs and environmental impact. Compliance and Safety Issues: Ensuring compliance with environmental regulations and maintaining safe operating conditions is challenging without continuous monitoring and data analysis. The Solution XMPro’s Cooling Tower Fin Fan Monitoring Solution, leveraging our Intelligent Digital Twin Suite (XMPro iDTS), is designed to optimize the performance of cooling towers in industrial settings. By integrating real-time data collection from various sensors installed on the cooling tower fans, the solution provides critical insights into performance metrics and maintenance needs. Key Metrics Monitored: Fan vibration and noise levels. Airflow rate and temperature. Fan motor power consumption. Cooling tower water temperature. Overall fan efficiency. Why XMPro iDTS? XMPro’s Intelligent Digital Twin Suite (iDTS) offers a unique approach to addressing the challenges in cooling tower fin fan monitoring. Here are some of the distinctive ways XMPro’s iDTS can solve the problems outlined earlier: Digital Twin Technology for Real-Time Simulation: XMPro’s iDTS creates a digital twin of the cooling tower, allowing for real-time simulation and analysis. This enables operators to visualize and understand the current state of the cooling system, predict future performance, and identify potential issues before they lead to downtime. Advanced Analytics and Predictive Maintenance: Leveraging AI and machine learning, XMPro’s iDTS can analyze historical and real-time data to predict maintenance needs. This predictive approach helps in scheduling maintenance activities proactively, reducing unplanned downtime and extending the lifespan of equipment. Customizable Workflow Automation: The platform allows for the creation of customizable workflows. These can automate responses to certain conditions, like adjusting fan speeds or triggering maintenance protocols when certain thresholds are reached, thus optimizing performance and energy efficiency. Integration with Existing Systems: XMPro’s iDTS can integrate with existing control systems and IoT devices, providing a unified platform for monitoring and management. This integration ensures that data from all sources is collected and analyzed cohesively, leading to more accurate insights and decision-making. Enhanced Data Visualization and User Dashboards: The platform provides advanced data visualization tools and customizable dashboards. These tools make it easier for different stakeholders to access relevant information, understand trends, and make informed decisions. Scalability and Flexibility: XMPro’s solution is scalable and flexible, accommodating the expansion of monitoring capabilities as the needs of the facility grow. This means that additional cooling towers or new sensor types can be integrated seamlessly into the existing system. Compliance and Safety Management: By continuously monitoring operational parameters, XMPro’s iDTS helps ensure that the cooling towers operate within regulatory compliance and safety standards. This proactive approach to compliance management can prevent violations and enhance overall safety. Energy Efficiency Optimization: Through continuous monitoring and analysis, XMPro’s iDTS can identify areas where energy efficiency can be improved, helping to reduce operational costs and the environmental impact of cooling tower operations. In summary, XMPro’s Intelligent Digital Twin Suite addresses the challenges in cooling tower fin fan monitoring by providing a comprehensive, real-time, predictive, and integrated solution. Its use of digital twin technology, combined with advanced analytics, customizable automation, and effective data visualization, makes it a powerful tool for optimizing cooling tower operations."
  },
  "docs/resources/faqs/external-content/use-cases/copy-me.html": {
    "href": "docs/resources/faqs/external-content/use-cases/copy-me.html",
    "title": "Scraped Content | XMPro",
    "summary": "Scraped Content Aging Pipe Predictive Maintenance in Water Utilities Air Quality Monitoring For Agriculture Alarm Management and Triage Asset Condition Monitoring for Surface Processing Plants in the Mining Industry Bogie Health Monitoring in the Rail Industry Boiler Feed Water Pumps CHPP Throughput Loss Monitoring Casting Guidance Conveyor Belt System Monitoring and Optimization in Automotive Manufacturing Cooling Tower Fin Fan Monitoring Cyclone/Slurry Pump Monitoring Demand Planning to Reduce Stockholding in Stores Demin Water Monitoring for Boiler Tube Corrosion EV Battery Assembly Process Optimization for the Car Manufacturing Industry Flood Prediction & Response in Water Utilities Golden Batch For Culture Addition In The Dairy processing Industry. Golden Batch Monitoring Improve First Pass Yield (FPY) Induced Draft (ID) Fan Monitoring Long Conveyor Monitoring Monitor Process Health to Reduce Cash-to-Cash Cycle Monitor Storm Water Reservoirs For Flood Prevention Monitor and Reduce Energy Consumption Oil Well Maintenance Planning Oil Well RTP Monitoring Pipe Scaling Prediction for Roller Cooling Precision Irrigation in Agriculture Predict Heat Exchanger Fouling Predictive Maintenance & Asset Health Monitoring For Haul Trucks In The Mining Industry Predictive Maintenance For Mobile Assets Within The Mining Industry Predictive Maintenance for Robotic Arms in the Automotive Industry Predictive Maintenance for Wind Turbines Pump Health Monitoring in Water Utilities Pumping Station OEE Real-time Balanced Business Scorecard (BBS) Real-time Safety Monitoring Short Term Inventory Planning Strategic Performance & Safety Oversight for Global Mining Operations Wheel and Track Wear Monitoring In The Rail Industry Wind Turbine Performance Optimization"
  },
  "docs/resources/faqs/external-content/use-cases/cyclone-slurry-pump-.html": {
    "href": "docs/resources/faqs/external-content/use-cases/cyclone-slurry-pump-.html",
    "title": "Cyclone/Slurry Pump Monitoring | XMPro",
    "summary": "Cyclone/Slurry Pump Monitoring url XMPro Cyclone/Slurry Pump Monitoring Solution: Enhancing Efficiency and Reliability in Harsh Industrial Environments The Problem Cyclone and slurry pump systems are critical components in many industrial processes, particularly in mining, manufacturing, and wastewater management. However, they face several challenges: Wear and Tear: These systems are prone to rapid wear due to abrasive materials, leading to XMPro Cyclone/Slurry Pump Monitoring Solution: Enhancing Efficiency and Reliability in Harsh Industrial Environments The Problem Cyclone and slurry pump systems are critical components in many industrial processes, particularly in mining, manufacturing, and wastewater management. However, they face several challenges: Wear and Tear: These systems are prone to rapid wear due to abrasive materials, leading to frequent maintenance and downtime. Efficiency Optimization: Achieving optimal efficiency in cyclone separation and slurry pumping is challenging due to varying process conditions. Predictive Maintenance: Traditional maintenance schedules may not align with actual wear patterns, leading to either premature maintenance or unexpected failures. Energy Consumption: Slurry pumps are energy-intensive, and optimizing their energy use is crucial for cost management and environmental sustainability. Operational Safety: Ensuring the safe operation of these systems, especially under high-pressure conditions, is essential to prevent accidents and equipment damage. The Solution XMPro’s Cyclone/Slurry Pump Monitoring Solution, utilizing XMPro’s Intelligent Digital Twin Suite (iDTS), offers a comprehensive approach to monitoring and optimizing these critical systems. Key Metrics Monitored: Pump Vibration and Noise Levels: Indicators of potential wear or mechanical issues. Flow Rates and Pressure: Essential for assessing the efficiency and performance of the system. Power Consumption: Monitoring energy use for cost-saving and environmental considerations. Wear Patterns: Tracking wear and tear on pump components and cyclone liners. Temperature Monitoring: Essential for detecting overheating, which can indicate mechanical issues. Why XMPro iDTS? XMPro’s Intelligent Digital Twin Suite (iDTS) offers unique solutions for the challenges associated with monitoring and optimizing cyclone and slurry pump operations. Here’s how XMPro’s iDTS can be specifically applied to enhance the Cyclone/Slurry Pump Monitoring Solution: Digital Twin for Process Simulation: XMPro’s iDTS can create a digital twin of the cyclone and slurry pump systems, allowing for real-time simulation and analysis. This enables operators to visualize the internal dynamics of these systems under various conditions, helping to predict wear and performance issues before they occur. Predictive Maintenance through Advanced Analytics: Leveraging machine learning and predictive analytics, the iDTS can analyze data from sensors to predict maintenance needs. This approach allows for maintenance to be scheduled based on actual wear and performance data, reducing downtime and extending equipment life. Operational Efficiency Optimization: The iDTS can process real-time data to optimize the efficiency of cyclone separation and slurry pumping. This includes adjusting for varying material properties, flow rates, and pressure levels to maintain optimal performance while minimizing energy consumption. Automated Alerts and Recommendations: XMPro’s solution can automate the process of generating alerts and recommendations when potential issues are detected. This feature ensures timely interventions, preventing damage and maintaining operational safety. Integration with Existing Control Systems: The iDTS is designed to integrate seamlessly with existing control systems and IoT devices, ensuring a unified platform for data collection and analysis. This integration is crucial for accurate monitoring and control of cyclone and slurry pump operations. Customizable Dashboards and Reporting: XMPro provides advanced data visualization tools and customizable dashboards, which are crucial for monitoring complex systems like cyclone and slurry pumps. These tools enable different stakeholders to access relevant information and make informed decisions. Scalability and Flexibility: The iDTS platform is scalable, meaning it can be expanded to accommodate additional sensors or systems as operational needs grow. Enhanced Safety and Compliance Monitoring: Continuous monitoring of operational parameters helps ensure that the systems operate safely and within regulatory compliance. This proactive approach to safety and compliance can prevent accidents and reduce the risk of non-compliance penalties. In summary, XMPro’s Intelligent Digital Twin Suite addresses the unique challenges in cyclone and slurry pump monitoring by providing a comprehensive, real-time, predictive, and integrated solution. Its use of digital twin technology, combined with advanced analytics, automated guidance, and effective data visualization, makes it a powerful tool for optimizing the performance and reliability of these critical industrial systems."
  },
  "docs/resources/faqs/external-content/use-cases/demand-planning-to-r.html": {
    "href": "docs/resources/faqs/external-content/use-cases/demand-planning-to-r.html",
    "title": "Demand Planning to Reduce Stockholding in Stores - XMPRO | XMPro",
    "summary": "Demand Planning to Reduce Stockholding in Stores - XMPRO url XMPro Demand Planning Solution: Streamlining Inventory Management for Retail Efficiency The Problem Effective demand planning is crucial for retail operations to maintain optimal stock levels. Challenges in this area: Overstocking and Understocking: Balancing inventory levels to avoid excess stock while preventing stockouts. Accurate Demand Forecasting: Predicting consumer demand accurately to inform inventory decisions. Supply Chain XMPro Demand Planning Solution: Streamlining Inventory Management for Retail Efficiency The Problem Effective demand planning is crucial for retail operations to maintain optimal stock levels. Challenges in this area: Overstocking and Understocking: Balancing inventory levels to avoid excess stock while preventing stockouts. Accurate Demand Forecasting: Predicting consumer demand accurately to inform inventory decisions. Supply Chain Coordination: Aligning inventory management with supply chain dynamics. Data Integration and Analysis: Effectively utilizing data from sales, market trends, and consumer behavior for informed decision-making. Cost Management: Minimizing costs associated with inventory holding, storage, and potential markdowns. #text-box-448503877 { width: 60%; } #text-box-448503877 .text-box-content { font-size: 100%; } #banner-610605364 { padding-top: 500px; } #banner-610605364 .bg.bg-loaded { background-image: url(https://xmpro.com/wp-content/uploads/2020/04/21.jpg); } The Solution XMPro’s Demand Planning Solution to Reduce Stockholding in Stores, utilizing XMPro iDTS, offers a data-driven approach to optimize inventory levels based on accurate demand forecasting. Key Metrics Monitored: Sales Data Analysis: Monitoring past sales data to identify trends and patterns. Market Trends and Consumer Behavior: Analyzing market trends and consumer behavior to anticipate changes in demand. Inventory Levels: Tracking current stock levels across different store locations. Supply Chain Dynamics: Monitoring supply chain factors that impact inventory replenishment. Cost Implications: Assessing the financial impact of inventory decisions. Initial Goals: Implement data integration tools for comprehensive analysis of sales, market, and supply chain data. Develop predictive models for accurate demand forecasting. Optimize inventory levels to reduce excess stock and minimize stockouts. Ensure robust data security and integrity. Provide user-friendly dashboards for real-time monitoring and decision-making. Why XMPro iDTS? XMPro iDTS offers specialized solutions to address the challenges associated with demand planning for reducing stockholding in stores. Here’s how XMPro iDTS can be specifically applied to enhance this solution: Digital Twin for Inventory and Supply Chain: XMPro iDTS can create a digital twin of the store’s inventory and supply chain, providing a virtual representation for analysis and optimization. This allows for the simulation of different inventory scenarios, helping to predict the impact of various demand patterns and supply chain changes on stock levels. Advanced Predictive Analytics for Demand Forecasting: Utilizing machine learning and predictive analytics, XMPro iDTS can analyze historical sales data, market trends, and consumer behavior to forecast demand accurately. This helps in maintaining optimal inventory levels, reducing the risk of overstocking or stockouts. Real-Time Data Integration and Analysis: XMPro iDTS can integrate and analyze data from various sources, including point-of-sale systems, market research, and supply chain information. This comprehensive view enables more accurate and dynamic demand planning. Automated Alerts and Recommendations: XMPro iDTS can generate automated alerts and recommendations for inventory replenishment and adjustment. This feature ensures timely and efficient inventory management decisions. Customizable Dashboards for Inventory Management: XMPro iDTS provides advanced data visualization tools and customizable dashboards tailored to the needs of inventory managers and retail planners. These dashboards offer insights into sales trends, inventory levels, and supply chain dynamics. Supply Chain Coordination: By monitoring supply chain dynamics, XMPro iDTS helps in aligning inventory management with supplier capabilities and logistics, ensuring timely replenishment and reducing lead times. Cost Impact Analysis: XMPro iDTS can assess the financial impact of inventory decisions, helping businesses to manage costs related to stockholding, storage, and potential markdowns. Scalability and Flexibility: XMPro iDTS is scalable, meaning it can be adapted to different store sizes, product ranges, and can integrate additional data sources as operational needs evolve. In summary, XMPro iDTS addresses the unique challenges in demand planning for retail stores by providing a comprehensive, real-time, predictive, and integrated solution. Its use of digital twin technology, combined with advanced analytics, automated guidance, and effective data visualization, makes it a powerful tool for optimizing inventory levels, enhancing retail efficiency, and reducing costs."
  },
  "docs/resources/faqs/external-content/use-cases/demin-water-monitori.html": {
    "href": "docs/resources/faqs/external-content/use-cases/demin-water-monitori.html",
    "title": "Demin Water Monitoring for Boiler Tube Corrosion - XMPRO | XMPro",
    "summary": "Demin Water Monitoring for Boiler Tube Corrosion - XMPRO url Demin Water Monitoring for Boiler Tube Corrosion Thermal power plants experience large generation losses due to boiler tube leaks. A failure of the demineralized water deaerators that heat feedwater and reduce oxygen is a major cause of boiler tube erosion/corrosion. Solution Monitor the demin water deaerators that remove the oxygen that lead to pit corrosion Demin Water Monitoring for Boiler Tube Corrosion Thermal power plants experience large generation losses due to boiler tube leaks. A failure of the demineralized water deaerators that heat feedwater and reduce oxygen is a major cause of boiler tube erosion/corrosion. Solution Monitor the demin water deaerators that remove the oxygen that lead to pit corrosion on the boiler tubes. Operations are not always aware of the impact of deaeration on the tubes and often ignore these DCS alerts. Benefits Reduce losses due to boiler tube leaks. Improve collaboration between reliability engineering and operations."
  },
  "docs/resources/faqs/external-content/use-cases/ev-battery-assembly-.html": {
    "href": "docs/resources/faqs/external-content/use-cases/ev-battery-assembly-.html",
    "title": "EV Battery Assembly Process Optimization for the Car Manufacturing Industry | XMPro",
    "summary": "EV Battery Assembly Process Optimization for the Car Manufacturing Industry url EV Battery Assembly Process Optimization for the Car Manufacturing Industry Introduction The transition to electric vehicles (EVs) represents a significant shift in the automotive industry, with the battery assembly process playing a crucial role in the production of EVs. This process involves complex steps, from cell sorting and module assembly to the integration of battery EV Battery Assembly Process Optimization for the Car Manufacturing Industry Introduction The transition to electric vehicles (EVs) represents a significant shift in the automotive industry, with the battery assembly process playing a crucial role in the production of EVs. This process involves complex steps, from cell sorting and module assembly to the integration of battery management systems and final pack assembly. Optimizing this process is essential for manufacturers to meet the growing demand for EVs, ensure high quality and safety standards, and maintain cost efficiency. The Challenge Complexity of Battery Assembly: The EV battery assembly process involves multiple intricate steps, each requiring precision and consistency to ensure the final product’s performance and safety. Quality Control: Maintaining high-quality standards throughout the assembly process is critical, as any defects can significantly impact the battery’s performance and the vehicle’s overall safety. Scalability: As demand for EVs grows, manufacturers must scale their battery assembly processes without compromising quality or efficiency. Supply Chain Coordination: Efficient coordination with suppliers is essential to ensure the timely delivery of high-quality components, such as battery cells, modules, and management systems. Energy Efficiency: Optimizing energy consumption during the assembly process can significantly reduce production costs and contribute to the sustainability goals of the manufacturing plant. Adaptability to New Technologies: The rapid evolution of battery technology requires assembly processes to be flexible and adaptable, allowing for the integration of new materials and designs. Data Integration and Analysis: Collecting and analyzing data from the assembly process to identify inefficiencies and areas for improvement can be challenging due to the complexity of the systems involved. The Solution: XMPro’s Intelligent EV Battery Assembly Process Optimization for the Car Manufacturing Industry. XMPro’s Intelligent Business Operations Suite (iBOS) is precisely engineered to address the intricate challenges of optimizing the EV battery assembly process. By leveraging a data-driven approach, it significantly improves the precision, efficiency, and scalability of battery production, crucial for meeting the high standards of safety, quality, and performance demanded in the EV industry. XMPro utilizes cutting-edge technologies to streamline the battery assembly process, transforming it into a highly efficient and predictable operation. Key Features Real-time Data Integration and Process Adjustment: XMPro integrates seamlessly with sensors and control systems throughout the battery assembly line, collecting real-time data on key parameters such as temperature, voltage, and assembly accuracy. This real-time monitoring is vital for maintaining optimal conditions across the manufacturing process, ensuring that each battery unit consistently meets strict quality and performance criteria. Advanced Analytics for Process Insights: XMPro applies sophisticated analytics to the collected data, uncovering patterns, trends, and deviations from the ideal assembly conditions. This deep dive into the data helps identify the impact of various factors on the battery assembly process, pinpointing areas for improvement and guaranteeing uniform product quality. Predictive Modeling for Assembly Optimization: With predictive modeling capabilities, XMPro forecasts the outcomes of various assembly scenarios, allowing manufacturers to test and refine assembly parameters. This optimization extends across the entire battery assembly process, from cell alignment to module packaging, maximizing both efficiency and quality. Automated Optimization and Control: Leveraging predictive insights and real-time data, XMPro can automate the adjustment of assembly parameters. This ensures that the battery assembly process remains within optimal conditions, minimizing manual intervention, enhancing production efficiency, and reducing the likelihood of errors. Configurable Dashboards for Centralized Monitoring: XMPro features customizable dashboards that offer a comprehensive view of the battery assembly process. These dashboards display essential metrics, alert operators to any deviations, and provide actionable recommendations, enabling swift and informed decision-making to maintain process integrity. Continuous Improvement Loop: XMPro promotes a culture of continuous improvement by analyzing data from each production batch. Insights from this ongoing analysis are used to refine predictive models and optimization strategies, leading to steady enhancements in the battery assembly process, further improving efficiency and product quality. Through its sophisticated integration of real-time data collection, advanced analytics, and predictive modeling, XMPro transforms the EV battery assembly process into a highly efficient, scalable, and quality-driven operation. Discover XMPro’s Process Optimization Solution for EV Battery Assembly Figure 1. Real-Time EV Battery Assembly Process Overview Dashboard Overview: This state-of-the-art dashboard is crafted for electric vehicle industry professionals managing EV battery assembly across dedicated lines. It provides an all-encompassing view of the entire battery assembly process, from component inspection to final assembly, emphasizing operational efficacy, system integrity, and adherence to quality benchmarks. The dashboard’s interactive elements offer live updates, representing the status of pivotal stages in the EV battery assembly line, such as cell stack assembly, thermal management, and module assembly, within a specific facility. Key Features: Integrated Process Monitoring: Showcases real-time data on vital EV battery assembly stages, including cell inspection and module assembly, for each line. Color-coded progress tracking differentiates the quality status across the batch timeline, while the line status bar reflects the working condition of the assembly line, signaling performance metrics and potential alerts. Optimization Alerts for Battery Assembly: Leverages sensor readings and advanced analytics to pinpoint optimization points within the battery assembly process. It provides alerts for areas needing attention, like temperature regulation or component alignment, to ensure product integrity and assembly line efficiency. XMPro AI Assistant Integration: Features the XMPro Co-Pilot system, harnessing the power of AI and machine learning to deliver informed decisions and automated processes based on in-depth data analysis. Line-Specific Analysis: Offers granular insights for individual assembly lines, including performance data, maintenance logs, and predictive upkeep forecasts, enabling focused operational tactics. Actionable Alerts and Recommendations: Produces tailored recommendations for operational refinement and maintenance activities, tailored to the specific challenges of EV battery assembly, such as automation calibration and precision module assembly. Comprehensive Status Overview: Summarizes the status and efficacy of the EV battery assembly equipment, providing managers with a swift assessment tool to oversee and direct strategic operations within the facility. Enhanced Navigation and Accessibility: Boasts a user-friendly interface with robust search capabilities, facilitating the retrieval of detailed information about EV battery assembly processes, thereby enhancing managerial effectiveness. Benefits The Real-Time EV Battery Assembly Process Overview Dashboard equips managers in the electric vehicle sector to efficiently oversee and refine the EV battery assembly. It ensures that leaders are well-equipped with the necessary insights and tools to sustain exceptional product standards, optimize operational effectiveness, and comply with quality requirements. By consolidating data and analytics, the dashboard aids in orchestrating strategic assembly workflows, preemptive problem-solving, and maintaining excellence in EV battery production for each line. Figure 2. Detailed View of the EV Battery Assembly Thermal Management Monitoring Dashboard This specialized Dashboard for Thermal Management Monitoring is a crucial tool for overseeing the thermal application step in the EV battery assembly process. It provides an in-depth look at the application of thermal materials, vital for ensuring battery safety and longevity. Comprehensive Thermal Application Monitoring: XMPro’s Golden Batch Monitoring Dashboard is expertly crafted to give electric vehicle manufacturing professionals a precise view of the thermal management phase. It displays a comprehensive data set for each battery batch, including batch number, start date/time, battery tray model, modules required, reception timestamp, as well as critical quality indicators like cell type, module capacity, protein content, voltage, and cell inspection pass rate. Batch Progress and Thermal Material Application: The dashboard includes a Current Batch Assembly Step Timeline, clearly delineating the current status of the thermal management step. A color-coded Thermal Compartment Material Application Monitoring visual illustrates the compliance of material application with predefined specifications, facilitating immediate corrective actions. Real-time Quality Metrics and AI Predictions: Operators can gauge current thermal uniformity metrics against ideal values through dynamic charts, offering real-time comparisons. The AI Analytics section provides a predictive quality score, in this case, 72% for good quality, alongside intelligent suggestions for process adjustments. Historical Data and Predictive Trends: Historical deviation charts allow for monitoring consistency in material application over time. Predictive trend lines for application thickness, thermal uniformity, and material conductivity offer insight into possible deviations, each accompanied by a confidence level to inform decision-making. Actionable Recommendations: XMPro offers concrete recommendations for optimization. It may suggest realigning the thermal compartment application pattern when deviations are detected, or checking cell integrity in specific modules if thermal uniformity breaches thresholds, promoting preventative maintenance. Operator Information: The dashboard displays the operator or supervisor’s name, promoting responsibility and traceability within the thermal management phase. In-Progress Batch Status: A status indicator at the top of the dashboard shows the batch’s real-time progress within the thermal management step, providing a straightforward, overall status update. This Detailed View of the EV Battery Assembly Thermal Management Monitoring Dashboard is essential for maintaining the precision and efficiency of thermal management in EV battery production, allowing for the proactive and informed management of this critical assembly process. Why XMPro iBOS for EV Battery Assembly Plant Operations? XMPro’s Intelligent Business Operations Suite (iBOS) provides a set of tailored solutions for the intricate demands of managing EV battery assembly operations across various production lines. Here’s how XMPro iBOS transforms EV battery assembly plant management: Advanced Intelligent Digital Twin Modeling: XMPro iBOS creates detailed models of EV battery assembly operations, producing a digital representation that mirrors the complex processes of production lines. This feature allows for in-depth analysis and simulation of equipment performance, such as assembly robots, testing stations, and thermal management systems, under different scenarios. It is crucial for refining processes in assembly plants with varying environmental and production conditions. Advanced Sensor Data Integration & Transformation: Incorporating live data from sensors on all assembly line equipment, XMPro iBOS tracks essential metrics such as voltage, current, and temperature. This comprehensive monitoring detects and analyses opportunities for performance enhancement throughout the battery assembly sequence, assuring consistent quality and efficiency. Predictive Analytics for Performance Enhancement: With state-of-the-art predictive analytics, XMPro iBOS predicts potential issues and fine-tunes operational parameters for each segment of the assembly line. This proactive strategy enables adjustments in critical assembly stages, boosting product quality and efficiency while reducing waste and stoppages. Maintenance Scheduling Optimization: XMPro iBOS evaluates performance data to refine maintenance schedules, shifting from a reactive to a predictive maintenance approach. This strategy is vital for synchronizing maintenance tasks across different production lines, improving equipment lifespan and minimizing interruptions in operation. Real-Time Monitoring and Predictive Alerting: XMPro iBOS generates automatic recommendations and alerts for assembly line adjustments based on ongoing and forecasted data analysis. This feature ensures that each component, from welding stations to inspection cameras, functions at peak performance, greatly diminishing the necessity for manual checks. Configurable and Interactive Dashboards: XMPro iBOS offers adaptable dashboards that give immediate insights into the condition and performance of assembly line equipment. These user interfaces are crafted to be interactive, permitting detailed examination of specific operational elements and aiding centralized decision-making. Scalability and Flexibility – Start Small, Scale Fast: With a modular design, XMPro iBOS ensures easy integration and scalability. This adaptability guarantees that EV battery assembly operations can effectively manage activities as they grow or adjust to evolving market needs. Enhanced Safety & Operational Efficiency: XMPro iBOS improves operational safety by pinpointing potential risks and inefficiencies in the assembly sequence, ensuring that machinery works within secure and optimal limits. This leads to a safer workplace and more efficient assembly processes. XMPro Blueprints – Quick Time to Value: XMPro Blueprints enable fast implementation of battery operations solutions, with templates based on industry best practices for rapid benefits realization. These blueprints ensure swift adoption of digital advancements across EV battery assembly operations. XMPro iBOS is specifically tailored to meet the challenges of EV battery assembly plant operations, offering a comprehensive, predictive, and integrated management solution. Its sophisticated operations modeling, coupled with extensive data analytics and personalized dashboards, allows EV battery assembly plants to achieve exceptional operational efficiency, product quality, and safety across all production lines. Not Sure How To Get Started? No matter where you are on your digital transformation journey, the expert team at XMPro can help guide you every step of the way - We have helped clients successfully implement and deploy projects with Over 10x ROI in only a matter of weeks! Request a free online consultation for your business problem. \"*\" indicates required fields Name*Email*Company / Organization*Job Role*Contact Department*SalesSupportMessage*CaptchaPhoneThis field is for validation purposes and should be left unchanged. Δdocument.getElementById( \"ak_js_3\" ).setAttribute( \"value\", ( new Date() ).getTime() ); /* <![CDATA[ */ gform.initializeOnLoaded( function() {gformInitSpinner( 3, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery('#gform_ajax_frame_3').on('load',function(){var contents = jQuery(this).contents().find('*').html();var is_postback = contents.indexOf('GF_AJAX_POSTBACK') >= 0;if(!is_postback){return;}var form_content = jQuery(this).contents().find('#gform_wrapper_3');var is_confirmation = jQuery(this).contents().find('#gform_confirmation_wrapper_3').length > 0;var is_redirect = contents.indexOf('gformRedirect(){') >= 0;var is_form = form_content.length > 0 && ! is_redirect && ! is_confirmation;var mt = parseInt(jQuery('html').css('margin-top'), 10) + parseInt(jQuery('body').css('margin-top'), 10) + 100;if(is_form){jQuery('#gform_wrapper_3').html(form_content.html());if(form_content.hasClass('gform_validation_error')){jQuery('#gform_wrapper_3').addClass('gform_validation_error');} else {jQuery('#gform_wrapper_3').removeClass('gform_validation_error');}setTimeout( function() { /* delay the scroll by 50 milliseconds to fix a bug in chrome */ }, 50 );if(window['gformInitDatepicker']) {gformInitDatepicker();}if(window['gformInitPriceFields']) {gformInitPriceFields();}var current_page = jQuery('#gform_source_page_number_3').val();gformInitSpinner( 3, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery(document).trigger('gform_page_loaded', [3, current_page]);window['gf_submitting_3'] = false;}else if(!is_redirect){var confirmation_content = jQuery(this).contents().find('.GF_AJAX_POSTBACK').html();if(!confirmation_content){confirmation_content = contents;}setTimeout(function(){jQuery('#gform_wrapper_3').replaceWith(confirmation_content);jQuery(document).trigger('gform_confirmation_loaded', [3]);window['gf_submitting_3'] = false;wp.a11y.speak(jQuery('#gform_confirmation_message_3').text());}, 50);}else{jQuery('#gform_3').append(contents);if(window['gformRedirect']) {gformRedirect();}}jQuery(document).trigger(\"gform_pre_post_render\", [{ formId: \"3\", currentPage: \"current_page\", abort: function() { this.preventDefault(); } }]); if (event.defaultPrevented) { return; } const gformWrapperDiv = document.getElementById( \"gform_wrapper_3\" ); if ( gformWrapperDiv ) { const visibilitySpan = document.createElement( \"span\" ); visibilitySpan.id = \"gform_visibility_test_3\"; gformWrapperDiv.insertAdjacentElement( \"afterend\", visibilitySpan ); } const visibilityTestDiv = document.getElementById( \"gform_visibility_test_3\" ); let postRenderFired = false; function triggerPostRender() { if ( postRenderFired ) { return; } postRenderFired = true; jQuery( document ).trigger( 'gform_post_render', [3, current_page] ); gform.utils.trigger( { event: 'gform/postRender', native: false, data: { formId: 3, currentPage: current_page } } ); if ( visibilityTestDiv ) { visibilityTestDiv.parentNode.removeChild( visibilityTestDiv ); } } function debounce( func, wait, immediate ) { var timeout; return function() { var context = this, args = arguments; var later = function() { timeout = null; if ( !immediate ) func.apply( context, args ); }; var callNow = immediate && !timeout; clearTimeout( timeout ); timeout = setTimeout( later, wait ); if ( callNow ) func.apply( context, args ); }; } const debouncedTriggerPostRender = debounce( function() { triggerPostRender(); }, 200 ); if ( visibilityTestDiv && visibilityTestDiv.offsetParent === null ) { const observer = new MutationObserver( ( mutations ) => { mutations.forEach( ( mutation ) => { if ( mutation.type === 'attributes' && visibilityTestDiv.offsetParent !== null ) { debouncedTriggerPostRender(); observer.disconnect(); } }); }); observer.observe( document.body, { attributes: true, childList: false, subtree: true, attributeFilter: [ 'style', 'class' ], }); } else { triggerPostRender(); } } );} ); /* ]]&gt; */"
  },
  "docs/resources/faqs/external-content/use-cases/flood-prediction--re.html": {
    "href": "docs/resources/faqs/external-content/use-cases/flood-prediction--re.html",
    "title": "Flood Prediction & Response in Water Utilities | XMPro",
    "summary": "Flood Prediction & Response in Water Utilities url XMPro Solution for Flood Prediction and Response in Water Utilities Introduction Effective flood management is crucial for water utilities, especially in the face of climate change and increasing urbanization. XMPro's solution focuses on leveraging advanced data analytics and predictive models to forecast flood events and enable swift, coordinated responses. The Challenge Water utilities face several XMPro Solution for Flood Prediction and Response in Water Utilities Introduction Effective flood management is crucial for water utilities, especially in the face of climate change and increasing urbanization. XMPro’s solution focuses on leveraging advanced data analytics and predictive models to forecast flood events and enable swift, coordinated responses. The Challenge Water utilities face several challenges in flood management: Accurate Flood Prediction: Utilizing diverse data sources to accurately predict flood events and their potential impact. Effective Response Planning: Coordinating response efforts to mitigate flood impacts on water infrastructure and service delivery. Public Safety and Communication: Ensuring public safety and maintaining clear communication with stakeholders during flood events. The Solution: XMPro’s Flood Prediction and Response XMPro’s solution employs advanced data integration, predictive analytics, and emergency response planning to manage flood risks effectively. Key Features Data Integration and Analysis: Integrating meteorological, hydrological, and geographical data to monitor flood risk factors. XMPro’s Data Stream Designer aggregates this data, providing a comprehensive view of potential flood scenarios. Predictive Flood Modeling: Utilizing machine learning algorithms to analyze data and predict flood events, including timing, location, and severity. Predictive insights assist in proactive flood response planning and resource allocation. Real-Time Monitoring and Alerting: Providing real-time monitoring of weather conditions and water levels, with an alert system that notifies utility operators and emergency responders of impending flood risks. Emergency Response Coordination: Facilitating coordinated response efforts, including mobilizing emergency crews, activating flood barriers, and managing water storage and diversion. Public Safety and Communication Tools: Offering tools for timely public communication, including automated alerts and updates to residents and businesses in affected areas. Customizable Dashboards and Reporting: Customizable dashboards display key flood risk data and response plans, alongside comprehensive reporting features for post-event analysis and regulatory compliance. Discover This Solution In Our Product Tour Figure 1. Real-Time Flood Prediction and Response Dashboard for Water Utilities Real-Time Flood Prediction and Response Dashboard This advanced dashboard provides water utility operators with a comprehensive, real-time view of their infrastructure, focusing on flood prediction and response. It features an interactive map that dynamically updates with live and predicted precipitation data, flood risk assessments from aggregated sources including meteorological data, Hydrological data, and Geographical Information System data. In addition to these data sources, the real time condition and the condition of various assets such as treatment plants, pump stations, reservoirs, and pipe networks may also indicate flooding. Color-Coded Asset Risk Indicators: Each water utility asset on the map is represented by colored circles indicating flood risk or operational problems: green for no problem, yellow for medium risk, and red for high risk requiring immediate action. Active Recommendations and Protocols: The dashboard highlights active recommendations generated by smart rule logic and AI, such as initiating flood prevention protocols, adjusting pump operation parameters, implementing staff safety protocols, intensifying water quality testing, and initiating controlled water releases from specific reservoirs. Asset-Specific Flood Risk and Health Metrics: Users can view flood risk factors and health metrics for each asset. For example, Treatment Plant TP007 might show no current flood risk, with all metrics within the healthy range and located outside the predicted flood zone. Weather Forecast and Warnings: The dashboard displays a 7-day weather forecast and live weather warnings, providing essential information for proactive planning and response. Maintenance and Operational Efficiency: A detailed graph tracks maintenance requirements across assets, prioritizing them based on upcoming service needs and flood risk assessments, ensuring efficient maintenance scheduling. Drill-Down Capability for Detailed Analysis: Each section of the dashboard allows for deeper exploration. Users can drill down into specific asset details, recommendation insights, and flood risk assessments, enabling targeted actions based on the system’s predictive analytics and recommendations. This Real-Time Flood Prediction and Response Dashboard is designed to provide water utility operators with critical insights for effective flood management. It combines live weather data, predictive flood modeling, and asset health monitoring to ensure informed decision-making, optimal operational efficiency, and enhanced public safety in managing water utility infrastructure. Why XMPro iDTS? XMPro’s Intelligent Digital Twin Suite (iDTS) offers several unique capabilities that can effectively address the challenges of flood prediction and response in water utilities. Here’s how XMPro iDTS can be particularly beneficial for this use case: Digital Twin for Water Utility Infrastructure: XMPro iDTS can create digital twins of the entire water utility infrastructure, including treatment plants, pump stations, reservoirs, and pipe networks. These digital twins provide a virtual representation of the physical assets, enabling real-time monitoring and scenario analysis for flood impacts. Integration with Environmental Data Sources: The suite can integrate diverse environmental data sources, including meteorological, hydrological, and geographical data, to provide a comprehensive view of potential flood scenarios. This integration is crucial for accurate flood prediction and planning. Predictive Analytics for Flood Forecasting: Utilizing advanced machine learning algorithms, XMPro iDTS can analyze historical and real-time data to predict flood events. This predictive capability allows utilities to proactively prepare for and respond to flood risks. Automated Response Protocols: The suite can automate response protocols based on predictive insights, such as initiating flood prevention measures, adjusting operational parameters, and implementing safety protocols. Real-Time Monitoring and Alerting: XMPro iDTS provides real-time monitoring of environmental conditions and water utility assets. It can generate instant alerts for impending flood risks, enabling quick decision-making and response coordination. Customizable Dashboards for Enhanced Decision-Making: XMPro iDTS includes customizable dashboards that display key data on flood risks and asset conditions. These dashboards can be tailored to the specific needs of water utility operators, providing actionable insights for flood management. Scalability and Flexibility – Start Small, Scale Fast: XMPro iDTS is scalable and flexible, capable of adapting to projects of all sizes, from single asset solutions, to comprehensive Common Operating Pictures of multiple asset classes. Enhanced Safety and Operational Efficiency: XMPro iDTS can include tools for public communication, ensuring timely alerts and updates to residents and businesses in affected areas, enhancing public safety and trust. Quick Time To Value – XMPro Blueprints Utilize XMPro blueprints, pre-configured for flood prediction monitoring to quickly set up the digital twin dashboard. These blueprints integrate industry best practices, ensuring a swift and effective implementation. In summary, XMPro iDTS addresses the flood prediction and response use case by providing a comprehensive, real-time, and predictive solution. Its capabilities in creating digital twins, integrating diverse data sources, predictive analytics, and customizable dashboards make it a powerful tool for enhancing flood management and resilience in water utilities. Not Sure How To Get Started? No matter where you are on your digital transformation journey, the expert team at XMPro can help guide you every step of the way - We have helped clients successfully implement and deploy projects with Over 10x ROI in only a matter of weeks! Request a free online consultation for your business problem. \"*\" indicates required fields Name*Email*Company / Organization*Job Role*Contact Department*SalesSupportMessage*CaptchaCommentsThis field is for validation purposes and should be left unchanged. Δdocument.getElementById( \"ak_js_3\" ).setAttribute( \"value\", ( new Date() ).getTime() ); /* <![CDATA[ */ gform.initializeOnLoaded( function() {gformInitSpinner( 3, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery('#gform_ajax_frame_3').on('load',function(){var contents = jQuery(this).contents().find('*').html();var is_postback = contents.indexOf('GF_AJAX_POSTBACK') >= 0;if(!is_postback){return;}var form_content = jQuery(this).contents().find('#gform_wrapper_3');var is_confirmation = jQuery(this).contents().find('#gform_confirmation_wrapper_3').length > 0;var is_redirect = contents.indexOf('gformRedirect(){') >= 0;var is_form = form_content.length > 0 && ! is_redirect && ! is_confirmation;var mt = parseInt(jQuery('html').css('margin-top'), 10) + parseInt(jQuery('body').css('margin-top'), 10) + 100;if(is_form){jQuery('#gform_wrapper_3').html(form_content.html());if(form_content.hasClass('gform_validation_error')){jQuery('#gform_wrapper_3').addClass('gform_validation_error');} else {jQuery('#gform_wrapper_3').removeClass('gform_validation_error');}setTimeout( function() { /* delay the scroll by 50 milliseconds to fix a bug in chrome */ }, 50 );if(window['gformInitDatepicker']) {gformInitDatepicker();}if(window['gformInitPriceFields']) {gformInitPriceFields();}var current_page = jQuery('#gform_source_page_number_3').val();gformInitSpinner( 3, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery(document).trigger('gform_page_loaded', [3, current_page]);window['gf_submitting_3'] = false;}else if(!is_redirect){var confirmation_content = jQuery(this).contents().find('.GF_AJAX_POSTBACK').html();if(!confirmation_content){confirmation_content = contents;}setTimeout(function(){jQuery('#gform_wrapper_3').replaceWith(confirmation_content);jQuery(document).trigger('gform_confirmation_loaded', [3]);window['gf_submitting_3'] = false;wp.a11y.speak(jQuery('#gform_confirmation_message_3').text());}, 50);}else{jQuery('#gform_3').append(contents);if(window['gformRedirect']) {gformRedirect();}}jQuery(document).trigger(\"gform_pre_post_render\", [{ formId: \"3\", currentPage: \"current_page\", abort: function() { this.preventDefault(); } }]); if (event.defaultPrevented) { return; } const gformWrapperDiv = document.getElementById( \"gform_wrapper_3\" ); if ( gformWrapperDiv ) { const visibilitySpan = document.createElement( \"span\" ); visibilitySpan.id = \"gform_visibility_test_3\"; gformWrapperDiv.insertAdjacentElement( \"afterend\", visibilitySpan ); } const visibilityTestDiv = document.getElementById( \"gform_visibility_test_3\" ); let postRenderFired = false; function triggerPostRender() { if ( postRenderFired ) { return; } postRenderFired = true; jQuery( document ).trigger( 'gform_post_render', [3, current_page] ); gform.utils.trigger( { event: 'gform/postRender', native: false, data: { formId: 3, currentPage: current_page } } ); if ( visibilityTestDiv ) { visibilityTestDiv.parentNode.removeChild( visibilityTestDiv ); } } function debounce( func, wait, immediate ) { var timeout; return function() { var context = this, args = arguments; var later = function() { timeout = null; if ( !immediate ) func.apply( context, args ); }; var callNow = immediate && !timeout; clearTimeout( timeout ); timeout = setTimeout( later, wait ); if ( callNow ) func.apply( context, args ); }; } const debouncedTriggerPostRender = debounce( function() { triggerPostRender(); }, 200 ); if ( visibilityTestDiv && visibilityTestDiv.offsetParent === null ) { const observer = new MutationObserver( ( mutations ) => { mutations.forEach( ( mutation ) => { if ( mutation.type === 'attributes' && visibilityTestDiv.offsetParent !== null ) { debouncedTriggerPostRender(); observer.disconnect(); } }); }); observer.observe( document.body, { attributes: true, childList: false, subtree: true, attributeFilter: [ 'style', 'class' ], }); } else { triggerPostRender(); } } );} ); /* ]]&gt; */"
  },
  "docs/resources/faqs/external-content/use-cases/golden-batch-for-cul.html": {
    "href": "docs/resources/faqs/external-content/use-cases/golden-batch-for-cul.html",
    "title": "Golden Batch For Culture Addition In The Dairy processing Industry. | XMPro",
    "summary": "Golden Batch For Culture Addition In The Dairy processing Industry. url Golden Batch for Culture Addition in the Dairy Processing Industry Introduction In the dairy processing industry, the addition of cultures (beneficial bacteria) is a critical step in the production of yogurt, cheese, and other fermented dairy products. Achieving the \"Golden Batch\" in culture addition means consistently reaching the optimal mix of cultures, temperature, and incubation Golden Batch for Culture Addition in the Dairy Processing Industry Introduction In the dairy processing industry, the addition of cultures (beneficial bacteria) is a critical step in the production of yogurt, cheese, and other fermented dairy products. Achieving the “Golden Batch” in culture addition means consistently reaching the optimal mix of cultures, temperature, and incubation times to produce dairy products of superior quality, taste, and texture. This use case explores how XMPro’s Intelligent Digital Twin Suite (iDTS) can optimize the culture addition process, ensuring each batch meets these high standards. The Challenge Achieving the Golden Batch in culture addition within the dairy processing industry entails mastering a series of intricate challenges: ensuring the viability and precise measurement of cultures, maintaining optimal fermentation conditions, and consistently producing high-quality fermented dairy products. These challenges are compounded by the need to adhere to stringent safety and quality standards while optimizing production efficiency and minimizing waste. Addressing these issues effectively is crucial for dairy processors aiming to maintain product excellence, operational efficiency, and compliance with regulatory standards, highlighting the importance of adopting advanced solutions like the Golden Batch for Culture Addition. Consistency in Culture Viability: Ensuring the added cultures are alive and active to initiate proper fermentation. Optimal Fermentation Conditions: Maintaining precise temperature and pH levels throughout the fermentation process. Product Quality and Safety: Achieving consistent product quality while adhering to safety and regulatory standards. Efficiency in Production: Maximizing yield and minimizing waste during the culture addition and fermentation processes. The Solution: XMPro’s Intelligent Golden Batch Solution for Culture Addition in the Dairy Industry. XMPro’s Intelligent Digital Twin Suite (iDTS) is expertly crafted to tackle the specific challenges of culture addition in the dairy processing industry. By delivering a targeted and integrated solution, it revolutionizes the process of managing culture viability, precision in addition, and fermentation conditions, ensuring the highest standards of product consistency and quality. Leveraging the advanced functionalities of XMPro iDTS, this strategy transforms culture addition into a controlled, replicable process, ensuring each batch meets the golden standard for fermented dairy products, while also upholding efficiency, quality assurance, and compliance with safety standards. Key Features Advanced Sensor Data Integration & Transformation: XMPro’s iDTS platform is engineered to integrate sensor-derived data in real-time, capturing essential parameters such as temperature, quality measures, and flow rates at the initial milk collection phase. This integration is crucial for the prompt detection of raw milk quality variances and potential process bottlenecks. The system ensures continuous monitoring at milk reception, quickly identifying any discrepancies that could affect quality or cause logistical inefficiencies. Real-time Batch Monitoring and AI Quality Prediction: This solution offers comprehensive oversight of milk batches by capturing details like batch identity, volume, and supplier specifics. It continuously compares real-time metrics such as temperature, fat and protein contents, pH levels, and somatic cell counts to ideal standards. Leveraging AI, the platform forecasts milk quality, offering actionable insights such as blending options to achieve desired fat content, thereby ensuring quality benchmarks are met. Visualization of Process Timelines and Storage Capacities: The platform enables users to visually track each phase in the dairy processing sequence, from the initial milk collection to the final aging stage, with up-to-the-minute updates on storage capacity levels. This functionality ensures efficient progression through the various stages of production. Operational Performance and Energy Utilization Insights: XMPro delivers valuable insights into the plant’s operational performance, featuring key metrics such as energy efficiency, Overall Equipment Effectiveness (OEE), and Total Effective Equipment Performance (TEEP). These insights support strategic decision-making to enhance energy consumption efficiency and boost production yields. Predictive Recommendations: The XMPro system anticipates and issues suggestions to preemptively address process-related issues, including concerns about culture activity, texture adjustments, and the optimization of parameters for processes like pasteurization. Guidance is also provided for critical pre-commencement checks and throughout the aging process. Interactive and Customizable Dashboards: Equipped with an interactive dashboard interface, XMPro provides operators with real-time batch status, a retrospective line status over the past day, and analysis of the current month’s downtime causes. Process enhancement recommendations are also delivered through this interface, promoting informed decision-making and operational efficiency. Discover XMPro’s Golden Batch Solution for Culture Addition in the Dairy Processing Industry Figure1. Real-time Dairy Processing Plant Operations Dashboard Comprehensive Dashboard for Dairy Plant Operations This comprehensive operations dashboard is tailored for dairy plant managers and operators to oversee the cheese production process. It provides a complete view of the workflow from milk reception to cheese aging, displaying the current status of different batches and processing stages like standardization, pasteurization, and culture addition. Batch Progress and Line Status: The dashboard displays the progress of various cheese batches through their production stages, with detailed information on each batch’s status. Additionally, the ‘Last 24 Hours – Line Status’ section gives a color-coded timeline showing periods of running, maintenance, cleaning, quality control, and downtime. Operational Efficiency Metrics: Key performance indicators such as Overall Equipment Effectiveness (OEE) and energy efficiency are presented, alongside real-time updates on the fill rate of cheese production. The dashboard illustrates the plant’s operational efficiency, providing insights for potential improvements. Predictive Recommendations: XMPro’s system issues targeted recommendations to improve the cheese-making process, including alerts on culture viability concerns and adjustments for firmness and pasteurization parameters. It also suggests checks for pH balance in the fermentation tank and final aging checks, ensuring quality and consistency. Granular Insights and Predictive Maintenance: Detailed insights are available for each batch, with predictive analytics on equipment health and process efficiency. This includes monitoring Total Effective Equipment Performance (TEEP) and providing a breakdown of the current month’s downtime causes, helping to strategically plan maintenance and reduce potential interruptions. Summary and Trend Analysis: The dashboard offers a summary view of recent batch performance with trends in first-pass yield and a comprehensive look at operational data over time. It allows for quick assessments and informed decision-making to enhance production quality and efficiency. User-Centric Interface: With its user-friendly interface, the dashboard ensures that all information—from batch progress to efficiency metrics—is easily accessible, enabling efficient navigation and rapid retrieval of detailed data for strategic management. This Dairy Processing Plant Operations Dashboard is an essential tool for monitoring and optimizing the cheese production line, enabling a proactive approach to quality control, maintenance, and operational excellence.” Figure 2. In-Depth Culture Addition Monitoring Dashboard for Dairy Processing This dashboard offers a specialized view into the culture addition phase of dairy processing, an essential step in cheese and yogurt production. It provides dairy professionals with real-time data and analytics, ensuring precision and quality in the addition of cultures to milk batches. Detailed Batch Analysis: The XMPro Golden Batch Monitoring Dashboard presents comprehensive details of each culture addition batch, including the batch number, start date/time, and tank volume. It further provides the exact culture being used, in this case, Lactococcus lactis subsp. lactis, and tracks the reception timestamp along with critical parameters like temperature, fat content, protein content, pH level, and somatic cell count. Process Timeline and Current Metrics: A dynamic Batch Step Timeline is showcased, indicating the current stage and estimated time to completion for the culture addition process. Concurrently, the dashboard displays real-time metrics such as pH level and acidity, juxtaposed against benchmark values for immediate assessment of the process’s adherence to quality standards. AI-Driven Quality Assessment: The AI Analytics module offers a predictive quality rating, which in this example is 80% for good quality, accompanied by intelligent suggestions to optimize the process, such as increasing temperature or culture addition. Adjustment Levers and Predictive Recommendations: Operators are equipped with adjustment levers for fine-tuning the process parameters like temperature and culture dosage. The system also proactively generates recommendations based on real-time data; for example, it suggests incrementing culture dosage by 10 DCU to address fermentation pace or adjusting temperature to maintain optimal culture activity. Operator Engagement: The dashboard highlights the operator on duty, here identified as Jordan Menzies, linking process management directly to an individual for accountability and precision in operation handling. Progress Indicators and Status Updates: The progress bar at the top provides a quick glance at the batch status, clearly marked as ‘In Progress’, while a timeline graph beneath it offers insight into the specification adherence of the ongoing process. The Culture Addition Monitoring Dashboard serves as a critical tool for dairy processors, enabling meticulous management of the culture addition stage to ensure product consistency, quality, and the successful production of dairy products. Why XMPro iDTS for Dairy Processing Plant Operations? XMPro’s Intelligent Digital Twin Suite (iDTS) delivers a suite of solutions specifically designed for the complex needs of managing dairy processing operations across multiple facilities. Here’s how XMPro iDTS revolutionizes dairy processing plant management: Advanced Intelligent Digital Twin Modeling: XMPro iDTS constructs detailed digital twins of dairy processing plants, offering a virtual model that reflects the intricate operations of facilities spread across different regions. This capability facilitates in-depth analysis and simulation of processing equipment performance, including pasteurizers, separators, and fermentation tanks, under various operational conditions. It’s instrumental for optimizing processes in plants with diverse environmental and production demands. Advanced Sensor Data Integration & Transformation: Integrating real-time data from sensors across all dairy processing equipment, XMPro iDTS captures critical metrics such as temperature, flow rates, and pressure levels. This extensive monitoring enables the identification and analysis of performance optimization opportunities throughout the dairy processing chain, ensuring uniform quality and efficiency. Predictive Analytics for Performance Enhancement: Leveraging cutting-edge predictive analytics, XMPro iDTS anticipates potential issues and optimizes operational settings for each piece of equipment. This foresight allows for proactive adjustments in processes like pasteurization and fermentation, maximizing product quality and throughput while minimizing waste and downtime. Maintenance Scheduling Optimization: By analyzing equipment performance data, XMPro iDTS streamlines maintenance schedules, adopting a predictive maintenance model over a reactive one. This approach is crucial for coordinating maintenance activities across multiple facilities, enhancing equipment longevity and reducing operational interruptions. Real-Time Monitoring and Predictive Alerting: Automated recommendations and alerts for equipment adjustments are generated based on real-time and predictive data analyses. This ensures that each piece of equipment, from homogenizers to cooling tanks, operates at optimal efficiency, significantly reducing the need for manual oversight. Customizable and Interactive Dashboards: XMPro iDTS provides customizable dashboards that offer real-time insights into the health and performance of equipment across all dairy processing plants. These dashboards are designed to be interactive, enabling detailed scrutiny of specific operational aspects and supporting centralized management decisions. Scalability and Flexibility – Start Small, Scale Fast: Designed to accommodate dairy operations of any scale, XMPro iDTS’s modular architecture allows for seamless integration and adaptability. This scalability ensures that dairy processing plants can efficiently manage operations as they expand or adapt to changing market demands. Enhanced Safety & Operational Efficiency: XMPro iDTS boosts operational safety by identifying potential hazards and inefficiencies in the processing line, ensuring that all equipment operates within safe and optimal parameters. This contributes to a safer working environment and more efficient production processes. XMPro Blueprints – Quick Time to Value: Offering quick time-to-value, XMPro Blueprints facilitate rapid deployment of digital twin solutions across dairy processing operations. These templates are built on industry best practices, ensuring that plants can quickly realize the benefits of digital transformation. XMPro iDTS uniquely addresses the challenges of dairy processing plant operations by offering a holistic, predictive, and integrated management solution. Its advanced digital twin technology, combined with comprehensive data analytics and customizable dashboards, empowers dairy processors to achieve unparalleled operational efficiency, product quality, and safety across multiple facilities. Not Sure How To Get Started? No matter where you are on your digital transformation journey, the expert team at XMPro can help guide you every step of the way - We have helped clients successfully implement and deploy projects with Over 10x ROI in only a matter of weeks! Request a free online consultation for your business problem. \"*\" indicates required fields Name*Email*Company / Organization*Job Role*Contact Department*SalesSupportMessage*CaptchaEmailThis field is for validation purposes and should be left unchanged. Δdocument.getElementById( \"ak_js_3\" ).setAttribute( \"value\", ( new Date() ).getTime() ); /* <![CDATA[ */ gform.initializeOnLoaded( function() {gformInitSpinner( 3, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery('#gform_ajax_frame_3').on('load',function(){var contents = jQuery(this).contents().find('*').html();var is_postback = contents.indexOf('GF_AJAX_POSTBACK') >= 0;if(!is_postback){return;}var form_content = jQuery(this).contents().find('#gform_wrapper_3');var is_confirmation = jQuery(this).contents().find('#gform_confirmation_wrapper_3').length > 0;var is_redirect = contents.indexOf('gformRedirect(){') >= 0;var is_form = form_content.length > 0 && ! is_redirect && ! is_confirmation;var mt = parseInt(jQuery('html').css('margin-top'), 10) + parseInt(jQuery('body').css('margin-top'), 10) + 100;if(is_form){jQuery('#gform_wrapper_3').html(form_content.html());if(form_content.hasClass('gform_validation_error')){jQuery('#gform_wrapper_3').addClass('gform_validation_error');} else {jQuery('#gform_wrapper_3').removeClass('gform_validation_error');}setTimeout( function() { /* delay the scroll by 50 milliseconds to fix a bug in chrome */ }, 50 );if(window['gformInitDatepicker']) {gformInitDatepicker();}if(window['gformInitPriceFields']) {gformInitPriceFields();}var current_page = jQuery('#gform_source_page_number_3').val();gformInitSpinner( 3, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery(document).trigger('gform_page_loaded', [3, current_page]);window['gf_submitting_3'] = false;}else if(!is_redirect){var confirmation_content = jQuery(this).contents().find('.GF_AJAX_POSTBACK').html();if(!confirmation_content){confirmation_content = contents;}setTimeout(function(){jQuery('#gform_wrapper_3').replaceWith(confirmation_content);jQuery(document).trigger('gform_confirmation_loaded', [3]);window['gf_submitting_3'] = false;wp.a11y.speak(jQuery('#gform_confirmation_message_3').text());}, 50);}else{jQuery('#gform_3').append(contents);if(window['gformRedirect']) {gformRedirect();}}jQuery(document).trigger(\"gform_pre_post_render\", [{ formId: \"3\", currentPage: \"current_page\", abort: function() { this.preventDefault(); } }]); if (event.defaultPrevented) { return; } const gformWrapperDiv = document.getElementById( \"gform_wrapper_3\" ); if ( gformWrapperDiv ) { const visibilitySpan = document.createElement( \"span\" ); visibilitySpan.id = \"gform_visibility_test_3\"; gformWrapperDiv.insertAdjacentElement( \"afterend\", visibilitySpan ); } const visibilityTestDiv = document.getElementById( \"gform_visibility_test_3\" ); let postRenderFired = false; function triggerPostRender() { if ( postRenderFired ) { return; } postRenderFired = true; jQuery( document ).trigger( 'gform_post_render', [3, current_page] ); gform.utils.trigger( { event: 'gform/postRender', native: false, data: { formId: 3, currentPage: current_page } } ); if ( visibilityTestDiv ) { visibilityTestDiv.parentNode.removeChild( visibilityTestDiv ); } } function debounce( func, wait, immediate ) { var timeout; return function() { var context = this, args = arguments; var later = function() { timeout = null; if ( !immediate ) func.apply( context, args ); }; var callNow = immediate && !timeout; clearTimeout( timeout ); timeout = setTimeout( later, wait ); if ( callNow ) func.apply( context, args ); }; } const debouncedTriggerPostRender = debounce( function() { triggerPostRender(); }, 200 ); if ( visibilityTestDiv && visibilityTestDiv.offsetParent === null ) { const observer = new MutationObserver( ( mutations ) => { mutations.forEach( ( mutation ) => { if ( mutation.type === 'attributes' && visibilityTestDiv.offsetParent !== null ) { debouncedTriggerPostRender(); observer.disconnect(); } }); }); observer.observe( document.body, { attributes: true, childList: false, subtree: true, attributeFilter: [ 'style', 'class' ], }); } else { triggerPostRender(); } } );} ); /* ]]&gt; */"
  },
  "docs/resources/faqs/external-content/use-cases/golden-batch-monitor.html": {
    "href": "docs/resources/faqs/external-content/use-cases/golden-batch-monitor.html",
    "title": "Golden Batch Monitoring | XMPro",
    "summary": "Golden Batch Monitoring url XMPro Golden Batch Production Quality Solutions in the Chemical Industry The Problem Chemical industry plants face significant challenges in maintaining consistent product quality, particularly due to variations in batch cycle time, raw material usage, and product quality: Batch Variability: Significant fluctuations in product batches impacting quality and efficiency. Inefficient Monitoring: Traditional monitoring methods are inadequate XMPro Golden Batch Production Quality Solutions in the Chemical Industry The Problem Chemical industry plants face significant challenges in maintaining consistent product quality, particularly due to variations in batch cycle time, raw material usage, and product quality: Batch Variability: Significant fluctuations in product batches impacting quality and efficiency. Inefficient Monitoring: Traditional monitoring methods are inadequate for real-time quality control. Quality Assurance: Difficulty in ensuring each batch meets the “golden batch” standard. The Solution XMPro leverages its Intelligent Digital Twin Suite, AI capabilities, and real-time data integration to ensure, maintain, and optimize production quality through a model-based Golden Batch approach. Key Features: Real-Time Batch Monitoring: Continuously monitor batches against the “golden batch” profile. Prescriptive Recommendations: Provide guidance on raw material feed rate and process adjustments based on expertly created and maintained business rules. AI-Driven Insights: Utilize AI models for continuous improvement and goal-seeking behavior in production. Data Stream Designer: Integrate and transform data in real-time, embedding AI models within the data flow. Comprehensive Visualizations: Interactive dashboards for a holistic view of production operations, customizable for informed decision-making. Benefits: Improved Product Quality: Consistently achieve and replicate the golden batch, enhancing product quality. Reduced Operational Variability: Minimize variations in batch cycle time and raw material usage. Enhanced Decision Support: Leverage AI-driven insights for better decision-making and automation. XMPro’s 3-Step Process To Golden Batch Success Identify & Standardize Optimal Production Parameters: Use Intelligent Digital Twins for continuous analysis of production variables, standardizing ideal conditions for the Golden Batch. Optimize in Real-Time with a Hybrid Model-Based Approach: Combine traditional process engineering with dynamic AI models for real-time adjustments and preemptive actions. Quick Time to Value with Blueprints and Templates: Implement XMPro’s blueprints and templates for rapid deployment, ensuring consistent quality in every batch. Implementation Strategy Integration with Production Systems: Seamlessly integrate XMPro solutions with existing production systems for comprehensive monitoring and control. User Training and System Customization: Provide training for staff and customize the system with user-friendly interfaces and dashboards. Continuous Improvement: Monitor outcomes and continuously refine the system for optimal performance. Future Developments Expanding AI Capabilities: Enhance AI models for more sophisticated predictive and prescriptive analytics. Broader Industry Applications: Adapt the solution for broader applications across different sectors within the chemical industry. Advanced Data Analytics: Further develop data analytics capabilities for deeper insights and more effective production optimization. Why XMPro iDTS? XMPro iDTS offers unique and innovative solutions for Golden Batch Monitoring in the chemical industry, particularly in enhancing production quality and efficiency. Here’s how XMPro iDTS can be specifically applied to this challenge: Intelligent Digital Twin for Process Simulation: XMPro iDTS creates a digital twin of the chemical production process, allowing for real-time simulation and analysis. This digital twin helps in visualizing and understanding the complex interactions within production processes, aiding in the identification and replication of Golden Batch conditions. Real-Time Data Integration and Analysis: By integrating data from various sources in real-time, including equipment sensors, production systems, and external data, XMPro iDTS provides a comprehensive view of the production process. This integration is crucial for accurate monitoring and quick decision-making to maintain Golden Batch standards. AI-Driven Predictive and Prescriptive Analytics: XMPro iDTS leverages AI models to predict potential deviations from the Golden Batch and prescribes corrective actions. These AI-driven insights enable proactive adjustments to maintain batch quality and reduce variability. Model-Based Optimization: The hybrid model-based approach of XMPro iDTS combines traditional process engineering practices with AI models. This method allows for real-time optimization of production parameters, ensuring consistent quality across batches. Blueprints and Templates for Rapid Deployment: XMPro iDTS offers blueprints and templates that encapsulate best practices and proven models for Golden Batch production. These resources facilitate quick implementation and customization, accelerating the time to value. Customizable Dashboards and Reporting Tools: The solution provides interactive and customizable dashboards, offering a holistic view of production operations. These dashboards are essential for monitoring key performance indicators and making informed decisions. Continuous Improvement and Learning: XMPro iDTS’s AI capabilities include self-learning models that continuously improve over time. This feature ensures that the system becomes more effective at predicting and maintaining Golden Batch conditions as more data is collected. Scalability and Flexibility: XMPro iDTS is scalable and flexible, allowing it to adapt to different scales of operation and to integrate with various types of production equipment and systems. In summary, XMPro iDTS addresses the challenges of Golden Batch Monitoring in the chemical industry by providing a comprehensive, real-time, predictive, and integrated solution. Its use of intelligent digital twins, combined with advanced AI analytics, model-based optimization, and effective data visualization, makes it a powerful tool for achieving and maintaining optimal production quality."
  },
  "docs/resources/faqs/external-content/use-cases/improve-first-pass-y.html": {
    "href": "docs/resources/faqs/external-content/use-cases/improve-first-pass-y.html",
    "title": "Improve First Pass Yield (FPY) | XMPro",
    "summary": "Improve First Pass Yield (FPY) url XMPro Solution to Improve First Pass Yield (FPY): Enhancing Manufacturing Efficiency and Product Quality The Problem Achieving a high First Pass Yield is crucial for manufacturing efficiency and product quality. However, manufacturers often face challenges that impact FPY: Process Variabilities: Inconsistencies in manufacturing processes can lead to defects and lower yield rates. Quality Control Issues:Tags: Discrete Manufacturing / Solutions XMPro Solution to Improve First Pass Yield (FPY): Enhancing Manufacturing Efficiency and Product Quality The Problem Achieving a high First Pass Yield is crucial for manufacturing efficiency and product quality. However, manufacturers often face challenges that impact FPY: Process Variabilities: Inconsistencies in manufacturing processes can lead to defects and lower yield rates. Quality Control Issues: Inadequate quality control measures can result in a high rate of rework or scrap. Equipment Performance: Suboptimal performance of machinery and equipment can adversely affect product quality. Material Quality: Variability in raw material quality can lead to inconsistent product outputs. Data Integration and Analysis: Difficulty in integrating and analyzing data from various stages of the manufacturing process to identify root causes of yield issues. The Solution XMPro’s solution to improve First Pass Yield utilizes XMPro iDTS to offer a data-driven and proactive approach to enhancing manufacturing processes and product quality. Key Metrics Monitored: Defect Rates: Monitoring and analyzing defect rates at each stage of the manufacturing process. Machine Performance: Tracking the performance and condition of manufacturing equipment. Process Parameters: Monitoring critical process parameters like temperature, pressure, and speed. Material Quality: Assessing the quality of raw materials used in the manufacturing process. Cycle Times: Measuring the cycle times of different manufacturing stages for efficiency analysis. Why XMPro iDTS? XMPro iDTS offers specialized solutions to address the challenges associated with improving First Pass Yield (FPY) in manufacturing processes. Here’s how XMPro iDTS can be specifically applied to enhance the FPY Improvement Solution: Digital Twin for Manufacturing Processes: XMPro iDTS can create a digital twin of the manufacturing process, providing a virtual representation that mirrors the physical production line. This allows for real-time simulation, analysis, and optimization of the manufacturing process, enabling a deeper understanding of how various factors affect FPY. Predictive Analytics for Quality Control: Utilizing advanced analytics and machine learning, XMPro iDTS can analyze data from the manufacturing process to predict potential quality issues before they occur. This predictive approach allows for proactive adjustments to the process, reducing defects and increasing FPY. Real-Time Monitoring of Process Parameters: XMPro iDTS can continuously monitor critical process parameters such as temperature, pressure, speed, and machine performance. By analyzing this data in real-time, the system can identify deviations from optimal conditions and recommend adjustments to maintain product quality. Integration with Quality Management Systems: XMPro iDTS can integrate with existing quality management systems to provide a comprehensive view of quality control measures and outcomes. This integration is crucial for identifying areas for improvement and implementing effective quality control strategies. Automated Alerts and Recommendations: XMPro iDTS can automate the generation of alerts and recommendations when potential quality issues are detected. This feature ensures timely interventions, reducing the rate of rework or scrap and maintaining high FPY. Customizable Dashboards and Reporting: XMPro iDTS provides advanced data visualization tools and customizable dashboards, which are essential for monitoring and analyzing manufacturing processes. These tools enable different stakeholders to access relevant information, understand trends, and make informed decisions quickly. Scalability and Flexibility: XMPro iDTS is scalable, meaning it can be expanded to accommodate additional manufacturing lines, processes, or data sources as operational needs evolve. Material Quality Analysis: By monitoring and analyzing the quality of raw materials, XMPro iDTS helps ensure that only materials meeting the required standards are used in the manufacturing process, further contributing to higher FPY. In summary, XMPro iDTS addresses the unique challenges in improving First Pass Yield by providing a comprehensive, real-time, predictive, and integrated solution. Its use of digital twin technology, combined with advanced analytics, automated guidance, and effective data visualization, makes it a powerful tool for optimizing manufacturing processes and enhancing product quality."
  },
  "docs/resources/faqs/external-content/use-cases/index.html": {
    "href": "docs/resources/faqs/external-content/use-cases/index.html",
    "title": "Use Cases | XMPro",
    "summary": "Use Cases Real-world use cases demonstrating how XMPro is used in various industries. Use Cases Aging Pipe Predictive Maintenance in Water Utilities Air Quality Monitoring For Agriculture Alarm Management and Triage - XMPRO Asset Condition Monitoring for Surface Processing Plants in the Mining Industry Bogie Health Monitoring in the Rail Industry Boiler Feed Water Pumps - XMPRO Casting Guidance CHPP Throughput Loss Monitoring - XMPRO Conveyor Belt System Monitoring and Optimization in Automotive Manufacturing Cooling Tower Fin Fan Monitoring Scraped Content Cyclone/Slurry Pump Monitoring Demand Planning to Reduce Stockholding in Stores - XMPRO Demin Water Monitoring for Boiler Tube Corrosion - XMPRO EV Battery Assembly Process Optimization for the Car Manufacturing Industry Flood Prediction & Response in Water Utilities Golden Batch For Culture Addition In The Dairy processing Industry. Golden Batch Monitoring Improve First Pass Yield (FPY) Induced Draft (ID) Fan Monitoring - XMPRO Long Conveyor Monitoring Monitor and Reduce Energy Consumption - XMPRO Monitor Process Health to Reduce Cash-to-Cash Cycle Monitor Storm Water Reservoirs For Flood Prevention Oil Well Maintenance Planning - XMPRO Oil Well RTP Monitoring - XMPRO Pipe Scaling Prediction for Roller Cooling - XMPRO Precision Irrigation in Agriculture Predict Heat Exchanger Fouling - XMPRO Predictive Maintenance & Asset Health Monitoring For Haul Trucks In The Mining Industry Pump Health Monitoring in Water Utilities Pumping Station OEE Real-time Balanced Business Scorecard (BBS) - XMPRO Real-time Safety Monitoring - XMPRO Short Term Inventory Planning Strategic Performance & Safety Oversight for Global Mining Operations Wheel and Track Wear Monitoring In The Rail Industry Wind Turbine Performance Optimization"
  },
  "docs/resources/faqs/external-content/use-cases/induced-draft-(id)-f.html": {
    "href": "docs/resources/faqs/external-content/use-cases/induced-draft-(id)-f.html",
    "title": "Induced Draft (ID) Fan Monitoring - XMPRO | XMPro",
    "summary": "Induced Draft (ID) Fan Monitoring - XMPRO url XMPro Induced Draft (ID) Fan Monitoring Solution: Enhancing Efficiency and Reliability in Industrial Ventilation The Problem Effective monitoring and optimization of Induced Draft (ID) Fans are crucial for maintaining operational efficiency in various industrial processes. Challenges include the following: Equipment Wear and Tear: Continuous operation leads to wear and tear of ID fan components, risking XMPro Induced Draft (ID) Fan Monitoring Solution: Enhancing Efficiency and Reliability in Industrial Ventilation The Problem Effective monitoring and optimization of Induced Draft (ID) Fans are crucial for maintaining operational efficiency in various industrial processes. Challenges include the following: Equipment Wear and Tear: Continuous operation leads to wear and tear of ID fan components, risking breakdowns and inefficiencies. Energy Efficiency: ID fans can consume significant energy, making their efficient operation crucial for cost management. Predictive Maintenance: Implementing effective predictive maintenance is challenging due to the critical nature and continuous operation of these fans. Operational Downtime: Unplanned downtime due to ID fan failures can cause significant disruptions in industrial processes. Environmental Compliance: Ensuring ID fans operate within environmental regulations is critical to prevent excess emissions. #text-box-1508906576 { width: 60%; } #text-box-1508906576 .text-box-content { font-size: 100%; } #banner-1702011829 { padding-top: 521px; } #banner-1702011829 .bg.bg-loaded { background-image: url(https://xmpro.com/wp-content/uploads/2020/04/7.jpg); } The Solution XMPro’s ID Fan Monitoring Solution, utilizing XMPro iDTS, offers a comprehensive approach to monitoring and optimizing the performance and reliability of ID fans in industrial settings. Key Metrics Monitored: Vibration and Noise Levels: Monitoring for signs of mechanical issues or imbalances. Temperature and Pressure: Tracking operational temperatures and pressures for optimal performance. Energy Consumption: Monitoring energy use to identify inefficiencies. Fan Speed and Airflow: Assessing fan speed and airflow to ensure effective ventilation. Component Wear and Tear: Evaluating the condition of bearings, blades, and other critical components. Initial Goals: Implement IoT sensors for comprehensive monitoring of ID fan performance and condition. Develop predictive maintenance models to anticipate and prevent equipment failures. Optimize fan operation for maximum efficiency and minimal energy consumption. Ensure robust data security and integrity. Provide user-friendly dashboards for real-time monitoring and decision-making. Why XMPro iDTS? XMPro iDTS offers specialized solutions to address the challenges associated with monitoring and optimizing Induced Draft (ID) Fans in industrial settings. Here’s how XMPro iDTS can be specifically applied to enhance the ID Fan Monitoring Solution: Digital Twin for ID Fan Systems: XMPro iDTS can create a digital twin of the ID fan system, providing a virtual representation that mirrors the physical fan and its operation. This allows for real-time simulation and analysis, enabling operators to visualize the impact of various operational parameters on the fan’s performance and health. Predictive Maintenance for Fan Components: Utilizing advanced analytics and machine learning, XMPro iDTS can analyze data from sensors to predict maintenance needs for critical components of the ID fan. This predictive approach allows for maintenance to be scheduled based on actual equipment condition, reducing unplanned downtime and extending equipment life. Optimization of Fan Operation: XMPro iDTS can analyze operational data to optimize the efficiency of the ID fan. This includes adjusting fan speed, airflow, and monitoring temperature and pressure to maximize performance while minimizing energy consumption. Real-Time Monitoring of Key Performance Indicators: XMPro iDTS enables real-time monitoring of key performance indicators such as vibration, noise, temperature, pressure, and energy consumption. By tracking these metrics, operators can identify and address issues promptly to maintain optimal efficiency. Automated Alerts and Recommendations: XMPro iDTS can generate automated alerts and recommendations when potential issues are detected. This feature ensures timely interventions, maintaining operational efficiency and safety. Customizable Dashboards and Reporting: XMPro iDTS provides advanced data visualization tools and customizable dashboards, which are essential for monitoring complex systems like ID fans. These tools enable different stakeholders to access relevant information, understand trends, and make informed decisions quickly. Scalability and Flexibility: XMPro iDTS is scalable, meaning it can be expanded to accommodate additional sensors or ID fan systems as operational needs evolve. Environmental Compliance Monitoring: Continuous monitoring of operational parameters helps ensure that the ID fan operates within environmental compliance standards. This proactive approach to compliance can prevent excess emissions and reduce the risk of non-compliance penalties. In summary, XMPro iDTS addresses the unique challenges in ID fan monitoring by providing a comprehensive, real-time, predictive, and integrated solution. Its use of digital twin technology, combined with advanced analytics, automated guidance, and effective data visualization, makes it a powerful tool for optimizing the performance and reliability of ID fan systems in industrial environments."
  },
  "docs/resources/faqs/external-content/use-cases/long-conveyor-monito.html": {
    "href": "docs/resources/faqs/external-content/use-cases/long-conveyor-monito.html",
    "title": "Long Conveyor Monitoring | XMPro",
    "summary": "Long Conveyor Monitoring url XMPro Long Conveyor Monitoring Solution: Ensuring Reliability and Efficiency in Material Handling The Problem Long conveyor systems are crucial in various industries for efficient material handling. However, they face several challenges: Wear and Tear: Due to their extensive use, long conveyors are prone to wear and tear, leading to potential breakdowns. Energy Efficiency: Long conveyors XMPro Long Conveyor Monitoring Solution: Ensuring Reliability and Efficiency in Material Handling The Problem Long conveyor systems are crucial in various industries for efficient material handling. However, they face several challenges: Wear and Tear: Due to their extensive use, long conveyors are prone to wear and tear, leading to potential breakdowns. Energy Efficiency: Long conveyors can consume significant energy, making their efficient operation crucial for cost management. Preventive Maintenance: Implementing effective preventive maintenance is challenging due to the length and complexity of these systems. Operational Downtime: Unplanned downtime due to conveyor failures can cause significant disruptions in operations. Safety Risks: Ensuring the safety of long conveyor operations is critical to prevent accidents and equipment damage. The Solution XMPro’s Long Conveyor Monitoring Solution, utilizing XMPro iDTS, offers a comprehensive approach to monitoring and optimizing the performance and reliability of long conveyor systems. Key Metrics Monitored: Belt Health: Monitoring the condition of conveyor belts for wear, tear, and misalignment. Motor Performance: Tracking the performance of conveyor motors, including energy consumption and operating temperatures. Roller Functionality: Monitoring the condition and performance of rollers along the conveyor. Operational Efficiency: Analyzing the overall efficiency of the conveyor system, including speed and material throughput. Safety Indicators: Monitoring for any safety hazards or deviations from standard operating procedures. Goals: Implement IoT sensors for comprehensive monitoring of conveyor health and performance. Develop predictive maintenance models to anticipate and prevent equipment failures. Optimize energy consumption and operational efficiency. Ensure robust data security and integrity. Provide user-friendly dashboards for real-time monitoring and decision-making. Why XMPro iDTS? XMPro iDTS offers specialized solutions to address the challenges associated with monitoring and optimizing long conveyor systems. Here’s how XMPro iDTS can be specifically applied to enhance the Long Conveyor Monitoring Solution: Digital Twin for Conveyor System Simulation: XMPro iDTS can create a digital twin of the entire conveyor system, providing a virtual representation that mirrors the physical conveyor. This allows for real-time simulation and analysis, enabling operators to visualize the impact of various factors on the conveyor’s performance and health. Predictive Maintenance for Conveyor Components: Utilizing advanced analytics and machine learning, XMPro iDTS can analyze data from sensors to predict maintenance needs for critical components like belts, rollers, and motors. This predictive approach allows for maintenance to be scheduled based on actual equipment condition, reducing downtime and extending equipment life. Energy Consumption Optimization: XMPro iDTS can analyze the energy consumption patterns of the conveyor system to identify opportunities for reducing energy usage, leading to cost savings and environmental benefits. Real-Time Monitoring of Key Performance Indicators: XMPro iDTS enables real-time monitoring of key performance indicators such as belt health, motor performance, roller functionality, and operational efficiency. By tracking these metrics, businesses can identify and address issues promptly. Automated Alerts and Recommendations: XMPro iDTS can generate automated alerts and recommendations when potential issues are detected. This feature ensures timely interventions, maintaining operational efficiency and safety. Customizable Dashboards and Reporting: XMPro iDTS provides advanced data visualization tools and customizable dashboards, which are essential for monitoring complex systems like long conveyors. These tools enable different stakeholders to access relevant information, understand trends, and make informed decisions quickly. Scalability and Flexibility: XMPro iDTS is scalable, meaning it can be expanded to accommodate additional sensors or conveyor systems as operational needs evolve. Safety Monitoring and Compliance: Continuous monitoring of safety indicators helps ensure that the conveyor operates safely and within compliance standards. This proactive approach to safety can prevent accidents and reduce the risk of non-compliance penalties. In summary, XMPro iDTS addresses the unique challenges in long conveyor monitoring by providing a comprehensive, real-time, predictive, and integrated solution. Its use of digital twin technology, combined with advanced analytics, automated guidance, and effective data visualization, makes it a powerful tool for optimizing the performance and reliability of long conveyor systems. CASE STUDY CLIENT CHALLENGE: In order to maximize underground mining operations, the underground conveyor system, a frequent cause of unplanned downtime, needed to reduce its downtime by 30% as an initial target for a predictive maintenance solution\\ XMPRO SOLUTION XMPro actively monitors 52 conveyors (spanning over 80+km) in real time, predicting fluid coupling and lagging failures with prescriptive recommendations.\\ BENEFITS: Within five months, the solution identified a potential saving of 184 hours of borer downtime, equating to 44k product tonnes. Exceeding the target, the solution achieved over a 80% reduction in downtime for fluid coupling failures. It now monitors multiple asset types across several mines\\"
  },
  "docs/resources/faqs/external-content/use-cases/monitor-and-reduce-e.html": {
    "href": "docs/resources/faqs/external-content/use-cases/monitor-and-reduce-e.html",
    "title": "Monitor and Reduce Energy Consumption - XMPRO | XMPro",
    "summary": "Monitor and Reduce Energy Consumption - XMPRO url XMPro Solution for Monitoring and Reducing Energy Consumption in Food Manufacturing The Problem In food manufacturing, energy costs constitute a significant portion of total operational expenses, presenting opportunities for cost reduction and enhanced competitiveness: High Energy Costs: A substantial part of operational costs in food manufacturing is attributed to energy usage. Inefficient Motor Drives: Variable XMPro Solution for Monitoring and Reducing Energy Consumption in Food Manufacturing The Problem In food manufacturing, energy costs constitute a significant portion of total operational expenses, presenting opportunities for cost reduction and enhanced competitiveness: High Energy Costs: A substantial part of operational costs in food manufacturing is attributed to energy usage. Inefficient Motor Drives: Variable speed drives (VSD) and other electric motor drives can become inefficient, leading to increased energy consumption. Lack of Continuous Monitoring: Without continuous monitoring, it’s challenging to identify and address energy inefficiencies. Operational Process Inefficiencies: Certain operational processes may lead to higher energy usage without optimized management. #text-box-706998815 { width: 60%; } #text-box-706998815 .text-box-content { font-size: 100%; } #banner-1580553900 { padding-top: 398px; } #banner-1580553900 .bg.bg-loaded { background-image: url(https://xmpro.com/wp-content/uploads/2020/04/17.jpg); } The Solution XMPro’s solution focuses on continuous monitoring of energy consumption, particularly in VSDs and electric motor drives, to identify inefficiencies and guide process and maintenance actions. Key Features: Continuous Monitoring of Drives: Real-time tracking of VSDs and other electric motor drives to identify inefficient units. Predictive Maintenance Recommendations: Providing guidance on maintenance actions to improve energy efficiency. Process Optimization: Identifying operational process issues that lead to higher energy use and recommending adjustments. Customizable Dashboards: User-friendly dashboards for real-time energy tracking and decision-making. #gap-348930103 { padding-top: 30px; } Why XMPro iDTS? XMPro iDTS offers specialized capabilities that are particularly effective in solving the challenges associated with monitoring and reducing energy consumption in food manufacturing, especially in managing variable speed drives (VSD) and other electric motor drives. Here’s how XMPro iDTS can be specifically applied to enhance this solution: Real-Time Monitoring and Digital Twin Simulation: XMPro iDTS can create a digital twin of the manufacturing process, including VSDs and electric motor drives. This allows for real-time monitoring and simulation, enabling a comprehensive understanding of energy usage patterns and identification of inefficiencies. Predictive Analytics for Energy Management: Utilizing advanced analytics, XMPro iDTS can predict when drives and motors are becoming less efficient and require maintenance or adjustment. This predictive approach enables proactive measures to optimize energy consumption. Process Optimization Recommendations: XMPro iDTS can analyze operational processes to identify areas where energy usage is higher than necessary. It can then provide recommendations for process adjustments to reduce energy consumption without compromising production efficiency. Automated Alerts for Maintenance and Optimization: The system can generate automated alerts when it detects potential energy inefficiencies or impending equipment issues. This feature aids in timely maintenance and operational adjustments to maintain energy efficiency. Integration with Existing Systems: XMPro iDTS seamlessly integrates with existing operational systems and energy monitoring tools, ensuring that data from all sources is consolidated and analyzed effectively for accurate energy management. Customizable Dashboards for Energy Monitoring: XMPro iDTS provides customizable dashboards that offer insights into energy consumption patterns, equipment performance, and operational efficiency. These dashboards are tailored to the needs of different stakeholders, from facility managers to maintenance teams. Scalability and Flexibility: XMPro iDTS is scalable and flexible, allowing the energy monitoring solution to be adapted to different sizes of operations and to accommodate evolving energy management needs. Sustainability and Compliance: By optimizing energy use, XMPro iDTS contributes to the sustainability goals of the food manufacturing industry and ensures compliance with environmental standards and regulations. In summary, XMPro iDTS addresses the unique challenges in monitoring and reducing energy consumption in food manufacturing by providing a comprehensive, real-time, predictive, and integrated solution. Its capabilities in digital twin technology, predictive analytics, process optimization, and effective data visualization make it a powerful tool for enhancing energy efficiency and operational sustainability."
  },
  "docs/resources/faqs/external-content/use-cases/monitor-process-heal.html": {
    "href": "docs/resources/faqs/external-content/use-cases/monitor-process-heal.html",
    "title": "Monitor Process Health to Reduce Cash-to-Cash Cycle | XMPro",
    "summary": "Monitor Process Health to Reduce Cash-to-Cash Cycle url XMPro Solution to Monitor Process Health for Reducing Cash-to-Cash Cycle Time: Streamlining Operations for Financial Efficiency The Problem Optimizing the cash-to-cash cycle time is crucial for improving a company's liquidity and operational efficiency. However, businesses often face challenges that can elongate this cycle: Inefficient Processes: Delays in production, supply chain inefficiencies, and poor inventory managementTags: Discrete Manufacturing / Solutions XMPro Solution to Monitor Process Health for Reducing Cash-to-Cash Cycle Time: Streamlining Operations for Financial Efficiency The Problem Optimizing the cash-to-cash cycle time is crucial for improving a company’s liquidity and operational efficiency. However, businesses often face challenges that can elongate this cycle: Inefficient Processes: Delays in production, supply chain inefficiencies, and poor inventory management can extend the time between spending cash and recovering it from sales. Data Silos: Lack of integrated data across different departments (procurement, production, sales, etc.) hinders effective decision-making. Supply Chain Visibility: Limited visibility into the supply chain can lead to unexpected delays and inventory issues. Demand Forecasting: Inaccurate demand forecasting leads to excess or insufficient inventory, impacting the cash conversion cycle. Receivables and Payables Management: Inefficient processes in managing receivables and payables can further delay the cash conversion cycle. The Solution XMPro’s solution to monitor process health for reducing cash-to-cash cycle time utilizes XMPro iDTS to offer a comprehensive, data-driven approach for streamlining operations and financial processes. Key Metrics Monitored: Inventory Levels: Real-time monitoring of inventory turnover and aging. Production Efficiency: Tracking production cycle times and downtime. Supplier Performance: Monitoring supplier lead times and reliability. Order-to-Cash Cycle: Analyzing the time taken from order receipt to cash collection. Procure-to-Pay Cycle: Monitoring the efficiency of the procurement process and payment cycles. Goals: Implement data integration tools for comprehensive monitoring across various processes. Develop predictive models for accurate demand forecasting and efficient inventory management. Optimize supplier selection and management processes. Ensure robust data security and integrity. Provide user-friendly dashboards for real-time monitoring and decision-making. Why XMPro iDTS? XMPro iDTS offers specialized solutions to address the challenges associated with monitoring process health to reduce the cash-to-cash cycle time. Here’s how XMPro iDTS can be specifically applied to enhance this solution: Digital Twin for End-to-End Process Visualization: XMPro iDTS can create a digital twin of the entire business process, from procurement to production, and through to sales and receivables. This comprehensive visualization allows for real-time monitoring and analysis of each stage in the cash-to-cash cycle, identifying bottlenecks and inefficiencies. Integrated Data Analysis Across Departments: Utilizing XMPro iDTS, companies can integrate data from various departments, including procurement, inventory management, production, and finance. This integrated approach provides a holistic view of the cash-to-cash cycle, facilitating more informed decision-making. Predictive Analytics for Demand and Supply Chain Management: XMPro iDTS can employ predictive analytics to forecast demand accurately and optimize inventory levels, reducing excess stock and associated holding costs. It can also predict supply chain disruptions, allowing for proactive adjustments to maintain smooth operations. Real-Time Monitoring of Key Financial Metrics: XMPro iDTS enables real-time monitoring of critical financial metrics such as inventory turnover, order-to-cash, and procure-to-pay cycles. By tracking these metrics, businesses can identify opportunities to shorten the cash-to-cash cycle. Automated Alerts and Recommendations: XMPro iDTS can generate automated alerts and recommendations for process improvements. For instance, it can suggest adjustments in procurement or inventory management based on current market trends and internal demand forecasts. Customizable Dashboards for Different Stakeholders: XMPro iDTS provides customizable dashboards tailored to the needs of different stakeholders, from operational managers to financial analysts. These dashboards offer insights into various aspects of the cash-to-cash cycle, supporting quick and effective decision-making. Scalability and Flexibility: XMPro iDTS is scalable, meaning it can adapt to growing business needs and can integrate additional data sources or processes as the company expands. Optimization of Receivables and Payables: By analyzing transaction patterns and historical data, XMPro iDTS helps optimize the management of receivables and payables, contributing to a more efficient cash-to-cash cycle. In summary, XMPro iDTS addresses the unique challenges in reducing the cash-to-cash cycle time by providing a comprehensive, real-time, predictive, and integrated solution. Its use of digital twin technology, combined with advanced analytics, automated guidance, and effective data visualization, makes it a powerful tool for streamlining operations and enhancing financial efficiency."
  },
  "docs/resources/faqs/external-content/use-cases/monitor-storm-water-.html": {
    "href": "docs/resources/faqs/external-content/use-cases/monitor-storm-water-.html",
    "title": "Monitor Storm Water Reservoirs For Flood Prevention | XMPro",
    "summary": "Monitor Storm Water Reservoirs For Flood Prevention url XMPro Solution for Monitoring Storm Water Reservoirs for Flood Prevention The Problem Metropolitan cities often face flooding challenges during prolonged rain spells, exacerbated by inefficient management of stormwater reservoirs: Inadequate Reservoir Management: Reservoirs frequently near full capacity at the onset of rain due to fixed-route maintenance schedules. Lack of Predictive Management: Services teams lack dynamic XMPro Solution for Monitoring Storm Water Reservoirs for Flood Prevention The Problem Metropolitan cities often face flooding challenges during prolonged rain spells, exacerbated by inefficient management of stormwater reservoirs: Inadequate Reservoir Management: Reservoirs frequently near full capacity at the onset of rain due to fixed-route maintenance schedules. Lack of Predictive Management: Services teams lack dynamic scheduling based on reservoir levels and weather forecasts. Water Quality Concerns: Potential for industrial runoff into stormwater systems, posing environmental and safety risks. Discover This Solution In Our Product Tour The Solution XMPro’s solution leverages continuous monitoring of stormwater reservoirs, integrating level data with weather predictions to optimize service crew deployment and monitor water quality for safety. Key Features: Continuous Reservoir Monitoring: Real-time monitoring of stormwater reservoir levels to anticipate flooding risks. Weather Prediction Integration: Combining reservoir level data with weather forecasts for dynamic scheduling of maintenance activities. Water Quality Monitoring: Assessing water quality to detect industrial runoff and other contaminants. Dynamic Truck Rolls: Implementing flexible service crew dispatch based on reservoir levels and weather predictions. Automated Alerts and Recommendations: Generating alerts for potential flooding and hazardous conditions, guiding service crew actions. Benefits: Reduced Flooding Incidents: Proactive reservoir management to minimize residential flooding. Enhanced Safety: Timely warnings to service crews about hazardous conditions. Improved Environmental Protection: Monitoring for industrial runoff to safeguard water quality. Why XMPro iDTS? XMPro iDTS offers unique and innovative solutions for monitoring stormwater reservoirs for flood prevention, especially in metropolitan cities. Here’s how XMPro iDTS can be specifically applied to enhance this solution: Digital Twin for Stormwater Systems: XMPro iDTS can create a digital twin of the stormwater reservoir system, providing a virtual representation that mirrors the physical reservoirs. This allows for real-time simulation and analysis of water levels, flow rates, and potential flood scenarios, enabling proactive flood prevention strategies. Integration with Weather Forecasting Systems: XMPro iDTS can integrate real-time data from weather forecasting systems, allowing for predictive analysis of rainfall and its potential impact on reservoir levels. This integration is crucial for dynamic scheduling of maintenance activities and preemptive flood management. Continuous Water Quality Monitoring: The solution includes continuous monitoring of water quality to detect industrial runoff and other contaminants. XMPro iDTS can analyze this data to identify environmental risks and guide appropriate response actions. Dynamic Service Crew Dispatch (Dynamic Truck Rolls): Leveraging real-time data and predictive insights, XMPro iDTS can optimize the deployment of service crews, ensuring they are dispatched dynamically based on reservoir levels and weather predictions, rather than fixed routes. Automated Alerts for Flooding and Hazardous Conditions: XMPro iDTS can generate automated alerts about potential flooding and hazardous conditions, enhancing the safety of service crews and residents in affected areas. Customizable Dashboards for Operational Management: The solution provides customizable dashboards that offer insights into reservoir levels, weather forecasts, water quality, and crew deployment. These dashboards are tailored to the needs of operational managers and field service teams for effective decision-making. Scalability and Flexibility: XMPro iDTS is scalable and flexible, allowing the solution to be adapted to different sizes of urban areas and to integrate with various types of environmental monitoring equipment and systems. Data-Driven Decision Support: Leveraging AI-driven insights, XMPro iDTS enhances decision support, enabling urban water management authorities to make informed decisions about flood prevention, environmental protection, and resource allocation. In summary, XMPro iDTS addresses the unique challenges in monitoring stormwater reservoirs for flood prevention by providing a comprehensive, real-time, predictive, and integrated solution. Its capabilities in digital twin technology, predictive analytics, dynamic crew dispatch, and effective data visualization make it a powerful tool for enhancing urban flood prevention and water quality management. Use XMPro Blueprints for Quick Time To Value​ Easily import Blueprints, Accelerators and Patterns into your environment, providing a starting point for configuring your own solutions. View BlueprintsView AcceleratorsView Patterns"
  },
  "docs/resources/faqs/external-content/use-cases/oil-well-maintenance.html": {
    "href": "docs/resources/faqs/external-content/use-cases/oil-well-maintenance.html",
    "title": "Oil Well Maintenance Planning - XMPRO | XMPro",
    "summary": "Oil Well Maintenance Planning - XMPRO url XMPro Oil Well Maintenance Planning Solution: Streamlining Maintenance for Optimal Performance and Longevity The Problem Effective maintenance planning is critical for the optimal performance and longevity of oil wells. However, several challenges can complicate maintenance activities: Predicting Maintenance Needs: Accurately predicting when maintenance is required to prevent unplanned downtime and costly repairs. Maintenance Scheduling: Coordinating XMPro Oil Well Maintenance Planning Solution: Streamlining Maintenance for Optimal Performance and Longevity The Problem Effective maintenance planning is critical for the optimal performance and longevity of oil wells. However, several challenges can complicate maintenance activities: Predicting Maintenance Needs: Accurately predicting when maintenance is required to prevent unplanned downtime and costly repairs. Maintenance Scheduling: Coordinating maintenance activities to minimize disruption to production. Resource Allocation: Efficiently allocating resources, including personnel and equipment, for maintenance tasks. Data Integration: Integrating data from various sources to make informed maintenance decisions. Compliance and Safety: Ensuring maintenance activities comply with safety and environmental regulations. #text-box-1536336923 { width: 60%; } #text-box-1536336923 .text-box-content { font-size: 100%; } #banner-2060888616 { padding-top: 500px; } #banner-2060888616 .bg.bg-loaded { background-image: url(https://xmpro.com/wp-content/uploads/2020/04/1.jpg); } The Solution XMPro’s Oil Well Maintenance Planning Solution, utilizing XMPro iDTS, offers a data-driven approach to effectively plan and execute maintenance activities for oil wells. Key Metrics Monitored: Equipment Health: Monitoring the condition of critical equipment components. Operational Performance: Tracking performance metrics to identify signs of potential equipment failure. Historical Maintenance Data: Analyzing past maintenance activities to predict future needs. Resource Utilization: Monitoring the usage of resources to optimize maintenance planning. Compliance and Safety Metrics: Ensuring maintenance activities adhere to regulatory standards. Goals: Develop predictive models to anticipate maintenance needs. Optimize maintenance scheduling to reduce operational disruptions. Ensure robust data security and integrity. Provide user-friendly dashboards for maintenance planning and decision-making. #gap-1110363370 { padding-top: 30px; } Why XMPro iDTS? XMPro iDTS offers specialized solutions to address the challenges associated with planning and executing maintenance for oil wells. Here’s how XMPro iDTS can be specifically applied to enhance the Oil Well Maintenance Planning Solution: Digital Twin for Maintenance Simulation and Planning: XMPro iDTS can create a digital twin of the oil well and its associated equipment, providing a virtual representation for simulation and analysis. This allows for detailed visualization of the well’s condition and performance, aiding in the identification of maintenance needs and the planning of maintenance activities. Predictive Maintenance Analytics: Utilizing advanced analytics and machine learning, XMPro iDTS can analyze data from sensors and historical maintenance records to predict when maintenance is required. This predictive approach enables proactive maintenance scheduling, reducing unplanned downtime and extending the life of equipment. Resource Optimization for Maintenance Activities: XMPro iDTS can assist in efficiently allocating resources for maintenance, including personnel, equipment, and materials. By analyzing resource utilization and maintenance requirements, the system can help plan maintenance activities to minimize operational disruptions and costs. Real-Time Monitoring and Alerts: XMPro iDTS enables real-time monitoring of equipment health and operational performance. Automated alerts can be generated when potential issues are detected, allowing for timely maintenance interventions. Integration with Maintenance Management Systems: XMPro iDTS can integrate with existing maintenance management systems, ensuring a seamless flow of data and enabling more effective maintenance planning and execution. Customizable Dashboards for Maintenance Management: XMPro iDTS provides advanced data visualization tools and customizable dashboards tailored to the needs of maintenance teams and operational managers. These tools offer insights into maintenance requirements, scheduling, and resource allocation. Compliance and Safety Monitoring: Continuous monitoring of compliance and safety metrics ensures that maintenance activities adhere to regulatory standards and best practices, reducing the risk of accidents and non-compliance issues. Scalability and Flexibility: XMPro iDTS is scalable, meaning it can be adapted to different sizes of oil wells and can integrate additional data sources or maintenance scenarios as operational needs evolve. In summary, XMPro iDTS addresses the unique challenges in oil well maintenance planning by providing a comprehensive, real-time, predictive, and integrated solution. Its use of digital twin technology, combined with advanced analytics, automated guidance, and effective data visualization, makes it a powerful tool for optimizing maintenance activities, enhancing operational efficiency, and ensuring safety and compliance."
  },
  "docs/resources/faqs/external-content/use-cases/oil-well-rtp-monitor.html": {
    "href": "docs/resources/faqs/external-content/use-cases/oil-well-rtp-monitor.html",
    "title": "Oil Well RTP Monitoring - XMPRO | XMPro",
    "summary": "Oil Well RTP Monitoring - XMPRO url XMPro Oil Well Return to Production (RTP) Monitoring Solution: Maximizing Efficiency in Oil Production Recovery The Problem Bringing oil wells back to production (RTP) presents several challenges that can impact efficiency and profitability: Restoration Efficiency: Quickly and efficiently restoring non-operational or underperforming wells to full production is a complex task. Predictive Maintenance: Anticipating maintenance needs XMPro Oil Well Return to Production (RTP) Monitoring Solution: Maximizing Efficiency in Oil Production Recovery The Problem Bringing oil wells back to production (RTP) presents several challenges that can impact efficiency and profitability: Restoration Efficiency: Quickly and efficiently restoring non-operational or underperforming wells to full production is a complex task. Predictive Maintenance: Anticipating maintenance needs to prevent future downtimes is crucial in the RTP process. Production Optimization: Ensuring that the well returns to its optimal production capacity while maintaining equipment health. Data Integration and Analysis: Effectively integrating and analyzing data from various sources to make informed decisions during the RTP process. Environmental and Safety Compliance: Maintaining compliance with environmental and safety regulations during the RTP process. #text-box-1764678312 { width: 60%; } #text-box-1764678312 .text-box-content { font-size: 100%; } #banner-1348588171 { padding-top: 514px; } #banner-1348588171 .bg.bg-loaded { background-image: url(https://xmpro.com/wp-content/uploads/2020/04/2.jpg); } #banner-1348588171 .bg { background-position: 54% 79%; } The Solution XMPro’s Oil Well RTP Monitoring Solution, utilizing XMPro iDTS, offers a comprehensive approach to efficiently managing the return to production of oil wells. Key Metrics Monitored: Well Performance: Monitoring production rates and operational efficiency of the well. Equipment Health: Tracking the condition of critical equipment such as pumps, valves, and sensors. Operational Parameters: Monitoring parameters like pressure, temperature, and flow rates to optimize production. Maintenance Needs: Predicting maintenance requirements to prevent unplanned downtimes. Environmental Impact: Monitoring for any environmental or safety issues during the RTP process. Goals: Implement IoT sensors for comprehensive monitoring of well and equipment performance. Develop predictive models for maintenance and production optimization. Optimize the RTP process for maximum efficiency and minimal environmental impact. Ensure robust data security and integrity. Provide user-friendly dashboards for real-time monitoring and decision-making. #gap-174075967 { padding-top: 30px; } Why XMPro iDTS? XMPro iDTS offers specialized solutions to address the challenges associated with monitoring and optimizing the Return to Production (RTP) process in oil wells. Here’s how XMPro iDTS can be specifically applied to enhance the Oil Well RTP Monitoring Solution: Digital Twin for Oil Well RTP Process: XMPro iDTS can create a digital twin of the oil well RTP process, providing a virtual representation that mirrors the physical well and its associated systems. This allows for real-time simulation and analysis, enabling operators to visualize the impact of various operational parameters on the well’s return to production and identify the most efficient strategies for restoration. Predictive Maintenance for Well Equipment: Utilizing advanced analytics and machine learning, XMPro iDTS can analyze data from sensors to predict maintenance needs for critical well equipment. This predictive approach allows for maintenance to be scheduled proactively, reducing unplanned downtime and ensuring a smooth RTP process. Optimization of Production Parameters: XMPro iDTS can analyze operational data to optimize the production parameters of the well during the RTP process. This includes adjusting pressure, temperature, and flow rates to maximize oil extraction efficiency while maintaining equipment health. Real-Time Monitoring of Key Performance Indicators: XMPro iDTS enables real-time monitoring of key performance indicators such as production rates, equipment condition, and operational parameters. By tracking these metrics, operators can make informed decisions to optimize the RTP process. Automated Alerts and Recommendations: XMPro iDTS can generate automated alerts and recommendations when potential issues are detected or when there are opportunities for optimization. This feature ensures timely interventions, maintaining operational efficiency and safety. Customizable Dashboards and Reporting: XMPro iDTS provides advanced data visualization tools and customizable dashboards, which are essential for monitoring the RTP process. These tools enable different stakeholders to access relevant information, understand trends, and make informed decisions quickly. Scalability and Flexibility: XMPro iDTS is scalable, meaning it can be expanded to accommodate additional wells or adapted to different RTP scenarios as operational needs evolve. Environmental and Safety Compliance Monitoring: Continuous monitoring of environmental and safety parameters helps ensure that the RTP process is conducted safely and in compliance with regulatory standards. This proactive approach can prevent accidents and reduce the risk of non-compliance penalties. In summary, XMPro iDTS addresses the unique challenges in the oil well Return to Production process by providing a comprehensive, real-time, predictive, and integrated solution. Its use of digital twin technology, combined with advanced analytics, automated guidance, and effective data visualization, makes it a powerful tool for optimizing the RTP process, enhancing efficiency, and ensuring safety and environmental compliance."
  },
  "docs/resources/faqs/external-content/use-cases/pipe-scaling-predict.html": {
    "href": "docs/resources/faqs/external-content/use-cases/pipe-scaling-predict.html",
    "title": "Pipe Scaling Prediction for Roller Cooling - XMPRO | XMPro",
    "summary": "Pipe Scaling Prediction for Roller Cooling - XMPRO url XMPro Pipe Scaling Monitoring Solution for Roller Cooling: Ensuring Efficiency and Longevity in Industrial Cooling Systems The Problem Pipe scaling in roller cooling systems is a significant issue in industries where precise temperature control is crucial, such as metalworking, manufacturing, and processing plants. Key challenges includes: Scaling Build-Up: Mineral deposits and scaling in pipes reduce XMPro Pipe Scaling Monitoring Solution for Roller Cooling: Ensuring Efficiency and Longevity in Industrial Cooling Systems #gap-136333096 { padding-top: 30px; } The Problem Pipe scaling in roller cooling systems is a significant issue in industries where precise temperature control is crucial, such as metalworking, manufacturing, and processing plants. Key challenges includes: Scaling Build-Up: Mineral deposits and scaling in pipes reduce cooling efficiency and can lead to overheating of equipment. Maintenance and Downtime: Frequent maintenance is required to remove scale, causing downtime and increased operational costs. Energy Inefficiency: Scaling forces cooling systems to work harder, consuming more energy and increasing costs. Equipment Wear and Tear: Persistent scaling can lead to permanent damage to pipes and cooling equipment. Quality Control Issues: Inconsistent cooling due to scaling can affect product quality, especially where precise temperature control is needed. #text-box-217483190 { width: 60%; } #text-box-217483190 .text-box-content { font-size: 100%; } #banner-183872687 { padding-top: 500px; } #banner-183872687 .bg.bg-loaded { background-image: url(https://xmpro.com/wp-content/uploads/2020/04/14.jpg); } The Solution XMPro’s Pipe Scaling Monitoring Solution for Roller Cooling, utilizing the Intelligent Digital Twin Suite (iDTS), offers a comprehensive approach to monitoring and managing pipe scaling. Key Metrics Monitored: Water Flow Rate: Monitoring for reductions in flow rate that can indicate scaling. Temperature Variations: Detecting uneven temperatures that may result from scaling. Pipe Pressure: Monitoring pressure changes as an indicator of scale build-up. Water Quality: Analyzing water for mineral content that can lead to scaling. Energy Consumption: Tracking energy usage to identify inefficiencies caused by scaling. #gap-255165200 { padding-top: 30px; } Why XMPro iDTS? XMPro iDTS offers unique and innovative solutions to the challenges associated with monitoring and managing pipe scaling in roller cooling systems. Here’s how XMPro iDTS can be specifically applied to enhance the Pipe Scaling Monitoring Solution for Roller Cooling: Digital Twin for Cooling System Simulation: XMPro iDTS can create a digital twin of the entire roller cooling system, including pipes and cooling mechanisms. This allows for real-time simulation and analysis of the system, enabling operators to visualize the impact of scaling on cooling efficiency and system performance. Predictive Analytics for Scaling Detection: Utilizing advanced analytics and machine learning, XMPro iDTS can analyze sensor data to predict the onset of pipe scaling. This predictive approach allows for proactive maintenance and cleaning schedules, reducing downtime and maintaining system efficiency. Water Quality Monitoring and Optimization: XMPro iDTS continuously monitors water quality parameters that contribute to scaling, such as mineral content, pH levels, and temperature. By analyzing this data, the system can recommend adjustments to water treatment processes to minimize scaling risk. Automated Alerts and Recommendations: XMPro iDTS can automate the generation of alerts and recommendations when potential scaling issues are detected. This feature ensures timely interventions, preventing significant scale build-up and maintaining operational efficiency. Integration with Existing Control Systems: XMPro iDTS is designed to integrate seamlessly with existing roller cooling systems and water treatment controls, ensuring a unified platform for data collection and analysis. This integration is crucial for accurate monitoring and effective management of scaling issues. Customizable Dashboards and Reporting: XMPro iDTS provides advanced data visualization tools and customizable dashboards, which are essential for monitoring complex cooling systems. These tools enable operators and managers to understand trends, monitor key performance indicators, and make informed decisions quickly. Scalability and Flexibility: XMPro iDTS is scalable, meaning it can be expanded to accommodate additional sensors or systems as operational needs evolve. Energy Efficiency Analysis: By monitoring and optimizing the cooling system’s performance, XMPro iDTS helps reduce energy consumption, leading to cost savings and environmental benefits. In summary, XMPro iDTS addresses the unique challenges in monitoring and managing pipe scaling in roller cooling systems by providing a comprehensive, real-time, predictive, and integrated solution. Its use of digital twin technology, combined with advanced analytics, automated guidance, and effective data visualization, makes it a powerful tool for optimizing the performance and longevity of these critical industrial systems."
  },
  "docs/resources/faqs/external-content/use-cases/precision-irrigation.html": {
    "href": "docs/resources/faqs/external-content/use-cases/precision-irrigation.html",
    "title": "Precision Irrigation in Agriculture | XMPro",
    "summary": "Precision Irrigation in Agriculture url XMPro Solution for Precision Irrigation in Agriculture Introduction In the agriculture industry, efficient water management is crucial for sustainability and productivity. XMPro's solution for Soil Moisture Monitoring for Precision Irrigation leverages advanced IoT technologies to optimize irrigation practices, ensuring water is used effectively to meet crop needs. The Challenge Farmers often face challenges in irrigation XMPro Solution for Precision Irrigation in Agriculture Introduction In the agriculture industry, efficient water management is crucial for sustainability and productivity. XMPro’s solution for Soil Moisture Monitoring for Precision Irrigation leverages advanced IoT technologies to optimize irrigation practices, ensuring water is used effectively to meet crop needs. The Challenge Farmers often face challenges in irrigation management, including: Over or Under-Watering: Determining the right amount of water for crops can be challenging, leading to wastage or insufficient irrigation. Resource Utilization: Efficient use of water resources is essential, especially in areas with limited water supply. Impact on Crop Yield: Inconsistent watering can adversely affect crop health and yield. The Solution: XMPro’s Precision Irrigation System XMPro’s solution employs IoT soil moisture sensors and data analytics to provide precise irrigation control. Key Features Real-Time Soil Moisture Monitoring: Deploying IoT sensors across fields to measure soil moisture levels in real-time, providing data-driven insights for irrigation needs. Data Integration and Transformation: XMPro’s platform integrates soil moisture data with weather forecasts and crop models to optimize irrigation schedules, ensuring crops receive the right amount of water at the right time. Automated Irrigation Control: The system can automatically adjust irrigation based on sensor data, reducing manual intervention and ensuring consistent watering practices. Customizable Alerts and Recommendations: Farmers receive alerts and recommendations for irrigation based on real-time soil conditions, preventing over or under-watering. Efficient Water Usage: Precision irrigation leads to more efficient water use, reducing waste and conserving resources. Figure 1. Real-Time Agriculture Asset Overview Dashboard Real-Time Agriculture Asset Overview Dashboard This comprehensive dashboard is expertly designed for agricultural operators, offering an integrated view of various agricultural assets and environmental conditions. It provides a multifaceted perspective on farm operations, covering asset management, irrigation needs, and weather monitoring. Multi-View Functionality: The dashboard is equipped with distinct views: Asset View, Irrigation View, and Weather View, allowing users to switch seamlessly between different monitoring goals. Interactive Map with Extensive Asset Monitoring: The map showcases a variety of agricultural assets such as tractors, combines, seeders, irrigation systems, and balers, each represented by distinct icons for easy recognition. In the Asset View, the operational status and upcoming maintenance requirements of each piece of equipment are clearly indicated, facilitating proactive management and scheduling. Soil Moisture Monitoring and Irrigation Overview: The Irrigation View highlights soil moisture levels across different fields, using data from IoT soil moisture sensors. Fields are color-coded based on moisture status: green for optimal, yellow for moderate, and red for low moisture levels. Alerts for fields needing immediate irrigation are prominently displayed, ensuring efficient water use and optimal crop health. Weather View for Comprehensive Climate Insights: The Weather View offers current weather conditions and forecasts, essential for daily and long-term agricultural planning. Agricultural Asset Health and Performance Metrics: The dashboard provides key metrics on agricultural assets, including efficiency, usage patterns, and maintenance schedules. Graphs and charts visualize trends and performance, aiding in decision-making. Customizable Alerts and Recommendations: Tailored alerts and recommendations are generated based on real-time data and predictive analytics, covering irrigation scheduling, crop rotation, and equipment maintenance. Drill-Down Capability for In-Depth Asset Information: Users can delve into detailed information about specific assets by clicking on their icons, accessing historical performance data, recent activities, and targeted maintenance or irrigation recommendations. This Real-Time Agriculture Asset Overview Dashboard is a vital tool for agricultural operators, enabling effective monitoring and management of diverse farm operations. By providing real-time data, predictive insights, and actionable recommendations across multiple specialized views, it enhances operational efficiency, optimizes resource management, and supports informed agricultural practices. Figure 2. Asset Analysis View – Field F006 Soil Moisture Health Asset Analysis View – Field Soil Moisture Health This Asset Analysis View provides in-depth insights into specific fields within the agricultural system, with a focus on Field F006, which is currently showing low soil moisture levels and requires immediate attention. Comprehensive Soil Moisture Health Metrics: This section displays key health indicators for Field F006, including real-time soil moisture levels, temperature variability, and crop growth stage. Enhanced with predictive analytics, the data enables accurate forecasts of irrigation needs, aiding in proactive water management to optimize crop health. Interactive Field Models: The dashboard presents detailed 2D and 3D models of Field F006. Features allow for an expanded view of the field, highlighting areas of concern. Zones showing low soil moisture levels, critical for immediate irrigation, are distinctly color-marked for quick identification and action. Error Identification and Proactive Recommendations: Clickable sections in the field model lead users to specific error details and associated recommendations. This integration with XMPro’s Recommendation Manager streamlines the process for identifying and addressing the urgent irrigation needs of Field F006. Detailed Information on Field F006: The dashboard provides a comprehensive profile of Field F006, including soil type, planting date, crop variety, and recent irrigation history. This information is essential for understanding its specific water requirements and planning future irrigation strategies. XMPro Co-Pilot Integration: Incorporating XMPro Co-Pilot, this feature utilizes AI, trained on datasets such as historical soil moisture data, weather patterns, and crop-specific water needs, to offer targeted guidance for managing Field F006. This AI-driven assistance supports informed irrigation decision-making and strategy optimization. This Asset Analysis View is tailored to provide a complete and actionable picture of the soil moisture health of Field F006. By combining sophisticated visual models with data-driven insights and AI-powered recommendations, it enables effective and timely irrigation management, ensuring the health and productivity of the crops in Field F006. Why XMPro iDTS? XMPro’s Intelligent Digital Twin Suite (iDTS) offers innovative solutions tailored for precision irrigation in agriculture, leveraging its advanced capabilities to optimize irrigation practices and enhance crop yields. Here’s how XMPro iDTS effectively addresses the challenges in this domain: Digital Twin Modeling for Agricultural Land: XMPro iDTS can create a digital twin of agricultural fields, providing a virtual representation that mirrors real-world conditions. This allows for continuous monitoring and analysis of soil moisture, crop health, and environmental factors, aiding in precise irrigation planning. Advanced Sensor Data Integration & Transformation: The suite integrates data from soil moisture sensors, weather stations, and satellite imagery. This comprehensive data integration is crucial for accurately assessing irrigation needs and optimizing water usage. Predictive Analytics for Irrigation Scheduling: Utilizing machine learning algorithms, XMPro iDTS analyzes diverse data sets to predict optimal irrigation times and quantities. This approach enables proactive, data-driven irrigation, ensuring that crops receive the right amount of water at the right time. Irrigation Scheduling Optimization: XMPro iDTS optimizes irrigation schedules based on real-time data and predictive insights, maximizing resource efficiency and crop yield while minimizing water waste and operational costs. Real-Time Soil Monitoring and Alerting: The platform provides real-time monitoring of soil and crop conditions, generating instant alerts for deviations from optimal moisture levels, enabling timely irrigation decisions. Customizable Dashboards for Decision Support: XMPro iDTS includes customizable dashboards that display key agricultural data, such as soil moisture trends and crop growth stages, in an easy-to-understand format, aiding in informed decision-making. Scalability and Flexibility – Start Small, Scale Fast: The platform offers scalable and flexible solutions, allowing for incremental adoption in agriculture. Its modular design ensures easy integration with existing agricultural systems, facilitating quick deployment and adaptability. Enhanced Crop Safety & Operational Efficiency: By providing precise irrigation control and real-time insights, XMPro iDTS enhances crop safety and overall operational efficiency, leading to higher yields and sustainable farming practices. XMPro Blueprints – Quick Time to Value: XMPro Blueprints for agriculture offer a rapid path to implementation, featuring pre-configured templates that incorporate best practices for precision irrigation, ensuring quick and effective deployment. In summary, XMPro iDTS addresses the precision irrigation use case in agriculture by offering a comprehensive, real-time, and predictive solution. Its capabilities in digital twin technology, sensor data integration, and advanced analytics make it a powerful tool for optimizing irrigation practices, enhancing crop yields, and promoting sustainable agriculture. Not Sure How To Get Started? No matter where you are on your digital transformation journey, the expert team at XMPro can help guide you every step of the way - We have helped clients successfully implement and deploy projects with Over 10x ROI in only a matter of weeks! Request a free online consultation for your business problem. \"*\" indicates required fields Name*Email*Company / Organization*Job Role*Contact Department*SalesSupportMessage*CaptchaEmailThis field is for validation purposes and should be left unchanged. Δdocument.getElementById( \"ak_js_3\" ).setAttribute( \"value\", ( new Date() ).getTime() ); /* <![CDATA[ */ gform.initializeOnLoaded( function() {gformInitSpinner( 3, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery('#gform_ajax_frame_3').on('load',function(){var contents = jQuery(this).contents().find('*').html();var is_postback = contents.indexOf('GF_AJAX_POSTBACK') >= 0;if(!is_postback){return;}var form_content = jQuery(this).contents().find('#gform_wrapper_3');var is_confirmation = jQuery(this).contents().find('#gform_confirmation_wrapper_3').length > 0;var is_redirect = contents.indexOf('gformRedirect(){') >= 0;var is_form = form_content.length > 0 && ! is_redirect && ! is_confirmation;var mt = parseInt(jQuery('html').css('margin-top'), 10) + parseInt(jQuery('body').css('margin-top'), 10) + 100;if(is_form){jQuery('#gform_wrapper_3').html(form_content.html());if(form_content.hasClass('gform_validation_error')){jQuery('#gform_wrapper_3').addClass('gform_validation_error');} else {jQuery('#gform_wrapper_3').removeClass('gform_validation_error');}setTimeout( function() { /* delay the scroll by 50 milliseconds to fix a bug in chrome */ }, 50 );if(window['gformInitDatepicker']) {gformInitDatepicker();}if(window['gformInitPriceFields']) {gformInitPriceFields();}var current_page = jQuery('#gform_source_page_number_3').val();gformInitSpinner( 3, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery(document).trigger('gform_page_loaded', [3, current_page]);window['gf_submitting_3'] = false;}else if(!is_redirect){var confirmation_content = jQuery(this).contents().find('.GF_AJAX_POSTBACK').html();if(!confirmation_content){confirmation_content = contents;}setTimeout(function(){jQuery('#gform_wrapper_3').replaceWith(confirmation_content);jQuery(document).trigger('gform_confirmation_loaded', [3]);window['gf_submitting_3'] = false;wp.a11y.speak(jQuery('#gform_confirmation_message_3').text());}, 50);}else{jQuery('#gform_3').append(contents);if(window['gformRedirect']) {gformRedirect();}}jQuery(document).trigger(\"gform_pre_post_render\", [{ formId: \"3\", currentPage: \"current_page\", abort: function() { this.preventDefault(); } }]); if (event.defaultPrevented) { return; } const gformWrapperDiv = document.getElementById( \"gform_wrapper_3\" ); if ( gformWrapperDiv ) { const visibilitySpan = document.createElement( \"span\" ); visibilitySpan.id = \"gform_visibility_test_3\"; gformWrapperDiv.insertAdjacentElement( \"afterend\", visibilitySpan ); } const visibilityTestDiv = document.getElementById( \"gform_visibility_test_3\" ); let postRenderFired = false; function triggerPostRender() { if ( postRenderFired ) { return; } postRenderFired = true; jQuery( document ).trigger( 'gform_post_render', [3, current_page] ); gform.utils.trigger( { event: 'gform/postRender', native: false, data: { formId: 3, currentPage: current_page } } ); if ( visibilityTestDiv ) { visibilityTestDiv.parentNode.removeChild( visibilityTestDiv ); } } function debounce( func, wait, immediate ) { var timeout; return function() { var context = this, args = arguments; var later = function() { timeout = null; if ( !immediate ) func.apply( context, args ); }; var callNow = immediate && !timeout; clearTimeout( timeout ); timeout = setTimeout( later, wait ); if ( callNow ) func.apply( context, args ); }; } const debouncedTriggerPostRender = debounce( function() { triggerPostRender(); }, 200 ); if ( visibilityTestDiv && visibilityTestDiv.offsetParent === null ) { const observer = new MutationObserver( ( mutations ) => { mutations.forEach( ( mutation ) => { if ( mutation.type === 'attributes' && visibilityTestDiv.offsetParent !== null ) { debouncedTriggerPostRender(); observer.disconnect(); } }); }); observer.observe( document.body, { attributes: true, childList: false, subtree: true, attributeFilter: [ 'style', 'class' ], }); } else { triggerPostRender(); } } );} ); /* ]]&gt; */"
  },
  "docs/resources/faqs/external-content/use-cases/predict-heat-exchang.html": {
    "href": "docs/resources/faqs/external-content/use-cases/predict-heat-exchang.html",
    "title": "Predict Heat Exchanger Fouling - XMPRO | XMPro",
    "summary": "Predict Heat Exchanger Fouling - XMPRO url XMPro Predictive Heat Exchanger Fouling Solution for Polyethylene Process Manufacturers The Problem Heat exchanger fouling in polyethylene process manufacturing presents significant challenges including: Extended Batch Cycles: Fouling leads to longer batch cycles, reducing overall plant efficiency and yield. Inadequate Monitoring: Traditional U coefficient monitoring methods are intermittent and often insufficient for early detection of fouling. XMPro Predictive Heat Exchanger Fouling Solution for Polyethylene Process Manufacturers The Problem Heat exchanger fouling in polyethylene process manufacturing presents significant challenges including: Extended Batch Cycles: Fouling leads to longer batch cycles, reducing overall plant efficiency and yield. Inadequate Monitoring: Traditional U coefficient monitoring methods are intermittent and often insufficient for early detection of fouling. Unplanned Maintenance: Lack of predictive insights results in unplanned maintenance activities, causing operational disruptions. Decreased Equipment Efficiency: Progressive fouling diminishes heat exchanger efficiency, impacting the manufacturing process. #text-box-370682599 { width: 60%; } #text-box-370682599 .text-box-content { font-size: 100%; } #banner-402887262 { padding-top: 415px; } #banner-402887262 .bg.bg-loaded { background-image: url(https://xmpro.com/wp-content/uploads/2020/04/15.jpg); } #banner-402887262 .bg { background-position: 51% 63%; } The Solution XMPro’s solution involves advanced monitoring and predictive analytics to proactively manage heat exchanger fouling: Key Features Weekly U Coefficient Monitoring: Regular and frequent calculations to track heat exchanger efficiency. Predictive Modeling: Advanced analytics to forecast when performance will decline below required standards. Maintenance Recommendations: Automated suggestions based on predictive data and business rules. Real-Time Data Integration: Using XMPro iDTS for continuous data analysis, enhancing predictive accuracy. Benefits: Early Warning System Over 9 months of advance notice before potential failure events. Minimized Unplanned Maintenance: Significant reduction in unplanned maintenance for heat exchangers. Optimized Plant Yield: Maintaining efficient batch cycles for higher plant yield. #image_1383975067 { width: 100%; } #gap-420723639 { padding-top: 30px; } Why XMPro iDTS? XMPro iDTS offers specialized capabilities that are particularly effective in solving the challenges associated with predicting and managing heat exchanger fouling in polyethylene process manufacturing. Here’s how XMPro iDTS can be specifically applied to enhance the Predictive Heat Exchanger Fouling Solution: Advanced Data Analytics for Fouling Prediction: XMPro iDTS utilizes advanced data analytics to process and analyze the data collected from heat exchangers. By regularly calculating the U coefficient and other relevant metrics, the system can predict fouling trends and forecast when performance might fall below required standards. Real-Time Monitoring and Digital Twin Simulation: The digital twin capability of XMPro iDTS allows for real-time simulation and monitoring of the heat exchanger’s performance. This virtual representation helps in understanding the impact of various operational parameters on fouling and efficiency, enabling proactive adjustments. Automated Maintenance Recommendations: Based on the predictive analytics and real-time data, XMPro iDTS can generate automated maintenance recommendations. These recommendations are informed by predefined business rules and predictive insights, ensuring timely and effective maintenance actions. Integration with Operational Systems: XMPro iDTS seamlessly integrates with existing operational systems, ensuring that data from the heat exchangers and other relevant sources are consolidated and analyzed cohesively. This integration is crucial for accurate monitoring and effective decision-making. Customizable Dashboards for Enhanced Decision-Making: XMPro iDTS provides customizable dashboards that offer a comprehensive view of the heat exchanger’s performance, including real-time data on U coefficients, temperature differentials, flow rates, and pressure drops. These dashboards are tailored to the needs of process engineers and maintenance teams, enhancing usability and decision-making. Predictive Maintenance Scheduling: The solution enables predictive maintenance scheduling, reducing the reliance on fixed maintenance intervals. This approach minimizes unplanned downtime and ensures that maintenance is performed only when necessary, based on the actual condition of the heat exchanger. Scalability and Flexibility: XMPro iDTS is scalable and flexible, allowing the solution to be adapted to different sizes of heat exchangers and to accommodate evolving operational needs. Energy Efficiency and Environmental Compliance: By maintaining optimal heat exchanger efficiency, XMPro iDTS helps in reducing energy consumption and ensuring environmental compliance, contributing to sustainable and cost-effective operations. In summary, XMPro iDTS addresses the unique challenges in predicting and managing heat exchanger fouling in polyethylene process manufacturing by providing a comprehensive, real-time, predictive, and integrated solution. Its advanced analytics, digital twin technology, automated recommendations, and effective data visualization make it a powerful tool for enhancing operational efficiency and reducing downtime."
  },
  "docs/resources/faqs/external-content/use-cases/predictive-maintenan.html": {
    "href": "docs/resources/faqs/external-content/use-cases/predictive-maintenan.html",
    "title": "Predictive Maintenance & Asset Health Monitoring For Haul Trucks In The Mining Industry | XMPro",
    "summary": "Predictive Maintenance & Asset Health Monitoring For Haul Trucks In The Mining Industry url Predictive Maintenance & Asset Health Monitoring For Haul Trucks In The Mining Industry. Introduction The mining sector is heavily dependent on robust machinery like haul trucks, which are indispensable for material excavation and transport. The effectiveness of these machines is a cornerstone for optimizing production and managing expenses. The Challenge Haul trucks and other mobile Predictive Maintenance & Asset Health Monitoring For Haul Trucks In The Mining Industry. Introduction The mining sector is heavily dependent on robust machinery like haul trucks, which are indispensable for material excavation and transport. The effectiveness of these machines is a cornerstone for optimizing production and managing expenses. The Challenge Haul trucks and other mobile equipment in mining operations are subject to demanding conditions and intensive use, leading to deterioration that can culminate in unanticipated equipment failures. Key challenges include – Rigorous Operating Conditions: Continuous exposure to harsh elements, persistent vibrations, and substantial burdens hasten the wear of equipment. Complex Maintenance Forecasting: Anticipating maintenance for an assorted array of equipment, each subject to unique operational stresses and life cycles, is intricate. Maintenance Scheduling vs. Downtime: Efficiently planning maintenance to curtail downtime and avoid disrupting operations poses an ongoing challenge. Fleet Diversity Issues: The presence of a heterogeneous mix of equipment varying in age, technology, and servicing needs adds complexity to the standardization of upkeep protocols. Data Management: Modern mobile assets generate copious amounts of data, which can be daunting to sift through to pinpoint critical maintenance information. Technology Integration: Blending advanced technologies with legacy assets to enhance maintenance efficiency poses integration challenges. Regulatory Compliance and Safety: Adhering to strict industry standards and maintaining operator safety is paramount, especially since oversights in maintenance can result in severe failures. Resource Optimization: Effectively allocating scarce resources, such as personnel and parts, particularly in isolated mining locations with constrained access, is a pivotal strategic decision. Navigating these challenges is crucial for extending the lifespan of mobile assets and upholding the productivity and safety requisites of the mining industry. The Solution: XMPro Co-Pilot for Predictive Maintenance & Asset Health Monitoring of Haul Trucks in Mining XMPro Co-Pilot is meticulously designed to tackle the specific challenges of managing haul trucks in mining operations. It delivers superior operational efficiency with a suite of predictive maintenance and asset health monitoring tools. Key Features Holistic Real-Time Asset Tracking: XMPro Co-Pilot consolidates telemetry from haul trucks, offering a live feed of the equipment’s health, from engine metrics to drivetrain condition, pivotal for proactive maintenance strategies. Enhanced Predictive Analytics: Leveraging sophisticated algorithms, the platform anticipates the service needs of mobile assets, projecting potential issues and facilitating preventive measures that are both timely and cost-efficient. Dynamic Simulation Modeling: The software creates dynamic simulations of haul trucks, mirroring actual operational conditions for in-depth analysis and preemptive maintenance scheduling. Predictive Maintenance Scheduling: XMPro Co-Pilot harnesses predictive data to generate maintenance recommendations, which prompt the initiation of prescriptive work orders. This ensures that maintenance activities are strategically planned, aligning with mining operations to minimize workflow disruption. Configurable Dashboard Interface: Tailored dashboards offer essential insights and clear visuals of asset conditions, enabling operators to make informed, strategic decisions with ease. With XMPro Co-Pilot, mining enterprises are equipped with a cutting-edge predictive maintenance framework that reduces equipment downtime, extends the service life of haul trucks, and upholds the highest standards of safety and regulatory compliance. Discover XMPro’s Predictive Maintenance & Asset Health Monitoring For Haul Trucks In The Mining Industry. Figure 1: Asset Analysis View – Haul Truck HT2002 System Health The Asset Analysis View is instrumental for monitoring the operational health of mining haul trucks like HT2002. This heavy-duty machinery, fundamental to mining operations, demands meticulous oversight to forestall common failure modes including engine issues, hydraulic malfunctions, and tire wear. Comprehensive Haul Truck Health Metrics The dashboard illuminates essential health metrics for the haul truck, concentrating on parameters crucial to its robust performance. These encompass engine temperature, oil viscosity, vibration levels, and power output, all indicative of the truck’s current condition. Utilizing predictive analytics in tandem with live health data, the system gauges the remaining useful life (RUL) of critical truck elements, guiding preemptive maintenance to thwart impending malfunctions, thus curtailing downtime and securing efficient operation compliant with mining industry benchmarks. Engine Temperature: At 71%, nearing high-risk thresholds, suggesting potential overheat risks that could compromise engine integrity. Oil Viscosity: Measured at 75%, may signify the necessity for oil change to ensure proper lubrication and engine protection. Vibration: Indicated at 75%, this could be a harbinger of emerging mechanical issues or misalignments needing attention. Power Output: Monitored at 1.9 kW, ensuring the truck’s performance is within expected ranges to avert mechanical stress or inefficiency. Fuel Efficiency: Observed at 92 L/h, is pivotal for assessing the engine’s health, with fluctuations from the norm pointing to possible engine concerns. Interactive 2D and 3D Haul Truck Models Providing interactive 2D and 3D models, the dashboard facilitates an exhaustive examination of the haul truck, focusing on components susceptible to wear or failure. This visual tool accentuates imperative areas such as the engine and hydraulic systems, directing maintenance focus toward preventing deterioration or operational inefficiency. Error Identification and Prescriptive Recommendations Proactive in its approach, the system identifies issues like “Engine Health Low” and “Hydraulic Return Fluid Overtemp,” while also proposing proactive maintenance steps. It emphasizes preventive measures to avoid operational stops and preserve truck health. Detailed Haul Truck Information The dashboard provides a comprehensive profile of the haul truck, which includes: Truck Type: Dump Truck Model: Komatsu 810e Total Running Hours: 2498, suggesting the machine’s activity level and potential maintenance timelines. Location Coordinates: Precise GPS positioning for easy asset tracking. Odometer: Marking 128,000 KM, vital for planning maintenance schedules. XMPro Co-Pilot Integration The XMPro Co-Pilot is adept at querying both real-time and historical data, empowering the system with the capability to conduct detailed analyses. This AI-enabled co-pilot is designed to sift through extensive maintenance records and operational metrics to render targeted, data-driven maintenance suggestions. It accentuates predictive maintenance practices by identifying trends and patterns that predict potential failures. Work Request History The truck’s maintenance history is thoroughly documented within the dashboard, showcasing service dates, performed actions, and outcomes. This meticulous logging ensures transparency in maintenance procedures and assists in the continuous improvement of the truck’s performance. Overall, the Asset Analysis View for the Haul Truck HT2002 merges cutting-edge visualizations with analytic insights and AI-augmented prognostics to sustain peak operational efficiency and safety standards. An indispensable tool for the mining sector, it empowers operators to maintain exemplary performance through predictive maintenance and efficient asset health management. Why XMPro iBOS for Mining Plant Operations? XMPro’s Intelligent Business Operations Suite (iBOS) is expertly devised for the intricate challenges faced in the predictive maintenance of mobile assets within the mining industry. Here’s the transformation it brings: Advanced Intelligent Digital Twin Modeling: XMPro iBOS constructs sophisticated digital twins, reflecting the detailed operations of mining equipment. It allows comprehensive performance analysis under varied conditions, vital for operational optimization. Advanced Sensor Data Integration & Transformation: Real-time sensor data across mobile assets offer critical insights into performance metrics like vibration, load capacity, and engine status, which are essential for detecting early signs of potential failures and maintenance needs. Predictive Analytics for Performance Enhancement: Utilizing advanced analytics, XMPro iBOS predicts potential asset failures, enhancing operational parameters and enabling preventive adjustments, thereby ensuring continuous mining operations with minimized downtimes. Maintenance Scheduling Optimization: Performance data drives XMPro iBOS’s maintenance scheduling, transforming the approach from reactive to proactive, optimizing the maintenance cycle for various assets, and significantly reducing breakdowns. Real-Time Monitoring and Predictive Alerting: Real-time monitoring and predictive alerting are critical components of XMPro’s iBOS for managing mobile assets within the mining industry. This ensures each mobile asset, from haul trucks to dozers, functions within the optimal parameters, thus enhancing efficiency and reducing reliance on manual intervention. Configurable and Interactive Dashboards: XMPro provides configurable dashboards that offer real-time insights into the health and performance of equipment across all dairy processing plants. These dashboards are designed to be interactive, enabling detailed scrutiny of specific operational aspects and supporting centralized management decisions. Scalability and Flexibility – Start Small, Scale Fast: Designed to accommodate dairy operations of any scale, XMPro’s modular architecture allows for seamless integration and adaptability. This scalability ensures that mining plants can efficiently manage operations as they expand or adapt to changing market demands. Enhanced Safety & Operational Efficiency: XMPro boosts operational safety by identifying potential hazards and inefficiencies in the processing line, ensuring that all equipment operates within safe and optimal parameters. This contributes to a safer working environment and more efficient production processes. XMPro Blueprints – Quick Time to Value: Offering quick time-to-value, XMPro Blueprints facilitate rapid deployment of intelligent operations solutions across mining operations. These templates are built on industry best practices, ensuring that plants can quickly realize the benefits of digital transformation. XMPro iBOS caters to the predictive maintenance needs of the mining industry’s mobile assets with a suite that promises comprehensive, predictive, and integrated solutions, driving efficiency and safety across operations. Not Sure How To Get Started? No matter where you are on your digital transformation journey, the expert team at XMPro can help guide you every step of the way - We have helped clients successfully implement and deploy projects with Over 10x ROI in only a matter of weeks! Request a free online consultation for your business problem. \"*\" indicates required fields Name*Email*Company / Organization*Job Role*Contact Department*SalesSupportMessage*CaptchaPhoneThis field is for validation purposes and should be left unchanged. Δdocument.getElementById( \"ak_js_3\" ).setAttribute( \"value\", ( new Date() ).getTime() ); /* <![CDATA[ */ gform.initializeOnLoaded( function() {gformInitSpinner( 3, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery('#gform_ajax_frame_3').on('load',function(){var contents = jQuery(this).contents().find('*').html();var is_postback = contents.indexOf('GF_AJAX_POSTBACK') >= 0;if(!is_postback){return;}var form_content = jQuery(this).contents().find('#gform_wrapper_3');var is_confirmation = jQuery(this).contents().find('#gform_confirmation_wrapper_3').length > 0;var is_redirect = contents.indexOf('gformRedirect(){') >= 0;var is_form = form_content.length > 0 && ! is_redirect && ! is_confirmation;var mt = parseInt(jQuery('html').css('margin-top'), 10) + parseInt(jQuery('body').css('margin-top'), 10) + 100;if(is_form){jQuery('#gform_wrapper_3').html(form_content.html());if(form_content.hasClass('gform_validation_error')){jQuery('#gform_wrapper_3').addClass('gform_validation_error');} else {jQuery('#gform_wrapper_3').removeClass('gform_validation_error');}setTimeout( function() { /* delay the scroll by 50 milliseconds to fix a bug in chrome */ }, 50 );if(window['gformInitDatepicker']) {gformInitDatepicker();}if(window['gformInitPriceFields']) {gformInitPriceFields();}var current_page = jQuery('#gform_source_page_number_3').val();gformInitSpinner( 3, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery(document).trigger('gform_page_loaded', [3, current_page]);window['gf_submitting_3'] = false;}else if(!is_redirect){var confirmation_content = jQuery(this).contents().find('.GF_AJAX_POSTBACK').html();if(!confirmation_content){confirmation_content = contents;}setTimeout(function(){jQuery('#gform_wrapper_3').replaceWith(confirmation_content);jQuery(document).trigger('gform_confirmation_loaded', [3]);window['gf_submitting_3'] = false;wp.a11y.speak(jQuery('#gform_confirmation_message_3').text());}, 50);}else{jQuery('#gform_3').append(contents);if(window['gformRedirect']) {gformRedirect();}}jQuery(document).trigger(\"gform_pre_post_render\", [{ formId: \"3\", currentPage: \"current_page\", abort: function() { this.preventDefault(); } }]); if (event.defaultPrevented) { return; } const gformWrapperDiv = document.getElementById( \"gform_wrapper_3\" ); if ( gformWrapperDiv ) { const visibilitySpan = document.createElement( \"span\" ); visibilitySpan.id = \"gform_visibility_test_3\"; gformWrapperDiv.insertAdjacentElement( \"afterend\", visibilitySpan ); } const visibilityTestDiv = document.getElementById( \"gform_visibility_test_3\" ); let postRenderFired = false; function triggerPostRender() { if ( postRenderFired ) { return; } postRenderFired = true; jQuery( document ).trigger( 'gform_post_render', [3, current_page] ); gform.utils.trigger( { event: 'gform/postRender', native: false, data: { formId: 3, currentPage: current_page } } ); if ( visibilityTestDiv ) { visibilityTestDiv.parentNode.removeChild( visibilityTestDiv ); } } function debounce( func, wait, immediate ) { var timeout; return function() { var context = this, args = arguments; var later = function() { timeout = null; if ( !immediate ) func.apply( context, args ); }; var callNow = immediate && !timeout; clearTimeout( timeout ); timeout = setTimeout( later, wait ); if ( callNow ) func.apply( context, args ); }; } const debouncedTriggerPostRender = debounce( function() { triggerPostRender(); }, 200 ); if ( visibilityTestDiv && visibilityTestDiv.offsetParent === null ) { const observer = new MutationObserver( ( mutations ) => { mutations.forEach( ( mutation ) => { if ( mutation.type === 'attributes' && visibilityTestDiv.offsetParent !== null ) { debouncedTriggerPostRender(); observer.disconnect(); } }); }); observer.observe( document.body, { attributes: true, childList: false, subtree: true, attributeFilter: [ 'style', 'class' ], }); } else { triggerPostRender(); } } );} ); /* ]]&gt; */"
  },
  "docs/resources/faqs/external-content/use-cases/pump-health-monitori.html": {
    "href": "docs/resources/faqs/external-content/use-cases/pump-health-monitori.html",
    "title": "Pump Health Monitoring in Water Utilities | XMPro",
    "summary": "Pump Health Monitoring in Water Utilities url XMPro Solution for Pump Health Monitoring in Water Utilities Introduction Effective pump operation is crucial for water utilities to ensure reliable water distribution and wastewater management. XMPro's solution focuses on monitoring pump health to prevent failures and optimize performance. The Challenge Water utilities face several challenges in pump operation: Early Failure Detection: Identifying signs of XMPro Solution for Pump Health Monitoring in Water Utilities Introduction Effective pump operation is crucial for water utilities to ensure reliable water distribution and wastewater management. XMPro’s solution focuses on monitoring pump health to prevent failures and optimize performance. The Challenge Water utilities face several challenges in pump operation: Early Failure Detection: Identifying signs of wear or impending failure in pump components to prevent breakdowns. Efficiency Optimization: Ensuring pumps operate at optimal efficiency to reduce energy consumption and operational costs. Maintenance Scheduling: Balancing the need for regular maintenance with minimizing downtime and service disruptions. The Solution: XMPro’s Pump Health Monitoring XMPro’s solution employs advanced sensors, data analytics, and predictive maintenance strategies to monitor and maintain pump health. Key Features Sensor Data Integration: Utilizing existing sensors to continuously monitor critical pump parameters such as vibration, temperature, flow rate, and pressure. XMPro’s Data Stream Designer integrates this sensor data, providing a comprehensive view of pump performance and condition. Predictive Analytics for Maintenance: Implementing machine learning algorithms to analyze sensor data and predict potential pump issues, such as bearing failures or seal leaks. Predictive insights assist in scheduling maintenance activities before issues lead to pump failures. Real-Time Monitoring and Alerts: Providing real-time monitoring of pump conditions, with an alert system that notifies maintenance teams of any detected anomalies requiring immediate attention. Customizable Dashboards and Reporting: Offering customizable dashboards that present key data on pump health, alongside comprehensive reporting features for maintenance planning and regulatory compliance. Figure 1. Real-Time Water Utilities Asset Overview Dashboard Real-Time Water Utilities Asset Overview Dashboard This comprehensive dashboard provides users with an up-to-the-minute view of their water utility assets. It features an interactive map that dynamically updates with the condition of various assets such as treatment plants, pump stations, reservoirs, and the pipe network, offering a clear visual representation of the water distribution system. Each asset on the map is marked with a color-coded status icon, indicating its current operational state, including active status and any alerts or error messages. The dashboard comprehensively displays the overall status of various asset categories, including treatment plants, pump stations, reservoirs, pipe networks, and metering systems. It also highlights all active recommendations generated by the system’s rule logic. This includes critical alerts like abnormal discharge pressures or potential leaks, ensuring immediate attention to potential issues. Additionally, the dashboard includes a detailed graph that tracks maintenance requirements across assets. It prioritizes assets based on their upcoming service needs, facilitating efficient maintenance scheduling. Each section of the dashboard is designed for deeper exploration. Users can drill down into specific asset and recommendation details, gaining granular insights and enabling targeted actions based on the system’s recommendations. This level of detail ensures that users can make informed decisions quickly and maintain optimal operational efficiency in managing water utility infrastructure. Figure 2. Asset Class Drill Down View – Pumps Asset Class Drilldown View – Pumps This specialized asset view for pumps in water utilities provides a comprehensive dashboard, offering essential insights into pump operations and health. Alerts Overview: This section graphically displays open alerts related to pump performance and condition, categorized by severity levels – Low , medium, and high severity. This visualization assists in quickly pinpointing pumps that require immediate attention due to potential issues like abnormal vibrations, temperature fluctuations, or efficiency drops. Work Order Status: The dashboard shows the current status of maintenance activities for pumps, categorized as available (no immediate action needed), in planning (maintenance scheduled), or waiting (urgent maintenance required). This helps in prioritizing maintenance tasks and allocating resources efficiently. Performance Metrics (Last 30 Days): It summarizes key metrics related to pump health, including new alerts, number of work orders initiated, open work orders, and open work requests. The dashboard also tracks the duration from alert initiation to work order completion, providing a comparative analysis with the previous 30-day period. Pump Filtering and Maintenance Information: Users can filter and view specific pumps, accessing detailed information such as the last maintenance date, upcoming scheduled maintenance, and due dates. This feature is crucial for planning preventive maintenance and avoiding unexpected downtimes. Recent Recommendations: This area lists the latest recommendations generated for pump maintenance, based on predictive analysis and real-time monitoring data. Users can view detailed information for each recommendation and take proactive steps to address potential issues. XMPro Co-Pilot Integration: The dashboard includes an interactive XMPro Co-Pilot feature, where users can input queries related to pump maintenance or operational issues. The AI model, trained on relevant internal data such as pump specifications and historical performance data, provides specific guidance on addressing identified issues. This advice can be directly linked to work order requests and triage instructions. This Asset Drill Down View is tailored for effective pump management in water utilities, enabling operators to swiftly access critical information, make informed decisions, and ensure the optimal performance and reliability of their pump assets. Figure 3. Asset Analysis View – Pump Health Asset Analysis View – Pump Health This Asset Analysis View offers detailed insights into specific pumps within the water utility system, focusing on a particular pump identified as Pump PMP001 Comprehensive Pump Health Metrics: This section displays vital health indicators for Pump PMP001, including vibration levels, temperature, flow rate, and pressure. Real-time data is combined with predictive analytics, enabling forecasts of potential issues and aiding in proactive maintenance. Interactive 2D and 3D Pump Models: The dashboard presents detailed 2D and 3D models of Pump PMP001, with features that allow for an expanded view of specific components. Areas flagged for potential wear or failure, such as impeller degradation or seal leaks, are highlighted for quick identification. For instance, components showing abnormal vibration or temperature readings are distinctly color-marked. Error Identification and Proactive Recommendations: Clickable sections in the pump model lead users to specific error details and associated recommendations. This integration with XMPro’s Recommendation Manager streamlines the process for identifying and addressing pump-related issues. Detailed Information on Pump PMP001: The dashboard provides a comprehensive profile of Pump PMP001, including its type, operational history, and unique characteristics. This information is crucial for understanding its maintenance and operational requirements. XMPro Co-Pilot Integration: Incorporating XMPro Co-Pilot, this feature utilizes AI, trained on datasets such as historical performance data and maintenance records, to offer specific guidance for issues related to Pump PMP001. This AI-driven assistance supports informed decision-making and enhances the efficiency of maintenance processes. This Asset Analysis View is specifically designed to provide a complete picture of the health of Pump PMP001, combining sophisticated visual models with data-driven insights and AI-powered recommendations for effective pump management in the water utility industry. Why XMPro iDTS? XMPro’s Intelligent Digital Twin Suite (iDTS) is uniquely equipped to address the complexities of Bogie Health Monitoring in the rail industry, utilizing cutting-edge technology and analytics. Here’s how XMPro iDTS excels in this application: Intelligent Digital Twin Creation for Pumps: XMPro iDTS can create a digital twin of each pump, providing a virtual representation that mirrors the real-world conditions of the pump. This digital twin continuously updates with data from sensors, allowing for real-time monitoring and analysis. Advanced Sensor Data Integration & Transformation: The suite integrates data from various sensors installed on pumps, such as vibration, temperature, flow rate, and pressure sensors. XMPro’s ability to aggregate and interpret this data is key to monitoring pump health and identifying potential issues early. Predictive Analytics for Early Warning: Utilizing machine learning algorithms, XMPro iDTS analyzes historical and real-time sensor data to predict potential pump failures or maintenance needs. This predictive capability allows for proactive maintenance scheduling, reducing downtime and preventing catastrophic failures. Maintenance Scheduling Optimization: The suite helps optimize maintenance schedules based on actual pump conditions and predictive insights, shifting from a reactive to a proactive maintenance approach. Real-Time Alerts and Proactive Decision Making: XMPro iDTS provides real-time monitoring of pump conditions. It can generate instant alerts when parameters like temperature or vibration exceed predefined thresholds, as in the case of an overheating pump. Customizable Dashboards for Enhanced Decision-Making: XMPro iDTS includes customizable dashboards that display key pump health data in an easy-to-understand format. These dashboards can be tailored to the specific needs of water utility operators, providing them with actionable insights. Scalability and Flexibility – Start Small, Scale Fast: XMPro iDTS is scalable and flexible, capable of adapting to projects of all sizes, from single asset solutions, to comprehensive Common Operating Pictures of multiple asset classes. Enhanced Safety and Operational Efficiency: XMPro iDTS provides decision support tools that help prioritize maintenance activities based on the severity and urgency of detected issues. Quick Time To Value – XMPro Blueprints Utilize XMPro blueprints, pre-configured for pump health monitoring to quickly set up the digital twin dashboard. These blueprints integrate industry best practices, ensuring a swift and effective implementation. In summary, XMPro iDTS addresses the pump health monitoring use case by offering a comprehensive, real-time, predictive, and integrated solution. Its capabilities in creating digital twins, advanced sensor data integration, predictive analytics, and customizable dashboards make it a powerful tool for enhancing the reliability and efficiency of pump operations in water utilities. Not Sure How To Get Started? No matter where you are on your digital transformation journey, the expert team at XMPro can help guide you every step of the way - We have helped clients successfully implement and deploy projects with Over 10x ROI in only a matter of weeks! Request a free online consultation for your business problem. \"*\" indicates required fields Name*Email*Company / Organization*Job Role*Contact Department*SalesSupportMessage*CaptchaNameThis field is for validation purposes and should be left unchanged. Δdocument.getElementById( \"ak_js_3\" ).setAttribute( \"value\", ( new Date() ).getTime() ); /* <![CDATA[ */ gform.initializeOnLoaded( function() {gformInitSpinner( 3, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery('#gform_ajax_frame_3').on('load',function(){var contents = jQuery(this).contents().find('*').html();var is_postback = contents.indexOf('GF_AJAX_POSTBACK') >= 0;if(!is_postback){return;}var form_content = jQuery(this).contents().find('#gform_wrapper_3');var is_confirmation = jQuery(this).contents().find('#gform_confirmation_wrapper_3').length > 0;var is_redirect = contents.indexOf('gformRedirect(){') >= 0;var is_form = form_content.length > 0 && ! is_redirect && ! is_confirmation;var mt = parseInt(jQuery('html').css('margin-top'), 10) + parseInt(jQuery('body').css('margin-top'), 10) + 100;if(is_form){jQuery('#gform_wrapper_3').html(form_content.html());if(form_content.hasClass('gform_validation_error')){jQuery('#gform_wrapper_3').addClass('gform_validation_error');} else {jQuery('#gform_wrapper_3').removeClass('gform_validation_error');}setTimeout( function() { /* delay the scroll by 50 milliseconds to fix a bug in chrome */ }, 50 );if(window['gformInitDatepicker']) {gformInitDatepicker();}if(window['gformInitPriceFields']) {gformInitPriceFields();}var current_page = jQuery('#gform_source_page_number_3').val();gformInitSpinner( 3, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery(document).trigger('gform_page_loaded', [3, current_page]);window['gf_submitting_3'] = false;}else if(!is_redirect){var confirmation_content = jQuery(this).contents().find('.GF_AJAX_POSTBACK').html();if(!confirmation_content){confirmation_content = contents;}setTimeout(function(){jQuery('#gform_wrapper_3').replaceWith(confirmation_content);jQuery(document).trigger('gform_confirmation_loaded', [3]);window['gf_submitting_3'] = false;wp.a11y.speak(jQuery('#gform_confirmation_message_3').text());}, 50);}else{jQuery('#gform_3').append(contents);if(window['gformRedirect']) {gformRedirect();}}jQuery(document).trigger(\"gform_pre_post_render\", [{ formId: \"3\", currentPage: \"current_page\", abort: function() { this.preventDefault(); } }]); if (event.defaultPrevented) { return; } const gformWrapperDiv = document.getElementById( \"gform_wrapper_3\" ); if ( gformWrapperDiv ) { const visibilitySpan = document.createElement( \"span\" ); visibilitySpan.id = \"gform_visibility_test_3\"; gformWrapperDiv.insertAdjacentElement( \"afterend\", visibilitySpan ); } const visibilityTestDiv = document.getElementById( \"gform_visibility_test_3\" ); let postRenderFired = false; function triggerPostRender() { if ( postRenderFired ) { return; } postRenderFired = true; jQuery( document ).trigger( 'gform_post_render', [3, current_page] ); gform.utils.trigger( { event: 'gform/postRender', native: false, data: { formId: 3, currentPage: current_page } } ); if ( visibilityTestDiv ) { visibilityTestDiv.parentNode.removeChild( visibilityTestDiv ); } } function debounce( func, wait, immediate ) { var timeout; return function() { var context = this, args = arguments; var later = function() { timeout = null; if ( !immediate ) func.apply( context, args ); }; var callNow = immediate && !timeout; clearTimeout( timeout ); timeout = setTimeout( later, wait ); if ( callNow ) func.apply( context, args ); }; } const debouncedTriggerPostRender = debounce( function() { triggerPostRender(); }, 200 ); if ( visibilityTestDiv && visibilityTestDiv.offsetParent === null ) { const observer = new MutationObserver( ( mutations ) => { mutations.forEach( ( mutation ) => { if ( mutation.type === 'attributes' && visibilityTestDiv.offsetParent !== null ) { debouncedTriggerPostRender(); observer.disconnect(); } }); }); observer.observe( document.body, { attributes: true, childList: false, subtree: true, attributeFilter: [ 'style', 'class' ], }); } else { triggerPostRender(); } } );} ); /* ]]&gt; */"
  },
  "docs/resources/faqs/external-content/use-cases/pumping-station-oee-.html": {
    "href": "docs/resources/faqs/external-content/use-cases/pumping-station-oee-.html",
    "title": "Pumping Station OEE | XMPro",
    "summary": "Pumping Station OEE url XMPro Pumping Station OEE Solution for Water Utilities The Problem Water utilities face significant challenges in maintaining the efficiency and reliability of pumping stations, particularly in preventing catastrophic engine room failures that can impact the entire treatment process: Risk of Catastrophic Failures: Potential for major engine room failures leading to extensive equipment damage. Impact on Treatment Stages: Failures affecting primary, secondary, and tertiary water treatment stages. Need for Proactive Maintenance: Difficulty in scheduling maintenance work to prevent disruptions and shutdowns. The Solution XMPro’s OEE monitoring solution provides critical insights into the overall health of equipment at all stages of water treatment, enabling proactive maintenance and reliability management. Key Features: Comprehensive Equipment Health Monitoring: Real-time monitoring of equipment across primary, secondary, and tertiary treatment stages. Predictive Analytics for Maintenance Planning: Utilizing predictive models to anticipate maintenance needs and schedule short-duration shutdowns effectively. Customizable Dashboards for Insightful Reporting: User-friendly dashboards providing insights into equipment health and operational efficiency. Automated Alerts for Early Warning: Generating alerts to inform maintenance and reliability engineers of potential issues. Benefits: Prevention of Major Failures: Early detection and intervention to prevent catastrophic engine room failures. Enhanced Treatment Process Reliability: Maintaining consistent operation across all water treatment stages. Optimized Maintenance Scheduling: Providing foresight for maintenance planning, minimizing the impact of shutdowns. Why XMPro iDTS? XMPro iDTS offers unique and innovative solutions for enhancing Overall Equipment Effectiveness (OEE) in pumping stations, particularly in the context of water utilities. Here’s how XMPro iDTS can be specifically applied to address the challenges in this area: Digital Twin for Comprehensive System Monitoring: XMPro iDTS can create a digital twin of the entire pumping station and associated water treatment systems. This digital representation allows for real-time monitoring and analysis of equipment across primary, secondary, and tertiary treatment stages, providing a holistic view of the system’s health. Predictive Analytics for Maintenance and Reliability: Utilizing advanced analytics and machine learning, XMPro iDTS can predict maintenance needs, identifying potential issues before they lead to catastrophic failures. This predictive approach enables water utilities to schedule maintenance proactively, minimizing the impact of short-duration shutdowns. Real-Time Data Integration and Analysis: XMPro iDTS integrates data from various sensors and systems in real-time, offering a comprehensive view of the operational health of the pumping station. This integration is crucial for accurate monitoring and effective decision-making. Automated Alerts and Early Warning System The system can generate automated alerts to inform maintenance and reliability engineers of potential issues, facilitating early intervention and preventing major failures. Customizable Dashboards for Operational Insights: XMPro iDTS provides customizable dashboards that offer insights into equipment performance, energy consumption, and operational efficiency. These dashboards are tailored to the needs of different stakeholders, enhancing usability and decision-making. Optimization of Treatment Processes: By continuously monitoring and analyzing data, XMPro iDTS helps optimize various treatment processes, ensuring they operate at peak efficiency and within environmental compliance standards. Scalability and Flexibility: XMPro iDTS is scalable and flexible, allowing the solution to be adapted to different sizes of operations and to integrate with various types of equipment and systems. Enhanced Decision Support: Leveraging AI-driven insights, XMPro iDTS enhances decision support, enabling water utilities to make informed decisions about maintenance, operations, and energy management. In summary, XMPro iDTS addresses the unique challenges in pumping station OEE for water utilities by providing a comprehensive, real-time, predictive, and integrated solution. Its capabilities in digital twin technology, predictive analytics, automated alerts, and effective data visualization make it a powerful tool for enhancing the efficiency, reliability, and sustainability of pumping station operations."
  },
  "docs/resources/faqs/external-content/use-cases/real-time-balanced-b.html": {
    "href": "docs/resources/faqs/external-content/use-cases/real-time-balanced-b.html",
    "title": "Real-time Balanced Business Scorecard (BBS) - XMPRO | XMPro",
    "summary": "Real-time Balanced Business Scorecard (BBS) - XMPRO url XMPro Real-Time Balanced Business Scorecard Solution: Dynamic Performance Management for Strategic Decision-Making The Problem Implementing a Balanced Business Scorecard (BBS) effectively in real-time presents several challenges: Integrating Diverse Data Sources: Consolidating data from various business functions for a comprehensive view. Real-Time Performance Metrics: Providing up-to-date metrics across different business areas. Aligning with Strategic Goals: Ensuring XMPro Real-Time Balanced Business Scorecard Solution: Dynamic Performance Management for Strategic Decision-Making The Problem Implementing a Balanced Business Scorecard (BBS) effectively in real-time presents several challenges: Integrating Diverse Data Sources: Consolidating data from various business functions for a comprehensive view. Real-Time Performance Metrics: Providing up-to-date metrics across different business areas. Aligning with Strategic Goals: Ensuring that scorecard metrics align with and support the organization’s strategic objectives. User Accessibility and Understanding: Making the scorecard accessible and understandable to all relevant stakeholders. Adaptability and Responsiveness: Quickly adapting the scorecard to reflect changes in business strategy or market conditions. #text-box-991742782 { width: 60%; } #text-box-991742782 .text-box-content { font-size: 100%; } #banner-2098329198 { padding-top: 449px; } #banner-2098329198 .bg.bg-loaded { background-image: url(https://xmpro.com/wp-content/uploads/2020/04/22.jpg); } The Solution XMPro’s Real-Time Balanced Business Scorecard Solution, utilizing XMPro iDTS, offers a dynamic approach to managing and monitoring key performance indicators aligned with business strategies. Key Features: Real-Time Data Integration: Aggregating data from finance, operations, customer service, and other relevant sources for a holistic view. Dynamic Performance Indicators: Tracking financial, customer, internal process, and learning/growth metrics in real-time. Strategic Alignment: Ensuring that all metrics are aligned with the organization’s strategic goals and objectives. Customizable Dashboards: Providing user-friendly dashboards tailored to different stakeholder needs. Alerts and Notifications: Generating automated alerts for deviations from expected performance levels. #gap-651732668 { padding-top: 30px; } Why XMPro iDTS? XMPro iDTS offers specialized capabilities that can significantly enhance the implementation and effectiveness of a Real-Time Balanced Business Scorecard (BBS). Here’s how XMPro iDTS can be specifically applied to this solution: Digital Twin for Organizational Processes: XMPro iDTS can create a digital twin of an organization’s various processes and operations. This virtual representation allows for real-time monitoring and analysis of different business areas, providing a comprehensive view that is crucial for an effective BBS. Real-Time Data Integration and Analysis: XMPro iDTS excels in integrating data from diverse sources in real-time. This capability ensures that the BBS reflects current operational realities, making it a powerful tool for immediate decision-making and strategic adjustments. Dynamic Performance Indicators: With XMPro iDTS, key performance indicators (KPIs) on the scorecard can be updated in real-time. This includes financial metrics, customer satisfaction indices, internal process efficiencies, and learning and growth metrics, providing a balanced and up-to-date view of organizational performance. Customizable and Interactive Dashboards: XMPro iDTS provides customizable dashboards that can be tailored to the needs of different stakeholders. These dashboards allow for interactive exploration of data, helping users understand the nuances behind the metrics and how they relate to strategic objectives. Automated Alerts and Notifications: The system can be configured to send automated alerts and notifications when certain thresholds are met or when there are significant deviations in performance metrics. This feature aids in proactive management and swift response to emerging issues. Alignment with Strategic Goals: XMPro iDTS ensures that the metrics and data used in the BBS are aligned with the organization’s strategic goals. This alignment is crucial for ensuring that the scorecard serves its intended purpose of guiding strategic decision-making. Scalability and Flexibility: XMPro iDTS is scalable and flexible, allowing the BBS to evolve as business strategies and market conditions change. This adaptability is key to maintaining the relevance and effectiveness of the scorecard over time. Predictive Analytics: Beyond just monitoring current performance, XMPro iDTS can utilize predictive analytics to forecast future trends based on current data. This capability can provide foresight into potential challenges and opportunities, adding a forward-looking dimension to the BBS. In summary, XMPro iDTS enhances the Real-Time Balanced Business Scorecard by providing a comprehensive, dynamic, and strategically aligned view of organizational performance. Its capabilities in real-time data integration, dynamic KPI tracking, customizable dashboards, and predictive analytics make it a powerful tool for effective performance management and strategic decision-making."
  },
  "docs/resources/faqs/external-content/use-cases/real-time-safety-mon.html": {
    "href": "docs/resources/faqs/external-content/use-cases/real-time-safety-mon.html",
    "title": "Real-time Safety Monitoring - XMPRO | XMPro",
    "summary": "Real-time Safety Monitoring - XMPRO url XMPro Real-Time Safety Monitoring Solution: Enhancing Workplace Safety and Compliance The Problem Ensuring real-time safety in industrial and operational environments is crucial but poses several challenges: Hazard Detection: Identifying potential safety hazards in real-time to prevent accidents. Compliance Monitoring: Continuously ensuring operations comply with safety regulations and standards. Incident Response: Quickly responding to safety incidents XMPro Real-Time Safety Monitoring Solution: Enhancing Workplace Safety and Compliance The Problem Ensuring real-time safety in industrial and operational environments is crucial but poses several challenges: Hazard Detection: Identifying potential safety hazards in real-time to prevent accidents. Compliance Monitoring: Continuously ensuring operations comply with safety regulations and standards. Incident Response: Quickly responding to safety incidents to minimize harm and operational disruption. Worker Health Monitoring: Keeping track of worker health and safety, especially in hazardous conditions. Data Integration and Analysis: Effectively integrating and analyzing data from various sources for comprehensive safety monitoring. #text-box-2117095243 { width: 60%; } #text-box-2117095243 .text-box-content { font-size: 100%; } #banner-1947213751 { padding-top: 460px; } #banner-1947213751 .bg.bg-loaded { background-image: url(https://xmpro.com/wp-content/uploads/2020/04/23.jpg); } The Solution XMPro’s Real-Time Safety Monitoring Solution, utilizing XMPro iDTS, offers a comprehensive approach to monitoring and enhancing safety across various operational environments. Key Metrics Monitored: Environmental Conditions: Monitoring conditions such as temperature, gas levels, and air quality. Equipment Operation: Tracking the operation of machinery and equipment for safety compliance. Worker Location and Health: Using wearables to monitor worker location, health indicators, and potential exposure to hazards. Incident Reports and Responses: Real-time tracking of incidents and the effectiveness of response measures. Compliance Metrics: Continuously monitoring compliance with safety regulations and standards. #gap-213258349 { padding-top: 30px; } Why XMPro iDTS? XMPro iDTS offers specialized solutions to address the challenges associated with real-time safety monitoring in various operational environments. Here’s how XMPro iDTS can be specifically applied to enhance the Real-Time Safety Monitoring Solution: Digital Twin for Safety Simulation and Analysis: XMPro iDTS can create a digital twin of the operational environment, providing a virtual representation that mirrors the physical workspace. This allows for real-time simulation and analysis of safety conditions, enabling predictive hazard detection and the planning of safety measures. Integrated Real-Time Monitoring: Utilizing IoT sensors and wearable technology, XMPro iDTS can integrate real-time data monitoring of environmental conditions, equipment operation, and worker health. This comprehensive monitoring enables immediate identification of potential safety risks. Predictive Analytics for Hazard Detection: XMPro iDTS employs advanced analytics and machine learning to predict potential safety hazards before they occur. By analyzing trends and patterns in the data, the system can alert safety managers to risks, allowing for proactive safety measures. Automated Alerts and Incident Response: XMPro iDTS can generate automated alerts in response to detected safety hazards, facilitating quick action. It can also guide effective incident response strategies, helping to minimize harm and operational disruption. Customizable Dashboards for Safety Management: XMPro iDTS provides advanced data visualization tools and customizable dashboards tailored to the needs of safety officers and operational managers. These dashboards offer real-time insights into safety metrics, enhancing decision-making and response capabilities. Compliance Monitoring and Reporting: Continuous monitoring of compliance metrics ensures that operations adhere to safety regulations and standards. XMPro iDTS can also facilitate automated reporting for regulatory compliance and internal safety audits. Worker Health and Location Tracking: By integrating wearable technology, XMPro iDTS can monitor the location and health indicators of workers, especially in hazardous conditions, contributing to enhanced worker safety and rapid response in case of emergencies. Scalability and Flexibility: XMPro iDTS is scalable and flexible, meaning it can be adapted to different operational scales and can integrate additional sensors or safety scenarios as needs evolve. In summary, XMPro iDTS addresses the unique challenges in real-time safety monitoring by providing a comprehensive, real-time, predictive, and integrated solution. Its use of digital twin technology, combined with advanced analytics, automated alerts, and effective data visualization, makes it a powerful tool for enhancing workplace safety, ensuring compliance, and protecting workers in various operational environments."
  },
  "docs/resources/faqs/external-content/use-cases/short-term-inventory.html": {
    "href": "docs/resources/faqs/external-content/use-cases/short-term-inventory.html",
    "title": "Short Term Inventory Planning | XMPro",
    "summary": "Short Term Inventory Planning url XMPro Short-Term Inventory Planning Solution: Optimizing Inventory Management for Agile and Efficient Operations The Problem Effective short-term inventory planning is crucial for businesses to maintain operational efficiency, meet customer demands, and minimize costs. However, several challenges can impede optimal inventory management: Demand Forecasting: Accurately predicting short-term product demand is challenging, leading to either stock shortagesTags: Discrete Manufacturing / Solutions XMPro Short-Term Inventory Planning Solution: Optimizing Inventory Management for Agile and Efficient Operations The Problem Effective short-term inventory planning is crucial for businesses to maintain operational efficiency, meet customer demands, and minimize costs. However, several challenges can impede optimal inventory management: Demand Forecasting: Accurately predicting short-term product demand is challenging, leading to either stock shortages or excess inventory. Supply Chain Disruptions: Unforeseen disruptions in the supply chain can quickly render inventory plans obsolete, causing delays and increased costs. Resource Allocation: Efficient allocation of resources for inventory management, including storage space and logistics, is complex and dynamic. Data Integration: Integrating data from various sources (sales, supply chain, market trends) for informed decision-making is often cumbersome. Rapid Response to Market Changes: Adapting quickly to market fluctuations or changes in customer preferences is essential for maintaining service levels and competitiveness. The Solution XMPro’s Short-Term Inventory Planning Solution, utilizing XMPro iDTS, offers a dynamic and data-driven approach to managing inventory effectively. Key Metrics Monitored: Inventory Levels: Real-time monitoring of stock levels across different locations. Demand Patterns: Analyzing sales data and market trends to predict short-term demand. Supplier Performance: Tracking supplier reliability and lead times. Storage Utilization: Monitoring storage space usage and optimizing warehouse operations. Order Fulfillment Rates: Tracking the rate of successful order fulfillments against customer demands. Why XMPro iDTS? XMPro iDTS offers specialized solutions to address the challenges associated with short-term inventory planning. Here’s how XMPro iDTS can be specifically applied to enhance the Short-Term Inventory Planning Solution: Digital Twin for Inventory and Supply Chain: XMPro iDTS can create a digital twin of the entire inventory and supply chain system. This allows for real-time visualization and simulation of inventory levels, supply chain dynamics, and warehouse operations, enabling more accurate and dynamic planning. Advanced Predictive Analytics for Demand Forecasting: Utilizing machine learning and predictive analytics, XMPro iDTS can analyze historical sales data, market trends, and other relevant factors to accurately predict short-term product demand. This helps in maintaining optimal inventory levels and reducing the risk of stockouts or overstocking. Real-Time Data Integration and Analysis: XMPro iDTS can integrate data from various sources, including ERP systems, supply chain management tools, and IoT sensors in warehouses. This integration provides a comprehensive view of the inventory status, supplier performance, and demand patterns, facilitating informed decision-making. Automated Alerts and Recommendations: XMPro iDTS can automate the generation of alerts and recommendations for inventory replenishment, reordering, and redistribution based on real-time data and predictive insights. This feature ensures timely and efficient inventory management. Optimization of Resource Allocation: XMPro iDTS can optimize the allocation of resources, such as storage space and logistics, by analyzing current inventory levels, predicted demand, and supply chain constraints. This leads to more efficient warehouse operations and reduced operational costs. Customizable Dashboards and Reporting: XMPro iDTS provides advanced data visualization tools and customizable dashboards, essential for monitoring inventory and supply chain operations. These tools enable different stakeholders to access relevant information, understand trends, and make quick decisions. Scalability and Flexibility: XMPro iDTS is scalable, meaning it can be expanded to accommodate additional data sources, warehouses, or supply chain components as the business grows. Rapid Response to Market Changes: By continuously monitoring market trends and customer demands, XMPro iDTS helps businesses adapt quickly to changes, maintaining service levels and competitiveness. In summary, XMPro iDTS addresses the unique challenges in short-term inventory planning by providing a comprehensive, real-time, predictive, and integrated solution. Its use of digital twin technology, combined with advanced analytics, automated guidance, and effective data visualization, makes it a powerful tool for optimizing inventory management and responding agilely to market demands."
  },
  "docs/resources/faqs/external-content/use-cases/strategic-performanc.html": {
    "href": "docs/resources/faqs/external-content/use-cases/strategic-performanc.html",
    "title": "Strategic Performance & Safety Oversight for Global Mining Operations | XMPro",
    "summary": "Strategic Performance & Safety Oversight for Global Mining Operations url Strategic Performance & Safety Oversight for Global Mining Operations Introduction: In the mining sector, strategic management of multi-site operations necessitates comprehensive oversight of performance and safety measures. The dynamic nature of global mining operations requires precise and effective handling of both production goals and the well-being of the workforce. The Challenge: The key challenges for Strategic Performance & Safety Oversight for Global Mining Operations Introduction: In the mining sector, strategic management of multi-site operations necessitates comprehensive oversight of performance and safety measures. The dynamic nature of global mining operations requires precise and effective handling of both production goals and the well-being of the workforce. The Challenge: The key challenges for strategic oversight in global mining include managing complex logistics across diverse geographical locations, ensuring the safety of personnel in hazardous conditions, and meeting production targets. Other challenges encompass: Environmental Impact: Balancing productivity with sustainable practices and minimizing environmental footprints. Compliance and Regulations: Adhering to strict regulations across different regions and maintaining high safety standards. Technological Integration: Incorporating advanced technologies for better data analysis and predictive maintenance. Risk Management: Identifying and mitigating operational risks in real-time to maintain safety and efficiency. Cross-Functional Coordination: Ensuring seamless communication and coordination among various departments and sites. Effective strategic oversight aims to address these multifaceted challenges through robust analytics, real-time monitoring, and proactive policy-making, ensuring resilience and sustainability in mining operations. The Solution: XMPro iBOS for Strategic Performance & Safety Oversight in Global Mining XMPro’s Intelligent Business Operations Suite (iBOS) expertly aligns with the strategic oversight needs of global mining operations, offering a powerful suite for monitoring performance and safety across multiple sites. Key Features: Holistic Asset Analytics: XMPro iBOS consolidates data across various assets, offering a 360-degree view of asset health and productivity, crucial for making informed maintenance decisions. Predictive Safety and Performance Modeling: Leveraging predictive analytics, XMPro iBOS anticipates potential risks and performance bottlenecks, facilitating preemptive actions to maintain high safety and performance standards. Comprehensive Digital Twin Interface: It provides detailed simulations of mining operations, enabling scenario planning and optimization strategies for asset maintenance. Maintenance Automation: Integrating predictive insights, the platform ensures maintenance activities are scheduled efficiently, reducing downtime and preserving operational continuity. Enhanced Executive Dashboards: Configurable dashboards present executives with a clear visualization of KPIs, safety metrics, and operational status, streamlining decision-making processes. XMPro iBOS equips leaders in the mining industry with a robust tool for maintaining oversight over safety, performance, and maintenance, bolstering productivity and compliance with safety regulations. Discover XMPro’s Strategic Performance & Safety Oversight Solution for Global Mining Operations Figure 1: Global Operations Overview for Strategic Performance & Safety in Mining Overview This executive dashboard is tailored for global mining operations, offering a strategic view that encompasses multiple sites. It integrates key performance indicators (KPIs) and safety metrics, giving leaders an expansive view of their operations. Key Features Global Production Tracking: Displays mining production data such as total tonnes mined, processed, and produced, against budgeted targets for comprehensive fiscal management. Safety and Compliance Monitoring: Incorporates safety statistics, including incident rates and compliance benchmarks, crucial for maintaining high safety standards. Executive Recommendations: Features actionable executive insights for strategic decisions, addressing immediate operational concerns. Site-Specific KPIs: Allows leaders to drill down into individual site performances, correlating them with global objectives. Interactive Global Map: Provides a geo-visual representation of production and safety data for all sites, enhancing global situational awareness. Benefits This Global Operations Overview is vital for executives to maintain a grasp on the pulse of multi-site mining operations, ensuring strategic alignment, performance optimization, and the upholding of stringent safety protocols. It supports informed decision-making with real-time data, facilitating a proactive approach to global mining management.” For detailed operational insights and a comprehensive understanding of each feature, the specific dashboard view can be referred to directly. Figure 2: Individual Site View – Strategic Performance & Safety Oversight for Mining Operations This dashboard delivers a concentrated analysis of production and safety KPIs at an individual mining site, just some of the features include the following: Key Features Production KPIs: The site-specific dashboard displays critical production metrics like tons mined and processed, providing a clear view of operational performance against set targets. Safety Metrics: A dedicated section for safety showcases the Total Recordable Injury Frequency Rate and fatality counts, emphasizing the site’s focus on safe operation practices. Environmental Impact: Displays environmental sustainability KPIs, such as Greenhouse Gas emissions, water usage, and tailings produced, ensuring environmental stewardship is measurable and front of mind. Executive Recommendations: High-priority alerts and recommended actions are highlighted to address operational exceptions swiftly, facilitating immediate executive intervention where necessary. Operational Trends: Offers a visual representation of historical production data, which is crucial for identifying trends and areas of improvement over time. Detailed Analysis: The site view supports drilling down into specific operational areas, enabling a closer look at individual performance indicators for a comprehensive understanding. Actionable Insights: The integration of data analytics provides strategic insights, allowing for informed decision-making to enhance site productivity and safety measures. The “Individual Site View” equips site managers and executives with the data to strategically steer site-specific operations, highlighting areas for improvement and enforcing high performance and safety standards. Why XMPro iBOS for Mining Plant Operations? XMPro’s Intelligent Business Operations Suite (iBOS) is expertly devised for the intricate challenges faced in the predictive maintenance of mobile assets within the mining industry. Here’s the transformation it brings: Advanced Intelligent Digital Twin Modeling: XMPro iBOS constructs sophisticated digital twins, reflecting the detailed operations of mining equipment. It allows comprehensive performance analysis under varied conditions, vital for operational optimization. Advanced Sensor Data Integration & Transformation: Real-time sensor data across mobile assets offer critical insights into performance metrics like vibration, load capacity, and engine status, which are essential for detecting early signs of potential failures and maintenance needs. Predictive Analytics for Performance Enhancement: Utilizing advanced analytics, XMPro iBOS predicts potential asset failures, enhancing operational parameters and enabling preventive adjustments, thereby ensuring continuous mining operations with minimized downtimes. Maintenance Scheduling Optimization: Performance data drives XMPro iBOS’s maintenance scheduling, transforming the approach from reactive to proactive, optimizing the maintenance cycle for various assets, and significantly reducing breakdowns. Real-Time Monitoring and Predictive Alerting: Real-time monitoring and predictive alerting are critical components of XMPro’s iBOS for managing mobile assets within the mining industry. This ensures each mobile asset, from haul trucks to dozers, functions within the optimal parameters, thus enhancing efficiency and reducing reliance on manual intervention. Configurable and Interactive Dashboards: XMPro provides configurable dashboards that offer real-time insights into the health and performance of equipment across all dairy processing plants. These dashboards are designed to be interactive, enabling detailed scrutiny of specific operational aspects and supporting centralized management decisions. Scalability and Flexibility – Start Small, Scale Fast: Designed to accommodate dairy operations of any scale, XMPro’s modular architecture allows for seamless integration and adaptability. This scalability ensures that mining plants can efficiently manage operations as they expand or adapt to changing market demands. Enhanced Safety & Operational Efficiency: XMPro boosts operational safety by identifying potential hazards and inefficiencies in the processing line, ensuring that all equipment operates within safe and optimal parameters. This contributes to a safer working environment and more efficient production processes. XMPro Blueprints – Quick Time to Value: Offering quick time-to-value, XMPro Blueprints facilitate rapid deployment of intelligent operations solutions across mining operations. These templates are built on industry best practices, ensuring that plants can quickly realize the benefits of digital transformation. XMPro iBOS caters to the predictive maintenance needs of the mining industry’s mobile assets with a suite that promises comprehensive, predictive, and integrated solutions, driving efficiency and safety across operations. Not Sure How To Get Started? No matter where you are on your digital transformation journey, the expert team at XMPro can help guide you every step of the way - We have helped clients successfully implement and deploy projects with Over 10x ROI in only a matter of weeks! Request a free online consultation for your business problem. \"*\" indicates required fields Name*Email*Company / Organization*Job Role*Contact Department*SalesSupportMessage*CaptchaPhoneThis field is for validation purposes and should be left unchanged. Δdocument.getElementById( \"ak_js_3\" ).setAttribute( \"value\", ( new Date() ).getTime() ); /* <![CDATA[ */ gform.initializeOnLoaded( function() {gformInitSpinner( 3, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery('#gform_ajax_frame_3').on('load',function(){var contents = jQuery(this).contents().find('*').html();var is_postback = contents.indexOf('GF_AJAX_POSTBACK') >= 0;if(!is_postback){return;}var form_content = jQuery(this).contents().find('#gform_wrapper_3');var is_confirmation = jQuery(this).contents().find('#gform_confirmation_wrapper_3').length > 0;var is_redirect = contents.indexOf('gformRedirect(){') >= 0;var is_form = form_content.length > 0 && ! is_redirect && ! is_confirmation;var mt = parseInt(jQuery('html').css('margin-top'), 10) + parseInt(jQuery('body').css('margin-top'), 10) + 100;if(is_form){jQuery('#gform_wrapper_3').html(form_content.html());if(form_content.hasClass('gform_validation_error')){jQuery('#gform_wrapper_3').addClass('gform_validation_error');} else {jQuery('#gform_wrapper_3').removeClass('gform_validation_error');}setTimeout( function() { /* delay the scroll by 50 milliseconds to fix a bug in chrome */ }, 50 );if(window['gformInitDatepicker']) {gformInitDatepicker();}if(window['gformInitPriceFields']) {gformInitPriceFields();}var current_page = jQuery('#gform_source_page_number_3').val();gformInitSpinner( 3, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery(document).trigger('gform_page_loaded', [3, current_page]);window['gf_submitting_3'] = false;}else if(!is_redirect){var confirmation_content = jQuery(this).contents().find('.GF_AJAX_POSTBACK').html();if(!confirmation_content){confirmation_content = contents;}setTimeout(function(){jQuery('#gform_wrapper_3').replaceWith(confirmation_content);jQuery(document).trigger('gform_confirmation_loaded', [3]);window['gf_submitting_3'] = false;wp.a11y.speak(jQuery('#gform_confirmation_message_3').text());}, 50);}else{jQuery('#gform_3').append(contents);if(window['gformRedirect']) {gformRedirect();}}jQuery(document).trigger(\"gform_pre_post_render\", [{ formId: \"3\", currentPage: \"current_page\", abort: function() { this.preventDefault(); } }]); if (event.defaultPrevented) { return; } const gformWrapperDiv = document.getElementById( \"gform_wrapper_3\" ); if ( gformWrapperDiv ) { const visibilitySpan = document.createElement( \"span\" ); visibilitySpan.id = \"gform_visibility_test_3\"; gformWrapperDiv.insertAdjacentElement( \"afterend\", visibilitySpan ); } const visibilityTestDiv = document.getElementById( \"gform_visibility_test_3\" ); let postRenderFired = false; function triggerPostRender() { if ( postRenderFired ) { return; } postRenderFired = true; jQuery( document ).trigger( 'gform_post_render', [3, current_page] ); gform.utils.trigger( { event: 'gform/postRender', native: false, data: { formId: 3, currentPage: current_page } } ); if ( visibilityTestDiv ) { visibilityTestDiv.parentNode.removeChild( visibilityTestDiv ); } } function debounce( func, wait, immediate ) { var timeout; return function() { var context = this, args = arguments; var later = function() { timeout = null; if ( !immediate ) func.apply( context, args ); }; var callNow = immediate && !timeout; clearTimeout( timeout ); timeout = setTimeout( later, wait ); if ( callNow ) func.apply( context, args ); }; } const debouncedTriggerPostRender = debounce( function() { triggerPostRender(); }, 200 ); if ( visibilityTestDiv && visibilityTestDiv.offsetParent === null ) { const observer = new MutationObserver( ( mutations ) => { mutations.forEach( ( mutation ) => { if ( mutation.type === 'attributes' && visibilityTestDiv.offsetParent !== null ) { debouncedTriggerPostRender(); observer.disconnect(); } }); }); observer.observe( document.body, { attributes: true, childList: false, subtree: true, attributeFilter: [ 'style', 'class' ], }); } else { triggerPostRender(); } } );} ); /* ]]&gt; */"
  },
  "docs/resources/faqs/external-content/use-cases/wheel-and-track-wear.html": {
    "href": "docs/resources/faqs/external-content/use-cases/wheel-and-track-wear.html",
    "title": "Wheel and Track Wear Monitoring In The Rail Industry | XMPro",
    "summary": "Wheel and Track Wear Monitoring In The Rail Industry url Wheel and Track Wear Monitoring Solution for the Rail Industry Introduction In the rail industry, the integrity of wheels and tracks is paramount for safe and efficient operations. XMPro's solution focuses on monitoring wear and tear to prevent derailments and reduce maintenance costs. The Challenge Rail systems face significant challenges in maintaining the health ofTags: Condition Monitoring Wheel and Track Wear Monitoring Solution for the Rail Industry Introduction In the rail industry, the integrity of wheels and tracks is paramount for safe and efficient operations. XMPro’s solution focuses on monitoring wear and tear to prevent derailments and reduce maintenance costs. The Challenge Rail systems face significant challenges in maintaining the health of wheels and tracks: Risk of Derailments: Abnormal wear in wheels and tracks can lead to increased risk of derailments, posing serious safety hazards. Maintenance Efficiency: Identifying the optimal frequency for maintenance activities to ensure safety while controlling costs. Operational Downtime: Unplanned maintenance and repairs can lead to significant operational downtime and disruptions. The Solution: XMPro Wheel and Track Wear Monitoring XMPro’s solution leverages data from advanced sensors and machine learning (ML) for anomaly detection, providing a proactive approach to wheel and track maintenance. Key Features Streamlining Sensor Data Integration and Transformation XMPro’s Data Stream Designer excels in integrating and transforming sensor data for rail systems. It seamlessly aggregates data from vibration and acoustic sensors on trains and tracks, utilizing XMPro’s comprehensive integration library. This system efficiently processes and interprets diverse sensor data, providing crucial insights into wheel and track wear patterns for proactive maintenance and operational decision-making. Machine Learning for Anomaly Detection: Implementing ML algorithms to analyze sensor data and detect anomalies indicating abnormal wear. Continuous learning and model refinement based on new data and identified wear patterns. Maintenance Scheduling Optimization: Using data-driven insights to optimize maintenance schedules, shifting from fixed intervals to condition-based maintenance. Real-Time Alerts and Reporting: Providing real-time alerts to maintenance teams about potential issues.Generating detailed reports on wheel and track conditions for maintenance planning and regulatory compliance. Figure 1. Real-Time Rail Asset Overview Dashboard Real-Time Rail Asset Overview Dashboard This comprehensive dashboard provides users with an up-to-the-minute view of their rail assets. It features an interactive map that dynamically updates with the GPS coordinates of trains in motion, offering a clear visual representation of their railway lines. Each asset on the map is marked with a color-coded status icon, indicating its current operational state, including active status and any alerts or error messages. The dashboard comprehensively displays the overall status of various asset categories, such as trains, crossings, tracks, maintenance vehicles, and substations. It also highlights all active recommendations generated by the system’s rule logic. This includes critical alerts like exceeded wheel wear thresholds, ensuring immediate attention to potential issues. Additionally, the dashboard includes a detailed graph that tracks maintenance requirements across assets. It prioritizes assets based on their upcoming service needs, facilitating efficient maintenance scheduling. Each section of the dashboard is designed for deeper exploration. Users can drill down into specific asset and recommendation details, gaining granular insights and enabling targeted actions based on the system’s recommendations. This level of detail ensures that users can make informed decisions quickly and maintain optimal operational efficiency. Figure 2. Asset Drill Down View – Trains Asset Drilldown View – Trains The specific asset view for trains provides users with a comprehensive and informative dashboard. Alerts Overview: Graphical representation of open alerts categorized by severity – no alerts, medium, and high severity. Work Order Status: Displays current status of each asset, categorized as available, in planning, or waiting. Performance Metrics (Last 30 Days): Summarizes key metrics such as new alerts, number of work orders, open work orders, and open work requests. It also tracks the duration from alert initiation to work order completion, comparing it with the previous 30-day period. Asset Filtering and Service Information: Enables filtering across all train assets, showing details like the last service date, upcoming service schedules, and due dates. Recent Recommendations: Lists recent recommendations triggered for train assets, with options for users to view details and take necessary actions. XMPro Co-Pilot Integration: Features an interactive block where users can query the AI model, trained on internal data like train engine manuals, for specific advice on errors, warnings, and issues. This information can be directly linked to work order requests and triage instructions. This dashboard is designed for ease of use, allowing quick access to vital information and efficient management of train assets. Figure 3. Asset Analysis View – Train T001 Asset Analysis View – Train T001 The Asset Analysis View offers detailed insights into specific assets, exemplified here by Train T001 within the train asset category. Comprehensive Asset Metrics: In this detailed view for Bogie B001, we concentrate on specific metrics crucial for monitoring wheel wear. Key metrics include Spring Compression, indicating load distribution and wear patterns; Bearing Temperature, signaling potential friction issues; Bearing Vibration Amplitude, identifying internal wear; and Wheel Vibration Amplitude, detecting uneven wear or defects. These metrics collectively provide a crucial overview for maintaining wheel health and ensuring operational safety. Interactive 2D and 3D Asset Models: Features detailed 2D and 3D models of Train T001, with capabilities to ‘explode’ the view for in-depth examination of specific components. Notably, the 3D model visually identifies issues, such as those highlighted by recommendation rules related to wheel and track wear. For instance, areas of concern, like the wheels and bogie of affected wheels, are marked in red for easy identification. Error Identification and Actionable Recommendations: Clicking on highlighted errors directs users to the associated recommendations, where a range of actions can be explored to address the issue. This functionality is integrated with XMPro’s Recommendation Manager for efficient problem resolution. Detailed Train Information: The view encompasses comprehensive information about Train T001, including its type, model, description, and manufacturer details. XMPro Co-Pilot Integration: This feature includes XMPro Co-Pilot, trained on internal company data, providing users with specific guidance and answers to queries related to Train T001. This AI-driven support enhances decision-making and problem-solving related to the asset. This Asset Analysis View is designed to provide users with a holistic understanding of Train T001, combining detailed visual models with actionable data and AI-assisted insights for effective asset management. Why XMPro iDTS? XMPro’s Intelligent Digital Twin Suite (iDTS) offers unique and innovative solutions for Wheel and Track Wear Monitoring in the rail industry, leveraging advanced technologies and analytics. Here’s how XMPro iDTS specifically addresses this challenge: Digital Twin Modeling for Rail Systems: XMPro iDTS creates a digital twin of the rail system, including detailed models of the tracks and wheels. This virtual representation allows for sophisticated simulation and analysis of wear patterns, enabling predictive maintenance and anomaly detection. Comprehensive Data Integration And Sophisticated Transformation XMPro iDTS features a comprehensive library of integrations that allow businesses to integrate and transform data from virtually any data source. In this case the solution integrates data from vibration and acoustic sensors installed on trains and tracks. Advanced Machine Learning for Anomaly Detection: Utilizing machine learning algorithms, XMPro iDTS analyzes sensor data to identify anomalies that indicate abnormal wear. This approach allows for early detection of potential issues that could lead to derailments or other safety hazards. Optimization of Maintenance Schedules: By analyzing wear patterns and predicting maintenance needs, XMPro iDTS helps optimize maintenance schedules. This shift from fixed-interval to condition-based maintenance reduces costs and prevents unnecessary downtime. Real-Time Alerts and Decision Support: XMPro iDTS provides real-time alerts to maintenance teams regarding potential wear issues. It also offers decision support tools to help prioritize maintenance activities based on the severity and urgency of detected anomalies. Customizable Dashboards and Reporting: The solution includes customizable dashboards that present key data and insights on wheel and track conditions. It also generates comprehensive reports for maintenance planning and regulatory compliance. Scalability and Flexibility – Start Small, Scale Fast: XMPro iDTS is scalable and flexible, capable of adapting to different sizes of rail networks and integrating with various types of sensor technologies. XMPro has been consistently deployed in only a matter of weeks. Enhanced Safety and Operational Efficiency: By enabling proactive maintenance and early detection of wear issues, XMPro iDTS enhances the safety and operational efficiency of rail systems, reducing the risk of accidents and improving service reliability. Quick Time To Value – XMPro Blueprints Leverage XMPro blueprints as pre-configured templates tailored for wheel and track wear monitoring. These blueprints provide a starting point for setting up the digital twin dashboard, incorporating best practices and industry standards. In summary, XMPro iDTS addresses the unique challenges of wheel and track wear monitoring in the rail industry by providing a comprehensive, real-time, predictive, and integrated solution. Its capabilities in digital twin technology, advanced sensor data integration, machine learning for anomaly detection, and effective visualization tools make it a powerful tool for enhancing rail safety, maintenance efficiency, and operational reliability. Not Sure How To Get Started? No matter where you are on your digital transformation journey, the expert team at XMPro can help guide you every step of the way - We have helped clients successfully implement and deploy projects with Over 10x ROI in only a matter of weeks! Request a free online consultation for your business problem. \"*\" indicates required fields Name*Email*Company / Organization*Job Role*Contact Department*SalesSupportMessage*CaptchaCommentsThis field is for validation purposes and should be left unchanged. Δdocument.getElementById( \"ak_js_3\" ).setAttribute( \"value\", ( new Date() ).getTime() ); /* <![CDATA[ */ gform.initializeOnLoaded( function() {gformInitSpinner( 3, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery('#gform_ajax_frame_3').on('load',function(){var contents = jQuery(this).contents().find('*').html();var is_postback = contents.indexOf('GF_AJAX_POSTBACK') >= 0;if(!is_postback){return;}var form_content = jQuery(this).contents().find('#gform_wrapper_3');var is_confirmation = jQuery(this).contents().find('#gform_confirmation_wrapper_3').length > 0;var is_redirect = contents.indexOf('gformRedirect(){') >= 0;var is_form = form_content.length > 0 && ! is_redirect && ! is_confirmation;var mt = parseInt(jQuery('html').css('margin-top'), 10) + parseInt(jQuery('body').css('margin-top'), 10) + 100;if(is_form){jQuery('#gform_wrapper_3').html(form_content.html());if(form_content.hasClass('gform_validation_error')){jQuery('#gform_wrapper_3').addClass('gform_validation_error');} else {jQuery('#gform_wrapper_3').removeClass('gform_validation_error');}setTimeout( function() { /* delay the scroll by 50 milliseconds to fix a bug in chrome */ }, 50 );if(window['gformInitDatepicker']) {gformInitDatepicker();}if(window['gformInitPriceFields']) {gformInitPriceFields();}var current_page = jQuery('#gform_source_page_number_3').val();gformInitSpinner( 3, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery(document).trigger('gform_page_loaded', [3, current_page]);window['gf_submitting_3'] = false;}else if(!is_redirect){var confirmation_content = jQuery(this).contents().find('.GF_AJAX_POSTBACK').html();if(!confirmation_content){confirmation_content = contents;}setTimeout(function(){jQuery('#gform_wrapper_3').replaceWith(confirmation_content);jQuery(document).trigger('gform_confirmation_loaded', [3]);window['gf_submitting_3'] = false;wp.a11y.speak(jQuery('#gform_confirmation_message_3').text());}, 50);}else{jQuery('#gform_3').append(contents);if(window['gformRedirect']) {gformRedirect();}}jQuery(document).trigger(\"gform_pre_post_render\", [{ formId: \"3\", currentPage: \"current_page\", abort: function() { this.preventDefault(); } }]); if (event.defaultPrevented) { return; } const gformWrapperDiv = document.getElementById( \"gform_wrapper_3\" ); if ( gformWrapperDiv ) { const visibilitySpan = document.createElement( \"span\" ); visibilitySpan.id = \"gform_visibility_test_3\"; gformWrapperDiv.insertAdjacentElement( \"afterend\", visibilitySpan ); } const visibilityTestDiv = document.getElementById( \"gform_visibility_test_3\" ); let postRenderFired = false; function triggerPostRender() { if ( postRenderFired ) { return; } postRenderFired = true; jQuery( document ).trigger( 'gform_post_render', [3, current_page] ); gform.utils.trigger( { event: 'gform/postRender', native: false, data: { formId: 3, currentPage: current_page } } ); if ( visibilityTestDiv ) { visibilityTestDiv.parentNode.removeChild( visibilityTestDiv ); } } function debounce( func, wait, immediate ) { var timeout; return function() { var context = this, args = arguments; var later = function() { timeout = null; if ( !immediate ) func.apply( context, args ); }; var callNow = immediate && !timeout; clearTimeout( timeout ); timeout = setTimeout( later, wait ); if ( callNow ) func.apply( context, args ); }; } const debouncedTriggerPostRender = debounce( function() { triggerPostRender(); }, 200 ); if ( visibilityTestDiv && visibilityTestDiv.offsetParent === null ) { const observer = new MutationObserver( ( mutations ) => { mutations.forEach( ( mutation ) => { if ( mutation.type === 'attributes' && visibilityTestDiv.offsetParent !== null ) { debouncedTriggerPostRender(); observer.disconnect(); } }); }); observer.observe( document.body, { attributes: true, childList: false, subtree: true, attributeFilter: [ 'style', 'class' ], }); } else { triggerPostRender(); } } );} ); /* ]]&gt; */"
  },
  "docs/resources/faqs/external-content/use-cases/wind-turbine-perform.html": {
    "href": "docs/resources/faqs/external-content/use-cases/wind-turbine-perform.html",
    "title": "Wind Turbine Performance Optimization | XMPro",
    "summary": "Wind Turbine Performance Optimization url XMPro Solution for Wind Turbine Performance Optimization Introduction In the renewable energy sector, optimizing the performance of wind turbines is crucial for maximizing energy output and efficiency. XMPro's solution for Wind Turbine Performance Optimization leverages advanced data analytics and intelligent digital twin technology to enhance the operational efficiency of wind turbines. The Challenge Wind turbines XMPro Solution for Wind Turbine Performance Optimization Introduction In the renewable energy sector, optimizing the performance of wind turbines is crucial for maximizing energy output and efficiency. XMPro’s solution for Wind Turbine Performance Optimization leverages advanced data analytics and intelligent digital twin technology to enhance the operational efficiency of wind turbines. The Challenge Wind turbines operate under varying environmental conditions, which can significantly impact their efficiency and energy output. Key challenges involve: Optimizing Turbine Performance: Adjusting turbine operations to maximize energy output under different wind conditions. Reducing Wear and Tear: Minimizing unnecessary stress on turbine components to extend their lifespan. Maximizing Energy Yield: Ensuring turbines operate at peak efficiency to maximize the energy yield. The Solution: XMPro’s Wind Turbine Performance Optimization XMPro’s solution employs real-time data monitoring, predictive analytics, and digital twin modeling to optimize wind turbine performance. Key Features Real-Time Data Monitoring: Utilizing IoT sensors to continuously monitor wind speed, direction, turbine rotation speed, and other critical parameters. Predictive Analytics for Performance Tuning: Analyzing sensor data with advanced algorithms to predict optimal turbine settings for different wind conditions. Digital Twin Modeling: Creating digital twins of wind turbines to simulate and analyze performance under various scenarios, aiding in decision-making for performance optimization. Automated Adjustment Recommendations: Providing automated recommendations for adjusting turbine settings such as blade pitch and rotation speed to optimize performance and energy output. Customizable Dashboards and Reporting: Offering customizable dashboards that display key performance metrics, enabling operators to monitor turbine efficiency and make informed decisions. See this Solution In Action In Our Product Tour Discover XMPro’s Wind Turbine Performance Solution Figure 1. Real-Time Renewable Asset Overview Dashboard for Wind and Solar Farms Real-Time Renewable Asset Overview Dashboard This advanced dashboard is specifically designed for operators of wind farms, providing a comprehensive view of wind turbine performance and optimization. It features an interactive map that dynamically updates with the operational status of different wind farms, offering a clear visual representation of their performance efficiency and health. Each wind farm on the map is marked with a color-coded status icon, indicating its current operational state, including active status and any alerts or error messages related to performance optimization or maintenance needs of individual turbines within that wind farm. Overview of Renewable Asset Health: The dashboard displays the overall performance status of wind turbines, highlighting areas with potential efficiency issues or optimization opportunities. It includes critical alerts such as suboptimal wind direction alignment, blade pitch adjustments, and gearbox efficiency. Performance Optimization Alerts: Utilizing data from existing sensors and advanced analytics, the dashboard provides real-time insights into optimization opportunities. It highlights turbines requiring adjustments for issues like wind direction misalignment or blade pitch inefficiencies. Maintenance Planning and Scheduling: A detailed graph tracks maintenance and performance optimization requirements across the wind farm. It prioritizes turbines based on their needs for maintenance or performance adjustments, facilitating efficient and proactive scheduling. Drill-Down Capability for In-Depth Analysis: Users can explore specific assets for detailed information, including historical performance data, recent maintenance activities, and predictive maintenance recommendations. This level of detail enables targeted actions based on the system’s predictive analytics. Customizable Alerts and Recommendations: The dashboard highlights active recommendations generated by the system’s smart rule logic and machine learning algorithms. This includes suggestions for enhancing turbine performance, addressing gearbox oil viscosity issues, and other optimization actions. Overall Asset Status Summary: At the bottom of the screen, there’s a summary of the status of different assets, including the number of active and inactive assets across various facilities like Wind Farm 1 and 2, Photovoltaic Plant 1 and 2, and Biomass Plant 1. Search Functionality: A search bar at the top allows users to search for specific data across the platform. This Real-Time Wind Turbine Performance Optimization Dashboard is an essential tool for wind farm operators, enabling them to effectively monitor and optimize the performance of their turbines. By providing real-time data, predictive insights, and actionable recommendations, it ensures informed decision-making and enhances the operational efficiency and energy output of wind turbines. Figure 2a. Real-Time Wind Farm Performance Management View Real-Time Wind Farm Performance Management Dashboard This XMPro digital dashboard, designed for Wind Farm Management, equips operators with essential tools for optimizing turbine operations and enhancing overall efficiency. Immediate Energy Output Assessment: The real-time power gauge showing current power generation in megawatts (MW) is crucial for assessing the farm’s immediate energy output. This feature allows operators to quickly identify any deviations from expected performance levels, which is key to maintaining optimal energy production. Long-Term Performance Analysis: The historical power chart displaying monthly power output data enables operators to analyze long-term performance trends. This insight is vital for strategic planning, identifying underperformance periods, and making informed decisions about maintenance and operational adjustments. Data Range: Monthly data from May to April. Targeted Turbine Maintenance and Optimization: The status table for individual wind turbines provides detailed information on each turbine’s status, power output, and performance. This targeted approach helps in pinpointing turbines that require maintenance or optimization, directly influencing the overall efficiency and reliability of the wind farm. Details: Asset name, status, power output, and performance percentage. Visual Management of Wind Farm Operations: The farm overview visualization offers a geographical representation of the wind farm, with clear indicators for each turbine’s status. This visual management tool is essential for large-scale operations, enabling quick identification and prioritization of turbines for performance optimization or maintenance. Optimization Based on Wind Conditions: The wind details section, showing real-time wind speed and direction, is critical for adjusting turbine operations to maximize energy capture. This real-time data ensures turbines are optimally aligned with current wind conditions, enhancing energy production efficiency. Metrics: Wind speed (m/s) and direction (degrees). Proactive Maintenance and Performance Alerts: The dashboard’s recommendations and alerts section provides actionable insights for proactive maintenance and performance optimization. These alerts address issues that can significantly impact energy output, ensuring timely interventions for optimal turbine performance. Enhanced Operational Efficiency and User Experience: The user-friendly interface with easy navigation and quick access to various functionalities enhances operational efficiency. This feature allows operators to manage complex wind farm operations effectively, ensuring optimal performance and maintenance scheduling. The Real-Time Wind Farm Performance Management Dashboard is a comprehensive tool that provides wind farm operators with the necessary data and insights for optimizing turbine performance and overall wind farm efficiency. Its combination of real-time monitoring, historical analysis, and actionable recommendations plays a crucial role in enhancing the operational efficiency and energy output of wind farms. Figure 2.b Real-Time Wind Farm Performance Management View – Individual Wind Turbine Real-Time Wind Farm Performance Management Dashboard – Individual Turbine Focus This XMPro dashboard view provides a detailed perspective on individual wind turbines within a wind farm, enhancing the ability to monitor and optimize each turbine’s performance. Interactive 3D Turbine Visualization: The central 3D visualization of a wind turbine provides an in-depth view of critical components like the rotor, pitch control, and blade. This interactive model allows for detailed monitoring of operational aspects such as rotational speed, rotor temperature, and hydraulic pitch pressure, crucial for proactive maintenance and performance optimization. Features: Rotor state, pitch control, blade status, rotational speed, and more. The Real-Time Wind Farm Performance Management Dashboard – Individual Turbine Focus is a sophisticated tool that provides wind farm operators with detailed insights into each turbine’s performance. Its combination of real-time monitoring, historical analysis, interactive 3D visualization, and actionable recommendations plays a crucial role in enhancing the operational efficiency and energy output of individual wind turbines within the farm. Figure 3. Asset Analysis View – Wind Turbine WT-10 Health Asset Analysis View – Wind Turbine WT-10 Health This Asset Analysis View on the XMPro dashboard offers detailed insights into a specific wind turbine within a renewable energy system, focusing on turbine WT-10. Comprehensive Production and Performance Data: The left section of the dashboard displays crucial production data for WT-10, including kilowatt-hours (2107 kWh), average power (1837 kW), and performance (63.7%). A green line graph illustrates the power output fluctuation over time, providing a visual representation of the turbine’s energy production efficiency. This data is essential for assessing the turbine’s current output and identifying trends or deviations in performance. Detailed Turbine Information: Below the production data, detailed information about WT-10 is listed, including turbine ID, wind farm location (West Rock), total power generated (7.8 GWh), operational hours (5345), and the turbine model (GE Haliade-X 14 MW). Geographical coordinates are also provided. This comprehensive profile is vital for understanding the turbine’s operational context and history, aiding in maintenance planning and performance analysis. Weather Forecast for Operational Planning: A 3-day weather forecast presents predictions for wind top speed and temperature highs, along with expected weather conditions. This forecast is crucial for anticipating environmental factors that could impact turbine performance and planning appropriate operational responses. Blade Damage Analysis: A detailed table outlines the damage to the turbine’s blades, including blade side, severity, damage type (LE Erosion), and the affected area in square meters. This information is critical for prioritizing maintenance activities and addressing blade health, which directly impacts turbine efficiency. Interactive 3D Turbine Visualization: The central 3D visualization of WT-10 highlights different parts, such as the rotor hub, and shows an alert symbol indicating issues. This interactive model allows for a deeper understanding of the turbine’s condition and aids in identifying areas requiring attention. Wind Speed and Direction Monitoring: On the right, a gauge displays the current wind speed (8.7 m/s) and direction (237°), along with an average wind speed indicator. Monitoring these conditions is essential for optimizing turbine alignment and settings to maximize energy capture. Targeted Recommendations and Alerts: Below the wind details, specific recommendations and alerts for WT-10 are listed, including high wind speed, suboptimal wind direction, and low wind speed warnings. These alerts, complete with timestamps, are key for proactive maintenance and operational adjustments. User Interface and Navigation: The dashboard includes a search function, user profile, and other interface icons for easy navigation and settings adjustments. This enhances the user experience, allowing for efficient management of turbine data and settings. This Asset Analysis View for Wind Turbine WT-10 on the XMPro dashboard provides a specialized and comprehensive analysis of the turbine’s performance, condition, and environmental factors. It is an invaluable tool for maintenance planning, operational decision-making, and optimizing the turbine’s energy output and efficiency. Why XMPro iDTS? XMPro’s Intelligent Digital Twin Suite (iDTS) offers several unique solutions for optimizing the performance of wind turbines, particularly in the context of the Wind Turbine Performance Optimization use case. Here’s how XMPro iDTS effectively addresses this challenge: Advanced Intelligent Digital Twin Modeling: XMPro iDTS creates sophisticated digital twins of individual wind turbines, providing a virtual representation that mirrors their real-world conditions. This enables detailed analysis and simulation of turbine performance under various environmental and operational scenarios. Advanced Sensor Data Integration & Transformation: The suite integrates real-time data from various sensors on the wind turbines, such as wind speed, direction, temperature, and turbine operational metrics. This integration allows for comprehensive monitoring and analysis of turbine performance, identifying areas for optimization. Predictive Analytics for Performance Enhancement: Utilizing advanced predictive analytics, XMPro iDTS can forecast potential performance issues and identify optimal operational settings for each turbine. This predictive approach enables proactive adjustments to maximize efficiency and energy output. Maintenance Scheduling Optimization: By analyzing performance data, XMPro iDTS helps optimize maintenance schedules, shifting from a reactive to a predictive maintenance approach. This reduces downtime and extends the lifespan of turbine components. Real-Time Monitoring and Predictive Alerting: The platform can generate automated recommendations for adjusting turbine settings, such as blade pitch or rotation speed, based on real-time data and predictive insights. This automation ensures turbines operate at their peak efficiency. Customizable and Interactive Dashboards: XMPro iDTS features customizable dashboards that provide real-time insights into turbine performance. These dashboards are interactive, allowing operators to drill down into specific aspects of turbine operation, such as power output, rotor speed, and blade health. Scalability and Flexibility – Start Small, Scale Fast: XMPro iDTS offers scalable and flexible solutions, allowing wind farms to start small and expand as needed. Its modular design ensures easy integration and adaptability, facilitating quick deployment and future-proof scalability. Enhanced Safety & Operational Efficiency: he suite enhances operational safety by predicting and mitigating potential risks associated with turbine operation. It also improves overall operational efficiency by ensuring turbines operate within optimal parameters. XMPro Blueprints – Quick Time to Value: XMPro Blueprints offer a rapid path to value realization for wind farms. These pre-configured templates are designed for quick implementation, incorporating best practices and industry standards. In summary, XMPro iDTS addresses the Wind Turbine Performance Optimization use case by providing a comprehensive, real-time, predictive, and integrated solution. Its capabilities in digital twin technology, advanced data integration, predictive analytics, and interactive dashboards make it a powerful tool for enhancing the performance, safety, and efficiency of wind turbines. Not Sure How To Get Started? No matter where you are on your digital transformation journey, the expert team at XMPro can help guide you every step of the way - We have helped clients successfully implement and deploy projects with Over 10x ROI in only a matter of weeks! Request a free online consultation for your business problem. \"*\" indicates required fields Name*Email*Company / Organization*Job Role*Contact Department*SalesSupportMessage*CaptchaPhoneThis field is for validation purposes and should be left unchanged. Δdocument.getElementById( \"ak_js_3\" ).setAttribute( \"value\", ( new Date() ).getTime() ); /* <![CDATA[ */ gform.initializeOnLoaded( function() {gformInitSpinner( 3, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery('#gform_ajax_frame_3').on('load',function(){var contents = jQuery(this).contents().find('*').html();var is_postback = contents.indexOf('GF_AJAX_POSTBACK') >= 0;if(!is_postback){return;}var form_content = jQuery(this).contents().find('#gform_wrapper_3');var is_confirmation = jQuery(this).contents().find('#gform_confirmation_wrapper_3').length > 0;var is_redirect = contents.indexOf('gformRedirect(){') >= 0;var is_form = form_content.length > 0 && ! is_redirect && ! is_confirmation;var mt = parseInt(jQuery('html').css('margin-top'), 10) + parseInt(jQuery('body').css('margin-top'), 10) + 100;if(is_form){jQuery('#gform_wrapper_3').html(form_content.html());if(form_content.hasClass('gform_validation_error')){jQuery('#gform_wrapper_3').addClass('gform_validation_error');} else {jQuery('#gform_wrapper_3').removeClass('gform_validation_error');}setTimeout( function() { /* delay the scroll by 50 milliseconds to fix a bug in chrome */ }, 50 );if(window['gformInitDatepicker']) {gformInitDatepicker();}if(window['gformInitPriceFields']) {gformInitPriceFields();}var current_page = jQuery('#gform_source_page_number_3').val();gformInitSpinner( 3, 'https://xmpro.com/wp-content/plugins/gravityforms/images/spinner.svg', true );jQuery(document).trigger('gform_page_loaded', [3, current_page]);window['gf_submitting_3'] = false;}else if(!is_redirect){var confirmation_content = jQuery(this).contents().find('.GF_AJAX_POSTBACK').html();if(!confirmation_content){confirmation_content = contents;}setTimeout(function(){jQuery('#gform_wrapper_3').replaceWith(confirmation_content);jQuery(document).trigger('gform_confirmation_loaded', [3]);window['gf_submitting_3'] = false;wp.a11y.speak(jQuery('#gform_confirmation_message_3').text());}, 50);}else{jQuery('#gform_3').append(contents);if(window['gformRedirect']) {gformRedirect();}}jQuery(document).trigger(\"gform_pre_post_render\", [{ formId: \"3\", currentPage: \"current_page\", abort: function() { this.preventDefault(); } }]); if (event.defaultPrevented) { return; } const gformWrapperDiv = document.getElementById( \"gform_wrapper_3\" ); if ( gformWrapperDiv ) { const visibilitySpan = document.createElement( \"span\" ); visibilitySpan.id = \"gform_visibility_test_3\"; gformWrapperDiv.insertAdjacentElement( \"afterend\", visibilitySpan ); } const visibilityTestDiv = document.getElementById( \"gform_visibility_test_3\" ); let postRenderFired = false; function triggerPostRender() { if ( postRenderFired ) { return; } postRenderFired = true; jQuery( document ).trigger( 'gform_post_render', [3, current_page] ); gform.utils.trigger( { event: 'gform/postRender', native: false, data: { formId: 3, currentPage: current_page } } ); if ( visibilityTestDiv ) { visibilityTestDiv.parentNode.removeChild( visibilityTestDiv ); } } function debounce( func, wait, immediate ) { var timeout; return function() { var context = this, args = arguments; var later = function() { timeout = null; if ( !immediate ) func.apply( context, args ); }; var callNow = immediate && !timeout; clearTimeout( timeout ); timeout = setTimeout( later, wait ); if ( callNow ) func.apply( context, args ); }; } const debouncedTriggerPostRender = debounce( function() { triggerPostRender(); }, 200 ); if ( visibilityTestDiv && visibilityTestDiv.offsetParent === null ) { const observer = new MutationObserver( ( mutations ) => { mutations.forEach( ( mutation ) => { if ( mutation.type === 'attributes' && visibilityTestDiv.offsetParent !== null ) { debouncedTriggerPostRender(); observer.disconnect(); } }); }); observer.observe( document.body, { attributes: true, childList: false, subtree: true, attributeFilter: [ 'style', 'class' ], }); } else { triggerPostRender(); } } );} ); /* ]]&gt; */"
  },
  "docs/resources/faqs/external-content/youtube/2012/index.html": {
    "href": "docs/resources/faqs/external-content/youtube/2012/index.html",
    "title": "2012 YouTube Videos | XMPro",
    "summary": "2012 YouTube Videos Video content from the XMPro YouTube channel published in 2012. Videos Note: Content will be migrated from GitBook in a future phase."
  },
  "docs/resources/faqs/external-content/youtube/2013/index.html": {
    "href": "docs/resources/faqs/external-content/youtube/2013/index.html",
    "title": "2013 YouTube Videos | XMPro",
    "summary": "2013 YouTube Videos Video content from the XMPro YouTube channel published in 2013. Videos Note: Content will be migrated from GitBook in a future phase."
  },
  "docs/resources/faqs/external-content/youtube/2016/index.html": {
    "href": "docs/resources/faqs/external-content/youtube/2016/index.html",
    "title": "2016 YouTube Videos | XMPro",
    "summary": "2016 YouTube Videos Video content from the XMPro YouTube channel published in 2016. Videos Note: Content will be migrated from GitBook in a future phase."
  },
  "docs/resources/faqs/external-content/youtube/2019/index.html": {
    "href": "docs/resources/faqs/external-content/youtube/2019/index.html",
    "title": "2019 YouTube Videos | XMPro",
    "summary": "2019 YouTube Videos Video content from the XMPro YouTube channel published in 2019. Videos Note: Content will be migrated from GitBook in a future phase."
  },
  "docs/resources/faqs/external-content/youtube/2020/index.html": {
    "href": "docs/resources/faqs/external-content/youtube/2020/index.html",
    "title": "2020 YouTube Videos | XMPro",
    "summary": "2020 YouTube Videos Video content from the XMPro YouTube channel published in 2020. Videos Note: Content will be migrated from GitBook in a future phase."
  },
  "docs/resources/faqs/external-content/youtube/2021/index.html": {
    "href": "docs/resources/faqs/external-content/youtube/2021/index.html",
    "title": "2021 YouTube Videos | XMPro",
    "summary": "2021 YouTube Videos Video content from the XMPro YouTube channel published in 2021. Videos Note: Content will be migrated from GitBook in a future phase."
  },
  "docs/resources/faqs/external-content/youtube/2022/index.html": {
    "href": "docs/resources/faqs/external-content/youtube/2022/index.html",
    "title": "2022 YouTube Videos | XMPro",
    "summary": "2022 YouTube Videos Video content from the XMPro YouTube channel published in 2022. Videos Note: Content will be migrated from GitBook in a future phase."
  },
  "docs/resources/faqs/external-content/youtube/2023/index.html": {
    "href": "docs/resources/faqs/external-content/youtube/2023/index.html",
    "title": "2023 YouTube Videos | XMPro",
    "summary": "2023 YouTube Videos Video content from the XMPro YouTube channel published in 2023. Videos Note: Content will be migrated from GitBook in a future phase."
  },
  "docs/resources/faqs/external-content/youtube/2024/index.html": {
    "href": "docs/resources/faqs/external-content/youtube/2024/index.html",
    "title": "2024 YouTube Videos | XMPro",
    "summary": "2024 YouTube Videos Video content from the XMPro YouTube channel published in 2024. Videos Note: Content will be migrated from GitBook in a future phase."
  },
  "docs/resources/faqs/external-content/youtube/index.html": {
    "href": "docs/resources/faqs/external-content/youtube/index.html",
    "title": "YouTube | XMPro",
    "summary": "YouTube Video content from the XMPro YouTube channel, including tutorials, webinars, and product demonstrations. By Year 2024 2023 2022 2021 2020 2019 2016 2013 2012"
  },
  "docs/resources/faqs/general.html": {
    "href": "docs/resources/faqs/general.html",
    "title": "General FAQs | XMPro",
    "summary": "General FAQs Find answers to some of the most frequently asked general questions. Training Is there a training outline? An end-to-end course can be found at XMPro Courses. It is broken down into 17 lessons - totaling 7 hours - covering an overview, Data Stream Designer, Recommendations, and App Designer. You can also find an extensive array of YouTube videos on various topics of interest on our YouTube channel XMPro learning. Are there any prerequisites for the training? As XMPro is a no-code platform, there are no technical prerequisites. We do recommend the following to make your training more effective: Start with a Use Case/problem statement in mind Have access to an XMPro instance so you can practice and work out issues Session Timeout How does the XMPro sign-in process work? When you try to sign in to XMPro Data Stream Designer or App Designer, you're redirected to the linked XMPro Subscription Manager's sign-in page. Enter your username and password to log in. Upon signing in, you receive credentials as browser cookies for both Subscription Manager and the site you wish to access. Subscription Manager redirects you back to the requested site, granting you an access token using those cookie credentials. The access token is valid for the duration specified in the site's Session Timeout setting, while cookies last for longer periods (e.g., an hour). Cookies enable access to the website's pages and assets like JavaScript and images, while the access token allows you to access the site's data, which is essential for activities like navigating, viewing live updates on app pages and saving changes. Note The Session Timeout setting is accessible by the Site Admin and can be found in Subscription Manager under the details blade for the product (e.g., App Designer). How long do I stay signed in? Without \"Remember Me\": you stay signed in for 1 hour or until your browser session ends (e.g. closing the browser window). With \"Remember Me\": you stay signed in for 30 days or until the site is unable to silently refresh the credentials. Note The Remember Me checkbox can be found when signing into XMPro. How does the Silent Refresh functionality work? When using XMPro App Designer, Data Stream Designer, or Subscription Manager, the site attempts a silent refresh of the credentials as the access token nears its expiry time. If successful, a new token is granted, valid for the duration specified in the Session Timeout setting. If unsuccessful, the site displays a login expired warning with a prompt to sign in again. Clicking the button redirects you to Subscription Manager for credentials. If the Subscription Manager cookie is still valid, it automatically issues a new access token and redirects back to the original site; otherwise, you're prompted to sign in again. Successful attempts through either path will refresh the cookie credentials if they're nearing expiry and meet refresh conditions. Refreshing the page will also update the credentials. Versioning What version of XMPro am I using? Whether you are logging a support issue or looking for a specific product feature, knowing the version of the XMPro Platform you're using is important. Click the help icon top right of the web applications to view the help blade. It contains the version number as well as helpful links such as release notes and logging a support ticket. From v4.4.4 onwards, you can access more details in the following ways: For web applications only, check the /version endpoint, which produces a response such as... { \"ApplicationTitle\": \"XMPro App Designer\", \"XMProPlatformVersion\": \"4.4.4\", \"InformationalVersion\": \"4.4.4.28+8c9912b045\", \"GitCommitId\": \"8c9912b04592ac2d54bf4fa9a522203cbd21bc98\", \"GitCommitDate\": \"2024-05-10T04:41:02Z\" } For all applications, check the log files. On application start, a log event like the following will be generated. [12:00:00.123] [Information] [XMPro.Runtime.Runtime] Running \"XMPro Stream Host\" \"4.4.4.33+bc71a3ebfc\" on \"Ubuntu\" [RuntimeInfo] RuntimeInfo { Title = XMPro Stream Host, XMProPlatformVersion = 4.4.4, AssemblyInformationalVersion = 4.4.4.33+bc71a3ebfc, GitCommitId = bc71a3ebfc82632d60586503878e8867fbf2e0b4, GitCommitDate = 2024-05-13T07:10:29Z }"
  },
  "docs/resources/faqs/implementation.html": {
    "href": "docs/resources/faqs/implementation.html",
    "title": "Implementation FAQs | XMPro",
    "summary": "Implementation Find answers to some of the most frequently asked implementation questions. Starting Out Where do I start with XMPro? XMPro strongly recommends that all projects should have a defined ROI. Therefore the initial Use Cases, collection of data pertaining to the Business Problems that you are seeking to resolve, and evaluation of project outcome at the midpoint and project close are vital to substantiate the ROI. A synopsis of our framework for a project is as follows: Define the Business Problem that needs to be addressed Identify Bad Actors Care must be taken to identify the initial Use Case Refer McKinsey study 4 steps on page 12 Refer McKinsey study Avoid Pilot Purgatory Gather data for the initial Proof of Value or Pilot Commence Proof of Value or Pilot Hold a mid-point review & measure success against these questions Does the tool work? Does it deliver value? Would the customer use it? Collect the ROI achieved in the Pilot Period Project Gate = Go / No Go decision Note Bad Actors are components or systems that frequently fail or cause issues in your operations. To build a Digital Twin of a factory, do I start with the whole factory or start small? Always start small. Digital transformation is achieved by many small projects that lock in success at each step. The overall goal is to achieve project success early and expand. There will be many challenges for you to achieve success, by starting with small projects you lower the overall risk of failure. The positive flip side of small projects is that you fail fast, and the costs are limited. Tip Focus on easy wins first - target high-impact, low-effort projects to demonstrate value quickly. I want Predictive Maintenance, and I want it now. We think it makes sense to think of two components of PdM (Predictive Maintenance): First Principle / Engineering Models – The starting point for most predictive solutions supported by real-time sensor data AI / Statistical Models – Use the AI / Statistical models to augment decision support but requires quality data and contextual models Subject to quality data, the First Principle / Engineering Models are the fast start as the formula is agreed upon and the model is ready to go. AI / Statistical Models require more effort and analysis – one will need the assets failure history and time and resources to refine your models. Another view of your predictive maintenance goal is to consider the maturity of the use cases. Smart Asset Management includes the smart use of condition monitoring, predictive maintenance, and process optimization. Note Image Note: A diagram showing the progression from condition monitoring to predictive maintenance to process optimization would be helpful here. This image needs to be created and added to the images directory. What is XMPro's approach to AI? XMPro is not an AI company. We can call the AI / Statistical models and use the output to trigger recommendation alerts, but the models are sourced from you or one of our AI partners. How does XMPro do it? We provide integration building blocks in our Data Streams to call the model and use the model output for computation of the real-time data stream, which is used in the creation of a recommendation for the asset. XMPro has the capability to pass data to Azure Machine Learning and train models. These models can be called by other XMPro Data Streams to use the model output for evaluation of real-time streaming data. For example, the RUL of a Secondary Crusher. Further Reading How to work with Data Streams Python Integration Azure ML Integration I've seen Mining examples. Can XMPro compose a Digital Twin for utilities? Yes, we can. In 2021/22 XMPro predominantly worked in the Mining and Oil & Gas sectors, and we are now expanding to other asset-intensive industries such as Utilities and Chemicals. These asset-intensive industries all have similar equipment, challenges, and use cases. We find that we can transport solutions from one to the other in these industries. The lessons learned from Mining can accelerate solutions for Utilities, bringing a slightly different perspective on how to approach challenges from a \"bad actor\" and failure analysis point of view rather than a traditional \"similar industry\" approach. Architecture Can XMPro be installed and used on-premise? Yes. Currently, the majority of XMPro deployments are Cloud-based. XMPro can be deployed on-premises, on Azure/AWS, or as a hybrid with parts on-premises and in the cloud. Typically for the hybrid deployment, the stream host is deployed on-premises with the core application being hosted on the cloud. On-premise deployments have been driven by the need for remote locations, as well as constraints on internet access from within a corporate network. Further Reading On-Premise deployment All deployment options If XMPro is in the Cloud, can I connect to my On-Premise application(s)? Yes. If your applications are internet-facing, then a cloud stream host can be used. If your applications are behind a firewall and internal to your network, then in this instance XMPro will be a hybrid deployment model, with the stream host located on-premises. Further Reading Azure Deployment Diagram AWS Deployment Diagram Do I need a separate server for the stream host, or will it run on the same application server? The preferred approach is to have a separate infrastructure for the stream host. It is possible that your XMPro deployment has only one Stream Host. However, this is rarely the case and we suggest the following considerations: Consider each location having a dedicated Stream Host A critical Data Stream, like pressure readings on a pressure vessel, may have its own dedicated Stream Host If a payload for a Data Stream is large and frequent, then consider a dedicated Stream Host for that Data Stream. Having several Stream Hosts can improve the resilience of your system Network latency and geographical spread could be factors in considering the case for multiple Stream Hosts Note Image Note: A diagram showing different Stream Host deployment scenarios would be helpful here. This image needs to be created and added to the images directory. Performance A look beneath the hood of XMPro Charts and Time Series Analysis Charts Often the first time that users are aware of the components that make up XMPro charts is when they experience slow rendering times. This article seeks to explain the structure behind the XMPro charts and the components that impact chart performance. Performance is not limited to the Time Series Analysis as the same pattern pertains to other XMPro charts too, such as the Chart Block. With an understanding of the influences on chart performance, the reader will be in an informed position to address their requirement. The current Time Series Analysis use SQL and Azure ADX as data sources. XMPro as an Event Intelligence platform XMPro is purpose-built to manage events in real-time. This means its technologies can be used for Business Intelligence purposes. The processing of large swathes of past time series data is possible, but there are circumstances where this will not be an optimal user experience as the connectors work on the basis of 'Get All Data' and not incremental steps like for example, Grafana. XMPro App Designer Server memory scenarios The XMPro App Designer Server will reserve the required memory for each unique query while there is at least 1 active user interacting with it. A unique query is defined as a Time Series Chart instance, with the same date selection and same data source. The number of concurrent queries with at least 1 active user will have an influence on the XMPro App Designer Server performance. If one user runs a query that consumes 2 GB of server memory and a different user performs that same query, then the server consumes no additional memory and instead used the cached query. If a different query is run that consumes 4 GB of server memory, then a total of 6 GB of memory would be consumed, and so on. Depending on the server configuration this could impact overall server performance and response time (user experience).\" Settings that could impact performance There are several influences on XMPro App Designer Server and XMPro App Designer Client performance. The main influences would include: Web Server memory Web Server CPU Load balancing / Auto Scaling (scale out) Data Size (size and number of rows) Data Complexity (type and relationship of data) Where aggregation is performed (Data Layer vs Web Server vs Chart Control) Network speed and reliability (Data Layer, Web Server, and Client Browser) Client Available CPU speed Client Available Memory Web Browser and Version XMPro Platform Version SQL vs ADX The first consideration that impacts response time is the amount of data queried at the data source and the volume of data transmitted in response. The second consideration is the volume and complexity of data that the App Designer Server needs to parse. Amending the aggregation period will be almost instantaneous on ADX. On SQL this is likely to be slower. Issues for consideration to address overall performance The Time Series Analysis data loads initially and then again whenever the date selection is changed. Thus the initial selection should be limited, e.g. 3 hours. The appropriate initial selection depends upon the frequency of data points, i.e., if the time interval is every second, every minute, or every hour. The number of assets affects performance - not whether they are selected. All data points for all assets for the data range selection are loaded so that the query does not need to be reloaded when a user changes their asset selection. Limit the number of records fetched from the source system, either by using aggregation at the source if possible (or) by limiting to a shorter date range selection. Consider whether this is tracking events in real-time or requires discovery in a BI environment. Consider the number of simultaneous users and unique query combinations and whether sufficient resources have been allocated."
  },
  "docs/resources/faqs/index.html": {
    "href": "docs/resources/faqs/index.html",
    "title": "Frequently Asked Questions | XMPro",
    "summary": "Frequently Asked Questions This section contains answers to frequently asked questions about XMPro. The FAQs are organized into the following categories: Implementation FAQs Questions related to implementing XMPro in your organization. View Implementation FAQs Configuration FAQs Questions related to configuring XMPro. View Configuration FAQs Agent FAQs Questions related to XMPro Agents. View Agent FAQs General FAQs General questions about XMPro. View General FAQs External Content Additional resources from external sources, including blogs, use cases, and YouTube videos. View External Content"
  },
  "docs/resources/icon-library.html": {
    "href": "docs/resources/icon-library.html",
    "title": "Icon Library | XMPro",
    "summary": "Icon Library The following is a list of icons that you can use when you create a new App, Data Stream, Recommendation, or any other XMPro Object. You can download these directly and upload them as part of the steps outlined in the various How-To guides. Note A zip file containing all the icons is available for download. The images listed below are for reference. Image Migration Note: The icons referenced in this document need to be copied from the original GitBook assets directory to the DocFX images directory. Once copied, each icon should be displayed inline using the format ![Icon Name](images/icon-name.png). Widgets These icons are available for use in widgets: widget-1.png widget-2.png widget-3.png widget-4.png widget-5.png widget-6.png widget-7.png widget-8.png widget-9.png widget-10.png widget-11.png Icons Analytics Analytics-related icons: analytics-1.png analytics-2.png analytics-3.png analytics-4.png analytics-5.png analytics-6.png analytics-7.png analytics-8.png analytics-9.png analytics-10.png analytics-11.png analytics-12.png analytics-13.png analytics-14.png analytics-15.png Borers Borer-related icons: borer-1.png borer-2.png borer-3.png borer-4.png borer-5.png Compressors Compressor-related icons: compressor-1.png compressor-2.png Conveyer Conveyor-related icons: conveyor-1.png conveyor-2.png conveyor-3.png conveyor-4.png Crusher Crusher-related icons: crusher-1.png crusher-2.png Fan Fan-related icons: fan-1.png fan-2.png fan-3.png Gears Gear-related icons: gears-1.png gears-2.png gears-3.png gears-4.png gears-5.png Maintenance Maintenance-related icons: maintenance-1.png maintenance-2.png maintenance-3.png maintenance-4.png maintenance-5.png maintenance-6.png maintenance-7.png maintenance-8.png Oil and Gas Oil and gas-related icons: oil-1.png oil-2.png oil-3.png oil-4.png oil-5.png oil-6.png oil-7.png oil-8.png oil-9.png oil-10.png oil-11.png oil-12.png oil-13.png oil-14.png oil-15.png oil-16.png Other Other miscellaneous icons: Admin.png Alert (Dark Theme).png Caution icon.png Grids (1).png Hydraulic.png Mill-Icon.png Mind.png My Sandbox.png Red Lock.png Other Equipment Other equipment-related icons: other-1.png other-2.png other-3.png other-4.png other-5.png other-6.png Pumps Pump-related icons: pump1 (1).png pump2 (1).png pump3.png pump4.png pump5.png pump6 (1).png pump7.png pump8 (1).png pump9 (1).png pump10 (1).png pump11.png pump12.png pump13.png pump14.png pump15 (1).png pump16 (1).png pump17 (1).png pump18 (1).png pump19.png pump20 (1).png pump21.png pump22 (1).png pump23 (1).png pump24 (1).png pump25.png pump26 (1).png pump27 (1).png pump28.png pump29 (1).png pump30.png pump31.png pump32 (1).png pump33.png pump34 (1).png pump35 (1).png pump36 (1).png pump37 (1).png pump38.png pump39 (1).png pump40.png pump41.png pump42 (1).png pump43 (1).png pump44 (1).png pump45.png pump46.png pump47 (1).png pump48 (1).png pump49 (1).png pump50.png Recommendations Dark Blue Dark blue recommendation icons: continuous-miner.png cooling.png gas-station.png pressure-gauge.png pump (1).png rubber-stamp.png temperature.png Light Blue Light blue recommendation icons: business-management.png factory.png humidity.png power-plant.png rock.png worker.png Orange Orange recommendation icons: account.png compressor.png expired.png factory-breakdown.png lock.png maintenance.png severity.png support.png temperature (1).png warning-shield.png Purple Purple recommendation icons: disconnected.png gear.png information.png mine-cart.png production-machine.png time.png Violet Violet recommendation icons: audio-wave.png gas-station (1).png maintenance (1).png pump.png screwdriver.png work.png Red Red recommendation icons: cancel.png decline (1).png engine-oil (1).png error.png gears.png heating.png oil-industry.png oil-storage-tank (1).png pressure.png question-mark.png scales (1).png Yellow Yellow recommendation icons: cancel (1).png decline.png engine-oil.png error (1).png gears (1).png heating (1).png oil-industry (1).png oil-storage-tank.png pressure (1).png question-mark (1).png scales.png yellow-notification.png Example Icon Display Below is an example of how icons should be displayed once the images are properly migrated: ![Widget 1](images/widget-1.png) widget-1.png ![Analytics 1](images/analytics-1.png) analytics-1.png ![Borer 1](images/borer-1.png) borer-1.png This format shows the icon followed by its filename for easy reference."
  },
  "docs/resources/index.html": {
    "href": "docs/resources/index.html",
    "title": "Resources | XMPro",
    "summary": "Resources Welcome to the XMPro Resources section. This section contains various resources to help you get the most out of XMPro. What's New Stay up to date with the latest features and improvements in XMPro. What's New in 4.4 Integrations Learn about the various integrations available for XMPro. Sizing Guideline Get guidance on sizing your XMPro deployment. Platform Security Learn about the security features of the XMPro platform. Icon Library Browse the XMPro icon library. FAQs Find answers to frequently asked questions about XMPro. Practice Notes Best practices and guidance for using XMPro effectively."
  },
  "docs/resources/integrations.html": {
    "href": "docs/resources/integrations.html",
    "title": "Integrations | XMPro",
    "summary": "Integrations Overview XMPro has several different integrations: Agents are the building blocks of Data Stream Designer (default Agents bulk download here). Connectors integrate to third-party data sources in App Designer (default Connectors bulk download here) Visualization Blocks are the building blocks of App Designer to create rich user experiences Agents An Agent is a reusable object that forms the building block of a Data Stream. When several Agents are connected, a Data Stream is formed. Each Agent is designed to perform a specific function in the stream. For example, they can be used to retrieve data from a database in real-time, display data, filter, sort the data, or save the data somewhere else, depending on the function of that individual Agent. Looking for an Agent that is not on the list? Send us a request or check out the Framework to create a new Agent yourself. Generative AI & Large Language Models Agent Category Tier Azure AI Document Intelligence Generative AI 5 Azure OpenAI Generative AI 4 OpenAI Assistant Generative AI 5 Ollama Generative AI 5 AI & Machine Learning Agent Category Tier Anomaly Detection AI & Machine Learning* 5 Azure ML AI & Machine Learning 5 Binary Classification AI & Machine Learning* 5 Boon Amber AI & Machine Learning 4 Forecasting AI & Machine Learning* 5 Kmeans Clustering AI & Machine Learning* 5 MLflow AI & Machine Learning 5 Multi Class Classification AI & Machine Learning* 5 Python AI & Machine Learning 5 Regression AI & Machine Learning* 5 RScript AI & Machine Learning 5 Social & Communication Agent Category Tier Email Listener, Action Agent 1 Twilio Action Agent 1 Database & Technology Agent Category Tier Azure Data Explorer Listener, Context Provider, Action Agent 2 Azure Data Factory Action Agent 2 Azure Data Lake Action Agent 2 Azure Digital Twin Listener, Context Provider, Action Agent 2 Azure Event Hub Listener, Action Agent 2 Azure IoT Hub Listener 2 Cognite Listener, Context Provider 2 Ethereum Smart Contract Listener, Action Agent 2 Excel File Reader Action Agent 2 ifm Listener 2 InfluxDB Listener, Context Provider, Action Agent 2 Litmus Edge OPC UA Listener, Action Agent 2 MongoDB Listener, Context Provider, Action Agent 2 MOVUS Listener, Context Provider, Action Agent 2 MySQL Listener, Context Provider, Action Agent 2 Neo4j Listener, Context Provider, Action Agent 2 OData Context Provider, Action Agent 2 ODBC Listener, Context Provider 2 OPC DA Listener, Action Agent 2 OPC UA Listener, Action Agent 2 Oracle Action Agent 2 Snowflake Listener, Context Provider, Action Agent 2 ERP Agent Category Tier Coupa Context Provider, Action Agent 3 Erbessed Listener, Context Provider 3 iPOS Action Agent 3 FinOps Context Provider, Action Agent 3 OSIsoft PI Listener, Context Provider, Action Agent 3 Salesforce Listener, Context Provider, Action Agent 3 SAP Context Provider, Action Agent 3 SAP HANA Context Provider, Action Agent 3 Advanced App Agent Category Tier FFT Function 4 Nanoprecise Listener, Context Provider, Action Agent 3 Signal Filter Function 4 Sparkplug B Listener, Action Agent 4 Tango Listener, Context Provider 3 Telit deviceWise Listener, Context Provider, Action Agent 3 Telit MQTT Listener, Action Agent 3 Telit OPC UA Listener, Action Agent 3 WebScraper Context Provider 4 Open Source Agent Category Tier Azure SQL Listener, Context Provider, Action Agent 5 Convert Flow Units Function 5 CRC16 Function 5 CSV Listener, Context Provider, Action Agent 5 Fixed Width File Reader Action Agent 5 Goal Seek Function 5 HiveMQ Listener, Action Agent 5 JSON Context Provider, Transformation 5 Linear Interpolation Function 5 Min Max Function 5 MQTT Listener, Action Agent 5 PDF Converter Action Agent 5 REST API Context Provider, Action Agent 5 Rounding Function 5 SQL Server Listener, Context Provider, Action Agent 5 XML File Reader Action Agent 5 XMPro Internal Agent Category Tier Aggregate Transformation 6 Alter Attributes Transformation 6 Area Under the Curve Function 6 Batch Identifier Transformation 6 Broadcast Transformation 6 Calculated Field Transformation 6 Concatenate Row Values Transformation 6 Data Conversion Transformation 6 Edge Analysis Transformation 6 Event Printer Action Agent 6 Event Simulator Listener 6 File Listener Listener 6 Filter Transformation 6 Geofence Function 6 Group & Merge Transformation 6 Join Transformation 6 Meta Action Agent 6 Missing Value Detector Transformation 6 Missing Value Substitutor Transformation 6 Normalize Fields Transformation 6 Pass Through Transformation 6 Pivot Table Transformation 6 Random Number Transformation 6 Row Count Transformation 6 Row Padding Transformation 6 Sort Transformation 6 Threshold Monitor Transformation 6 Transpose Transformation 6 Trim Transformation 6 Union Transformation 6 Unzip Action Agent 6 Window Transformation 6 XMPro App Action Agent 6 Recommendations Agent Category Tier Close Action Request Recommendation 6 Read Action Request Recommendation 6 Read Recommendation Recommendation 6 Resolve Recommendation Recommendation 6 Run Recommendation Recommendation 6 Update Recommendation Recommendation 6 Download Default Agents Tier 5 Download the tier 5 files here. Use these individual files if you are not on v4.1.13 or higher: Action Agents AI & ML Context Providers Functions Listeners *Note: Links for the larger AI & ML Agents are on their individual documentation pages. Tier 6 Download the tier 6 files here. Use these individual files if you are not on v4.1.13 or higher: Action Agents Functions Listeners Recommendations Transformations Connectors A Connector is a pre-built integration plug-in for the XMPro App Designer that allows you to connect to third-party data sources without coding. Whereas the Agents in a published Data Stream continuously poll for data, the Connectors in a published App retrieve data on an ad-hoc basis. Note Download the tier 5 & 6 Connector files here. Database & Technology Connector Tier Azure Data Explorer 2 TSA Azure Data Explorer 2 Azure Digital Twin 2 Neo4J 2 Snowflake 2 ERP Connector Tier Erbessd 3 OSIsoft PI 3 OSIsoftPI Histogram 3 Advanced App Connector Tier Nanoprecise 3 Open Source Connector Tier Azure SQL 5 HTTP 5 JSON 5 REST API 5 SQL Server 5 TSA SQL Server 5 XMPro Internal Connector Tier Data Streams 6 Visualization Blocks An App Designer visualization block allows a no-code way to integrate with third-party systems and create rich user experiences. Listed below are some of the integration blocks found in the App Designer toolbox: Visualization Block Autodesk Forge D3 Visualization Esri Map Pivot Grid Power BI Time Series Chart Unity"
  },
  "docs/resources/platform-security.html": {
    "href": "docs/resources/platform-security.html",
    "title": "Platform Security | XMPro",
    "summary": "Platform Security XMPro places a high priority on security, performing app security checks every 3 months using Veracode. Veracode's comprehensive analysis helps identify, prevent, and fix vulnerabilities through multiple testing methods: Static Analysis (white-box testing), Dynamic Analysis (black-box testing), and Software Composition Analysis. Security Technologies and Practices Our suite of products leverages robust technologies and practices to maintain a high security standard: App Designer, Data Stream Designer, and XMPro AI are built on dotnet 8. Subscription Manager is built on Microsoft .NET Framework 4.8 Runtime. In the event a vulnerability is identified in any of these technologies, Microsoft promptly releases an update. We integrate these updates into our products and regularly release new versions that include essential security fixes. Static Application Security Testing (SAST) Static Application Security Testing (SAST) is a form of white-box testing used to scan an application's source, binary, or byte code. Dynamic Application Security Testing (DAST) Dynamic Application Security Testing (DAST) analyzes a web application through the front end to find vulnerabilities through simulated attacks. This is also called Penetration testing. Software Composition Analysis (SCA) Software Composition Analysis scans all the components used in an application for security risks and vulnerabilities. Results Product SAST Score DAST Score Date Subscription Manager 4.4.18 92 95 20 Mar 25 App Designer 4.4.18 78 95 20 Mar 25 Data Stream Designer 4.4.18 89 95 20 Mar 25 XMPro AI 4.4.18 96 95 20 Mar 25 WorkFlow 97 91 20 Mar 25 Support and Recommendations .NET versions are supported by Microsoft for 3 years after release, as detailed in their support policy. To ensure you have the most secure XMPro offerings, we recommend upgrading at least every 3 months to take advantage of the latest security updates and features."
  },
  "docs/resources/practice-notes/index.html": {
    "href": "docs/resources/practice-notes/index.html",
    "title": "Practice Notes | XMPro",
    "summary": "Practice Notes This section contains practice notes and best practices for using XMPro effectively. These notes are based on real-world experience and provide guidance on how to get the most out of XMPro. Available Practice Notes Unified Recommendation Alert Management Learn how to implement a unified approach to managing recommendation alerts in XMPro. Performant Landing Pages in Real-Time Monitoring Best practices for creating performant landing pages for real-time monitoring applications."
  },
  "docs/resources/practice-notes/performant-landing-pages-in-real-time-monitoring.html": {
    "href": "docs/resources/practice-notes/performant-landing-pages-in-real-time-monitoring.html",
    "title": "Performant Landing Pages in Real-Time Monitoring | XMPro",
    "summary": "Performant Landing Pages in Real-Time Monitoring XMPro Design Patterns, Published Mar 2025 Problem Statement A poorly performing landing page creates frustrated, disconnected users. Imagine waiting for data to load, knowing it won't tell you what you need to know – so you'll have to change a filter or navigate elsewhere and wait all over again! One reason for a poor landing page is if it is loaded up with a Unity 3D model of a beautiful long-haul truck or wind farm. Seeing a Caterpillar truck in 3D with real-time sensor readings and alerts is a thing of beauty, but do you need to see it 1st time every time? This is the question for page designers who may confuse 'sizzle' with a real-life working dashboard. The usefulness of the page balanced with performance should be top of mind. \"What is the problem the user is seeking to answer\" should drive every component of every page. Introduction Landing page design requires a balanced approach that addresses both technical performance and user experience needs. Successful implementation depends on recognizing that these dimensions are interconnected—technical decisions directly shape how users interact with and perceive XMPro applications. In industrial environments, landing pages serve operational personnel who require immediate access to actionable information. The effectiveness of these pages directly impacts decision-making processes, situational awareness, and operational efficiency. Thus, technical implementation must align with specific operational contexts and user workflows. Key principles that connect technical implementation to user impact include: Performance directly shapes initial user perception - Load times establish immediate trust in the system and determine whether users engage effectively Data relevance is as important as data speed - Users need not just fast access, but immediate visibility of the specific information required for their role Interface design must balance visual clarity with resource efficiency - Technical optimization and intuitive design work together to guide users to essential functions Device context significantly affects both technical requirements and user needs - Implementation must account for varying operational environments and device capabilities Scale considerations affect both system architecture and user comprehension - As data volumes grow, adapt technical approaches and information presentation The following sections explore how these principles translate to specific implementation strategies across the key aspects of landing page design—balancing technical performance with meaningful user experiences. Performance & First Impressions Critical First Moments Initial page load performance establishes immediate user trust and engagement you're your XMPro industrial applications. Users form judgments about system reliability within milliseconds of their first interaction, making the technical optimization of landing pages a direct driver of user confidence and operational effectiveness. The \"time to first meaningful paint\"—when users first see and can interact with essential content—directly shapes their perception of the entire system. In industrial environments, this initial impression extends beyond mere satisfaction to affect operational readiness and decision-making capability. Make the user journey easy, fast and relevant. Performance Targets Landing pages must meet specific performance targets across different operational contexts: Fixed workstations and laptops: maximum 2 seconds Research shows user frustration begins at 2 seconds Enterprise infrastructure should support optimal performance Portable devices (tablets and phones) on 5G networks: maximum 3 seconds Field operations require rapid mobile access Enterprise security adds processing overhead These targets ensure that all users, regardless of their work environment, can immediately engage with critical operational data. Strategic Content Distribution Strategically distribute content across primary landing pages and drill-down views to optimize performance: Landing Page Essentials Include only critical metrics, alerts, and primary controls on initial landing pages (Data that matters) Focus on immediate visibility of essential operational indicators Utilize lightweight visualization components (tiles, simple charts, status indicators) Drill-down Resource Management Place resource-intensive visualizations (maps, Unity 3D models, complex dashboards) on separate drill-down pages, ensuring these elements load only when explicitly requested by users Provide clear visual indicators of what detailed information is available For example, a manufacturing operations landing page should immediately show critical KPIs and alert counts, with clear pathways to detailed floor visualizations that load only when selected. This approach ensures immediate access to essential information while preserving system responsiveness. Device-Specific Optimization Rather than a one-page-fits-all approach, each device category requires specific optimization approaches: Control Room Displays Optimized for large screens and multiple monitors Focused on comprehensive data visualization Designed for mouse and keyboard interaction Field Tablets Simplified interface for touch interaction Prioritized critical metrics for mobile viewing Adapted for variable lighting conditions Mobile Devices Streamlined for essential operational data Optimized for single-hand operation Designed for intermittent connectivity Note Summary: Performance optimization directly impacts first impressions and operational effectiveness Ensure landing pages load within device-specific target times Include only essential elements on initial landing pages Place resource-intensive visualizations on separate drill-down pages Adapt interface and content based on device context and user role Provide clear pathways to detailed information Data Strategy & Content Relevance Connecting Data Management with User Needs Data management and content prioritization work together to create effective landing pages. How we handle data directly affects what users see and how quickly they can access important information. Smart Data Architecture Choices Effective implementation requires careful consideration of data scope, storage location, and access patterns to achieve optimal response times. Consider these key architectural decisions: Data Volume Control Use focused data sets with clear time boundaries (like 30-day windows) Prioritize data that supports immediate operational decisions Provide clear paths to historical data when needed Storage Strategy Optimization Choose storage solutions based on access frequency and performance requirements Keep critical operational metrics in fast-access storage Balance query speed against storage costs (e.g. ADX vs SQL) For industrial applications, this means deciding which metrics need immediate visibility vs which can be accessed through drilldowns. For example, a plant manager's landing page might show current production rates and critical alerts, while detailed equipment metrics remain accessible through clear navigation options. Content Organization for Different Users XMPro landing pages must adapt content presentation based on both technical limits and user priorities: Role-Based Content Focus Show the most relevant information for each role immediately Adjust content organization based on device type and operational context Keep critical alerts and indicators visible regardless of device Structure information to support common decision workflows Adaptive Information Density Adjust information density based on device capabilities Present complex visualizations differently across device types Ensure important metrics remain prominent on all devices Provide consistent access to essential functions across all formats For example, when showing industrial processes on mobile devices, the system might display simplified status indicators with clear paths to detailed diagrams, while workstations receive more comprehensive initial visualizations. Data Currency Management Resource-intensive calculations and complex aggregations form the foundation of landing page metrics. However, daily sensor readings don't warrant hourly recalculation, while production metrics might require more frequent updates. Balancing data freshness with performance requires a strategic approach to updates: Scheduled Calculations Perform resource-intensive calculations during off-peak times Match refresh intervals to actual data change patterns Show clear timestamps indicating last calculation time Provide manual refresh capabilities for users Real-Time vs. Pre-Calculated Data Use pre-calculated metrics for landing page elements to improve load speed Reserve real-time queries for critical operational metrics and drill-down views Balance real-time capabilities with system performance Clearly indicate which metrics are real-time versus aggregated summaries The goal is to provide rapid access to summarized metrics while enabling detailed exploration of current data. The currency data should always be displayed for each component. Note Summary: Effective data strategy connects technical performance with operational relevance Match data scope and access methods to specific operational roles Prioritize content based on device context and user responsibilities Implement appropriate data currency strategies for different metric types Structure information to support operational decision-making Balance information density with clarity across all device types Interface Design & User Navigation Integrating Design Structure with User Interaction Pathways Interface architecture and user navigation function as interconnected systems that determine operational efficiency. Research-based design principles support efficient information access while enabling critical industrial functions[1]. Effective integration of these components creates landing pages that guide users naturally through their workflows. Layout Optimization User interaction research reveals that effective landing page design must strategically position elements to align with natural viewing patterns and business objectives. The dominant F-pattern[2] scanning behavior shows users first view the top-left area, then scan horizontally before moving downward. This research provides clear guidance for element placement. Critical Component Positioning Position critical metrics and status indicators in the top-left quadrant, for immediate visibility Structure navigation elements to start key processes, reducing cognitive load Organize visual elements by usage frequency and business priority - following the natural F-pattern scanning path In industrial control interfaces, this evidence-based approach to layout design directly impacts user engagement and task completion rates across all device formats. Visualization Strategy Implementation Component selection is a tradeoff between system performance and user engagement. While simpler visualizations deliver better performance, they may reduce user insight and satisfaction; conversely, sophisticated visualizations that delight users often impose significant performance costs. Effective visualization requires balancing these competing demands. Component Selection Parameters Choose visualization types based on data volume, update frequency, and performance impact Reserve resource-intensive components (maps, time series, Unity models) for drill-down views, i.e. requested by a user rather than automatically loaded Utilize efficient components (charts, grids, tiles) for primary metrics display Adapt visualization complexity inversely to data volume (simpler views for larger datasets) For example, when displaying sensor data across multiple sites, consider using a simplified grid view for the landing page rather than an interactive map visualization. These more complex visualizations should load when specifically requested, preserving system responsiveness while maintaining access to detailed information. Clear Action Pathways Conversion path clarity in effective landing pages ensures that primary user actions remain clear and accessible across devices. users can efficiently navigate to primary actions across all devices. Call-to-Action (CTA) elements require careful design consideration to maintain their prominence and effectiveness regardless of display context. This aspect of interface design supports operational effectiveness by minimizing cognitive effort during task execution. Action Element Consistency Maintain consistent CTA visibility across all device types Adapt interactive element sizes to device input methods Optimize touch targets for mobile and tablet interfaces In practical implementation, an \"Overdue Recommendations\" function might span the full width on mobile screens for easy touch access, while appearing as a compact button in a consistent location on desktop interfaces. Although the appearance adapts, the function and its importance remain clear to users regardless of device. Note Summary: Effective interface design integrates layout principles with navigation pathways Position key elements according to validated visual scanning patterns Select visualization components based on performance and usability requirements Design clear interaction paths for essential operational functions Preserve visual hierarchy across different screen sizes [1] https://www.finoit.com/articles/maximizing-user-experience-design-through-information-architecture/ [2] LinkedIn: Understanding and Leveraging the F-Pattern in UX Design Operational Context Adaptation Content Prioritization Across Devices How information is structured and presented must adapt to different screen sizes while maintaining operational relevance. Content prioritization ensures critical information remains accessible regardless of viewing context. Device-Specific Content Hierarchy Adapt content structure based on device constraints and operational context Maintain critical alert visibility across all form factors Implement progressive disclosure for complex information sets Provide clear paths to detailed information On smaller devices, complex features like process diagrams or digital twin demonstrations should be simplified initially, with clear options to access detailed views. This approach preserves XMPro's industrial process capabilities while adapting to device constraints, ensuring the strong value proposition is maintained without overwhelming users. Device-Specific Implementation Strategies Different operational environments demand tailored interface approaches while maintaining functional consistency. Understanding how each device type is used in industrial contexts shapes implementation decisions. Control Room Environments Design for extended monitoring sessions with comprehensive information display Support advanced interactions using precision input devices Enable systematic comparison between related process parameters Optimize for continuous operational oversight Field Operations Support Prioritize glance-based information access for mobile contexts Provide simplified status indicators with clear paths to details Design for variable environmental conditions (lighting, distractions) Optimize for potentially intermittent connectivity Cross-Device Integration Ensure users maintain consistent mental models across devices Preserve work state during device transitions Implement unified notification systems across all platforms Support seamless operational continuity regardless of access point Note Summary: Operational context variations necessitate adaptation while preserving core functionality Prioritize content based on device context and operational roles Implement device-specific interfaces while maintaining consistent functionality Ensure device choice never impedes operational workflows Scale Considerations System scale fundamentally impacts implementation decisions across data architecture and visualization strategies. Each order of magnitude (10^4, 10^6, and 10^8) presents distinct optimization requirements that affect both performance and user experience. Landing page design must anticipate and adapt to these scale transitions to maintain effective operation. In industrial environments, scale considerations directly impact implementation choices. For example, a dashboard displaying real-time sensor data might use detailed line graphs when monitoring ten devices but must switch to aggregated heat maps when tracking thousands of sensors. Similarly, a landing page designed for a single manufacturing site requires different optimization strategies than one managing global operations. As noted by Peter van Hardenberg, increasing scale changes everything about a system - from data architecture to visualizations. Rather than attempting to design for all scale possibilities, implementations should target specific scale ranges and plan for transitions as operational needs evolve. Note Summary: Key principles for managing scale Match implementation to current operational requirements Implement appropriate data aggregation methods Select visualization techniques suitable for data volume Design and test at production-equivalent scale Plan system rebuilds at major scale transition points (10^4, 10^6, 10^8) Avoid over-engineering for future scale requirements Implement appropriate monitoring for scale-related performance metrics Conclusion Successful landing page implementation in industrial environments requires careful balance across multiple dimensions: performance requirements, data architecture, and user experience. Key success metrics include initial load time performance, data currency management, and operational workflow efficiency. These metrics must be evaluated within the context of device-specific requirements and scale considerations. Implementation teams should focus on delivering robust core functionality that meets essential operational needs. This approach aligns with industrial reliability requirements, where consistent performance takes precedence over non-essential features. As with choosing between a Toyota Corolla and a Tesla, the goal is to match implementation complexity to actual requirements - sometimes reliability and simplicity are key, while other situations may warrant more sophisticated solutions. Acknowledgements I want to acknowledge the valuable feedback from projects, clients, and customers that motivated me to compile these guidelines. Their practical experiences have shaped this document and reinforced the importance of performant landing page design in industrial applications. I have leaned towards the XMPro teams' experiences and intentionally not made this document a rehash of the ASM Consortium Guidelines Effective Console Operator HMI Design[1]. [1] https://www.asmconsortium.net/Documents/ASM_Handout_Display.pdf Appendix 1 - Summary of the integration of interface architecture and user navigation The usefulness of an XMPro page is the integration[1] of interfacing architecture and user navigation to enhance operational efficiency and support critical industrial functions. Here are some references and explanations that support this assertion: Navigation Design and User Experience: Effective navigation design is crucial for creating a seamless user experience. It involves analyzing and implementing ways for users to navigate through digital platforms efficiently, which directly impacts operational efficiency[2]. Research-based design principles, like user-centered design and iterative testing, ensure that navigation systems are intuitive and easy to use. Information Architecture Principles: Dan Brown's 10 principles of information architecture provide a comprehensive framework for designing user-friendly information systems. Principles like \"Focused Navigation\" and \"Multiple Classifications\" help ensure that users can access information efficiently, which is essential for operational efficiency[3]. These principles support the idea that well-structured information architectures enable critical functions by making information accessible and organized. Best Practices in Navigation and Information Architecture: Best practices in UX/UI design emphasize the importance of intuitive menus and navigation systems. These systems are fundamental in helping users explore and navigate through digital interfaces, which is critical for guiding users through workflows[4]. Effective organization of information and visual hierarchy are key considerations for ensuring that users can easily find what they need, thereby enhancing operational efficiency. Landing Page Design: While the statement specifically mentions landing pages, effective landing page design often incorporates principles of navigation and information architecture. For example, landing pages like those of Workable and Trello use minimalistic designs and intuitive navigation to guide users through workflows efficiently[5]. These designs are based on understanding the target audience's needs and behaviors, which aligns with research-based design principles. In summary, the integration of interface architecture and user navigation is supported by research-based design principles that enhance operational efficiency and support critical industrial functions. Effective navigation and information architecture are crucial for creating seamless user experiences and guiding users naturally through workflows, including landing pages. [1] https://www.finoit.com/articles/maximizing-user-experience-design-through-information-architecture/ [2] https://www.justinmind.com/blog/navigation-design-almost-everything-you-need-to-know/ [3] https://adamfard.com/blog/10-principles-information-architecture [4] https://www.linkedin.com/pulse/best-practices-navigation-information-architecture-florencia-marelli/ [5] https://www.getresponse.com/blog/landing-page-examples"
  },
  "docs/resources/practice-notes/unified-recommendation-alert-management.html": {
    "href": "docs/resources/practice-notes/unified-recommendation-alert-management.html",
    "title": "Unified Recommendation Alert Management | XMPro",
    "summary": "Unified Recommendation Alert Management XMPro Capabilities and Design Patterns, Updated Feb 2025 Glossary Acronym Description ADT Asset Digital Twin APM Asset Performance Management CBC Composable Business Capabilities CMMS Computerized Maintenance Management System EAM Enterprise Asset Management EWMA Exponentially Weighted Moving Average LOESS Locally estimated scatterplot smoothing LOWESS Locally weighted scatterplot smoothing PBC Packaged Business Capabilities RPN Risk Priority Number Audience The readers who will find this documentation most useful will have a working knowledge of XMPro and use XMPro to address asset performance business problems. It is suggested that new users of XMPro should workshop their requirements with their XMPro partners. It is useful to emphasise that XMPro is an Intelligent Business Operations Solution (iBOS). Introduction This document has arisen from the work done with partners and seeks to align some APM capabilities pertaining to Recommendation Alert Management with XMPro configurations based on standard XMPro capability. For ease of use we have labelled those configurations 'Design Patterns\". We call these patterns 'Composable Business Capabilities'. (CBC[1]) The customer will need to adapt the design pattern to their own situation. Some of the Design Patterns have been further developed and are available as Apps in our GitHub[2] – these are enabled 'Packaged Business Capabilities' (PBC)[3]. [1] Gartner Reference Model for Intelligent Composable Business Applications [2] https://xmpro.github.io/Blueprints-Accelerators-Patterns/ [3] Gartner Reference Model for Intelligent Composable Business Applications Summary High Level Summarization APM Capability Associated Design Pattern Application Description Management of Prioritized Recommendation Alerts Unified view of Recommendation Alerts, reliability health and risk scores Asset and alert rating - Asset criticality - Recommendation severity - Recommendation alert priority Recommendation alert management Priority map of asset criticality Strategy and Aggregator Pattern - Recommendation Alert Scoring (including Asset Criticality) - Workbench - Asset Analysis meta tags - Recommendation Analysis meta tags - Shutdown Prioritization of Assets subject to open prioritized Recommendation Alerts Unified view of Recommendation Alerts by asset Asset and alert rating - Asset criticality - Recommendation severity - Recommendation alert priority Priority map of asset criticality Strategy and Aggregator Pattern - Recommendation Alert Scoring (including Asset Criticality) - Asset Analysis meta tags Management of Recommendation Alerts during Shutdowns Capability to suspend alerts during a Shutdown process. State and Observer Pattern - Shutdown Ability to track Work Orders arising from Recommendation Alerts Reliability-Centered Maintenance (RCM) and Work Order Management Aggregator Pattern - Work Bench - Configured Recommendation Alert page Categorization of Recommendation Alerts Recommendation alert management Decorator Pattern - Recommendation Analysis meta tags Categorization of Assets Asset hierarchy will cater for user defined categorization. Asset hierarchy capability within the system and not rely on the historian asset hierarchies. Decorator Pattern - Asset Analysis meta tags Part 1: APM Capability 1. Management of Prioritized Recommendation Alerts and Prioritized Assets subject to Recommendation Alerts APM capabilities Unified view of Recommendation Alerts Asset and alert rating Asset criticality Recommendation severity Recommendation alert priority Recommendation alert management Priority map of asset criticality Design Pattern Recommendation alert scoring – Strategy Pattern Workbench – Aggregator Pattern Asset Analysis Meta Tags – Decorator Pattern Recommendation Analysis Meta Tags – Decorator Pattern APM requirements A number of the requirements are addressed out of the box. Design patterns can be used to further enhance the systems capability. Unified view The APM system will provide a unified view of the reliability health and risk scores through integration of asset strategy, condition monitoring, analytics, and APM data systems to measure cost, failure rates and compliance metrics. Priority The APM system will provide a standard process for defining the criticality of assets. The APM system will provide a standard process for defining the Risk Priority Number (RPN) / severity score of recommendation alerts. The APM system will provide a standard process for prioritising recommendation alerts by the measure of an alert's Risk Priority Number (RPN) and an asset criticality score. The APM system will provide a modifiable risk matrix that can be adjusted to the company's definition of risk. Recommendation management The APM system will provide the ability to create recommendations within each area of functionality that can be associated to an equipment ID or functional location. The APM system will provide means to track and follow up recommendations from several hierarchical levels perspective in the organization (site, areas, units, system, and assets). The APM system can provide the ability to schedule an alert email message to be sent to the person responsible for ensuring that the recommendation is addressed. The APM system will provide concise reporting and alerting capability to track outstanding and past-due recommendations. The APM system will provide the ability to initiate recommendations for further planning and execution. 2. Management of Recommendation Alerts during Shutdowns The Shutdown Management App uses the State and Observer Patterns to manage preplanned shutdowns effectively. During a shutdown, the app transitions assets to a \"Disabled\" state using the State Pattern, silencing recommendations and preventing the creation of equipment alerts based on anomalies. The Observer Pattern ensures stakeholders are notified about the shutdown schedule via email, enhancing communication and coordination. This approach maintains system integrity and operational efficiency by ensuring that no unnecessary alerts are generated during maintenance periods. APM capability Capability to suspend alerts during a Shutdown process. Associated Design Pattern Shutdown – Object, Observer patterns APM requirements A number of the requirements are addressed out of the box. However, the design patterns can be used to enhance the systems capability. The APM system will provide the capability to automatically suspend generate alerts during a planned shutdown / start up. The APM system will provide the capability to amend planned shutdown and startup times for a planned shutdown / start up. The APM system will optionally categorize, and store alerts generated during the shutdown and start up procedures. 3. Ability to track Work Orders arising from Recommendation Alerts APM capability Paper free integration to CMMS for work order process. Associated Design Pattern Work Bench – Aggregator Pattern Custom Recommendation Alert page – Aggregator Pattern APM requirements The APM system will provide linkage from the analysis of recommendation alerts to the resulting work order. The APM system will provide integration to a maintenance management / paper free work order process. 4. Categorization of Recommendation Alerts A Recommendation Meta Tag App uses the Decorator Pattern to dynamically enhance asset data without altering the original schema. By assigning meta tags such as performance metrics, maintenance recommendations, and operational statuses to assets, the app enriches contextual data, enabling more informed decision-making. This approach allows for flexible and scalable data enhancement, improving predictive maintenance, performance monitoring, and overall asset management within the APM framework. APM capability Ability to contextualize and categorizes alerts by customizable metrics for targeted filtering. Design Pattern Workbench – Aggregator Pattern Recommendation Analysis meta tags – Decorator Pattern APM requirements A number of the requirements are addressed out of the box. However, the design patterns can be used to enhance the systems capability. Recommendation management The APM system will provide the capability to filter and categorize alerts, and by fault mechanism. 5. Categorization of Assets An Asset Master Hierarchy App utilizes the Decorator Pattern to dynamically enhance the hierarchical representation of assets without altering the original asset structure. This app assigns hierarchical meta tags, such as parent-child relationships, asset dependencies, and location mappings to assets, for more informed decision-making. APM capability Asset hierarchy will cater for user defined categorization. Asset hierarchy capability within the system and not rely on the historian asset hierarchies. Associated Associated Design Pattern Asset Analysis meta tags – Decorator Pattern APM requirements A number of the requirements are addressed out of the box. However, design patterns can be used to enhance the systems capability. Recommendation management The APM system will provide the capability to filter and categorize assets. The APM system will provide the capability to allocate a criticality score to each asset. Part 2: XMPro configurations as Design Patterns This section articulates how using XMPro capabilities the APM requirement is addressed. This assembly of configured XMPro capability is the foundation for a low code XMPro App. The authors have used standard XMPro functionality to create the various Design patterns. This Practice Note is primarily concerned with a unified view of prioritized Recommendation Alerts. The most popular Design Pattern is definitely 'Work Bench', but we recommend that the other Design Patterns should be reviewed and considered. 1. Design Pattern: Work Bench The objective of the Recommendation Alerts Workbench design pattern is to allow for the prioritization, categorization and filtering of alerts. The workbench addresses the following APM capabilities: Unified view of Recommendation Alerts, reliability health and risk scores Asset and alert rating Asset criticality Recommendation severity Recommendation alert priority Recommendation alert management Priority map of asset criticality Generic querying, reporting, graphing, and searching capabilities for all asset types, alert histories, and work orders. Users have a high-level unified view to assist in their workflow process by providing the ability to see XMPro alerts filtered by criticality and status as well as associated WO's. In the above example the first three tabs focus on the status of Recommendation Alerts – Open, Assigned without WO and Assigned with WO. The landing page alert tabs will include all unassigned alerts. The last three tabs focus on linked WO and the appropriate status; Open, Complete WO and Closed WO. The filtering, tabs, actions, and various status would be set for your circumstance. The aim is to give the user situational awareness to all the elements of the Recommendation Alert. The above example page provides the relevant information on an alert to allow users to: See a holistic view of a piece of equipment (ability to see all alerts related to that asset) Any associated discussion which may provide insights into investigation and actions take Data at time of alert triggering Relevant metrics (schematics, score history) In the Figure 3 this shows an example of assigning many Alerts to one WO. This page shows all open work orders and associated Recommendation Alerts. Additionally, users can create custom recommendation pages which display relevant data to the alert including: Other alerts associated with the Asset ID Metadata associated with the alert Ability to tie a WO/WR to the alert All WO/WR available 2. Design Pattern: Asset Prioritization Asset Prioritization= (Severity (Recommendation Alert Setting) X Occurrence X Detectability) X Asset Criticality settings (assigned at Asset level). Severity set at Recommendation level with Recommendation Category factor, Recommendation Factor and Recommendation Rule Factor. Occurrence is measured in the Data Stream and updated with the 'Run Recommendation' agent. Note: For calculating Occurrences with tags, log on all occurrences should be enabled for your recommendations Occurrence can be calculated with two methods which is based on the data. A) Event Frames B) Tags Refer to Appendix A Note An occurrence may not necessarily correspond to a new Recommendation Alert. For example, when polling OSI PI tags, the current occurrence might be part of an existing fault that previously triggered an open Recommendation Alert, or it could trigger a new alert if none is currently open. In such cases, an occurrence count is used to indicate the number of polling intervals during which the alert logic is met. Conversely, if the Data Stream is ingesting OSI Event Frames (EF), then each new EF is treated as a separate occurrence. This distinction is vital for accurately defining the measure for occurrence count. Detectability is omitted as we assume that the Failure Mode is detectable if a recommendation rule exists. Default score = 1 Asset Criticality is assigned at an Asset level in the Asset Master, Asset Hierarchy or Asset meta tags. Ideally, this is pulled from a source system to match the existing criticalities you are utilizing for other reliability activities. If this is unavailable, this can be stored in XMPro. XMPro capability on Recommendation scoring [4]. Note We suggest the initial default values be 1. We would expect that there will be several cycles as the various score settings are fine tuned. [4] How is the scoring calculated 3. Design Pattern: Asset Shutdown The purpose of the app is to allow users to silence alert generation for specific assets in recommendations while still maintaining the published state of the recommendation. This activation/deactivation will take place automatically based on predefined start/stop dates. The application will also notify users when the shutdown will start and end to determine if a modification is necessary and as a verification step. In the examples below we have explained the concept detailing a shutdown by asset. However, we strongly suggest that the first iteration allows for shutdown by a level in the asset hierarchy or some other mechanism the sites may have. Example you may wish to shut assets within a certain location. If your organization analyses Recommendation Alerts across the organization, the analysis is not clouded by the need to omit alerts triggered during a shutdown and the subsequent start up. This page will be used to create new shutdowns and edit existing shutdowns. For existing shutdowns, assigned assets will appear below the shutdown grid. This page will be used to assign Assets to an existing shutdown. The \"Previously Selected\" column tells shutdown planners what Assets are already assigned to a shutdown. 4. Design Pattern: Recommendation Meta Tags Administration The Recommendation Meta Tag Application enriches asset data by dynamically adding meta tags with maintenance recommendations, operational statuses, and performance metrics. This contextual information aids in making informed maintenance decisions and optimizing asset performance. By enhancing data without altering the original asset schema, the application supports predictive maintenance and improves overall asset management, leading to increased operational efficiency and reliability. In this page we have opted to distinguish between 'Not Allocated', or 'Not Reviewed'. 'Not Allocated' means that the Meta Tag is not relevant to the Recommendation (In the above this is the preferred allocation as not blank) and 'Not Reviewed' means that no selection has been decided for this asset. In the page above we have assigned Meta Tag Values to a Recommendation. This page creates or edits the Meta Tag Values. The Meta Tag column will populate a dropdown of existing options from the list of available options Created on the Meta Tags page. This page contains a grid where the user can create or edit the Meta Tags. 5. Design Pattern: Asset Meta Tags Administration The Asset Meta Tag Application enhances asset management by dynamically adding meta tags to asset data. These tags include performance metrics, maintenance recommendations, and operational statuses, providing enriched contextual information. This additional data helps in predictive maintenance, performance monitoring, and informed decision-making without altering the original asset schema. The application enables better tracking and management of assets, leading to improved operational efficiency and reliability. In this page we have opted to distinguish between 'Not Allocated', or 'Not Reviewed'. 'Not Allocated' means that the Meta Tag is not relevant to the Asset (A preferred allocation rather than blank) and 'Not Reviewed' means that no selection has been made for this asset. This page will be used to assign Meta Tag Values to an Asset. Each Meta Tag will populate a row with the corresponding Meta Tag Values for selection from the dropdown. This page contains a grid where the user can create or edit the Meta Tag Values. The Meta Tag column will populate a dropdown of existing options from the list of available options Created on the Meta Tags page. This page contains a grid where the user can create or edit the Meta Tags. Appendix 1 – Calculation of Recommendation Alert and Asset Priority Scores Definitions Term Description Asset Criticality Score Asset Criticality is a value (1-10) you set for your site assets based on how critical those site assets are to you. A value of 1 indicates the lowest level of importance, while a value of 10 is absolutely critical to your company. Asset Criticality is assigned at an Asset level in the Asset Master, Asset Hierarchy, or Asset meta tags. Asset Priority Score Asset Criticality * Weighted Risk Priority Number for all applicable Recommendation Alerts Recommendation Alert Priority Score Risk Priority Number * Asset Criticality Risk Priority Number (RPN) Severity * Detectability * Occurrence Severity The severity of the failure mode is rated on a scale from 1 (low) to 1000 (high). A high severity rating indicates severe risk. Recommendation Category Factor * Recommendation Factor * Recommendation Rule Factor. Detectability Set at 1 as all asset conditions measured are detectable Occurrence The logarithmic function Frequency = 1/a * logb(mx) + c to measuring operational alarms in a system. 1. Approach to Scoring Occurrence for an individual Recommendation Alert 1.1 Event Frames (Logarithmic Formula) Event Frames monitor all occurrences unlike the Recommendation Alerts where the occurrence is measured at the polling. Consequently, a Recommendation Alert does not equal all occurrences. Outcome – Recommendation Alert Priority Score Formula: \\(Recommendation Alert Priority Score = (Severity * Occurrence* X Detectability) * Asset Criticality settings (assigned at Asset level)\\) Severity & Detectability as stated in 'Definitions' above. Occurrence is a measure of the count that the recommendation logic is met. We use the logarithmic function to calculate 'Occurrence'. The logarithmic function to measuring operational alarms in a system. \\(Occurance= 1/a* 〖log〗_b (mx)+c\\) Where: x is the number of alerts for that specific alert type a is a scaling factor m is the multiplier for x c is a vertical shift Examples for each constant: x: Number of alerts in the last x hours [x is a global parameter] Let's say x = 100 alerts were recorded in the past 24 hours. a: Scaling factor for the overall frequency Example: a = 0.5 A smaller value of 'a' will increase the overall frequency, making the system more sensitive to changes in the number of alerts. b: Base of the logarithm Example: b = 10 Using base 10 is common and makes the scale easy to interpret. Each order of magnitude increase in alerts will correspond to a unit increase in the log value. m: Multiplier for x (alert count) Example: m = 0.1 This scales down the number of alerts. If m < 1, it reduces the impact of large numbers of alerts, preventing the frequency from growing too quickly. c: Vertical shift Example: c = 3 This adds a constant to the result, effectively setting a minimum frequency even when there are very few alerts. The logarithmic function helps to compress the range of occurrence values, preventing the RPN from growing too quickly for assets with very frequent alerts. Adjusting the parameters a, b, m, and c allows you to fine-tune the sensitivity of the occurrence calculation to best fit your risk assessment needs. The formula can be enhanced to incorporate multiple variables. Example for a pump \\(1/a * logb(mx * V * T * F * E) + c\\) Where: x = number of alerts in the last 24 hours V = vibration level factor T = temperature factor F = flow rate factor E = efficiency metric factor 1.2 Tags (Using time decay) In the situation where Recommendation Alerts are created from polling asset tags, we normalize the occurrence count with a time decay function. \\[ Recommendation Alert Priority Score = (Severity * Occurrence* X Detectability) * Asset Criticality settings (assigned at Asset level) \\] Severity & Detectability as stated in 'Definitions' above. Occurrence is a measure of the count that the recommendation logic is met. This approach accounts for two factors: The gradual decrease in an alert's significance over time The varying sampling frequencies across different data sources. The resulting score reflects both the event's severity and its temporal relevance. The following equation can be used: \\(Occurrence= n* e ^ (-λt)\\) Where: \\(λ\\) is the decay rate \\(t\\) is the duration from the current time \\(n\\) is percentage of time an alert was open normalized To calculate \\(n\\), use the following ratio \\[ n=(Actual number of Occurrence for a duration)/(Total Potential Occurance in that duration) \\] For example, we have two alerts open. Alert 1: \\(λ\\) is .01 \\(t\\) is every 24 hours \\(n\\) is percentage of time an alert was open normalized Polling Rate = 12 hours Polling Duration = 24 hours Total Polling Time Frame = 3 Days Alert 2: \\(λ\\) is .01 \\(t\\) is every 24 hours \\(n\\) is percentage of time an alert was open normalized Polling Rate = 24 hours Polling Duration = 24 hours Total Polling Time Frame = 3 Days The duration in this scenario is set largest duration between polls (in this case 24 hours). If we had the following data for the alerts: Days 1 1 2 2 3 3 Alert 1 Poll x x x x 0 x Alert 2 Poll - x - x - x In the grid above, \"x\" is an occurrence, \"0\" is no occurrence, and \"–\" indicates no poll. To normalize so we would get the same number of time frames to calculate occurrence for, we would divide the number of total occurrences in each time frame by the number of potential occurrences Days Alert 1 Alert 2 Time (hrs) Alert 1 Occ. Score by Day Alert 2 Occ. Score by Day 1 2/2 1/1 48 .618 .618 2 2/2 1/1 24 .786 .786 3 1/2 1/1 0 .5 1 Total 1.90 2.40 Alert 2, which occurs at every interval, has an occurrence score of 2.4 with an aggressive decay factor. While the most recent occurrence receives the maximum score of 1.0, the high decay factor significantly reduces the weight of occurrences from three days ago. For comparison, we applied the same decay factor to Alert 1. However, Alert 1's latest value was normalized to 0.5 since the condition was true for only half of the potential instances. This normalization reduced Alert 1's occurrence score to 1.90, compared to Alert 2's score of 2.4. The difference in scores demonstrates how recent occurrences have a stronger influence than older ones. The combination of normalization and decay rates effectively handles both varying polling frequencies and the diminishing relevance of historical occurrences Scores can also be adjusted based on other factors. Below shows an example based on the type of alert and how different decay rates affect the scores if decay rates are based on recommendations. Hours Elapsed Critical (rate: 0.05) Vibration (rate: 0.1) Temperature (rate: 0.3) Pressure (rate: 0.5) Transient (rate: 0.8) 0 1.000 1.000 1.000 1.000 1.000 1 0.951 0.905 0.741 0.607 0.449 2 0.905 0.819 0.549 0.368 0.202 4 0.819 0.670 0.301 0.135 0.041 8 0.670 0.449 0.091 0.018 0.002 12 0.549 0.301 0.027 0.002 0.000 24 0.301 0.091 0.001 0.000 0.000 Decay rates can also be adjusted based on an asset's criticality. For example: Critical assets: Use lower decay rates (0.05 - 0.1) Non-critical assets: Use higher decay rates (0.3 - 0.5) Maintenance-dependent alerts: Use very low decay rates (0.01 - 0.05) 2. Priority by Asset for Multiple Recommendation Alert Types Outcome – Priority score by Asset For the various recommendation alert types for an asset, we need to calculate individual occurrence scores for each Recommendation Alert type and then aggregate them into a single asset-level occurrence score. This approach ensures that we consider all alert types while maintaining a single score for use in the RPN (Risk Priority Number) calculation. Step 1: Calculate Individual Occurrence Scores Calculated as per Section 1 or Section 2 Step 2: Aggregate Occurrence Scores After calculating individual occurrence scores, aggregate them into a single asset-level score. Here are three methods to consider: Maximum Method: Asset Occurrence = max(O₁, O₂, ..., Oₙ) Where O₁, O₂, etc. are occurrence scores for each alert type. Weighted Average: (Recommended) Asset Occurrence = (w₁ * O₁ + w₂ * O₂ + ... + wₙ * Oₙ) / (w₁ + w₂ + ... + wₙ) Where w₁, w₂, etc. are weights assigned to each alert type based on their importance. Logarithmic Sum: Asset Occurrence = exp(log(O₁) + log(O₂) + ... + log(Oₙ)) Weighted Average is recommended as the preferred method. the method that best fits your system's requirements and risk assessment strategy. Step 3: Normalize the Score Ensure the final asset occurrence score falls within your desired range (e.g., 1-10) by applying appropriate scaling or normalization. Step 4: Integrate with RPN Calculation for each asset Use this aggregated asset occurrence score in your RPN formula: RPN = Severity * Asset Occurrence * Detectability Example Calculation using Event Frames. Refer Section 1.2 for the calculation of occurrence for tags. Let's consider an asset with two alert types: Alert Type 1: x₁ = 100 alerts a = 0.5, m = 0.1, c = 3 Occurrence₁ = 1/0.5 * log₂(0.1 * 100) + 3 = 9.64 Alert Type 2: x₂ = 50 alerts a = 0.5, m = 0.2, c = 3 Occurrence₂ = 1/0.5 * log₂(0.2 * 50) + 3 = 9.32 Using the weighted average method with w₁ = 1.2 and w₂ = 1: Asset Occurrence = (1.2 * 9.64 + 1 * 9.32) / (1.2 + 1) = 9.50 Step 5: Calculate the Asset Priority Asset Priority = Asset Criticality * RPN Considerations Address assets with no alerts of a particular type by setting a minimum occurrence value (e.g., 1) or using zero, depending on your risk assessment strategy. The 'Run Recommendation' agent should update the 'x' values (number of alerts) for each alert type. The logarithmic function moderates the growth of the occurrence score for assets with frequent alerts while still capturing the significance of alert frequency. Regularly review and adjust the parameters (a, m, c, and weights) to ensure the system accurately reflects your organization's risk priorities. Appendix 2 - Analysis of Risk Priority Number (RPN) Implementation in Asset Performance Management: Limitations and Opportunities Abstract We examine the theoretical foundations and practical limitations of Risk Priority Number (RPN) implementation in Asset Performance Management (APM) systems. While RPN serves as a cornerstone metric in failure mode and effects analysis (FMEA), significant challenges emerge in its practical application. Through critical analysis of current literature and industry practices, this study identifies key limitations in RPN methodology and proposes potential enhancements for more effective risk assessment in industrial settings. Introduction Risk Priority Number (RPN) has long served as a fundamental tool in risk assessment and failure mode analysis. However, as industrial systems grow in complexity and the demands for precise risk quantification increase, the traditional RPN methodology faces several challenges that warrant careful examination. This paper aims to critically analyse these limitations and explore potential solutions for modern industrial applications. 1. Theoretical Framework 1.1 Traditional RPN Calculation The traditional RPN calculation follows the formula: RPN = Severity × Occurrence × Detection Where: Severity (S) represents the seriousness of failure Occurrence (O) represents the likelihood of failure Detection (D) represents the probability of detecting failure before impact 1.2 Mathematical Limitations 1.2.1 Scale Constraints Despite the theoretical range of 1-1000, the multiplicative nature of RPN produces only 120 unique values, creating significant gaps in the risk assessment spectrum. This limitation impacts the granularity of risk differentiation and can lead to clustering of risk scores. 1.2.2 Equal Weighting Problem The multiplicative relationship between factors assumes equal importance of S,"
  },
  "docs/resources/sizing-guideline.html": {
    "href": "docs/resources/sizing-guideline.html",
    "title": "Sizing Guideline | XMPro",
    "summary": "Sizing Guideline This is a guideline for the compute resources needed for the different components in a deployment. Small, medium, and large sizing estimates are provided. The small option starts with the minimum recommended resources and, generally, each subsequent size doubles the number of CPU cores and available RAM. Not all components experience the same increase in load, so the estimates may not increase at the same rate for all components. Many factors influence the number of Apps and Data Streams a deployment can effectively run. These factors include: the number of data streams, how frequently the streams process data, the size of the data payload, the number of recommendations to be monitored, the number of apps and event boards being served, the complexity of apps and event boards (the number of elements and integration points), and the number of concurrent users accessing the apps and event boards. As a rough guide, an example workload for a Medium-sized deployment would be: ~200 Data Streams running across ~15 Stream Hosts, serving data and triggering recommendations for ~10 Apps On-Premise Component Small Medium Large Subscription Manager (SM) 1 2 CPU 8GB RAM 2 CPU 8GB RAM 4 CPU 16GB RAM Application Designer (AD) 2 CPU 8GB RAM 4 CPU 16GB RAM 8 CPU 32GB RAM Data Stream Designer (DS) 2 CPU 8GB RAM 4 CPU 16GB RAM 8 CPU 32GB RAM Stream Host Server (SH) 2,3 2 CPU 8GB RAM 4 CPU 16GB RAM 8 CPU 32GB RAM SQL Database Server (Combined for SM, AD, DS) 4 2 CPU 8GB RAM 4 CPU 16GB RAM 8 CPU 32GB RAM Note Footnotes 1 High volumes of concurrent users may require additional compute. 2 Multiple Stream Hosts can be deployed to the Stream Host Server. 3 If the Stream Host needs more resources, consider increasing the RAM before adding additional CPU cores as Stream Hosts perform in-memory processing of events. 4 High volumes of recommendations may require additional compute and storage. Azure Estimates for Azure target the Premium v3 service plan for applications, and Azure SQL Database for the databases. Azure SQL database estimates are based on the General-Purpose service tier and use the DTU-based purchasing model (a blended measure of compute, storage, and IO resources). Component Small Medium Large Subscription Manager (SM) App Service Plan 1 P1v3 P1v3 P2v3 Application Designer (AD) App Service Plan P1v3 P2v3 P3v3 Data Stream Designer (DS) App Service Plan P1v3 P1v3 P2v3 Stream Host Server (SH) App Service Plan 2,3 P1v3 P2v3 P3v3 Azure SQL Database (For each of SM, AD, DS) 4 Standard – 20 DTUs Standard – 50 DTUs Standard – 100 DTUs Note Footnotes 1 High volumes of concurrent users may require additional compute. 2 Multiple Stream Hosts can be deployed to the Stream Host App Service Plan. 3 If the Stream Host needs more resources, consider increasing the RAM before adding additional CPU cores as Stream Hosts perform in-memory processing of events. 4 High volumes of recommendations may require additional compute and storage. For additional details please see Azure App Service Pricing and Azure SQL Database Pricing. AWS Estimates for AWS target Amazon EC2 T3 instances for applications, and an Amazon RDS T3 instance for the databases. Component Small Medium Large Subscription Manager (SM) EC2 Instance 1 t3.large t3.large t3.xlarge Application Designer (AD) EC2 Instance t3.large t3.xlarge t3.2xlarge Data Stream Designer (DS) EC2 Instance t3.large t3.large t3.xlarge Stream Host Server (SH) EC2 Instance 2,3 t3.large t3.xlarge t3.2xlarge Amazon RDS for SQL (Combined for SM, AD, DS) 4 t3.large t3.xlarge t3.2xlarge Note Footnotes 1 High volumes of concurrent users may require additional compute. 2 Multiple Stream Hosts can be deployed to the Stream Host Server. 3 If the Stream Host needs more resources, consider increasing the RAM before adding additional CPU cores as Stream Hosts perform in-memory processing of events. 4 High volumes of recommendations may require additional compute and storage. For additional details please see AWS EC2 and RDS instance types."
  },
  "docs/resources/whats-new/index.html": {
    "href": "docs/resources/whats-new/index.html",
    "title": "What's New in 4.4 | XMPro",
    "summary": "What's New in 4.4 Overview In this release we have concentrated our efforts on two key initiatives: the cloud-to-edge continuum and AI & engineering excellence. These areas reflect our commitment to delivering cutting-edge solutions and ensuring excellence in artificial intelligence and engineering practices. This page shows a curated selection of features. For more details on what's in the latest version, please read the Release Notes. Note Image Migration Note: Fig 1: The areas in focus this release - This image needs to be created and placed in the images directory. Cloud-to-Edge Continuum Theme The XMPro platform needs to be performant, scalable, and monitored to be fully cloud-agnostic. We strive to implement industry best practices to achieve this. Stream Host Improvements Stream Hosts are vital to Data Stream performance. We've rewritten the Stream Hosts to guarantee higher levels of performance. As part of the site's upgrade to v4.4.0, you will also need to install the new Stream Hosts. Stream Hosts are optimized for reliable orchestration of data streams and interactions such as publish, unpublish, sync with Data Stream Designer, and handling network disconnections. This ensures more reliable and performance-driven data flow management across cloud and edge environments. Enhanced logging capabilities, including detailed agent lifecycle events, provide deeper insights into the operation of Stream Hosts, facilitating better monitoring and troubleshooting. AI and Engineering Excellence Theme AI and Engineering is a core pillar, aiming to harness algorithms, data insights, and computational methodologies that enhance our product's capability to build innovative solutions that address complex challenges. Metablocks The new feature, Metablocks, is the first step towards plug-and-play Blocks. The first two Metablocks, Unity and Unity (Legacy), demonstrate how the modular approach improves performance. It opens the ability to support different web technologies. Metablocks represent a significant advancement in XMPro's Application Development Platform, offering a flexible, secure, and performance-oriented solution for web application development. These blocks are designed to empower engineers and developers by enhancing app performance, security posture, and overall development experience within the App Designer (AD) environment. See Metablocks for a full description. Tree Map Block Note Image Migration Note: Fig 2: Tree Map Block - This image needs to be created and placed in the images directory. This new App Designer block (v4.4.0) allows you to visualize hierarchical data as a set of nested rectangles whose sizes are proportional to the visualized values. You can create tactical visualizations that showcase both hierarchical and magnitude data about operations, rather than 2D lists. For example, the relative number of recommendation alerts across different asset class and location segments. Live Feed Block Note Image Migration Note: Fig 3: Live Feed Block - This image needs to be created and placed in the images directory. This new block (v4.4.0) allows you to incorporate your IP Live Feed cameras into an application. Recommendation Analytics Block and Recommendation Alert Discussion Block These two new blocks (v4.3.2) pave the way for composable Recommendation Alert pages. In future releases, we will build on the composable recommendation functionality, enabling you to design your recommendation application – should you choose not to use the out-of-the-box option. Recommendation Analytics gives a quick view of the percentage change and alerts generated for an asset – ideal for analyzing recommendations and supporting decision-making processes. Recommendation Alert Discussion facilitates collaboration by allowing teams to discuss and act upon recommendations within the platform. Global Notification Communicate important information to all users across the XMPro suite with the new Global Notification feature. Global Administrators can display a global notification for a specific period, such as planned maintenance downtime along with a hyperlink to release notes. Choose from a type of hint, warning, or error to style the notification icon and banner color. Note Image Migration Note: Fig 4: A global notification appears on the landing page - This image needs to be created and placed in the images directory. Admin Reports Do you want to upgrade your Agent and Connector integrations but are unsure which versions are currently loaded or where they've been used? We've added reports (v4.3.7) to support designers in creating an upgrade path. A master report to find the most recent version installed, and a detail report to identify Applications and Data Streams that are using older versions. Note Image Migration Note: Fig 5: Data Stream Designer's Agent Usage Report - This image needs to be created and placed in the images directory. Support Issues Addressed We addressed two major support issues. Subscription Manager Memory Issue We've resolved a critical memory management issue, ensuring smoother operation and enhancing the platform stability (v4.4.0). Live View Issue We've resolved a connection management issue by automatically turning off Live View (v4.3.7). This reduces the additional load on the Data Stream Designer. Conclusion This update is primarily about the improvement and scaling of Stream Hosts as part of our commitment to performance within our Cloud to Edge Continuum theme. We also have many new features as part of the AI & Engineering Intelligence Themes. Previous Versions What's New in 4.3 What's New in 4.2 What's New in 4.1.13 What's New in 4.1 What's New in 4.0"
  },
  "docs/resources/whats-new/whats-new-in-4.0.html": {
    "href": "docs/resources/whats-new/whats-new-in-4.0.html",
    "title": "What's New in 4.0 | XMPro",
    "summary": "What's New in 4.0 Overview This page shows a curated selection of features we've released in XMPro version 4. For more details on what's in the latest version, please read the Release Notes. Recommendations Forms Create forms for your recommendations to help your team send requests, complete on-site inspections and create work orders in other systems like SAP EAM. Discussions Discussions provide a space to collaborate with colleagues while keeping the conversation part of the recommendation alert audit trail. Analytics See which recommendations get triggered most often for a specific asset or entity to help you identify recurring issues. Alert Timeline Get full visibility into the timeline of a recommendation and who has interacted with it. Notifications Set up rule-based notifications to trigger when an alert gets generated, if it has been pending for a period of time, or if a team member leaves a note. Users can also manage their own notification settings to decide which alerts they would like to receive. Mobile Experience Whether your team works underground or out in the field, they can use XMPRO in the browser on their mobile device to get access to the recommendation alerts they need to respond to critical events. This update also provides offline capability and is available on both iOS and Android devices. Azure Digital Twins XMPro version 4 includes two new Agents and a Connector to help you make the most of Azure Digital Twins without coding. Azure DT Context Provider The new Context Provider allows you to fetch contextual data like the asset make and model from Azure Digital Twins. Azure DT Action Agent The Action Agent enables you to create instances in Azure Digital Twins, upload DTDL models and set up new resources without having to leave the XMPro interface. You can also automatically send data to Time Series Insights when the Azure Digital Twin receives new real-time data. Azure DT App Designer Connector This addition to the toolbox makes it easy to integrate Azure Digital Twins with your XMPro applications. You can use Azure DT as a data source or even create custom XMPro Apps with forms that update your digital twins in Microsoft Azure DT. Recurring Data Streams This functionality allows you to run recurring Data Streams on a customizable schedule, for instance, once a day at 12 am. This may be useful if you only want to read data or perform an action with the data at certain points during the day, or if you want to perform actions on the data once a week, month or year. App Designer Toolbox We're constantly adding new blocks to the App Designer toolbox to provide you with more no-code functionality for your real-time apps. Here are some of the latest additions to the toolbox: Unity - leverage the powerful gaming engine for real-time 3D visualizations and simulations D3 - produce custom interactive data visualizations with fast loading times Esri Maps - display maps with geographical, terrain, or topographical overlays PowerBI - embed reports and historical data analytics visualizations into your applications Autodesk Forge - embed 2D and 3D engineering designs in your apps"
  },
  "docs/resources/whats-new/whats-new-in-4.1.13.html": {
    "href": "docs/resources/whats-new/whats-new-in-4.1.13.html",
    "title": "What's New in 4.1.13 | XMPro",
    "summary": "What's New in 4.1.13 Overview This page shows a curated selection of features we've released in XMPro version 4.1.13. For more details on what's in the latest version, please read the Release Notes. App Designer Azure Digital Twin Hierarchy Block This new block in the XMPro App Designer toolbox allows you to connect your asset hierarchy in Azure Digital Twins to your time series data in Azure Data Explorer (all without coding). You can now visually navigate your asset hierarchy across regions, plants and sites and compare parameters across different assets. Image Map Block The updated Image Map block allows you to overlay dynamic and static content onto a background image. You can add indicators that change color onto assets in your plant, show text values that update with live sensor data, and move markers based on x and y coordinates in a database. The new version of this block maintains the aspect ratio for both the background image and overlay content, making it much simpler to create a design that works on different screen sizes. Unity Block Take advantage of the Unity platform and performance improvements with the new Unity block that supports Unity 2020 and above. The legacy Unity block is still available for use in your existing applications. Connector Logs Spend less time troubleshooting in the XMPro App Designer with the new Connector Logs feature. Whether you're an App composer, the developer writing a new Connector, or an Administrator, you can now easily view messages generated by a Connector. Data Stream Designer 'Started On' Stream Metric Stream Metrics provides helpful information about the health of your Data Stream. We've added a Started On metric that shows the number of Stream Hosts on which the Data Stream started vs the total number of Stream Hosts online across all Collections. This metric is especially useful to alert you when the Data Stream is first published that it failed to start on one or more Stream Hosts. Subscription Manager Sync Business Roles to Azure Active Directory We're continuously working to simplify access management for our users. If you're using Azure AD as your External Identity Provider, you can now manage your XMPro business roles from within Azure AD as well. You can set this up by specifying a claim name that Azure AD or the graph API will pass to the XMPro Subscription Manager. When a user logs in, Subscription Manager will look at the value specified in the Azure AD claim and assign them to the XMPro Business Role with the same name."
  },
  "docs/resources/whats-new/whats-new-in-4.1.html": {
    "href": "docs/resources/whats-new/whats-new-in-4.1.html",
    "title": "What's New in 4.1 | XMPro",
    "summary": "What's New in 4.1 Overview This page shows a curated selection of features we've released in XMPro version 4.1. For more details on what's in the latest version, please read the Release Notes. App Designer Time Series Chart Block This new block in the App Designer toolbox allows you to visualize large amounts of time series data to give your engineers and operators a real-time view of your asset health. You can use a variety of data sources to populate your Time Series Chart, but a key benefit is that the asset hierarchy can be integrated with Azure Digital Twins and the telemetry data can be loaded from Azure Data Explorer. Useful features built into the Time Series Chart block include the ability to drop markers on the chart to denote important events, as well as panning and zooming to change the timescale. You can show or hide assets and parameters via the hierarchy panel. This visualization makes it easy to compare different assets with each other across multiple parameters, like temperature, pressure and vibration. Grid Block We've expanded the functionality of the Grid Block in the App Designer toolbox to include visual indicators, which allow you to show the state of an item in the grid. This is particularly useful if you want to use colours to indicate the health of an asset or the status of a root cause analysis. There are also new column options for select boxes and hyperlinks, making it easier to choose from a predefined list or navigate to a subpage from the grid. You can set default filters to automatically organize your grid data when you load the page. And we've added support for column reordering, resizing and state persistence. Notification Templates XMPro Recommendations allow you to send automated email and SMS notifications to inform your team about a critical event occurring in your data. You can now create custom templates for those notifications simply by uploading an HTML file for email or adding your text message for SMS. Placeholders in your templates enable you to replace static text in your notifications with real-time data about the recommendation alert like its title, how long it has been pending, and the real-time data that triggered the alert. Data Stream Designer Stream Metrics We've added Stream Metrics to Data Streams to give you more visibility into the health of your Data Streams. Once a Data Stream is published, you will be able to see how much data is being processed per minute as well as how many errors were generated by Agents in the Stream. You'll also have better visibility into how many of your Stream Hosts are online. Error Flow How do you know when your Data Stream health is deteriorating? With the new Error Flow feature you can create a separate set of actions that trigger when your Data Stream encounters errors. For example, the Data Stream can send out an email to notify you if the issues if there are more than a certain number of errors occurring per minute. Connector Documentation We've made major updates to our documentation for the growing XMPro connector library. Our team has added more example videos and GIFs to help you confidently configure agents on your own. Platform Security As part of our continuous commitment to security, we've implemented quarterly scans of our platform. Veracode scans the software and tests it in multiple ways, including using Static Analysis (white-box testing), Dynamic Analysis (black-box testing), and Software Composition Analysis. The results and reports from the testing are available for review here."
  },
  "docs/resources/whats-new/whats-new-in-4.2.html": {
    "href": "docs/resources/whats-new/whats-new-in-4.2.html",
    "title": "What's New in 4.2 | XMPro",
    "summary": "What's New in 4.2 Overview This page shows a curated selection of features we've released in XMPro version 4.2. For more details on what's in the latest version, please read the Release Notes. Application Designer Recommendation and Alert Scoring When setting up a Recommendation, authors are now able to fine-tune the Alert Recommendation priority by changing its score. By contrast, Alert Ranking only has options for High, Medium, and Low. Alert Scores are calculated based on these factors: Recommendation - The importance of the recommendation itself Recommendation Category - The importance of the recommendation's category Recommendation Rule - The importance of the specific rule Recommendation Optional- Additional Rule Factor value retrieved from the Data Stream. As a recommendation creator, assigning a score to an alert lets you control its importance level more precisely. This Score helps the alert recipient to understand its relative importance. Viewing the Alerts You can view the order of alerts in the Recommendation Alerts list. You can also view the Scores using the Score Factor Matrix. Follow the steps mentioned here. Auto-Assigning of an Escalated Recommendation Alert This enhancement automatically assigns an escalated recommendation alert to the previous owner of the original alert. To use this, simply check \"enable execution order\" and \"auto-escalate\" with the specified rules. You can view this as a timeline entry that creates an audit trail. Query Optimization for AD Experience faster performance and quicker AD queries through optimized Entity Framework settings for database queries. Subscription Manager New Permission - Hide Users Outside of Business Roles This new permission prevents users' information from being exposed to users who are not in the same business role group or any of the parent business role groups, enhancing privacy and security. Data Stream Designer Agent Category Visual Indicator Introduced color palettes as visual cues for agent categories. As a user, you can now quickly distinguish between listeners, context providers, transformations, etc."
  },
  "docs/resources/whats-new/whats-new.html": {
    "href": "docs/resources/whats-new/whats-new.html",
    "title": "What's New in 4.3 | XMPro",
    "summary": "What's New in 4.3 Overview Introducing the XMPro 4.3 release, where we continue to prioritize initiatives aligned with our higher goals of achieving faster time to value, distributed intelligence, and secure deployments. Our product roadmap outlines how each initiative serves a specific purpose within these categories. We have concentrated our efforts on two key initiatives: the cloud-to-edge continuum and AI & engineering excellence. These areas reflect our commitment to delivering cutting-edge solutions and ensuring excellence in artificial intelligence and engineering practices. Note Image: Fig 1: The areas in focus this release. (Image not available) Cloud-to-Edge Continuum The XMPro platform needs to be performant, scalable, and monitored in order to be fully cloud agnostic. We strive to implement industry best practices to achieve this. Auto Scale - Distributed Caching This feature aligns with the EDGE continuum bucket, enabling XMPro to run distributed infrastructure essential for HDT cloud computing. Auto Scale, XMPro's implementation of caching has been overhauled with a distributed storage feature that promises improved caching capabilities. It offers a superior caching approach that is highly recommended, particularly for larger production-ready implementations. It uses a technology called Redis, a flexible technology that offers a distributed storage feature that makes use of multiple smaller cache entries. In this setup, some nodes act as masters, handling the processing of data, while others serve as backups. This way, if one node goes down, the others can take over to keep the system running smoothly. These changes improve the performance and reliability of our caching system, ensuring that data is stored and accessed efficiently, particularly for larger production-ready implementations. Health Check Endpoints The introduction of health check endpoints is essential for cloud-agnostic applications as they enable platform independence and decouple it from specific cloud provider dependencies, like Azure's Application Insights. Health check endpoints play a crucial role by allowing easy identification of problems without requiring extensive technical knowledge or login credentials. They provide a snapshot of each XMPro product's health, including its interconnected components and overall system status. Administrators can configure health checks and add additional systems to monitor, further enhancing the product's stability and performance. The output can be consumed by your preferred provider, such as Azure's Application Insights, or use the default Health UI. Logging Provider Support Logging Provider Support is a new feature that introduces the implementation of Serilog, a diagnostic logging library for .NET applications. This library enables the capture of log events with structured data. Providing administrators with valuable insights into the behavior and performance of XMPro. Three logging outputs are supported: Logging to file support has been added for all XMPro products, whereas Application Insights and Datadog support has been added for all products aside from Subscription Manager. These are cloud-based application monitoring and analytics services. Deployment Automation We've changed the way our database installs and upgrades are applied. For new installs, our products will automatically install the required database changes. For upgrades, our products will detect what database changes are needed and make these. We are moving away from doing database installs and upgrades from the desktop installer, with all database installs and upgrades happening automatically from within the products. Accelerate time to value by choosing to automatically deploy the regular, smaller releases to your pre-prod environment, rather than less frequent, larger upgrades. AI and Engineering Excellence The new XMPro AI article describes various ways in which AI is infused into the Digital Twin Platform. XMPro Notebook This release introduces the XMPro Notebook, which is an embedded version of Jupyterhub and will be available for evaluation on new XMPro Freemium accounts. XMPro Notebook provides an intuitive and flexible interface for data analysis, scientific computing, machine learning, and more. Users can write code and execute cells independently, which facilitates step-by-step exploration and experimentation with real-time data. Existing customers and Freemium users can contact us for access and licensing options. Please visit XMPro AI for more information about XMPro AI and XMPro's Intelligent Digital Twin Suite. Application Designer's Time Series Chart Performance Performance of the Time Series Chart Block, when using the new TSC SQL Connector, has been significantly enhanced due to optimized client-side querying. The advantage of this Connector is that it is optimized for server-side processing of the required grouping into buckets needed for the Time Series Chart data points, reducing the response time and size, and enabling quicker retrieval of data for longer periods. Look out for the future release of TSC-optimized Azure Data Explorer and Historian Connectors. Note Image: Fig 4: XMPro Integration: TSC SQL Server Connector. (Animation not available)"
  },
  "index.html": {
    "href": "index.html",
    "title": "What is XMPro? | XMPro",
    "summary": "What is XMPro? XMPro's Application Development Platform empowers engineers and subject matter experts to build real-time applications without coding. The platform consists of 3 main software components: XMPro App Designer A visual page designer that enables you to create custom page designs by dragging blocks from the toolbox onto your page, configure their properties and connect to your data sources, all without having to code. XMPro Data Stream Designer A drag-and-drop interface to visually design Data Streams (a streaming data pipeline). Use XMPro Connectors in your Data Streams to bring in real-time data from a variety of sources, add contextual data, apply analytics, and initiate actions based on events in your data. XMPro Notebook Harnessing the power of the Jupyter Notebook, XMPro Notebook provides an intuitive and flexible interface for data analysis, scientific computing, machine learning, and more. Users can write and execute code independently, facilitating step-by-step exploration and experimentation with real-time data. XMPro Connectors XMPro's extensible integration library includes 100+ Connectors for industrial automation solutions, IoT platforms, historians, enterprise applications, AI/ML, and collaboration solutions. Watch The Demo Video Watch the demo video to see XMPro's platform in action How The Documentation Is Organized Getting Started - New here? Sign up for a free trial and get started with the End-To-End Use Case sample project. Resources - A goldmine of general articles, such as release news, a sizing guideline, an icon library, and FAQs to elevate your product experience. Concepts - Get detailed explanations of the platform's essential concepts, like Data Streams, Recommendations, Applications, and Connectors. How-To Guides - Follow step-by-step tutorials to help you create Apps and Data Streams. Blocks - Get detailed descriptions of the components you can use to design your App pages and how to configure them. Administration - Find out how to manage users, licenses, and subscriptions in XMPro. This documentation is only relevant to administrators. Installation - Learn how to Install XMPro in a variety of environments. This documentation is only relevant to administrators. Release Notes - Stay up to date on the latest features and bug fixes. Note TODO remove Can't find what you're looking for? Search the docs for instant results or contact support. Try searching a phrase - such as a release version \"v4.3.2\" to find pages added for that release. Try asking a question - for best results use a full sentence rather than a phrase. DocFX powers Documentation.xmpro.com. The search functionality is driven by an index which includes all XMPro documentation, video tutorials, blogs, publications, and the website. Note The Beta tag indicates incremental functionality, added to prepare for a future feature."
  },
  "migration-docs/IMAGE-VERIFICATION-README.html": {
    "href": "migration-docs/IMAGE-VERIFICATION-README.html",
    "title": "Image Reference Verification | XMPro",
    "summary": "Image Reference Verification This tool helps verify and fix image references in markdown files, ensuring that all image links correctly match the actual image filenames. It's particularly useful after migrations where image filenames may have been updated (e.g., replacing spaces with underscores) but the markdown references weren't updated. The Problem During migration processes, image filenames often change to follow consistent naming conventions (e.g., replacing spaces with underscores), but the markdown files that reference these images aren't always updated to match. This results in broken image links. For example, a markdown file might reference: ![Alt text](images/XMPro%20Notebook License.png) But the actual image file is named: XMPro_Notebook_License.png The Solution The incremental-verify-image-references.ps1 script scans markdown files for image references, checks if the referenced images exist, and attempts to find alternatives if they don't. It can automatically fix the references to point to the correct files. Key features: Processes files in batches to avoid token limit issues Saves progress between runs, allowing you to resume interrupted verification Prioritizes Title_Snake_Case_Convention for image filenames Provides detailed reporting on issues found and fixed Exports results to CSV for further analysis Usage Basic Usage To check for image reference issues without making changes: .\\incremental-verify-image-references.ps1 To automatically fix image references: .\\incremental-verify-image-references.ps1 -fix Advanced Options The script supports several command-line options: Option Description -fix Fix issues automatically (default: check only) -dir <path> Specify the root directory to scan (default: current directory) -prefer-snake-case Prefer Title_Snake_Case_Convention for filenames (default) -prefer-kebab-case Prefer kebab-case for filenames -batch-size <number> Number of files to process in each batch (default: 10) -progress-file <path> Path to the progress file (default: image-verification-progress.json) -csv-report <path> Path to the CSV report file (default: image-verification-report.csv) -continue Continue from the last run using the progress file -no-csv Disable CSV report generation Examples Process files in batches of 20 and fix issues: .\\incremental-verify-image-references.ps1 -fix -batch-size 20 Continue a previous run: .\\incremental-verify-image-references.ps1 -continue Scan a specific directory: .\\incremental-verify-image-references.ps1 -dir docs-docfx/docs How It Works The script scans markdown files for image references (both ![alt](path) and <img src=\"path\"> formats) For each reference, it checks if the image exists at the specified path If the image doesn't exist, it tries to find alternatives by: Checking multiple directories (same directory, images subdirectory, parent's images directory, etc.) Trying different filename formats (spaces, underscores, hyphens) URL decoding the filename If an alternative is found, it can automatically update the reference in the markdown file Progress is saved after each batch, allowing you to resume interrupted verification Reporting The script generates two types of reports: Console output: Detailed information about issues found and fixed CSV report: A file containing all issues, which can be opened in Excel or other tools for further analysis Avoiding Token Limits When running the script with AI assistance, you might encounter token limits that prevent the script from completing. To address this issue, we've created wrapper scripts that run the verification with optimized settings. Standard Wrapper Script .\\run-image-verification.ps1 -fix This wrapper script has the following benefits: Uses a default batch size of 3 (configurable) Handles the fix and continue parameters Helps avoid token limit issues Options for run-image-verification.ps1 Option Description -fix Fix issues automatically (default: check only) -batchSize <number> Number of files to process in each batch (default: 3) -continue Continue from the last run using the progress file -resetProgress Reset the progress file before starting (useful when encountering token limits) Examples Run with default settings (batch size 3, check only): .\\run-image-verification.ps1 Run with fix mode enabled: .\\run-image-verification.ps1 -fix Continue a previous run with a custom batch size: .\\run-image-verification.ps1 -continue -batchSize 5 Optimized Scripts for Severe Token Limit Issues If you still encounter token limit issues with the standard wrapper script, we've created optimized versions that use minimal memory: .\\run-image-verification-optimized.ps1 -fix The optimized scripts include: incremental-verify-image-references-optimized.ps1: An enhanced version of the original script with: Minimal memory usage option that stores only essential data in memory Immediate CSV export of issues to reduce memory footprint More frequent progress saves Improved progress tracking with issue counts Better handling of large datasets run-image-verification-optimized.ps1: A wrapper for the optimized script that: Enables minimal memory usage by default Provides options for custom progress and report files Includes enhanced guidance for handling token limits Options for run-image-verification-optimized.ps1 Option Description -fix Fix issues automatically (default: check only) -batchSize <number> Number of files to process in each batch (default: 3) -continue Continue from the last run using the progress file -resetProgress Reset the progress file before starting -progressFile <path> Custom path for the progress file -csvReport <path> Custom path for the CSV report Examples Run with minimal memory usage: .\\run-image-verification-optimized.ps1 Continue a previous run with minimal memory usage: .\\run-image-verification-optimized.ps1 -continue Use custom progress and report files: .\\run-image-verification-optimized.ps1 -progressFile \"progress-part1.json\" -csvReport \"report-part1.csv\" Handling Token Limit Issues If you encounter token limit errors when running the script with AI assistance, try these approaches: Use the optimized script with minimal memory usage: .\\run-image-verification-optimized.ps1 -batchSize 1 Use custom progress files for different sections: # First run: .\\run-image-verification-optimized.ps1 -progressFile \"progress-part1.json\" -csvReport \"report-part1.csv\" # Second run (different section): .\\run-image-verification-optimized.ps1 -progressFile \"progress-part2.json\" -csvReport \"report-part2.csv\" -dir \"docs/another-section\" Reset progress and start with a small batch: .\\run-image-verification-optimized.ps1 -batchSize 1 -resetProgress Run in multiple sessions: Run with a small batch size until you hit a token limit Close the AI session to clear the token context Start a new session and continue with the -continue flag Troubleshooting If you encounter any issues: Check the progress file to see which files have been processed Try running with a smaller batch size if you're hitting token limits Use the -continue option to resume from where you left off Check the CSV report for details on specific issues If the progress file becomes too large, use the -resetProgress option to start fresh Switch to the optimized scripts if you're consistently hitting token limits Use separate progress files for different parts of the verification process"
  },
  "migration-docs/NAVIGATION-FIX-README.html": {
    "href": "migration-docs/NAVIGATION-FIX-README.html",
    "title": "Navigation Fix Tool | XMPro",
    "summary": "Navigation Fix Tool This tool helps identify and fix navigation issues in the DocFX documentation. It addresses problems that prevent proper verification of image references and other content. The Problem After migrating content from GitBook to DocFX, several navigation issues can occur: Broken TOC links: Links in table of contents (TOC) files that point to non-existent files TOC inconsistencies: Different TOC files referencing the same content with different paths Broken internal links: Links within markdown files that point to non-existent files These issues prevent proper navigation through the documentation and make it difficult to verify if image references are working correctly. The Solution The fix-navigation-issues.ps1 script scans the documentation for navigation issues and automatically fixes them. It: Finds and fixes broken links in TOC files Identifies and resolves inconsistencies between TOC files Locates and repairs broken internal links in markdown files Creates placeholder files when necessary Generates a summary report of issues found and fixed Usage To run the navigation fix script: cd docs-docfx .\\fix-navigation-issues.ps1 The script will: Scan all TOC files in the docs directory Identify issues and inconsistencies Fix the issues automatically Generate a summary report How It Works The script performs the following steps: TOC File Analysis: Finds all toc.yml files in the documentation Parses each TOC file to extract entries and links Verifies that each link points to an existing file TOC Issue Resolution: For missing files, tries to find files with different extensions (.md, .html) Looks for files with similar names using a similarity algorithm Creates placeholder files when no suitable alternative is found Updates TOC entries to point to the correct files TOC Inconsistency Detection: Identifies entries with the same name but different href values across TOC files Determines which href points to an existing file Updates TOC files to use consistent href values Creates placeholder files when neither href points to an existing file Broken Link Detection: Scans all markdown files for internal links Verifies that each link points to an existing file Identifies broken links Broken Link Resolution: Tries to find files with different extensions Looks for files with similar names Updates links to point to the correct files Verification After running the fix script, you should verify that the navigation issues have been resolved by: Running the navigation verification script: .\\verify-navigation.ps1 Building and serving the documentation locally: docfx build docfx serve _site Manually testing navigation through the documentation to ensure all links work correctly Summary Report The script generates a summary report (navigation-fix-summary.md) that includes: Total number of TOC issues found and fixed Total number of TOC inconsistencies found and fixed Total number of broken links found and fixed Overall summary of issues found and fixed Next Steps After fixing navigation issues, you can proceed with: Image Verification: Run the image verification script to check if image references are working correctly: .\\run-image-verification-optimized.ps1 -batchSize 1 -fix Manual Review: Review the documentation to ensure all content is accessible and displays correctly Final Deployment: Once all issues are resolved, deploy the documentation to the production environment Troubleshooting If you encounter any issues with the script: Check the console output for error messages Review the summary report for details on issues that couldn't be fixed Manually inspect problematic files and fix issues that the script couldn't resolve Run the script again after making manual fixes"
  },
  "migration-docs/NAVIGATION-MIGRATION-PLAN.html": {
    "href": "migration-docs/NAVIGATION-MIGRATION-PLAN.html",
    "title": "DocFX Navigation Migration Plan | XMPro",
    "summary": "DocFX Navigation Migration Plan This document outlines the detailed plan for migrating the navigation structure from GitBook to DocFX, ensuring that the DocFX site has like-for-like functionality with the GitBook site. Current State Analysis GitBook Navigation Structure Uses a single SUMMARY.md file with indentation to show hierarchy Has extensive external content (blogs, use cases, YouTube videos) organized by year Contains hundreds of individual entries, especially in the blogs section Deep hierarchical structure with multiple levels of nesting DocFX Navigation Structure Uses multiple toc.yml files distributed across directories Has references to external content sections but the actual directories and content are missing Structure is flatter and missing many of the deeper levels Missing most of the external content that exists in GitBook Implementation Plan The implementation is divided into distinct phases with checkpoints to allow for resumption if the task is interrupted. Phase 1: Create Base Directory Structure Objective: Create the basic directory structure needed for the external content. Steps: Create a state tracking file to record progress Create directories for blogs organized by year (2010-2024) Create directories for use cases Create directories for YouTube content organized by year (2012-2024) Update state tracking file after each major step Checkpoint: Verify all directories have been created successfully. State Tracking: { \"phase\": 1, \"completed\": true, \"directories_created\": [ \"blogs\", \"blogs/2024\", \"blogs/2023\", // ... other directories ] } Phase 2: Create TOC Files for Main Sections Objective: Create the main toc.yml files for the external content sections. Steps: Create toc.yml for external-content directory Create toc.yml for blogs directory with links to year subdirectories Create toc.yml for use cases directory Create toc.yml for YouTube directory with links to year subdirectories Update state tracking file after each toc.yml creation Checkpoint: Verify all main toc.yml files have been created successfully. State Tracking: { \"phase\": 2, \"completed\": true, \"toc_files_created\": [ \"external-content/toc.yml\", \"external-content/blogs/toc.yml\", // ... other toc files ] } Phase 3: Create Index Files for Main Sections Objective: Create index.md files for each main directory to provide content and navigation. Steps: Create index.md for external-content directory Create index.md for blogs directory with links to year subdirectories Create index.md for use cases directory Create index.md for YouTube directory with links to year subdirectories Update state tracking file after each index.md creation Checkpoint: Verify all main index.md files have been created successfully. State Tracking: { \"phase\": 3, \"completed\": true, \"index_files_created\": [ \"external-content/index.md\", \"external-content/blogs/index.md\", // ... other index files ] } Phase 4: Create Year-Specific TOC and Index Files Objective: Create toc.yml and index.md files for each year directory. Steps: For each year in blogs: a. Create toc.yml with entries for all blog posts in that year b. Create index.md with links to all blog posts in that year For each year in YouTube: a. Create toc.yml with entries for all videos in that year b. Create index.md with links to all videos in that year Update state tracking file after each year is completed Checkpoint: Verify all year-specific toc.yml and index.md files have been created successfully. State Tracking: { \"phase\": 4, \"completed\": true, \"years_completed\": { \"blogs\": [\"2024\", \"2023\", \"2022\", \"...\"], \"youtube\": [\"2024\", \"2023\", \"2022\", \"...\"] } } Phase 5: Migrate Blog Content Objective: Migrate all blog content from GitBook to DocFX. Steps: For each year in blogs: a. For each blog post in that year: i. Create markdown file with content from GitBook ii. Update state tracking file after each blog post is migrated Update state tracking file after each year is completed Checkpoint: Verify all blog content has been migrated successfully. State Tracking: { \"phase\": 5, \"completed\": true, \"blogs_migrated\": { \"2024\": [\"blog1.md\", \"blog2.md\", \"...\"], \"2023\": [\"blog1.md\", \"blog2.md\", \"...\"], // ... other years } } Phase 6: Migrate Use Cases Content Objective: Migrate all use cases content from GitBook to DocFX. Steps: For each use case: a. Create markdown file with content from GitBook b. Update state tracking file after each use case is migrated Checkpoint: Verify all use cases content has been migrated successfully. State Tracking: { \"phase\": 6, \"completed\": true, \"use_cases_migrated\": [ \"use-case1.md\", \"use-case2.md\", // ... other use cases ] } Phase 7: Migrate YouTube Content Objective: Migrate all YouTube content from GitBook to DocFX. Steps: For each year in YouTube: a. For each video in that year: i. Create markdown file with content from GitBook ii. Update state tracking file after each video is migrated Update state tracking file after each year is completed Checkpoint: Verify all YouTube content has been migrated successfully. State Tracking: { \"phase\": 7, \"completed\": true, \"youtube_migrated\": { \"2024\": [\"video1.md\", \"video2.md\", \"...\"], \"2023\": [\"video1.md\", \"video2.md\", \"...\"], // ... other years } } Phase 8: Update Main Resources TOC Objective: Update the main resources toc.yml to ensure the external content is properly linked. Steps: Update the FAQs section in resources/toc.yml to properly link to external content Update state tracking file after the toc.yml is updated Checkpoint: Verify the main resources toc.yml has been updated successfully. State Tracking: { \"phase\": 8, \"completed\": true, \"resources_toc_updated\": true } Phase 9: Test Navigation Structure Objective: Thoroughly test the navigation structure to ensure it works correctly. Steps: Verify all links work correctly Ensure the hierarchy matches the GitBook structure Test navigation on different devices Verify search functionality works with the new content Update state tracking file after testing is completed Checkpoint: Verify all navigation functionality works correctly. State Tracking: { \"phase\": 9, \"completed\": true, \"testing_completed\": true, \"issues_found\": [ // List any issues found during testing ] } Implementation Script To implement this plan, we'll create a PowerShell script that: Reads the state tracking file to determine the current state Executes the appropriate phase based on the current state Updates the state tracking file after each step Provides detailed logging for troubleshooting The script will be designed to be idempotent, so it can be run multiple times without causing issues. If it's interrupted, it can be resumed from the last completed step. Script Structure # Navigation Migration Script # Initialize state tracking $stateFile = \"navigation-migration-state.json\" $state = $null # Check if state file exists if (Test-Path $stateFile) { $state = Get-Content $stateFile | ConvertFrom-Json } else { # Initialize new state $state = @{ phase = 0 completed = $false directories_created = @() toc_files_created = @() index_files_created = @() years_completed = @{ blogs = @() youtube = @() } blogs_migrated = @{} use_cases_migrated = @() youtube_migrated = @{} resources_toc_updated = $false testing_completed = $false issues_found = @() } } # Save state function function Save-State { $state | ConvertTo-Json -Depth 10 | Set-Content $stateFile } # Phase 1: Create Base Directory Structure function Execute-Phase1 { # Implementation details... } # Phase 2: Create TOC Files for Main Sections function Execute-Phase2 { # Implementation details... } # Additional phase functions... # Main execution switch ($state.phase) { 0 { Write-Host \"Starting migration from the beginning...\" $state.phase = 1 Save-State Execute-Phase1 } 1 { if (-not $state.completed) { Write-Host \"Resuming Phase 1: Create Base Directory Structure...\" Execute-Phase1 } else { Write-Host \"Phase 1 already completed. Moving to Phase 2...\" $state.phase = 2 Save-State Execute-Phase2 } } # Additional cases for other phases... } Resumption Strategy If the migration task is interrupted or fails, it can be resumed by: Examining the state tracking file to determine the current state Running the migration script again, which will automatically resume from the last completed step Manually fixing any issues that caused the failure before resuming This approach ensures that the migration can be completed across multiple sessions if necessary, without losing progress or duplicating work. Verification and Rollback After each phase, the script will verify that the expected files and directories have been created successfully. If verification fails, the script will: Log the failure details Update the state tracking file to indicate the failure Provide instructions for manual intervention If necessary, each phase can be rolled back by: Deleting the files and directories created in that phase Updating the state tracking file to indicate that the phase needs to be re-executed Conclusion This detailed implementation plan provides a structured approach to migrating the navigation structure from GitBook to DocFX. By breaking the migration into distinct phases with checkpoints and state tracking, we ensure that the process can be resumed if interrupted, and that each step can be verified before moving on to the next. The end result will be a DocFX site with a navigation structure that matches the GitBook site, providing like-for-like functionality for users."
  },
  "migration-docs/PHASE6-COMPLETION.html": {
    "href": "migration-docs/PHASE6-COMPLETION.html",
    "title": "Phase 6 Completion: Use Cases Migration | XMPro",
    "summary": "Phase 6 Completion: Use Cases Migration Summary Phase 6 of the Navigation Migration Plan has been successfully completed. This phase involved migrating all use cases content from GitBook to DocFX. What Was Accomplished Created the necessary directory structure for use cases content: Created the external-content directory in docs/resources/faqs/ Created the use-cases directory in external-content/ Migrated 38 use case files from GitBook to DocFX: Each use case was copied with its original content File names were sanitized to ensure compatibility with DocFX Created navigation files: Created toc.yml for the use-cases directory with entries for all use cases Created index.md for the use-cases directory with links to all use cases Created toc.yml for the external-content directory Created index.md for the external-content directory Updated the main FAQs toc.yml to include the external content section Updated the state tracking file to reflect the completion of Phase 6 Fixed TOC files with invalid YAML syntax: Created a script to fix TOC files with unquoted colons in title names Applied the fix to all blog TOC files to ensure proper YAML syntax This fix was necessary to resolve build errors in GitHub Actions Verification All 38 use cases were successfully migrated and are now accessible through the DocFX navigation structure. The navigation hierarchy matches the GitBook structure, with use cases organized in a single directory. Next Steps The next phase (Phase 7) will involve migrating all YouTube content from GitBook to DocFX, following a similar approach to what was done for use cases. Scripts Created create-directory-structure.ps1: Creates the necessary directory structure for use cases run-phase6.ps1: Migrates use cases content from GitBook to DocFX update-state.ps1: Updates the state tracking file to reflect the completion of Phase 6 fix-toc-files.ps1: Fixes TOC files with invalid YAML syntax due to unquoted colons in title names These scripts can be used as a reference for implementing future phases of the migration plan. Lessons Learned YAML syntax in TOC files requires special attention: Colons in title names need to be quoted to avoid YAML parsing errors This is important to remember for future content migration phases The fix-toc-files.ps1 script can be reused for similar issues in the future Testing the build process is essential: GitHub Actions build errors helped identify issues that weren't apparent locally Fixing these issues early prevents problems in later phases"
  },
  "migration-docs/README.html": {
    "href": "migration-docs/README.html",
    "title": "Image Verification and Navigation Fix Tools | XMPro",
    "summary": "Image Verification and Navigation Fix Tools This folder contains the essential tools needed to complete the image verification process and fix navigation issues in the XMPro documentation. Unnecessary migration files have been moved to the backup folder to keep this directory focused on the current tasks. Current Issues The original image verification script was causing token limit issues when run with AI assistance, even with a small batch size of 2. The optimized scripts in this folder address this issue by using minimal memory and storing only essential information. Contents Navigation Fix Tools fix-navigation-issues.ps1: Script to identify and fix navigation issues run-navigation-fix.ps1: Wrapper script for running the navigation fix tool NAVIGATION-FIX-README.md: Documentation for the navigation fix tools verify-navigation.ps1: Script to verify navigation functionality Image Verification Tools incremental-verify-image-references-optimized.ps1: Optimized version with minimal memory usage run-image-verification-optimized.ps1: Wrapper script for the optimized verification IMAGE-VERIFICATION-README.md: Documentation for image verification tools image-verification-progress.json: Progress file for incremental verification image-verification-report.csv: Report of image verification results Usage Running Navigation Fix The navigation fix tools help identify and fix navigation issues that prevent proper verification of image references: cd docs-docfx ./migration-docs/run-navigation-fix.ps1 Running Image Verification The optimized image verification script can be run with: cd docs-docfx ./migration-docs/run-image-verification-optimized.ps1 -batchSize 1 -fix If you still encounter token limits, you can use different progress files for different sections: cd docs-docfx ./migration-docs/run-image-verification-optimized.ps1 -progressFile \"progress-part1.json\" -csvReport \"report-part1.csv\" Backup Other migration-related files that are no longer needed have been moved to the backup folder for reference. These include: Original and enhanced image verification scripts Migration plans and documentation Helper scripts and example files Verification results and reports Note The actual documentation content is in the docs folder. These tools are specifically for addressing the image reference issues and navigation problems."
  }
}